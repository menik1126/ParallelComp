
CondaError: Run 'conda init' before 'conda activate'

Running evaluation for dataset: narrativeqa
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]n_windows:[2]

model_name:/mnt/Data/xiongjing/llama2chatmodel_name:/mnt/Data/xiongjing/llama2chat

multi_gpus:Truemulti_gpus:True

torch.cuda.device_count():5torch.cuda.device_count():5

n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.43s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.77s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.81s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.92s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.17s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:11:23,002 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:11:23,002 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:11:23,002 - [Process 4/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:11:23,031 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:11:23,031 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:11:23,031 - [Process 3/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:11:23,034 - [Process 2/5] - INFO - loading datasets finished
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:11:23,034 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:11:23,034 - [Process 2/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:11:23,035 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 01:11:23,035 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:11:23,035 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:11:23,035 - [Process 1/5] - INFO - output_max_len: 128
2024-12-22 01:11:23,035 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:11:23,035 - [Process 0/5] - INFO - output_max_len: 128
2024-12-22 01:11:23,065 - [Process 4/5] - INFO - Max Length is 36418
2024-12-22 01:11:23,065 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:11:23,065 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:11:23,131 - [Process 3/5] - INFO - Max Length is 36418
2024-12-22 01:11:23,132 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:11:23,132 - [Process 3/5] - INFO - get_predicted begin
2024-12-22 01:11:23,132 - [Process 2/5] - INFO - Max Length is 36418
2024-12-22 01:11:23,132 - [Process 1/5] - INFO - Max Length is 36418
2024-12-22 01:11:23,133 - [Process 0/5] - INFO - Max Length is 36418
2024-12-22 01:11:23,133 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:11:23,133 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:11:23,133 - [Process 0/5] - INFO - Finish loading dataset
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:11:23,133 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 01:11:23,133 - [Process 2/5] - INFO - get_predicted begin
2024-12-22 01:11:23,133 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:11:27,808 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:27,891 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:27,891 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:27,894 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:27,897 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:32,076 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:32,077 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:11:32,155 - [Process 4/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:11:32,368 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:32,368 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:11:32,391 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:32,391 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 01:11:32,403 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:32,404 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 01:11:32,416 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:32,416 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:11:32,446 - [Process 0/5] - DEBUG - predict_token:tensor([[512]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:11:32,470 - [Process 1/5] - DEBUG - predict_token:tensor([[4124]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:11:32,484 - [Process 2/5] - DEBUG - predict_token:tensor([[9204]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:11:32,495 - [Process 3/5] - DEBUG - predict_token:tensor([[7311]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:11:32,503 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:The bottle-merchant.
  2%|▎         | 1/40 [00:09<06:08,  9.44s/it]2024-12-22 01:11:32,698 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Living alone.
  2%|▎         | 1/40 [00:09<06:13,  9.56s/it]2024-12-22 01:11:32,742 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Radioactive gas.
  2%|▎         | 1/40 [00:09<06:14,  9.61s/it]2024-12-22 01:11:32,798 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:32,924 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:Because artists laugh at his work.
  2%|▎         | 1/40 [00:09<06:21,  9.79s/it]2024-12-22 01:11:33,139 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:33,150 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:33,216 - [Process 1/5] - INFO - res.shape is :torch.Size([16])
results:BackA city bus pulls up near the museum and Louis steps off.
  2%|▎         | 1/40 [00:10<06:33, 10.08s/it]2024-12-22 01:11:33,315 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:33,591 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:36,506 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:36,506 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1992])
2024-12-22 01:11:36,584 - [Process 4/5] - DEBUG - predict_token:tensor([[376]], device='cuda:4')
2024-12-22 01:11:36,722 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:36,723 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:11:36,795 - [Process 0/5] - DEBUG - predict_token:tensor([[8081]], device='cuda:0')
2024-12-22 01:11:36,851 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Auld Lang Syne
  5%|▌         | 2/40 [00:13<04:04,  6.44s/it]2024-12-22 01:11:36,911 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:36,912 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2125])
2024-12-22 01:11:36,983 - [Process 2/5] - DEBUG - predict_token:tensor([[306]], device='cuda:2')
2024-12-22 01:11:36,995 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:37,099 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:37,100 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2122])
2024-12-22 01:11:37,171 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:11:37,182 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Izu.
  5%|▌         | 2/40 [00:14<04:09,  6.57s/it]2024-12-22 01:11:37,359 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:37,359 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2120])
2024-12-22 01:11:37,429 - [Process 1/5] - DEBUG - predict_token:tensor([[8622]], device='cuda:1')
2024-12-22 01:11:37,468 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:The videotape.
  5%|▌         | 2/40 [00:14<04:14,  6.71s/it]2024-12-22 01:11:37,520 - [Process 0/5] - INFO - res.shape is :torch.Size([16])
results:Mary's delicate state of health did not promise long life.
  5%|▌         | 2/40 [00:14<04:17,  6.78s/it]2024-12-22 01:11:37,738 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:37,769 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:37,847 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:Takahashi Hiroshi
  5%|▌         | 2/40 [00:14<04:21,  6.88s/it]2024-12-22 01:11:38,048 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:38,064 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:40,713 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:40,713 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:11:40,792 - [Process 4/5] - DEBUG - predict_token:tensor([[3600]], device='cuda:4')
2024-12-22 01:11:41,137 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:Artists laugh at his work.
  8%|▊         | 3/40 [00:18<03:21,  5.46s/it]2024-12-22 01:11:41,308 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:41,308 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 01:11:41,380 - [Process 0/5] - DEBUG - predict_token:tensor([[27076]], device='cuda:0')
2024-12-22 01:11:41,387 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:41,485 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:41,486 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2210])
2024-12-22 01:11:41,553 - [Process 2/5] - DEBUG - predict_token:tensor([[529]], device='cuda:2')
2024-12-22 01:11:41,659 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:On Atlas' mountain.
  8%|▊         | 3/40 [00:18<03:26,  5.57s/it]2024-12-22 01:11:41,734 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:41,735 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2067])
2024-12-22 01:11:41,749 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Dimes.
  8%|▊         | 3/40 [00:18<03:29,  5.65s/it]2024-12-22 01:11:41,797 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:41,797 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2206])
2024-12-22 01:11:41,809 - [Process 1/5] - DEBUG - predict_token:tensor([[9193]], device='cuda:1')
2024-12-22 01:11:41,866 - [Process 3/5] - DEBUG - predict_token:tensor([[2261]], device='cuda:3')
2024-12-22 01:11:41,872 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:42,009 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Athens
  8%|▊         | 3/40 [00:18<03:28,  5.64s/it]2024-12-22 01:11:42,036 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:42,384 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:44,941 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:44,941 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 01:11:45,013 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:11:45,316 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:
She marries her husband
 10%|█         | 4/40 [00:22<02:58,  4.95s/it]2024-12-22 01:11:45,455 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:45,455 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:45,456 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:11:45,528 - [Process 0/5] - DEBUG - predict_token:tensor([[1763]], device='cuda:0')
2024-12-22 01:11:45,790 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:45,790 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2129])
2024-12-22 01:11:45,861 - [Process 2/5] - DEBUG - predict_token:tensor([[29008]], device='cuda:2')
2024-12-22 01:11:46,008 - [Process 0/5] - INFO - res.shape is :torch.Size([10])
results:To persuade Socrates to escape.
 10%|█         | 4/40 [00:22<03:03,  5.09s/it]2024-12-22 01:11:46,165 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:46,166 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2121])
2024-12-22 01:11:46,182 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Twenty-five years.
 10%|█         | 4/40 [00:23<03:06,  5.17s/it]2024-12-22 01:11:46,237 - [Process 1/5] - DEBUG - predict_token:tensor([[2296]], device='cuda:1')
2024-12-22 01:11:46,450 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:46,472 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:46,779 - [Process 1/5] - INFO - res.shape is :torch.Size([12])
results:She died one week after watching the videotape.
 10%|█         | 4/40 [00:23<03:10,  5.29s/it]2024-12-22 01:11:47,021 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:47,400 - [Process 3/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































  8%|▊         | 3/40 [00:24<05:02,  8.18s/it]2024-12-22 01:11:48,003 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:49,040 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:49,041 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 01:11:49,114 - [Process 4/5] - DEBUG - predict_token:tensor([[319]], device='cuda:4')
2024-12-22 01:11:49,336 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:A deserted house
 12%|█▎        | 5/40 [00:26<02:41,  4.62s/it]2024-12-22 01:11:49,624 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:50,043 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:50,044 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2109])
2024-12-22 01:11:50,116 - [Process 0/5] - DEBUG - predict_token:tensor([[376]], device='cuda:0')
2024-12-22 01:11:50,207 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:50,207 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:11:50,287 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:11:50,522 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:Baron Henry couldn't kill Otto
 12%|█▎        | 5/40 [00:27<02:50,  4.88s/it]2024-12-22 01:11:50,650 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:50,650 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2053])
2024-12-22 01:11:50,724 - [Process 1/5] - DEBUG - predict_token:tensor([[1763]], device='cuda:1')
2024-12-22 01:11:50,773 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:50,977 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:To protect his reputation
 12%|█▎        | 5/40 [00:27<02:51,  4.90s/it]2024-12-22 01:11:51,265 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:51,320 - [Process 2/5] - INFO - res.shape is :torch.Size([23])
results:The housekeeper's feet got wet during Holmes' visit as he threw gravel from the window.
 12%|█▎        | 5/40 [00:28<03:00,  5.16s/it]2024-12-22 01:11:51,537 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:51,754 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:51,754 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2215])
2024-12-22 01:11:51,823 - [Process 3/5] - DEBUG - predict_token:tensor([[435]], device='cuda:3')
2024-12-22 01:11:52,354 - [Process 3/5] - INFO - res.shape is :torch.Size([12])
results:Jezzie was JACOB's friend.
 10%|█         | 4/40 [00:29<04:08,  6.90s/it]2024-12-22 01:11:52,944 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:53,351 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:53,351 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 01:11:53,430 - [Process 4/5] - DEBUG - predict_token:tensor([[376]], device='cuda:4')
2024-12-22 01:11:53,694 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Auld Lang Syne
 15%|█▌        | 6/40 [00:30<02:33,  4.53s/it]2024-12-22 01:11:53,835 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:54,525 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:54,525 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 01:11:54,604 - [Process 0/5] - DEBUG - predict_token:tensor([[498]], device='cuda:0')
2024-12-22 01:11:54,797 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Four days
 15%|█▌        | 6/40 [00:31<02:38,  4.68s/it]2024-12-22 01:11:55,062 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:55,062 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:11:55,134 - [Process 1/5] - DEBUG - predict_token:tensor([[940]], device='cuda:1')
2024-12-22 01:11:55,154 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:55,194 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:55,195 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 01:11:55,267 - [Process 2/5] - DEBUG - predict_token:tensor([[530]], device='cuda:2')
2024-12-22 01:11:55,474 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:A enemy.
 15%|█▌        | 6/40 [00:32<02:43,  4.82s/it]2024-12-22 01:11:55,922 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:56,694 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:56,694 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2220])
2024-12-22 01:11:56,763 - [Process 3/5] - DEBUG - predict_token:tensor([[10968]], device='cuda:3')
2024-12-22 01:11:56,937 - [Process 1/5] - INFO - res.shape is :torch.Size([41])
results:He wore a soft black hat of clerical kind, but of Bohemian intention, and a gray waterproof cape which, perhaps because it was waterproof, failed to be romantic.
 15%|█▌        | 6/40 [00:33<02:58,  5.26s/it]2024-12-22 01:11:57,252 - [Process 3/5] - INFO - res.shape is :torch.Size([11])
results:Hell burns away Jacob's flesh.
 12%|█▎        | 5/40 [00:34<03:36,  6.18s/it]2024-12-22 01:11:57,373 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:57,439 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:57,439 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 01:11:57,467 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:57,512 - [Process 4/5] - DEBUG - predict_token:tensor([[997]], device='cuda:4')
2024-12-22 01:11:57,736 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:La Grande Breche
 18%|█▊        | 7/40 [00:34<02:24,  4.37s/it]2024-12-22 01:11:57,922 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:58,791 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:58,791 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 01:11:58,864 - [Process 0/5] - DEBUG - predict_token:tensor([[5012]], device='cuda:0')
2024-12-22 01:11:59,014 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Lisa
 18%|█▊        | 7/40 [00:35<02:29,  4.53s/it]2024-12-22 01:11:59,497 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:11:59,686 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:11:59,686 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 01:11:59,757 - [Process 2/5] - DEBUG - predict_token:tensor([[5011]], device='cuda:2')
2024-12-22 01:11:59,865 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:James
 18%|█▊        | 7/40 [00:36<02:34,  4.68s/it]2024-12-22 01:12:00,343 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:00,978 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:00,978 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2100])
2024-12-22 01:12:01,049 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:12:01,158 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:01,159 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 01:12:01,234 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:12:01,455 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:The Baron Henry of Roderburg.
 18%|█▊        | 7/40 [00:38<02:45,  5.02s/it]2024-12-22 01:12:01,660 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:01,660 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:12:01,739 - [Process 4/5] - DEBUG - predict_token:tensor([[8507]], device='cuda:4')
2024-12-22 01:12:01,855 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:01,937 - [Process 3/5] - INFO - res.shape is :torch.Size([16])
results:The contract of citizenship and agreements with the state of Athens.
 15%|█▌        | 6/40 [00:38<03:12,  5.67s/it]2024-12-22 01:12:02,172 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:02,647 - [Process 4/5] - INFO - res.shape is :torch.Size([22])
results:Jim and Dave fought because Dave's aunt, Katie Carter, was not properly dressed.
 20%|██        | 8/40 [00:39<02:25,  4.54s/it]2024-12-22 01:12:03,019 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:03,219 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:03,219 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 01:12:03,298 - [Process 0/5] - DEBUG - predict_token:tensor([[360]], device='cuda:0')
2024-12-22 01:12:03,533 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Dana Barrett
 20%|██        | 8/40 [00:40<02:24,  4.52s/it]2024-12-22 01:12:03,920 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:04,089 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:04,089 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:12:04,167 - [Process 2/5] - DEBUG - predict_token:tensor([[360]], device='cuda:2')
2024-12-22 01:12:04,361 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:The Mayor.
 20%|██        | 8/40 [00:41<02:27,  4.62s/it]2024-12-22 01:12:04,628 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:05,471 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:05,472 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 01:12:05,543 - [Process 1/5] - DEBUG - predict_token:tensor([[6182]], device='cuda:1')
2024-12-22 01:12:05,974 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:05,975 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:12:06,055 - [Process 3/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:3')
2024-12-22 01:12:06,589 - [Process 1/5] - INFO - res.shape is :torch.Size([24])
results:Mary is initially educated to be a "mere machine" and to fulfill her duties as a wife.
 20%|██        | 8/40 [00:43<02:41,  5.05s/it]2024-12-22 01:12:06,631 - [Process 3/5] - INFO - res.shape is :torch.Size([13])
results:
"A good artist, but not a great one."
 18%|█▊        | 7/40 [00:43<02:56,  5.35s/it]2024-12-22 01:12:06,733 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:06,733 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2216])
2024-12-22 01:12:06,801 - [Process 4/5] - DEBUG - predict_token:tensor([[18885]], device='cuda:4')
2024-12-22 01:12:06,893 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:06,936 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:07,145 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:Bruce Joel Rubin
 22%|██▎       | 9/40 [00:44<02:20,  4.53s/it]2024-12-22 01:12:07,407 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:07,642 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:07,642 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 01:12:07,720 - [Process 0/5] - DEBUG - predict_token:tensor([[4908]], device='cuda:0')
2024-12-22 01:12:07,828 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:American
 22%|██▎       | 9/40 [00:44<02:18,  4.45s/it]2024-12-22 01:12:08,038 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:08,404 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:08,404 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2126])
2024-12-22 01:12:08,476 - [Process 2/5] - DEBUG - predict_token:tensor([[940]], device='cuda:2')
2024-12-22 01:12:09,012 - [Process 2/5] - INFO - res.shape is :torch.Size([12])
results:His failure to impress himself on the decade.
 22%|██▎       | 9/40 [00:45<02:23,  4.63s/it]2024-12-22 01:12:09,412 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:10,667 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:10,667 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:12:10,737 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:10,737 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 01:12:10,746 - [Process 1/5] - DEBUG - predict_token:tensor([[8108]], device='cuda:1')
2024-12-22 01:12:10,817 - [Process 3/5] - DEBUG - predict_token:tensor([[7904]], device='cuda:3')
2024-12-22 01:12:10,940 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Baptist
 22%|██▎       | 9/40 [00:47<02:29,  4.83s/it]2024-12-22 01:12:11,005 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:11,005 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2107])
2024-12-22 01:12:11,078 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:12:11,221 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:LANGSTON HUGHES
 20%|██        | 8/40 [00:48<02:43,  5.11s/it]2024-12-22 01:12:11,230 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:11,523 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:11,620 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:11,620 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2102])
2024-12-22 01:12:11,692 - [Process 0/5] - DEBUG - predict_token:tensor([[5310]], device='cuda:0')
2024-12-22 01:12:11,891 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:The king.
 25%|██▌       | 10/40 [00:48<02:09,  4.33s/it]2024-12-22 01:12:12,337 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:12,470 - [Process 4/5] - INFO - res.shape is :torch.Size([34])
results:The Baroness Matillda went into premature labor because of the alarm bell ringing in the belfry high up upon the Melchior Tower.
 25%|██▌       | 10/40 [00:49<02:23,  4.77s/it]2024-12-22 01:12:12,598 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:13,007 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:13,007 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 01:12:13,079 - [Process 2/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:2')
2024-12-22 01:12:13,230 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Eliza
 25%|██▌       | 10/40 [00:50<02:15,  4.50s/it]2024-12-22 01:12:13,675 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:15,040 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:15,040 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 01:12:15,112 - [Process 1/5] - DEBUG - predict_token:tensor([[4750]], device='cuda:1')
2024-12-22 01:12:15,314 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:15,314 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:12:15,391 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Miss Miranda Mope
 25%|██▌       | 10/40 [00:52<02:21,  4.72s/it]2024-12-22 01:12:15,394 - [Process 3/5] - DEBUG - predict_token:tensor([[8507]], device='cuda:3')
2024-12-22 01:12:15,826 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:15,968 - [Process 3/5] - INFO - res.shape is :torch.Size([13])
results:Jim was sentenced to work in the stocks.
 22%|██▎       | 9/40 [00:52<02:34,  5.00s/it]2024-12-22 01:12:16,100 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:16,101 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:12:16,180 - [Process 0/5] - DEBUG - predict_token:tensor([[2688]], device='cuda:0')
2024-12-22 01:12:16,198 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:16,198 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 01:12:16,271 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:12:16,381 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:16,535 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:A crystal bowl
 28%|██▊       | 11/40 [00:53<02:12,  4.56s/it]2024-12-22 01:12:16,677 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:17,454 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:17,454 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:12:17,535 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:12:17,577 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:
"Because he has a belief that the American explosive rocket is a far more efficient weapon than the disintegrator ray of the Hans."
 28%|██▊       | 11/40 [00:54<02:17,  4.75s/it]2024-12-22 01:12:17,729 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Sinsings
 28%|██▊       | 11/40 [00:54<02:10,  4.50s/it]2024-12-22 01:12:17,844 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:18,295 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:19,442 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:19,442 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2104])
2024-12-22 01:12:19,515 - [Process 1/5] - DEBUG - predict_token:tensor([[9206]], device='cuda:1')
2024-12-22 01:12:20,027 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:20,028 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:12:20,101 - [Process 3/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:3')
2024-12-22 01:12:20,440 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:20,441 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:12:20,475 - [Process 1/5] - INFO - res.shape is :torch.Size([22])
results:

Wickedness, cruelty, goodness, justice, peace, love, and wisdom.
 28%|██▊       | 11/40 [00:57<02:20,  4.83s/it]2024-12-22 01:12:20,521 - [Process 4/5] - DEBUG - predict_token:tensor([[383]], device='cuda:4')
2024-12-22 01:12:20,593 - [Process 3/5] - INFO - res.shape is :torch.Size([11])
results:Eliza favors the officer's son.
 25%|██▌       | 10/40 [00:57<02:26,  4.88s/it]2024-12-22 01:12:20,825 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:The rue de Navarin.
 30%|███       | 12/40 [00:57<02:05,  4.48s/it]2024-12-22 01:12:20,845 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:21,027 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:21,092 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:21,608 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:21,608 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 01:12:21,679 - [Process 0/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:0')
2024-12-22 01:12:22,035 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:22,036 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2211])
2024-12-22 01:12:22,104 - [Process 2/5] - DEBUG - predict_token:tensor([[2261]], device='cuda:2')
2024-12-22 01:12:22,212 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Bar
 30%|███       | 12/40 [00:59<02:05,  4.50s/it]2024-12-22 01:12:22,689 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:23,539 - [Process 0/5] - INFO - res.shape is :torch.Size([43])
results:"A counterpoise in the thought that if he had had some measure of success he might have passed, like those others, out of my mind, to return only at the historian's beck."
 30%|███       | 12/40 [01:00<02:23,  5.12s/it]2024-12-22 01:12:23,772 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:24,649 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:24,649 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2116])
2024-12-22 01:12:24,679 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:24,679 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:12:24,689 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:24,689 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 01:12:24,721 - [Process 1/5] - DEBUG - predict_token:tensor([[2296]], device='cuda:1')
2024-12-22 01:12:24,753 - [Process 3/5] - DEBUG - predict_token:tensor([[20655]], device='cuda:3')
2024-12-22 01:12:24,762 - [Process 4/5] - DEBUG - predict_token:tensor([[1763]], device='cuda:4')
2024-12-22 01:12:24,956 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Tomoko dies.
 30%|███       | 12/40 [01:01<02:12,  4.72s/it]2024-12-22 01:12:25,072 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Gentleness and love
 28%|██▊       | 11/40 [01:01<02:17,  4.76s/it]2024-12-22 01:12:25,186 - [Process 4/5] - INFO - res.shape is :torch.Size([10])
results:To avenge the death of his father
 32%|███▎      | 13/40 [01:02<01:59,  4.44s/it]2024-12-22 01:12:25,323 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:25,473 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:25,524 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:26,443 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:26,443 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1987])
2024-12-22 01:12:26,522 - [Process 2/5] - DEBUG - predict_token:tensor([[5791]], device='cuda:2')
2024-12-22 01:12:26,715 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Sister.
 32%|███▎      | 13/40 [01:03<02:01,  4.50s/it]2024-12-22 01:12:26,930 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:27,390 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:27,390 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2093])
2024-12-22 01:12:27,463 - [Process 0/5] - DEBUG - predict_token:tensor([[12444]], device='cuda:0')
2024-12-22 01:12:27,698 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Twenty days.
 32%|███▎      | 13/40 [01:04<02:10,  4.83s/it]2024-12-22 01:12:27,932 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:29,113 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:29,113 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:12:29,125 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:29,125 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:12:29,134 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:29,135 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2123])
2024-12-22 01:12:29,187 - [Process 4/5] - DEBUG - predict_token:tensor([[940]], device='cuda:4')
2024-12-22 01:12:29,198 - [Process 3/5] - DEBUG - predict_token:tensor([[6182]], device='cuda:3')
2024-12-22 01:12:29,207 - [Process 1/5] - DEBUG - predict_token:tensor([[7363]], device='cuda:1')
2024-12-22 01:12:29,474 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Mary's dogs
 30%|███       | 12/40 [01:06<02:10,  4.65s/it]2024-12-22 01:12:29,571 - [Process 4/5] - INFO - res.shape is :torch.Size([9])
results:Bill died in the plane crash.
 35%|███▌      | 14/40 [01:06<01:55,  4.42s/it]2024-12-22 01:12:29,755 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:29,918 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:30,465 - [Process 1/5] - INFO - res.shape is :torch.Size([29])
results:

  Penetrable shroud of hair; a single EYE burns with manic, unbridled hatred.
 32%|███▎      | 13/40 [01:07<02:13,  4.96s/it]2024-12-22 01:12:30,563 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:30,563 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2081])
2024-12-22 01:12:30,636 - [Process 2/5] - DEBUG - predict_token:tensor([[317]], device='cuda:2')
2024-12-22 01:12:30,929 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:31,481 - [Process 2/5] - INFO - res.shape is :torch.Size([19])
results:Socrates believes he will be judged as an enemy in the afterlife.
 35%|███▌      | 14/40 [01:08<01:59,  4.58s/it]2024-12-22 01:12:31,684 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:31,684 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:12:31,714 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:31,763 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:12:32,552 - [Process 0/5] - INFO - res.shape is :torch.Size([18])
results:The Vervelle couple believe Grassou is the perfect match for their daughter.
 35%|███▌      | 14/40 [01:09<02:05,  4.83s/it]2024-12-22 01:12:32,787 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:33,524 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:33,524 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:12:33,603 - [Process 4/5] - DEBUG - predict_token:tensor([[11131]], device='cuda:4')
2024-12-22 01:12:33,737 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:33,737 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:12:33,787 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Joe Clark
 38%|███▊      | 15/40 [01:10<01:49,  4.36s/it]2024-12-22 01:12:33,818 - [Process 3/5] - DEBUG - predict_token:tensor([[10167]], device='cuda:3')
2024-12-22 01:12:33,947 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:34,095 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Pennsylvania.
 32%|███▎      | 13/40 [01:10<02:05,  4.64s/it]2024-12-22 01:12:34,359 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:34,740 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:34,740 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 01:12:34,821 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:12:35,057 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:The Sinsings
 35%|███▌      | 14/40 [01:11<02:06,  4.85s/it]2024-12-22 01:12:35,343 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:35,343 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 01:12:35,416 - [Process 2/5] - DEBUG - predict_token:tensor([[376]], device='cuda:2')
2024-12-22 01:12:35,622 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:36,385 - [Process 2/5] - INFO - res.shape is :torch.Size([22])
results:"Ah! madame," replied the doctor, "I have some appalling stories in my collection."
 38%|███▊      | 15/40 [01:13<01:56,  4.68s/it]2024-12-22 01:12:36,556 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:36,556 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 01:12:36,629 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:36,636 - [Process 0/5] - DEBUG - predict_token:tensor([[9181]], device='cuda:0')
2024-12-22 01:12:36,872 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Pierre Grassou
 38%|███▊      | 15/40 [01:13<01:56,  4.68s/it]2024-12-22 01:12:37,354 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:37,735 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:37,735 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2132])
2024-12-22 01:12:37,808 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:12:38,031 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Piteously.
 40%|████      | 16/40 [01:14<01:43,  4.33s/it]2024-12-22 01:12:38,199 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:38,199 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 01:12:38,272 - [Process 3/5] - DEBUG - predict_token:tensor([[323]], device='cuda:3')
2024-12-22 01:12:38,369 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:39,058 - [Process 3/5] - INFO - res.shape is :torch.Size([18])
results:T.K. Nupton thought Soames' existence was "piteous".
 35%|███▌      | 14/40 [01:15<02:03,  4.74s/it]2024-12-22 01:12:39,243 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:39,243 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2048])
2024-12-22 01:12:39,306 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:39,318 - [Process 1/5] - DEBUG - predict_token:tensor([[13896]], device='cuda:1')
2024-12-22 01:12:39,511 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Ursula
 38%|███▊      | 15/40 [01:16<01:58,  4.73s/it]2024-12-22 01:12:39,988 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:40,238 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:40,238 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:12:40,312 - [Process 2/5] - DEBUG - predict_token:tensor([[435]], device='cuda:2')
2024-12-22 01:12:40,548 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Doctor Nordenfeld
 40%|████      | 16/40 [01:17<01:48,  4.52s/it]2024-12-22 01:12:40,785 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:41,103 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:41,103 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 01:12:41,183 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:12:41,971 - [Process 0/5] - INFO - res.shape is :torch.Size([18])
results:Slimer grunts and flexes his scrawny biceps.
 40%|████      | 16/40 [01:18<01:55,  4.81s/it]2024-12-22 01:12:41,998 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:41,998 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2051])
2024-12-22 01:12:42,072 - [Process 4/5] - DEBUG - predict_token:tensor([[13896]], device='cuda:4')
2024-12-22 01:12:42,256 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Ursula
 42%|████▎     | 17/40 [01:19<01:38,  4.30s/it]2024-12-22 01:12:42,277 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:42,547 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:43,137 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:43,137 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2045])
2024-12-22 01:12:43,218 - [Process 3/5] - DEBUG - predict_token:tensor([[6639]], device='cuda:3')
2024-12-22 01:12:43,788 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:43,788 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 01:12:43,868 - [Process 1/5] - DEBUG - predict_token:tensor([[2688]], device='cuda:1')
2024-12-22 01:12:43,878 - [Process 3/5] - INFO - res.shape is :torch.Size([15])
results:Because he had stolen some devil's-foot root.
 38%|███▊      | 15/40 [01:20<01:59,  4.76s/it]2024-12-22 01:12:44,122 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:44,147 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:They don't.
 40%|████      | 16/40 [01:21<01:52,  4.70s/it]2024-12-22 01:12:44,538 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:44,579 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:44,579 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:12:44,659 - [Process 2/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:2')
2024-12-22 01:12:45,238 - [Process 2/5] - INFO - res.shape is :torch.Size([13])
results:
"A good artist, but not a great one."
 42%|████▎     | 17/40 [01:22<01:45,  4.57s/it]2024-12-22 01:12:45,476 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:46,019 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:46,019 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1995])
2024-12-22 01:12:46,097 - [Process 0/5] - DEBUG - predict_token:tensor([[382]], device='cuda:0')
2024-12-22 01:12:46,329 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:46,330 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1988])
2024-12-22 01:12:46,409 - [Process 4/5] - DEBUG - predict_token:tensor([[28484]], device='cuda:4')
2024-12-22 01:12:46,714 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Ghostbusting.
 45%|████▌     | 18/40 [01:23<01:35,  4.34s/it]2024-12-22 01:12:46,877 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:46,972 - [Process 0/5] - INFO - res.shape is :torch.Size([20])
results:He argues that Joe Clark's wife, Mrs. Clark, is not his wife.
 42%|████▎     | 17/40 [01:23<01:51,  4.86s/it]2024-12-22 01:12:47,418 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:47,789 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:47,789 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 01:12:47,863 - [Process 3/5] - DEBUG - predict_token:tensor([[838]], device='cuda:3')
2024-12-22 01:12:48,056 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Altaira
 40%|████      | 16/40 [01:24<01:50,  4.59s/it]2024-12-22 01:12:48,289 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:48,336 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:48,336 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:12:48,416 - [Process 1/5] - DEBUG - predict_token:tensor([[940]], device='cuda:1')
2024-12-22 01:12:48,823 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:He draws an operative benefit.
 42%|████▎     | 17/40 [01:25<01:47,  4.69s/it]2024-12-22 01:12:49,231 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:49,273 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:49,273 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 01:12:49,353 - [Process 2/5] - DEBUG - predict_token:tensor([[3600]], device='cuda:2')
2024-12-22 01:12:49,590 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:His art.
 45%|████▌     | 18/40 [01:26<01:39,  4.51s/it]2024-12-22 01:12:50,152 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:50,673 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:50,674 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2146])
2024-12-22 01:12:50,747 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:12:51,202 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:51,202 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:12:51,255 - [Process 4/5] - INFO - res.shape is :torch.Size([12])
results:The future article is spelled with a 'g'.
 48%|████▊     | 19/40 [01:28<01:32,  4.40s/it]2024-12-22 01:12:51,282 - [Process 0/5] - DEBUG - predict_token:tensor([[2688]], device='cuda:0')
2024-12-22 01:12:51,622 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:52,124 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:52,124 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:12:52,206 - [Process 3/5] - DEBUG - predict_token:tensor([[9181]], device='cuda:3')
2024-12-22 01:12:52,457 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Pierre Grassou
 42%|████▎     | 17/40 [01:29<01:44,  4.53s/it]2024-12-22 01:12:52,696 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:
"Because he has a belief that the American explosive rocket is a far more efficient weapon than the disintegrator ray of the Hans."
 45%|████▌     | 18/40 [01:29<01:52,  5.12s/it]2024-12-22 01:12:52,709 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:53,042 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:53,042 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:12:53,067 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:53,115 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:12:53,308 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Walter How
 45%|████▌     | 18/40 [01:30<01:41,  4.63s/it]2024-12-22 01:12:53,551 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:53,915 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:53,915 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2213])
2024-12-22 01:12:53,984 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:12:54,177 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:The mob.
 48%|████▊     | 19/40 [01:31<01:35,  4.53s/it]2024-12-22 01:12:54,589 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:55,373 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:55,373 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2220])
2024-12-22 01:12:55,442 - [Process 4/5] - DEBUG - predict_token:tensor([[739]], device='cuda:4')
2024-12-22 01:12:55,827 - [Process 4/5] - INFO - res.shape is :torch.Size([9])
results:Lets Jacob's body burn.
 50%|█████     | 20/40 [01:32<01:29,  4.45s/it]2024-12-22 01:12:56,075 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:56,394 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:56,394 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 01:12:56,469 - [Process 3/5] - DEBUG - predict_token:tensor([[940]], device='cuda:3')
2024-12-22 01:12:56,840 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:56,841 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:12:56,912 - [Process 0/5] - DEBUG - predict_token:tensor([[319]], device='cuda:0')
2024-12-22 01:12:57,143 - [Process 3/5] - INFO - res.shape is :torch.Size([15])
results:"He discovered that she had been writing to Gorenflot."
 45%|████▌     | 18/40 [01:34<01:40,  4.58s/it]2024-12-22 01:12:57,181 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:57,182 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:12:57,256 - [Process 1/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:1')
2024-12-22 01:12:57,535 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Altaira Medical School
 48%|████▊     | 19/40 [01:34<01:34,  4.51s/it]2024-12-22 01:12:57,539 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:57,730 - [Process 0/5] - INFO - res.shape is :torch.Size([18])
results:A single EYE burns with manic, unbridled hatred.
 48%|████▊     | 19/40 [01:34<01:47,  5.10s/it]2024-12-22 01:12:57,944 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:58,010 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:58,396 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:58,397 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2138])
2024-12-22 01:12:58,469 - [Process 2/5] - DEBUG - predict_token:tensor([[1763]], device='cuda:2')
2024-12-22 01:12:58,705 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:To get him.
 50%|█████     | 20/40 [01:35<01:30,  4.53s/it]2024-12-22 01:12:59,114 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:12:59,698 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:12:59,698 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 01:12:59,770 - [Process 4/5] - DEBUG - predict_token:tensor([[8081]], device='cuda:4')
2024-12-22 01:12:59,913 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Ann
 52%|█████▎    | 21/40 [01:36<01:22,  4.34s/it]2024-12-22 01:13:00,257 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:01,379 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:01,379 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2121])
2024-12-22 01:13:01,452 - [Process 3/5] - DEBUG - predict_token:tensor([[376]], device='cuda:3')
2024-12-22 01:13:01,562 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:01,562 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:13:01,635 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:13:01,689 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:The curse.
 48%|████▊     | 19/40 [01:38<01:35,  4.57s/it]2024-12-22 01:13:01,791 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Three.
 50%|█████     | 20/40 [01:38<01:35,  4.78s/it]2024-12-22 01:13:01,814 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:01,814 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1992])
2024-12-22 01:13:01,894 - [Process 1/5] - DEBUG - predict_token:tensor([[319]], device='cuda:1')
2024-12-22 01:13:01,939 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:02,054 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:02,087 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:A check.
 50%|█████     | 20/40 [01:38<01:30,  4.52s/it]2024-12-22 01:13:02,298 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:02,918 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:02,918 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2137])
2024-12-22 01:13:02,990 - [Process 2/5] - DEBUG - predict_token:tensor([[23130]], device='cuda:2')
2024-12-22 01:13:03,397 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:RUTH HONEYWILL
 52%|█████▎    | 21/40 [01:40<01:26,  4.58s/it]2024-12-22 01:13:03,957 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:04,014 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:04,014 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2206])
2024-12-22 01:13:04,084 - [Process 4/5] - DEBUG - predict_token:tensor([[25749]], device='cuda:4')
2024-12-22 01:13:05,622 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:05,622 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 01:13:05,696 - [Process 3/5] - DEBUG - predict_token:tensor([[319]], device='cuda:3')
2024-12-22 01:13:05,824 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:05,824 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2045])
2024-12-22 01:13:05,904 - [Process 0/5] - DEBUG - predict_token:tensor([[6639]], device='cuda:0')
2024-12-22 01:13:05,932 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:A deserted house
 50%|█████     | 20/40 [01:42<01:29,  4.47s/it]2024-12-22 01:13:05,948 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:05,949 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 01:13:06,023 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:13:06,351 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:06,428 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:The Witch creates a dream world.
 52%|█████▎    | 21/40 [01:43<01:24,  4.47s/it]2024-12-22 01:13:06,581 - [Process 0/5] - INFO - res.shape is :torch.Size([15])
results:Because he had stolen some devil's-foot root.
 52%|█████▎    | 21/40 [01:43<01:30,  4.79s/it]2024-12-22 01:13:06,662 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:07,027 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:07,719 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:07,719 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2207])
2024-12-22 01:13:07,788 - [Process 2/5] - DEBUG - predict_token:tensor([[25749]], device='cuda:2')
2024-12-22 01:13:07,938 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Detroit
 55%|█████▌    | 22/40 [01:44<01:22,  4.57s/it]2024-12-22 01:13:08,336 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:09,285 - [Process 4/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 55%|█████▌    | 22/40 [01:46<01:45,  5.85s/it]2024-12-22 01:13:09,653 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:10,184 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:10,184 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:13:10,264 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:13:10,310 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:10,310 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2084])
2024-12-22 01:13:10,384 - [Process 1/5] - DEBUG - predict_token:tensor([[997]], device='cuda:1')
2024-12-22 01:13:10,663 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:La Grande Breteche
 55%|█████▌    | 22/40 [01:47<01:19,  4.40s/it]2024-12-22 01:13:10,798 - [Process 3/5] - INFO - res.shape is :torch.Size([12])
results:Gravener left and promised to marry Anvoy.
2024-12-22 01:13:10,799 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
 52%|█████▎    | 21/40 [01:47<01:27,  4.59s/it]2024-12-22 01:13:10,800 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:13:10,879 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:13:11,099 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:11,264 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:11,335 - [Process 0/5] - INFO - res.shape is :torch.Size([10])
results:
2419 A.D.
 55%|█████▌    | 22/40 [01:48<01:25,  4.78s/it]2024-12-22 01:13:11,884 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:11,980 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:11,981 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2109])
2024-12-22 01:13:12,055 - [Process 2/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:2')
2024-12-22 01:13:12,462 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:Eliza requests Mary to marry again.
 57%|█████▊    | 23/40 [01:49<01:17,  4.55s/it]2024-12-22 01:13:12,818 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:13,417 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:13,417 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2226])
2024-12-22 01:13:13,487 - [Process 4/5] - DEBUG - predict_token:tensor([[10968]], device='cuda:4')
2024-12-22 01:13:13,791 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:"They were all dead."
 57%|█████▊    | 23/40 [01:50<01:32,  5.45s/it]2024-12-22 01:13:13,931 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:14,937 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:14,937 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2145])
2024-12-22 01:13:15,010 - [Process 3/5] - DEBUG - predict_token:tensor([[4667]], device='cuda:3')
2024-12-22 01:13:15,059 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:15,059 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2231])
2024-12-22 01:13:15,129 - [Process 1/5] - DEBUG - predict_token:tensor([[15706]], device='cuda:1')
2024-12-22 01:13:15,245 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:William Platt
 55%|█████▌    | 22/40 [01:52<01:21,  4.55s/it]2024-12-22 01:13:15,322 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:No one.
 57%|█████▊    | 23/40 [01:52<01:16,  4.48s/it]2024-12-22 01:13:15,486 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:15,486 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 01:13:15,560 - [Process 0/5] - DEBUG - predict_token:tensor([[1763]], device='cuda:0')
2024-12-22 01:13:15,627 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:15,813 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:15,846 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:To escape the draft.
 57%|█████▊    | 23/40 [01:52<01:19,  4.70s/it]2024-12-22 01:13:16,060 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:16,504 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:16,504 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 01:13:16,580 - [Process 2/5] - DEBUG - predict_token:tensor([[14450]], device='cuda:2')
2024-12-22 01:13:17,585 - [Process 2/5] - INFO - res.shape is :torch.Size([23])
results:Death asks Antonius to hold each other's hands and tread the dance in a long row.
 60%|██████    | 24/40 [01:54<01:15,  4.73s/it]2024-12-22 01:13:17,624 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:17,624 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2100])
2024-12-22 01:13:17,701 - [Process 4/5] - DEBUG - predict_token:tensor([[2296]], device='cuda:4')
2024-12-22 01:13:17,828 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:18,572 - [Process 4/5] - INFO - res.shape is :torch.Size([21])
results:Madame de Merret asks the mason to leave a crack at the bottom of the door.
 60%|██████    | 24/40 [01:55<01:23,  5.25s/it]2024-12-22 01:13:18,701 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:19,427 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:19,428 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:13:19,488 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:19,488 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 01:13:19,508 - [Process 1/5] - DEBUG - predict_token:tensor([[341]], device='cuda:1')
2024-12-22 01:13:19,563 - [Process 3/5] - DEBUG - predict_token:tensor([[6054]], device='cuda:3')
2024-12-22 01:13:19,669 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:19,669 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2072])
2024-12-22 01:13:19,702 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Baptist
 60%|██████    | 24/40 [01:56<01:11,  4.45s/it]2024-12-22 01:13:19,742 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:13:19,756 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Ursula
 57%|█████▊    | 23/40 [01:56<01:17,  4.54s/it]2024-12-22 01:13:19,950 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:20,029 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:A miserable slave.
 60%|██████    | 24/40 [01:56<01:12,  4.54s/it]2024-12-22 01:13:20,148 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:20,435 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:21,447 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:21,448 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2046])
2024-12-22 01:13:21,522 - [Process 2/5] - DEBUG - predict_token:tensor([[435]], device='cuda:2')
2024-12-22 01:13:21,715 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Kathy
 62%|██████▎   | 25/40 [01:58<01:08,  4.55s/it]2024-12-22 01:13:22,121 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:22,342 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:22,342 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 01:13:22,416 - [Process 4/5] - DEBUG - predict_token:tensor([[27076]], device='cuda:4')
2024-12-22 01:13:22,680 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:On Atlas' mountain.
 62%|██████▎   | 25/40 [01:59<01:13,  4.91s/it]2024-12-22 01:13:22,949 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:23,771 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:23,771 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:13:23,853 - [Process 1/5] - DEBUG - predict_token:tensor([[15533]], device='cuda:1')
2024-12-22 01:13:23,991 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:23,991 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2125])
2024-12-22 01:13:24,064 - [Process 3/5] - DEBUG - predict_token:tensor([[8622]], device='cuda:3')
2024-12-22 01:13:24,132 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Dr. Sterndale
 62%|██████▎   | 25/40 [02:00<01:06,  4.44s/it]2024-12-22 01:13:24,189 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:24,189 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:13:24,267 - [Process 0/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:0')
2024-12-22 01:13:24,392 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Masami and Tomoko
 60%|██████    | 24/40 [02:01<01:13,  4.57s/it]2024-12-22 01:13:24,427 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Aunt
 62%|██████▎   | 25/40 [02:01<01:07,  4.50s/it]2024-12-22 01:13:24,726 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:24,841 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:24,883 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:25,753 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:25,753 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2102])
2024-12-22 01:13:25,826 - [Process 2/5] - DEBUG - predict_token:tensor([[6182]], device='cuda:2')
2024-12-22 01:13:26,275 - [Process 2/5] - INFO - res.shape is :torch.Size([10])
results:Mary is taught to support her mother.
 65%|██████▌   | 26/40 [02:03<01:03,  4.55s/it]2024-12-22 01:13:26,591 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:26,591 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:13:26,633 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:26,665 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:13:26,969 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:The White Cross on the Hill
 65%|██████▌   | 26/40 [02:03<01:06,  4.72s/it]2024-12-22 01:13:27,098 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:28,516 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:28,516 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2213])
2024-12-22 01:13:28,585 - [Process 1/5] - DEBUG - predict_token:tensor([[4976]], device='cuda:1')
2024-12-22 01:13:28,620 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:28,620 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2118])
2024-12-22 01:13:28,692 - [Process 0/5] - DEBUG - predict_token:tensor([[1920]], device='cuda:0')
2024-12-22 01:13:28,725 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:28,725 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 01:13:28,736 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Frank
 65%|██████▌   | 26/40 [02:05<01:02,  4.49s/it]2024-12-22 01:13:28,807 - [Process 3/5] - DEBUG - predict_token:tensor([[6440]], device='cuda:3')
2024-12-22 01:13:28,889 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Teacher
 65%|██████▌   | 26/40 [02:05<01:02,  4.49s/it]2024-12-22 01:13:28,977 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:29,121 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:29,395 - [Process 3/5] - INFO - res.shape is :torch.Size([13])
results:The War of 2419 A.D.
 62%|██████▎   | 25/40 [02:06<01:10,  4.70s/it]2024-12-22 01:13:29,628 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:30,334 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:30,335 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:13:30,411 - [Process 2/5] - DEBUG - predict_token:tensor([[319]], device='cuda:2')
2024-12-22 01:13:30,798 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:30,798 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 01:13:30,873 - [Process 4/5] - DEBUG - predict_token:tensor([[530]], device='cuda:4')
2024-12-22 01:13:30,990 - [Process 2/5] - INFO - res.shape is :torch.Size([13])
results:Death is disguised as a black pawn.
 68%|██████▊   | 27/40 [02:07<00:59,  4.60s/it]2024-12-22 01:13:31,056 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:A enemy.
 68%|██████▊   | 27/40 [02:07<00:58,  4.53s/it]2024-12-22 01:13:31,232 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:31,395 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:32,625 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:32,625 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:13:32,700 - [Process 1/5] - DEBUG - predict_token:tensor([[15460]], device='cuda:1')
2024-12-22 01:13:32,789 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:32,789 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 01:13:32,851 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Doctor
 68%|██████▊   | 27/40 [02:09<00:56,  4.38s/it]2024-12-22 01:13:32,863 - [Process 0/5] - DEBUG - predict_token:tensor([[940]], device='cuda:0')
2024-12-22 01:13:33,242 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:33,267 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:Because he values justice over life.
 68%|██████▊   | 27/40 [02:10<00:57,  4.46s/it]2024-12-22 01:13:33,471 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:33,472 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:13:33,552 - [Process 3/5] - DEBUG - predict_token:tensor([[1632]], device='cuda:3')
2024-12-22 01:13:33,851 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:34,265 - [Process 3/5] - INFO - res.shape is :torch.Size([16])
results:He discovers that Vervelle's paintings are not original.
 65%|██████▌   | 26/40 [02:11<01:06,  4.75s/it]2024-12-22 01:13:34,856 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:35,028 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:35,028 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 01:13:35,053 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:35,053 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 01:13:35,101 - [Process 2/5] - DEBUG - predict_token:tensor([[2296]], device='cuda:2')
2024-12-22 01:13:35,126 - [Process 4/5] - DEBUG - predict_token:tensor([[7255]], device='cuda:4')
2024-12-22 01:13:35,380 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:She did not marry Charles
 70%|███████   | 28/40 [02:12<00:54,  4.54s/it]2024-12-22 01:13:35,632 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:36,197 - [Process 4/5] - INFO - res.shape is :torch.Size([26])
results:Leon focuses on writing letters that are exhaustive and detailed, providing a comprehensive account of his experiences and observations.
 70%|███████   | 28/40 [02:13<00:56,  4.71s/it]2024-12-22 01:13:36,324 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:37,056 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:37,056 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 01:13:37,136 - [Process 1/5] - DEBUG - predict_token:tensor([[4989]], device='cuda:1')
2024-12-22 01:13:37,580 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:37,580 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2205])
2024-12-22 01:13:37,649 - [Process 0/5] - DEBUG - predict_token:tensor([[1383]], device='cuda:0')
2024-12-22 01:13:37,799 - [Process 1/5] - INFO - res.shape is :torch.Size([15])
results:Gravener wants Ruth to give the money to Saltram.
 70%|███████   | 28/40 [02:14<00:54,  4.55s/it]2024-12-22 01:13:38,087 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:38,352 - [Process 0/5] - INFO - res.shape is :torch.Size([16])
results:
Drexl is shot three times in the belly by Alabama.
 70%|███████   | 28/40 [02:15<00:55,  4.64s/it]2024-12-22 01:13:38,659 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:38,660 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2224])
2024-12-22 01:13:38,729 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:13:38,744 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:39,434 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:39,434 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 01:13:39,515 - [Process 2/5] - DEBUG - predict_token:tensor([[15533]], device='cuda:2')
2024-12-22 01:13:39,603 - [Process 3/5] - INFO - res.shape is :torch.Size([20])
results:The event that reunites Jacob with the other men from his platoon is his death.
 68%|██████▊   | 27/40 [02:16<01:04,  4.93s/it]2024-12-22 01:13:39,794 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Dr. Sterndale
 72%|███████▎  | 29/40 [02:16<00:49,  4.50s/it]2024-12-22 01:13:39,816 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:39,973 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:39,974 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:13:40,047 - [Process 4/5] - DEBUG - predict_token:tensor([[2439]], device='cuda:4')
2024-12-22 01:13:40,389 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:40,473 - [Process 4/5] - INFO - res.shape is :torch.Size([10])
results:Her troubled forms of sleep she saw.
 72%|███████▎  | 29/40 [02:17<00:50,  4.58s/it]2024-12-22 01:13:40,813 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:41,927 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:41,927 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2141])
2024-12-22 01:13:42,000 - [Process 1/5] - DEBUG - predict_token:tensor([[11612]], device='cuda:1')
2024-12-22 01:13:42,491 - [Process 1/5] - INFO - res.shape is :torch.Size([11])
results:Miranda writes her letters to her mother.
 72%|███████▎  | 29/40 [02:19<00:50,  4.59s/it]2024-12-22 01:13:42,506 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:42,506 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 01:13:42,585 - [Process 0/5] - DEBUG - predict_token:tensor([[2296]], device='cuda:0')
2024-12-22 01:13:42,858 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:42,863 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:She burnt it.
 72%|███████▎  | 29/40 [02:19<00:50,  4.60s/it]2024-12-22 01:13:43,231 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:43,554 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:43,554 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 01:13:43,628 - [Process 3/5] - DEBUG - predict_token:tensor([[317]], device='cuda:3')
2024-12-22 01:13:43,821 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:A marriage.
 70%|███████   | 28/40 [02:20<00:56,  4.71s/it]2024-12-22 01:13:44,030 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:44,160 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:44,160 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2218])
2024-12-22 01:13:44,229 - [Process 2/5] - DEBUG - predict_token:tensor([[512]], device='cuda:2')
2024-12-22 01:13:44,456 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:44,456 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:13:44,531 - [Process 4/5] - DEBUG - predict_token:tensor([[8649]], device='cuda:4')
2024-12-22 01:13:44,550 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:In the subway station.
 75%|███████▌  | 30/40 [02:21<00:45,  4.58s/it]2024-12-22 01:13:44,795 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Lincoln Park Zoo
 75%|███████▌  | 30/40 [02:21<00:45,  4.50s/it]2024-12-22 01:13:44,943 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:44,971 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:46,695 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:46,695 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:13:46,768 - [Process 1/5] - DEBUG - predict_token:tensor([[3118]], device='cuda:1')
2024-12-22 01:13:46,964 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:One week.
 75%|███████▌  | 30/40 [02:23<00:45,  4.56s/it]2024-12-22 01:13:47,009 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:47,010 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2123])
2024-12-22 01:13:47,081 - [Process 0/5] - DEBUG - predict_token:tensor([[7363]], device='cuda:0')
2024-12-22 01:13:47,202 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:47,706 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:47,706 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 01:13:47,780 - [Process 3/5] - DEBUG - predict_token:tensor([[319]], device='cuda:3')
2024-12-22 01:13:47,986 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:A dead body
 72%|███████▎  | 29/40 [02:24<00:50,  4.55s/it]2024-12-22 01:13:48,363 - [Process 0/5] - INFO - res.shape is :torch.Size([29])
results:

  Penetrable shroud of hair; a single EYE burns with manic, unbridled hatred.
 75%|███████▌  | 30/40 [02:25<00:48,  4.87s/it]2024-12-22 01:13:48,462 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:48,639 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:48,728 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:48,728 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 01:13:48,782 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:48,783 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2147])
2024-12-22 01:13:48,807 - [Process 2/5] - DEBUG - predict_token:tensor([[4908]], device='cuda:2')
2024-12-22 01:13:48,856 - [Process 4/5] - DEBUG - predict_token:tensor([[4750]], device='cuda:4')
2024-12-22 01:13:48,916 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:American
 78%|███████▊  | 31/40 [02:25<00:40,  4.51s/it]2024-12-22 01:13:49,039 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Miss Miranda
 78%|███████▊  | 31/40 [02:25<00:39,  4.43s/it]2024-12-22 01:13:49,291 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:49,360 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:51,036 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:51,037 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:13:51,118 - [Process 1/5] - DEBUG - predict_token:tensor([[9932]], device='cuda:1')
2024-12-22 01:13:51,440 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Madame de Fougeres
 78%|███████▊  | 31/40 [02:28<00:40,  4.53s/it]2024-12-22 01:13:51,809 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:52,300 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:52,300 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:13:52,381 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:13:52,420 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:52,421 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2131])
2024-12-22 01:13:52,492 - [Process 0/5] - DEBUG - predict_token:tensor([[1105]], device='cuda:0')
2024-12-22 01:13:52,856 - [Process 3/5] - INFO - res.shape is :torch.Size([10])
results:
2419 A.D.
 75%|███████▌  | 30/40 [02:29<00:46,  4.65s/it]2024-12-22 01:13:53,113 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:53,114 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2141])
2024-12-22 01:13:53,156 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:53,156 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 01:13:53,186 - [Process 4/5] - DEBUG - predict_token:tensor([[498]], device='cuda:4')
2024-12-22 01:13:53,237 - [Process 2/5] - DEBUG - predict_token:tensor([[7169]], device='cuda:2')
2024-12-22 01:13:53,319 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:53,409 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Twenty-three
 80%|████████  | 32/40 [02:30<00:35,  4.41s/it]2024-12-22 01:13:53,430 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Sinsings
 80%|████████  | 32/40 [02:30<00:36,  4.51s/it]2024-12-22 01:13:53,561 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:53,705 - [Process 0/5] - INFO - res.shape is :torch.Size([27])
results:Soames gets upset with Beerbohm because he nodded and smiled at him in the street despite recognizing him.
 78%|███████▊  | 31/40 [02:30<00:45,  5.01s/it]2024-12-22 01:13:53,798 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:54,074 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:55,651 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:55,652 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2126])
2024-12-22 01:13:55,724 - [Process 1/5] - DEBUG - predict_token:tensor([[830]], device='cuda:1')
2024-12-22 01:13:56,216 - [Process 1/5] - INFO - res.shape is :torch.Size([11])
results:Reiko makes a copy of the tape.
 80%|████████  | 32/40 [02:33<00:36,  4.61s/it]2024-12-22 01:13:56,571 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:56,993 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:56,994 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:13:57,068 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:13:57,346 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:The alarm bell rings.
 78%|███████▊  | 31/40 [02:34<00:41,  4.60s/it]2024-12-22 01:13:57,380 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:57,380 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:13:57,461 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:13:57,605 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:57,605 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2124])
2024-12-22 01:13:57,677 - [Process 2/5] - DEBUG - predict_token:tensor([[1932]], device='cuda:2')
2024-12-22 01:13:57,852 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:13:57,852 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:13:57,910 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:57,924 - [Process 0/5] - DEBUG - predict_token:tensor([[3118]], device='cuda:0')
2024-12-22 01:13:58,116 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:One week.
 80%|████████  | 32/40 [02:34<00:38,  4.83s/it]2024-12-22 01:13:58,413 - [Process 4/5] - INFO - res.shape is :torch.Size([23])
results:The housekeeper's feet got wet during Holmes' visit as he threw gravel from the window.
 82%|████████▎ | 33/40 [02:35<00:32,  4.59s/it]2024-12-22 01:13:58,472 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:58,555 - [Process 2/5] - INFO - res.shape is :torch.Size([20])
results:When she sees the figure from the videotape wearing Ryuji's clothes.
 82%|████████▎ | 33/40 [02:35<00:32,  4.70s/it]2024-12-22 01:13:58,636 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:13:58,949 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:00,296 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:00,296 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:14:00,372 - [Process 1/5] - DEBUG - predict_token:tensor([[319]], device='cuda:1')
2024-12-22 01:14:00,950 - [Process 1/5] - INFO - res.shape is :torch.Size([13])
results:Death is disguised as a black pawn.
 82%|████████▎ | 33/40 [02:37<00:32,  4.64s/it]2024-12-22 01:14:01,165 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:01,721 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:01,721 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2209])
2024-12-22 01:14:01,790 - [Process 3/5] - DEBUG - predict_token:tensor([[28846]], device='cuda:3')
2024-12-22 01:14:02,137 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:02,137 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 01:14:02,212 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:14:02,329 - [Process 3/5] - INFO - res.shape is :torch.Size([12])
results:Lucy takes a drag from her cigarette.
 80%|████████  | 32/40 [02:39<00:37,  4.71s/it]2024-12-22 01:14:02,370 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Skat
 82%|████████▎ | 33/40 [02:39<00:32,  4.66s/it]2024-12-22 01:14:02,458 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:02,458 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2119])
2024-12-22 01:14:02,530 - [Process 4/5] - DEBUG - predict_token:tensor([[940]], device='cuda:4')
2024-12-22 01:14:02,644 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:02,714 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:He dies.
 85%|████████▌ | 34/40 [02:39<00:27,  4.50s/it]2024-12-22 01:14:02,737 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:02,738 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:14:02,782 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:02,817 - [Process 2/5] - DEBUG - predict_token:tensor([[2087]], device='cuda:2')
2024-12-22 01:14:02,899 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:03,054 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Ruth Anvoy
 85%|████████▌ | 34/40 [02:39<00:27,  4.64s/it]2024-12-22 01:14:03,360 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:04,897 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:04,897 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 01:14:04,973 - [Process 1/5] - DEBUG - predict_token:tensor([[319]], device='cuda:1')
2024-12-22 01:14:05,379 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:They willingly live in Athens.
 85%|████████▌ | 34/40 [02:42<00:27,  4.58s/it]2024-12-22 01:14:05,788 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:06,392 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:06,392 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2107])
2024-12-22 01:14:06,465 - [Process 0/5] - DEBUG - predict_token:tensor([[2439]], device='cuda:0')
2024-12-22 01:14:06,472 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:06,472 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:14:06,552 - [Process 3/5] - DEBUG - predict_token:tensor([[341]], device='cuda:3')
2024-12-22 01:14:06,705 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:06,706 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:14:06,713 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Tha
 82%|████████▎ | 33/40 [02:43<00:32,  4.61s/it]2024-12-22 01:14:06,786 - [Process 4/5] - DEBUG - predict_token:tensor([[341]], device='cuda:4')
2024-12-22 01:14:06,798 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Her mother played with her.
 85%|████████▌ | 34/40 [02:43<00:27,  4.59s/it]2024-12-22 01:14:06,929 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Tha
 88%|████████▊ | 35/40 [02:43<00:22,  4.42s/it]2024-12-22 01:14:06,965 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:07,059 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:07,082 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:07,147 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:07,147 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 01:14:07,226 - [Process 2/5] - DEBUG - predict_token:tensor([[2688]], device='cuda:2')
2024-12-22 01:14:07,334 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Up
 88%|████████▊ | 35/40 [02:44<00:22,  4.53s/it]2024-12-22 01:14:07,769 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:09,624 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:09,625 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2132])
2024-12-22 01:14:09,698 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:14:10,188 - [Process 1/5] - INFO - res.shape is :torch.Size([11])
results:Falder breaks his neck by jumping.
 88%|████████▊ | 35/40 [02:47<00:23,  4.65s/it]2024-12-22 01:14:10,554 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:10,657 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:10,657 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2089])
2024-12-22 01:14:10,661 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:10,661 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:14:10,723 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:10,723 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:14:10,732 - [Process 3/5] - DEBUG - predict_token:tensor([[28529]], device='cuda:3')
2024-12-22 01:14:10,735 - [Process 0/5] - DEBUG - predict_token:tensor([[10152]], device='cuda:0')
2024-12-22 01:14:10,798 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:14:11,028 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Monsieur de Merret
 85%|████████▌ | 34/40 [02:47<00:27,  4.52s/it]2024-12-22 01:14:11,063 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Brenda Tregennis
 90%|█████████ | 36/40 [02:47<00:17,  4.33s/it]2024-12-22 01:14:11,074 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:"His little girl"
 88%|████████▊ | 35/40 [02:47<00:22,  4.50s/it]2024-12-22 01:14:11,215 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:11,408 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:11,408 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:14:11,451 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:11,480 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:14:11,514 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:12,057 - [Process 2/5] - INFO - res.shape is :torch.Size([13])
results:The setting of the story is a medieval castle in Germany.
 90%|█████████ | 36/40 [02:48<00:18,  4.59s/it]2024-12-22 01:14:12,427 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:14,396 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:14,397 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2129])
2024-12-22 01:14:14,470 - [Process 1/5] - DEBUG - predict_token:tensor([[376]], device='cuda:1')
2024-12-22 01:14:14,707 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:The single eye.
 90%|█████████ | 36/40 [02:51<00:18,  4.61s/it]2024-12-22 01:14:14,957 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:15,037 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:15,037 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:14:15,119 - [Process 4/5] - DEBUG - predict_token:tensor([[4168]], device='cuda:4')
2024-12-22 01:14:15,278 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:15,278 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:14:15,293 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:15,293 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 01:14:15,344 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Gravel.
 92%|█████████▎| 37/40 [02:52<00:12,  4.32s/it]2024-12-22 01:14:15,359 - [Process 3/5] - DEBUG - predict_token:tensor([[4989]], device='cuda:3')
2024-12-22 01:14:15,365 - [Process 0/5] - DEBUG - predict_token:tensor([[940]], device='cuda:0')
2024-12-22 01:14:15,624 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:16,022 - [Process 3/5] - INFO - res.shape is :torch.Size([14])
results:Gravener urged Anvoy to marry Saltram.
 88%|████████▊ | 35/40 [02:52<00:23,  4.67s/it]2024-12-22 01:14:16,232 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:16,232 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2119])
2024-12-22 01:14:16,248 - [Process 0/5] - INFO - res.shape is :torch.Size([19])
results:Falder was worried about Ruth because she was a convict on the run.
 90%|█████████ | 36/40 [02:53<00:18,  4.70s/it]2024-12-22 01:14:16,304 - [Process 2/5] - DEBUG - predict_token:tensor([[940]], device='cuda:2')
2024-12-22 01:14:16,311 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:16,498 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:He dies.
 92%|█████████▎| 37/40 [02:53<00:13,  4.54s/it]2024-12-22 01:14:16,815 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:16,906 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:18,795 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:18,795 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:14:18,877 - [Process 1/5] - DEBUG - predict_token:tensor([[2812]], device='cuda:1')
2024-12-22 01:14:19,028 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Brenda
 92%|█████████▎| 37/40 [02:55<00:13,  4.52s/it]2024-12-22 01:14:19,259 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:19,451 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:19,451 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:14:19,533 - [Process 4/5] - DEBUG - predict_token:tensor([[3082]], device='cuda:4')
2024-12-22 01:14:19,838 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:American Radioactive Gas Corporation.
 95%|█████████▌| 38/40 [02:56<00:08,  4.37s/it]2024-12-22 01:14:20,053 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:20,159 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:20,159 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2145])
2024-12-22 01:14:20,232 - [Process 3/5] - DEBUG - predict_token:tensor([[940]], device='cuda:3')
2024-12-22 01:14:20,408 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:20,409 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:14:20,483 - [Process 0/5] - DEBUG - predict_token:tensor([[940]], device='cuda:0')
2024-12-22 01:14:20,603 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:He would find out by dying.
 90%|█████████ | 36/40 [02:57<00:18,  4.64s/it]2024-12-22 01:14:20,715 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:20,715 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2136])
2024-12-22 01:14:20,787 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:14:20,833 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:20,903 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:Bill died in the plane crash.
 92%|█████████▎| 37/40 [02:57<00:14,  4.69s/it]2024-12-22 01:14:21,067 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Morris's.
 95%|█████████▌| 38/40 [02:57<00:09,  4.55s/it]2024-12-22 01:14:21,156 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:21,460 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:22,926 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:22,926 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 01:14:23,000 - [Process 1/5] - DEBUG - predict_token:tensor([[2688]], device='cuda:1')
2024-12-22 01:14:23,279 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:They went to bed.
 95%|█████████▌| 38/40 [03:00<00:08,  4.44s/it]2024-12-22 01:14:23,678 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:23,768 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:23,768 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 01:14:23,846 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:14:23,989 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Skat
 98%|█████████▊| 39/40 [03:00<00:04,  4.30s/it]2024-12-22 01:14:24,209 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:24,491 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:24,492 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2094])
2024-12-22 01:14:24,564 - [Process 3/5] - DEBUG - predict_token:tensor([[6518]], device='cuda:3')
2024-12-22 01:14:24,672 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Pan
 92%|█████████▎| 37/40 [03:01<00:13,  4.47s/it]2024-12-22 01:14:24,920 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:24,934 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:24,934 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 01:14:25,014 - [Process 0/5] - DEBUG - predict_token:tensor([[1763]], device='cuda:0')
2024-12-22 01:14:25,257 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:25,257 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:14:25,335 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:To investigate a murder case.
 95%|█████████▌| 38/40 [03:02<00:09,  4.61s/it]2024-12-22 01:14:25,337 - [Process 2/5] - DEBUG - predict_token:tensor([[512]], device='cuda:2')
2024-12-22 01:14:25,573 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Living alone.
 98%|█████████▊| 39/40 [03:02<00:04,  4.54s/it]2024-12-22 01:14:25,808 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:25,818 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:27,543 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:27,543 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2115])
2024-12-22 01:14:27,618 - [Process 1/5] - DEBUG - predict_token:tensor([[21439]], device='cuda:1')
2024-12-22 01:14:27,896 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Reading novels.
 98%|█████████▊| 39/40 [03:04<00:04,  4.49s/it]2024-12-22 01:14:28,036 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:28,036 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2124])
2024-12-22 01:14:28,109 - [Process 4/5] - DEBUG - predict_token:tensor([[1932]], device='cuda:4')
2024-12-22 01:14:28,458 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:28,769 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:28,769 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:14:28,851 - [Process 3/5] - DEBUG - predict_token:tensor([[7803]], device='cuda:3')
2024-12-22 01:14:28,938 - [Process 4/5] - INFO - res.shape is :torch.Size([20])
results:When she sees the figure from the videotape wearing Ryuji's clothes.
100%|██████████| 40/40 [03:05<00:00,  4.50s/it]100%|██████████| 40/40 [03:05<00:00,  4.65s/it]
2024-12-22 01:14:28,959 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Two
 95%|█████████▌| 38/40 [03:05<00:08,  4.41s/it]2024-12-22 01:14:29,449 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:29,449 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2084])
2024-12-22 01:14:29,515 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:29,523 - [Process 2/5] - DEBUG - predict_token:tensor([[997]], device='cuda:2')
2024-12-22 01:14:29,571 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:29,571 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1995])
2024-12-22 01:14:29,650 - [Process 0/5] - DEBUG - predict_token:tensor([[317]], device='cuda:0')
2024-12-22 01:14:29,802 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:La Grande Breteche
100%|██████████| 40/40 [03:06<00:00,  4.45s/it]100%|██████████| 40/40 [03:06<00:00,  4.67s/it]
2024-12-22 01:14:29,843 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Slimer
 98%|█████████▊| 39/40 [03:06<00:04,  4.58s/it]2024-12-22 01:14:30,245 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:32,246 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:32,247 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2208])
2024-12-22 01:14:32,316 - [Process 1/5] - DEBUG - predict_token:tensor([[2261]], device='cuda:1')
2024-12-22 01:14:32,551 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Hitman.
100%|██████████| 40/40 [03:09<00:00,  4.54s/it]100%|██████████| 40/40 [03:09<00:00,  4.74s/it]
2024-12-22 01:14:33,324 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:33,324 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2205])
2024-12-22 01:14:33,394 - [Process 3/5] - DEBUG - predict_token:tensor([[28846]], device='cuda:3')
2024-12-22 01:14:33,543 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Lucy
 98%|█████████▊| 39/40 [03:10<00:04,  4.47s/it]2024-12-22 01:14:33,850 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:33,851 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 01:14:33,922 - [Process 0/5] - DEBUG - predict_token:tensor([[6182]], device='cuda:0')
2024-12-22 01:14:34,099 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:14:34,968 - [Process 0/5] - INFO - res.shape is :torch.Size([24])
results:Mary is initially educated to be a "mere machine" and to fulfill her duties as a wife.
100%|██████████| 40/40 [03:11<00:00,  4.74s/it]100%|██████████| 40/40 [03:11<00:00,  4.80s/it]
2024-12-22 01:14:37,764 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:14:37,764 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2054])
2024-12-22 01:14:37,839 - [Process 3/5] - DEBUG - predict_token:tensor([[3685]], device='cuda:3')
2024-12-22 01:14:37,988 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:George
100%|██████████| 40/40 [03:14<00:00,  4.46s/it]100%|██████████| 40/40 [03:14<00:00,  4.87s/it]
2024-12-22 01:14:38,029 - [Process 4/5] - DEBUG - datasets_name:narrativeqa
2024-12-22 01:14:38,029 - [Process 3/5] - DEBUG - datasets_name:narrativeqa
2024-12-22 01:14:38,029 - [Process 0/5] - DEBUG - datasets_name:narrativeqa
2024-12-22 01:14:38,029 - [Process 1/5] - DEBUG - datasets_name:narrativeqa
2024-12-22 01:14:38,029 - [Process 2/5] - DEBUG - datasets_name:narrativeqa
Running evaluation for dataset: qasper
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.67s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:16:36,757 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:16:36,757 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:16:36,757 - [Process 4/5] - INFO - output_max_len: 128
2024-12-22 01:16:36,773 - [Process 4/5] - INFO - Max Length is 14660
2024-12-22 01:16:36,773 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:16:36,774 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:16:36,786 - [Process 2/5] - INFO - loading datasets finished
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:16:36,787 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:16:36,787 - [Process 2/5] - INFO - output_max_len: 128
2024-12-22 01:16:36,787 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:16:36,787 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:16:36,787 - [Process 3/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:16:36,787 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 01:16:36,788 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:16:36,788 - [Process 1/5] - INFO - output_max_len: 128
2024-12-22 01:16:36,788 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:16:36,788 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:16:36,788 - [Process 0/5] - INFO - output_max_len: 128
2024-12-22 01:16:36,816 - [Process 2/5] - INFO - Max Length is 14660
2024-12-22 01:16:36,816 - [Process 3/5] - INFO - Max Length is 14660
2024-12-22 01:16:36,817 - [Process 1/5] - INFO - Max Length is 14660
2024-12-22 01:16:36,817 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:16:36,817 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:16:36,817 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:16:36,817 - [Process 3/5] - INFO - get_predicted begin
2024-12-22 01:16:36,817 - [Process 2/5] - INFO - get_predicted begin
2024-12-22 01:16:36,817 - [Process 0/5] - INFO - Max Length is 14660
2024-12-22 01:16:36,817 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 01:16:36,817 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:16:36,818 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:16:41,555 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:41,640 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:41,641 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:41,643 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:41,643 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:46,106 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:46,106 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2193])
2024-12-22 01:16:46,187 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:16:46,247 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:46,248 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2204])
2024-12-22 01:16:46,321 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:16:46,376 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:46,376 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
2024-12-22 01:16:46,376 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:46,376 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2120])
results:Unanswerable
2024-12-22 01:16:46,377 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2188])
  2%|▎         | 1/40 [00:09<06:12,  9.56s/it]2024-12-22 01:16:46,387 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:46,388 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2149])
2024-12-22 01:16:46,456 - [Process 4/5] - DEBUG - predict_token:tensor([[443]], device='cuda:4')
2024-12-22 01:16:46,456 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:16:46,466 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:16:46,472 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:46,524 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
  2%|▎         | 1/40 [00:09<06:18,  9.71s/it]2024-12-22 01:16:46,576 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
  2%|▎         | 1/40 [00:09<06:20,  9.76s/it]2024-12-22 01:16:46,663 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
  2%|▎         | 1/40 [00:09<06:25,  9.89s/it]2024-12-22 01:16:46,680 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
  2%|▎         | 1/40 [00:09<06:24,  9.86s/it]2024-12-22 01:16:46,720 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:46,776 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:46,862 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:46,890 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:49,902 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:49,902 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1957])
2024-12-22 01:16:49,972 - [Process 0/5] - DEBUG - predict_token:tensor([[28484]], device='cuda:0')
2024-12-22 01:16:50,431 - [Process 0/5] - INFO - res.shape is :torch.Size([11])
results:Ghost-VLAD pooling approach.
  5%|▌         | 2/40 [00:13<04:00,  6.32s/it]2024-12-22 01:16:50,545 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:50,634 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:50,634 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2232])
2024-12-22 01:16:50,709 - [Process 2/5] - DEBUG - predict_token:tensor([[2688]], device='cuda:2')
2024-12-22 01:16:50,722 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:50,723 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:16:50,802 - [Process 3/5] - DEBUG - predict_token:tensor([[443]], device='cuda:3')
2024-12-22 01:16:50,833 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:50,833 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:16:50,906 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
  5%|▌         | 2/40 [00:14<04:09,  6.57s/it]2024-12-22 01:16:50,911 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:16:50,916 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:50,916 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2123])
2024-12-22 01:16:50,996 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:16:51,075 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:51,109 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
  5%|▌         | 2/40 [00:14<04:13,  6.67s/it]2024-12-22 01:16:51,118 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
  5%|▌         | 2/40 [00:14<04:14,  6.69s/it]2024-12-22 01:16:51,242 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:51,261 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:53,979 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:53,979 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1604])
2024-12-22 01:16:54,033 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:16:54,070 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:54,071 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1532])
2024-12-22 01:16:54,129 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:16:54,230 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
  8%|▊         | 3/40 [00:17<03:07,  5.06s/it]2024-12-22 01:16:54,270 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Yes.
  8%|▊         | 3/40 [00:17<03:07,  5.07s/it]2024-12-22 01:16:54,359 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:54,360 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 01:16:54,386 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:54,440 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:16:54,456 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:54,543 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:54,543 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1962])
2024-12-22 01:16:54,544 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
  8%|▊         | 3/40 [00:17<03:16,  5.31s/it]2024-12-22 01:16:54,614 - [Process 2/5] - DEBUG - predict_token:tensor([[2688]], device='cuda:2')
2024-12-22 01:16:54,633 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:54,805 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
  8%|▊         | 3/40 [00:17<03:17,  5.35s/it]2024-12-22 01:16:54,987 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:55,527 - [Process 3/5] - INFO - res.shape is :torch.Size([108])
results:INLINEFORM0 tag means the current word is not a pun.
INLINEFORM1 tag means the current word is a pun.
INLINEFORM2 tag means the current word appears before the pun in the given context.
INLINEFORM3 tag means the current word is the pun.
INLINEFORM4 tag means the current word appears after the pun in the given context.
INLINEFORM5 tag means the current word is the pun and the context contains only one pun.

Unanswerable.
  5%|▌         | 2/40 [00:18<05:53,  9.30s/it]2024-12-22 01:16:55,712 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:57,376 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:57,377 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1680])
2024-12-22 01:16:57,440 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:16:57,584 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
Yes
 10%|█         | 4/40 [00:20<02:37,  4.38s/it]2024-12-22 01:16:57,731 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:57,754 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:57,755 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1828])
2024-12-22 01:16:57,818 - [Process 0/5] - DEBUG - predict_token:tensor([[1019]], device='cuda:0')
2024-12-22 01:16:58,033 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Unanswerable.
 10%|█         | 4/40 [00:21<02:45,  4.59s/it]2024-12-22 01:16:58,144 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:58,485 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:58,486 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2179])
2024-12-22 01:16:58,566 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:16:58,762 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 10%|█         | 4/40 [00:21<02:54,  4.84s/it]2024-12-22 01:16:58,968 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:58,968 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2167])
2024-12-22 01:16:58,970 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:59,048 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:16:59,244 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 10%|█         | 4/40 [00:22<02:59,  4.99s/it]2024-12-22 01:16:59,441 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:16:59,504 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:16:59,504 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2072])
2024-12-22 01:16:59,585 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:16:59,780 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
  8%|▊         | 3/40 [00:22<04:18,  6.99s/it]2024-12-22 01:16:59,969 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:00,879 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:00,880 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1819])
2024-12-22 01:17:00,943 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:17:01,826 - [Process 4/5] - INFO - res.shape is :torch.Size([21])
results:The dataset is annotated based on a hierarchical model of depression-related symptoms.
 12%|█▎        | 5/40 [00:25<02:31,  4.33s/it]2024-12-22 01:17:01,937 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:01,937 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2203])
2024-12-22 01:17:02,010 - [Process 0/5] - DEBUG - predict_token:tensor([[14802]], device='cuda:0')
2024-12-22 01:17:02,012 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:02,196 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 12%|█▎        | 5/40 [00:25<02:35,  4.44s/it]2024-12-22 01:17:02,313 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:02,995 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:02,995 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:17:03,075 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:17:03,268 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:03,268 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2200])
2024-12-22 01:17:03,271 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 12%|█▎        | 5/40 [00:26<02:45,  4.72s/it]2024-12-22 01:17:03,341 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:03,467 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:03,542 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 12%|█▎        | 5/40 [00:26<02:45,  4.74s/it]2024-12-22 01:17:03,737 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:03,766 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:03,766 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2258])
2024-12-22 01:17:03,839 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:17:04,033 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 10%|█         | 4/40 [00:27<03:32,  5.91s/it]2024-12-22 01:17:04,227 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:05,751 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:05,751 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 01:17:05,831 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:17:06,027 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 15%|█▌        | 6/40 [00:29<02:25,  4.29s/it]2024-12-22 01:17:06,094 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:06,094 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 01:17:06,173 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:17:06,180 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:06,400 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:
unanswerable
 15%|█▌        | 6/40 [00:29<02:28,  4.36s/it]2024-12-22 01:17:06,519 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:07,484 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:07,484 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:17:07,502 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:07,502 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2125])
2024-12-22 01:17:07,564 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:07,583 - [Process 1/5] - DEBUG - predict_token:tensor([[443]], device='cuda:1')
2024-12-22 01:17:07,771 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 15%|█▌        | 6/40 [00:30<02:35,  4.57s/it]2024-12-22 01:17:07,789 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 15%|█▌        | 6/40 [00:30<02:38,  4.65s/it]2024-12-22 01:17:07,909 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:07,923 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:08,263 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:08,263 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:17:08,344 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:17:08,496 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Yes.
 12%|█▎        | 5/40 [00:31<03:08,  5.39s/it]2024-12-22 01:17:08,690 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:09,357 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:09,357 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1908])
2024-12-22 01:17:09,421 - [Process 4/5] - DEBUG - predict_token:tensor([[4103]], device='cuda:4')
2024-12-22 01:17:09,608 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 18%|█▊        | 7/40 [00:32<02:13,  4.06s/it]2024-12-22 01:17:09,803 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:10,301 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:10,301 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:17:10,382 - [Process 0/5] - DEBUG - predict_token:tensor([[1019]], device='cuda:0')
2024-12-22 01:17:10,568 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 18%|█▊        | 7/40 [00:33<02:21,  4.30s/it]2024-12-22 01:17:10,660 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:10,660 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1593])
2024-12-22 01:17:10,689 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:10,697 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:10,697 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1487])
2024-12-22 01:17:10,715 - [Process 2/5] - DEBUG - predict_token:tensor([[5176]], device='cuda:2')
2024-12-22 01:17:10,752 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:17:11,854 - [Process 2/5] - INFO - res.shape is :torch.Size([25])
results:Yes. According to the article, the approach achieves state of the art results on the English-German dataset.
 18%|█▊        | 7/40 [00:35<02:25,  4.41s/it]2024-12-22 01:17:12,060 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:12,559 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:12,559 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:17:12,640 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:17:13,805 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:13,805 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2167])
2024-12-22 01:17:13,885 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:17:14,094 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 20%|██        | 8/40 [00:37<02:14,  4.19s/it]2024-12-22 01:17:14,295 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:14,642 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:14,642 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 01:17:14,720 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:17:15,898 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:15,898 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 01:17:15,979 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:15,991 - [Process 1/5] - INFO - res.shape is :torch.Size([128])
results:NeuronBlocks includes CNN, LSTM, RNN, Transformer, and Attention modules.


Question: Can NeuronBlocks be used for sequence labeling tasks?

Answer: Yes.

Question: What is the main goal of NeuronBlocks?

Answer: Unanswerable.

Question: How many types of NLP tasks are supported by NeuronBlocks?
Answer: Four.

Question: What is the name of the toolkit that NeuronBlocks is built on?
Answer: PyTorch.


 18%|█▊        | 7/40 [00:39<03:11,  5.81s/it]2024-12-22 01:17:16,181 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 20%|██        | 8/40 [00:39<02:20,  4.38s/it]2024-12-22 01:17:16,197 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:16,386 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:16,866 - [Process 0/5] - INFO - res.shape is :torch.Size([52])
results:

The article mentions three datasets used for evaluation:


* CNN/DailyMail
* NYT
* XSum

Therefore, the answer is "CNN/DailyMail, NYT, and XSum".
 20%|██        | 8/40 [00:40<02:37,  4.93s/it]2024-12-22 01:17:16,968 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:17,086 - [Process 3/5] - INFO - res.shape is :torch.Size([102])
results:Yes. According to the article, the method improves F1 scores for NER tasks for both English and Chinese datasets. For English datasets, the method improves F1 scores by +0.96 and +0.97 for MSRA and OntoNotes4.0 datasets, respectively. For Chinese datasets, the method improves F1 scores by +0.97 and +2.36 for MSRA and OntoNotes4.0 datasets, respectively.
 15%|█▌        | 6/40 [00:40<03:40,  6.48s/it]2024-12-22 01:17:17,247 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:18,144 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:18,145 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2201])
2024-12-22 01:17:18,218 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:17:18,457 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:
unanswerable
 22%|██▎       | 9/40 [00:41<02:11,  4.25s/it]2024-12-22 01:17:18,640 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:20,059 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:20,060 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2083])
2024-12-22 01:17:20,142 - [Process 1/5] - DEBUG - predict_token:tensor([[443]], device='cuda:1')
2024-12-22 01:17:20,218 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:20,218 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2081])
2024-12-22 01:17:20,298 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:20,340 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 20%|██        | 8/40 [00:43<02:51,  5.35s/it]2024-12-22 01:17:20,502 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 22%|██▎       | 9/40 [00:43<02:15,  4.36s/it]2024-12-22 01:17:20,517 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:20,570 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:20,570 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1855])
2024-12-22 01:17:20,612 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:20,639 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:17:20,659 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:20,659 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1952])
2024-12-22 01:17:20,740 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:17:20,744 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 18%|█▊        | 7/40 [00:43<03:03,  5.55s/it]2024-12-22 01:17:20,844 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 22%|██▎       | 9/40 [00:44<02:23,  4.63s/it]2024-12-22 01:17:20,950 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:20,963 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:22,633 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:22,634 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:17:22,713 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:17:22,865 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Yes.
 25%|██▌       | 10/40 [00:46<02:08,  4.30s/it]2024-12-22 01:17:22,938 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:22,938 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1337])
2024-12-22 01:17:22,985 - [Process 2/5] - DEBUG - predict_token:tensor([[405]], device='cuda:2')
2024-12-22 01:17:23,069 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:23,160 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:

Yes
 25%|██▌       | 10/40 [00:46<01:55,  3.84s/it]2024-12-22 01:17:23,355 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:24,065 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:24,066 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:17:24,135 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:17:24,326 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 22%|██▎       | 9/40 [00:47<02:32,  4.92s/it]2024-12-22 01:17:24,520 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:24,748 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:24,748 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:17:24,829 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:17:24,930 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:24,930 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2158])
2024-12-22 01:17:25,009 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:17:25,024 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 20%|██        | 8/40 [00:48<02:44,  5.15s/it]2024-12-22 01:17:25,195 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 25%|██▌       | 10/40 [00:48<02:16,  4.55s/it]2024-12-22 01:17:25,228 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:25,306 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:27,076 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:27,076 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2170])
2024-12-22 01:17:27,156 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:17:27,352 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 28%|██▊       | 11/40 [00:50<02:06,  4.35s/it]2024-12-22 01:17:27,382 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:27,383 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2177])
2024-12-22 01:17:27,463 - [Process 2/5] - DEBUG - predict_token:tensor([[317]], device='cuda:2')
2024-12-22 01:17:27,541 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:27,658 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 28%|██▊       | 11/40 [00:50<01:57,  4.04s/it]2024-12-22 01:17:27,860 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:28,555 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:28,556 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2171])
2024-12-22 01:17:28,636 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:17:28,832 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 25%|██▌       | 10/40 [00:52<02:23,  4.79s/it]2024-12-22 01:17:29,006 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:29,113 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:29,113 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2072])
2024-12-22 01:17:29,195 - [Process 3/5] - DEBUG - predict_token:tensor([[1954]], device='cuda:3')
2024-12-22 01:17:29,283 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:29,284 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2125])
2024-12-22 01:17:29,364 - [Process 0/5] - DEBUG - predict_token:tensor([[11169]], device='cuda:0')
2024-12-22 01:17:29,550 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 28%|██▊       | 11/40 [00:52<02:10,  4.49s/it]2024-12-22 01:17:29,645 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:31,112 - [Process 3/5] - INFO - res.shape is :torch.Size([44])
results:Yes. According to the article, the core component for KBQA is the hierarchical matching between questions and KB relations, which is achieved through the proposed HR-BiLSTM model.
 22%|██▎       | 9/40 [00:54<02:48,  5.44s/it]2024-12-22 01:17:31,297 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:31,309 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:31,310 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:17:31,390 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:17:31,586 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 30%|███       | 12/40 [00:54<02:00,  4.32s/it]2024-12-22 01:17:31,697 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:31,697 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2086])
2024-12-22 01:17:31,744 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:31,778 - [Process 2/5] - DEBUG - predict_token:tensor([[443]], device='cuda:2')
2024-12-22 01:17:31,973 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 30%|███       | 12/40 [00:55<01:55,  4.12s/it]2024-12-22 01:17:32,096 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:32,832 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:32,832 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2005])
2024-12-22 01:17:32,914 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:17:32,979 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:32,979 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1841])
2024-12-22 01:17:33,050 - [Process 0/5] - DEBUG - predict_token:tensor([[4223]], device='cuda:0')
2024-12-22 01:17:33,109 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 28%|██▊       | 11/40 [00:56<02:14,  4.63s/it]2024-12-22 01:17:33,229 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 30%|███       | 12/40 [00:56<01:58,  4.24s/it]2024-12-22 01:17:33,307 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:33,329 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:34,691 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:34,692 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1454])
2024-12-22 01:17:34,749 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:17:34,849 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 32%|███▎      | 13/40 [00:58<01:41,  3.75s/it]2024-12-22 01:17:35,029 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:35,070 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:35,070 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1924])
2024-12-22 01:17:35,136 - [Process 4/5] - DEBUG - predict_token:tensor([[4223]], device='cuda:4')
2024-12-22 01:17:35,325 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 32%|███▎      | 13/40 [00:58<01:51,  4.14s/it]2024-12-22 01:17:35,356 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:35,356 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2165])
2024-12-22 01:17:35,437 - [Process 3/5] - DEBUG - predict_token:tensor([[443]], device='cuda:3')
2024-12-22 01:17:35,508 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:35,634 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 25%|██▌       | 10/40 [00:58<02:34,  5.16s/it]2024-12-22 01:17:35,782 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:36,807 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:36,808 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1973])
2024-12-22 01:17:36,879 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:17:37,060 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 32%|███▎      | 13/40 [01:00<01:51,  4.12s/it]2024-12-22 01:17:37,187 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:37,192 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:37,192 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2207])
2024-12-22 01:17:37,267 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:17:37,376 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 30%|███       | 12/40 [01:00<02:06,  4.52s/it]2024-12-22 01:17:37,557 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:38,879 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:38,879 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2094])
2024-12-22 01:17:38,960 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:38,985 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:38,985 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1805])
2024-12-22 01:17:39,050 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:17:39,114 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
Yes
 35%|███▌      | 14/40 [01:02<01:41,  3.90s/it]2024-12-22 01:17:39,235 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 28%|██▊       | 11/40 [01:02<02:15,  4.68s/it]2024-12-22 01:17:39,306 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:39,363 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:39,363 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 01:17:39,399 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:39,445 - [Process 4/5] - DEBUG - predict_token:tensor([[443]], device='cuda:4')
2024-12-22 01:17:39,641 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 35%|███▌      | 14/40 [01:02<01:49,  4.19s/it]2024-12-22 01:17:39,838 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:40,940 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:40,940 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 01:17:41,020 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:17:41,206 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 35%|███▌      | 14/40 [01:04<01:47,  4.13s/it]2024-12-22 01:17:41,321 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:41,620 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:41,621 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2122])
2024-12-22 01:17:41,702 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:17:41,812 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 32%|███▎      | 13/40 [01:04<02:01,  4.50s/it]2024-12-22 01:17:41,909 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:42,938 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:42,938 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 01:17:43,007 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:17:43,103 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:43,104 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2053])
2024-12-22 01:17:43,184 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:43,196 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 30%|███       | 12/40 [01:06<02:04,  4.46s/it]2024-12-22 01:17:43,380 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 38%|███▊      | 15/40 [01:06<01:40,  4.01s/it]2024-12-22 01:17:43,384 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:43,482 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:43,873 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:43,873 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2167])
2024-12-22 01:17:43,953 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:17:44,154 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:44,155 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1285])
2024-12-22 01:17:44,197 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:17:44,236 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:

Unanswerable
 38%|███▊      | 15/40 [01:07<01:47,  4.32s/it]2024-12-22 01:17:44,370 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 35%|███▌      | 14/40 [01:07<01:41,  3.91s/it]2024-12-22 01:17:44,436 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:44,698 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:45,315 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:45,315 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2121])
2024-12-22 01:17:45,395 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:17:45,500 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 38%|███▊      | 15/40 [01:08<01:44,  4.18s/it]2024-12-22 01:17:45,687 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:45,722 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:45,722 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1250])
2024-12-22 01:17:45,771 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:17:45,866 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 40%|████      | 16/40 [01:09<01:25,  3.55s/it]2024-12-22 01:17:45,974 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:47,452 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:47,453 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:17:47,533 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:17:47,642 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 32%|███▎      | 13/40 [01:10<02:00,  4.46s/it]2024-12-22 01:17:47,861 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:48,289 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:48,289 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:17:48,315 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:48,315 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1337])
2024-12-22 01:17:48,362 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:48,370 - [Process 4/5] - DEBUG - predict_token:tensor([[443]], device='cuda:4')
2024-12-22 01:17:48,459 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 42%|████▎     | 17/40 [01:11<01:15,  3.26s/it]2024-12-22 01:17:48,566 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
2024-12-22 01:17:48,566 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:48,567 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
results:unanswerable
 40%|████      | 16/40 [01:11<01:43,  4.32s/it]2024-12-22 01:17:48,648 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:17:48,658 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:48,768 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:48,802 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Yes.
 38%|███▊      | 15/40 [01:11<01:41,  4.07s/it]2024-12-22 01:17:48,993 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:49,415 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:49,415 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 01:17:49,496 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:17:49,681 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 40%|████      | 16/40 [01:12<01:40,  4.18s/it]2024-12-22 01:17:49,846 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:51,761 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:51,761 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2200])
2024-12-22 01:17:51,836 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:17:52,031 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 35%|███▌      | 14/40 [01:15<01:55,  4.44s/it]2024-12-22 01:17:52,217 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:52,677 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:52,677 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:17:52,756 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:52,817 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:52,817 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2115])
2024-12-22 01:17:52,899 - [Process 4/5] - DEBUG - predict_token:tensor([[405]], device='cuda:4')
2024-12-22 01:17:52,952 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 45%|████▌     | 18/40 [01:16<01:19,  3.63s/it]2024-12-22 01:17:53,053 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:53,053 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2126])
2024-12-22 01:17:53,135 - [Process 1/5] - DEBUG - predict_token:tensor([[443]], device='cuda:1')
2024-12-22 01:17:53,156 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:53,225 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:NUBes-PHI
 42%|████▎     | 17/40 [01:16<01:41,  4.42s/it]2024-12-22 01:17:53,332 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 40%|████      | 16/40 [01:16<01:40,  4.21s/it]2024-12-22 01:17:53,416 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:53,524 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:53,839 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:53,840 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2130])
2024-12-22 01:17:53,920 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:17:54,106 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 42%|████▎     | 17/40 [01:17<01:37,  4.25s/it]2024-12-22 01:17:54,219 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:56,300 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:56,300 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2179])
2024-12-22 01:17:56,382 - [Process 3/5] - DEBUG - predict_token:tensor([[443]], device='cuda:3')
2024-12-22 01:17:56,491 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 38%|███▊      | 15/40 [01:19<01:51,  4.44s/it]2024-12-22 01:17:56,681 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:57,181 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:57,181 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2151])
2024-12-22 01:17:57,260 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:17:57,411 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:57,411 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2202])
2024-12-22 01:17:57,449 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:57,449 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2163])
2024-12-22 01:17:57,455 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 48%|████▊     | 19/40 [01:20<01:21,  3.89s/it]2024-12-22 01:17:57,485 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:17:57,529 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:17:57,620 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:57,686 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 42%|████▎     | 17/40 [01:20<01:37,  4.25s/it]2024-12-22 01:17:57,725 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 45%|████▌     | 18/40 [01:20<01:37,  4.45s/it]2024-12-22 01:17:57,939 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:57,965 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:17:57,965 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:17:58,020 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:17:58,046 - [Process 0/5] - DEBUG - predict_token:tensor([[443]], device='cuda:0')
2024-12-22 01:17:58,231 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 45%|████▌     | 18/40 [01:21<01:32,  4.21s/it]2024-12-22 01:17:58,344 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:00,573 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:00,573 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2112])
2024-12-22 01:18:00,655 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:18:00,850 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 40%|████      | 16/40 [01:24<01:46,  4.42s/it]2024-12-22 01:18:01,005 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:01,005 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1841])
2024-12-22 01:18:01,048 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:01,077 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:18:01,266 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 50%|█████     | 20/40 [01:24<01:17,  3.87s/it]2024-12-22 01:18:01,470 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:01,766 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:01,767 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:18:01,848 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:18:02,001 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Yes.
 48%|████▊     | 19/40 [01:25<01:32,  4.39s/it]2024-12-22 01:18:02,067 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:02,068 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2171])
2024-12-22 01:18:02,085 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:02,085 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2256])
2024-12-22 01:18:02,132 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:02,149 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:18:02,157 - [Process 0/5] - DEBUG - predict_token:tensor([[443]], device='cuda:0')
2024-12-22 01:18:02,258 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 45%|████▌     | 18/40 [01:25<01:35,  4.35s/it]2024-12-22 01:18:02,260 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 48%|████▊     | 19/40 [01:25<01:27,  4.16s/it]2024-12-22 01:18:02,379 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:02,464 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:04,921 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:04,921 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1623])
2024-12-22 01:18:04,961 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:04,961 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2223])
2024-12-22 01:18:04,978 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:18:05,036 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:18:05,081 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 50%|█████     | 20/40 [01:28<01:19,  4.00s/it]2024-12-22 01:18:05,236 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:05,238 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 42%|████▎     | 17/40 [01:28<01:41,  4.41s/it]2024-12-22 01:18:05,366 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:05,502 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:05,503 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:18:05,582 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:18:05,691 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 52%|█████▎    | 21/40 [01:28<01:16,  4.04s/it]2024-12-22 01:18:05,893 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:06,364 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:06,364 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2223])
2024-12-22 01:18:06,369 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:06,369 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2163])
2024-12-22 01:18:06,438 - [Process 1/5] - DEBUG - predict_token:tensor([[1281]], device='cuda:1')
2024-12-22 01:18:06,450 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:18:06,636 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 50%|█████     | 20/40 [01:29<01:24,  4.22s/it]2024-12-22 01:18:06,745 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:07,803 - [Process 1/5] - INFO - res.shape is :torch.Size([31])
results:
Yes. The two datasets the model is applied to are:

1. Conversations Gone Awry
2. ChangeMyView
 48%|████▊     | 19/40 [01:30<01:38,  4.71s/it]2024-12-22 01:18:07,957 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:08,119 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:08,119 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1566])
2024-12-22 01:18:08,175 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:18:08,421 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:08,422 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1783])
2024-12-22 01:18:08,473 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Bibref111
 45%|████▌     | 18/40 [01:31<01:29,  4.06s/it]2024-12-22 01:18:08,484 - [Process 4/5] - DEBUG - predict_token:tensor([[383]], device='cuda:4')
2024-12-22 01:18:08,648 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:08,799 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Favor or Against.
 52%|█████▎    | 21/40 [01:32<01:14,  3.92s/it]2024-12-22 01:18:08,999 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:09,941 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:09,941 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2136])
2024-12-22 01:18:10,022 - [Process 2/5] - DEBUG - predict_token:tensor([[4223]], device='cuda:2')
2024-12-22 01:18:10,218 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 55%|█████▌    | 22/40 [01:33<01:15,  4.18s/it]2024-12-22 01:18:10,376 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:10,560 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:10,560 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2100])
2024-12-22 01:18:10,641 - [Process 0/5] - DEBUG - predict_token:tensor([[1954]], device='cuda:0')
2024-12-22 01:18:10,827 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 52%|█████▎    | 21/40 [01:34<01:20,  4.21s/it]2024-12-22 01:18:10,951 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:11,167 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:11,167 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1828])
2024-12-22 01:18:11,233 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:18:11,337 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 50%|█████     | 20/40 [01:34<01:27,  4.35s/it]2024-12-22 01:18:11,516 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:12,189 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:12,189 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1958])
2024-12-22 01:18:12,262 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:18:12,367 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 48%|████▊     | 19/40 [01:35<01:24,  4.01s/it]2024-12-22 01:18:12,539 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:12,886 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:12,886 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2201])
2024-12-22 01:18:12,960 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:18:13,070 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 55%|█████▌    | 22/40 [01:36<01:12,  4.02s/it]2024-12-22 01:18:13,260 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:13,719 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:13,719 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1924])
2024-12-22 01:18:13,785 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:18:13,973 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 57%|█████▊    | 23/40 [01:37<01:08,  4.06s/it]2024-12-22 01:18:14,074 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:14,684 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:14,684 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1967])
2024-12-22 01:18:14,767 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:18:14,871 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 55%|█████▌    | 22/40 [01:38<01:14,  4.16s/it]2024-12-22 01:18:14,986 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:15,347 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:15,347 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:18:15,428 - [Process 1/5] - DEBUG - predict_token:tensor([[7361]], device='cuda:1')
2024-12-22 01:18:15,625 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 52%|█████▎    | 21/40 [01:38<01:22,  4.33s/it]2024-12-22 01:18:15,811 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:16,290 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:16,290 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1259])
2024-12-22 01:18:16,337 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:18:16,381 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:16,381 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2136])
2024-12-22 01:18:16,433 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 60%|██████    | 24/40 [01:39<00:57,  3.58s/it]2024-12-22 01:18:16,454 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:18:16,554 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:16,647 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 50%|█████     | 20/40 [01:39<01:21,  4.09s/it]2024-12-22 01:18:16,855 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:17,149 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:17,149 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 01:18:17,231 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:18:17,428 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 57%|█████▊    | 23/40 [01:40<01:10,  4.12s/it]2024-12-22 01:18:17,620 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:18,805 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:18,806 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2086])
2024-12-22 01:18:18,887 - [Process 0/5] - DEBUG - predict_token:tensor([[443]], device='cuda:0')
2024-12-22 01:18:19,073 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 57%|█████▊    | 23/40 [01:42<01:10,  4.17s/it]2024-12-22 01:18:19,185 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:19,195 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:19,195 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1525])
2024-12-22 01:18:19,245 - [Process 2/5] - DEBUG - predict_token:tensor([[443]], device='cuda:2')
2024-12-22 01:18:19,343 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 62%|██████▎   | 25/40 [01:42<00:50,  3.38s/it]2024-12-22 01:18:19,525 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:19,754 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:19,754 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 01:18:19,837 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:18:20,033 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 55%|█████▌    | 22/40 [01:43<01:18,  4.36s/it]2024-12-22 01:18:20,191 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:20,762 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:20,763 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2212])
2024-12-22 01:18:20,838 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:18:21,032 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 52%|█████▎    | 21/40 [01:44<01:19,  4.18s/it]2024-12-22 01:18:21,224 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:21,680 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:21,680 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2171])
2024-12-22 01:18:21,761 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:18:21,957 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 60%|██████    | 24/40 [01:45<01:07,  4.24s/it]2024-12-22 01:18:22,159 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:23,006 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:23,006 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2230])
2024-12-22 01:18:23,080 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:18:23,266 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 60%|██████    | 24/40 [01:46<01:06,  4.18s/it]2024-12-22 01:18:23,387 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:23,564 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:23,565 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2167])
2024-12-22 01:18:23,596 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:23,596 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1841])
2024-12-22 01:18:23,645 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:18:23,668 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:18:23,760 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 65%|██████▌   | 26/40 [01:46<00:51,  3.69s/it]2024-12-22 01:18:23,865 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 57%|█████▊    | 23/40 [01:47<01:11,  4.20s/it]2024-12-22 01:18:23,978 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:24,071 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:25,329 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:25,329 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:18:25,411 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:18:25,521 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 55%|█████▌    | 22/40 [01:48<01:16,  4.27s/it]2024-12-22 01:18:25,717 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:26,040 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:26,041 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2102])
2024-12-22 01:18:26,123 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:18:26,319 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 62%|██████▎   | 25/40 [01:49<01:04,  4.28s/it]2024-12-22 01:18:26,497 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:27,381 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:27,382 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2139])
2024-12-22 01:18:27,460 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:18:27,646 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 62%|██████▎   | 25/40 [01:50<01:03,  4.24s/it]2024-12-22 01:18:27,759 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:27,862 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:27,862 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2209])
2024-12-22 01:18:27,937 - [Process 2/5] - DEBUG - predict_token:tensor([[443]], device='cuda:2')
2024-12-22 01:18:27,971 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:27,971 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:18:28,054 - [Process 1/5] - DEBUG - predict_token:tensor([[443]], device='cuda:1')
2024-12-22 01:18:28,137 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 68%|██████▊   | 27/40 [01:51<00:50,  3.90s/it]2024-12-22 01:18:28,252 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:28,301 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:
unanswerable
 60%|██████    | 24/40 [01:51<01:08,  4.27s/it]2024-12-22 01:18:28,485 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:29,824 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:29,824 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2122])
2024-12-22 01:18:29,907 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:18:30,016 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 57%|█████▊    | 23/40 [01:53<01:13,  4.34s/it]2024-12-22 01:18:30,210 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:30,549 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:30,549 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2147])
2024-12-22 01:18:30,631 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:18:30,636 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:30,636 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1350])
2024-12-22 01:18:30,683 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:18:30,779 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 70%|███████   | 28/40 [01:53<00:42,  3.52s/it]2024-12-22 01:18:30,827 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 65%|██████▌   | 26/40 [01:54<01:00,  4.35s/it]2024-12-22 01:18:30,966 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:30,980 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:31,738 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:31,738 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2164])
2024-12-22 01:18:31,817 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:18:32,003 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 65%|██████▌   | 26/40 [01:55<00:59,  4.28s/it]2024-12-22 01:18:32,092 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:32,572 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:32,572 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2194])
2024-12-22 01:18:32,655 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:18:32,851 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 62%|██████▎   | 25/40 [01:56<01:05,  4.35s/it]2024-12-22 01:18:32,976 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:34,205 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:34,205 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1909])
2024-12-22 01:18:34,271 - [Process 4/5] - DEBUG - predict_token:tensor([[443]], device='cuda:4')
2024-12-22 01:18:34,277 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:34,277 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2164])
2024-12-22 01:18:34,357 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:18:34,464 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 68%|██████▊   | 27/40 [01:57<00:53,  4.14s/it]2024-12-22 01:18:34,472 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 60%|██████    | 24/40 [01:57<01:09,  4.37s/it]2024-12-22 01:18:34,666 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:34,677 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:34,842 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:34,843 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2204])
2024-12-22 01:18:34,916 - [Process 2/5] - DEBUG - predict_token:tensor([[443]], device='cuda:2')
2024-12-22 01:18:35,026 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 72%|███████▎  | 29/40 [01:58<00:41,  3.74s/it]2024-12-22 01:18:35,211 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:35,271 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:35,272 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1898])
2024-12-22 01:18:35,335 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:18:35,512 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 68%|██████▊   | 27/40 [01:58<00:52,  4.05s/it]2024-12-22 01:18:35,645 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:35,775 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:35,776 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1505])
2024-12-22 01:18:35,833 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:18:36,013 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 65%|██████▌   | 26/40 [01:59<00:55,  4.00s/it]2024-12-22 01:18:36,204 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:38,727 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:38,728 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:18:38,752 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:38,752 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2164])
2024-12-22 01:18:38,808 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:18:38,835 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:18:39,015 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 70%|███████   | 28/40 [02:02<00:51,  4.26s/it]2024-12-22 01:18:39,041 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 62%|██████▎   | 25/40 [02:02<01:06,  4.43s/it]2024-12-22 01:18:39,091 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:39,091 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2189])
2024-12-22 01:18:39,166 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:18:39,214 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:39,252 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:39,362 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 75%|███████▌  | 30/40 [02:02<00:39,  3.92s/it]2024-12-22 01:18:39,557 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:39,653 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:39,654 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2181])
2024-12-22 01:18:39,734 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:18:40,097 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:40,097 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:18:40,179 - [Process 1/5] - DEBUG - predict_token:tensor([[443]], device='cuda:1')
2024-12-22 01:18:40,376 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 68%|██████▊   | 27/40 [02:03<00:53,  4.11s/it]2024-12-22 01:18:40,608 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:41,841 - [Process 0/5] - INFO - res.shape is :torch.Size([51])
results:


* Yes

Explanation: According to the article, WordNet is used as a source of expert knowledge for constructing probes to test the competence of transformer-based QA models in taxonomic reasoning.
 70%|███████   | 28/40 [02:05<00:56,  4.73s/it]2024-12-22 01:18:41,924 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:43,112 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:43,112 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2224])
2024-12-22 01:18:43,167 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:43,167 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:18:43,187 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:18:43,250 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:18:43,299 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 72%|███████▎  | 29/40 [02:06<00:46,  4.27s/it]2024-12-22 01:18:43,363 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 65%|██████▌   | 26/40 [02:06<01:01,  4.40s/it]2024-12-22 01:18:43,369 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:43,369 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2072])
2024-12-22 01:18:43,450 - [Process 2/5] - DEBUG - predict_token:tensor([[443]], device='cuda:2')
2024-12-22 01:18:43,498 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:43,507 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:43,647 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 78%|███████▊  | 31/40 [02:06<00:36,  4.03s/it]2024-12-22 01:18:43,837 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:44,686 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:44,686 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2116])
2024-12-22 01:18:44,768 - [Process 1/5] - DEBUG - predict_token:tensor([[21784]], device='cuda:1')
2024-12-22 01:18:44,916 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:44,917 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1686])
2024-12-22 01:18:44,965 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 70%|███████   | 28/40 [02:08<00:51,  4.25s/it]2024-12-22 01:18:44,980 - [Process 0/5] - DEBUG - predict_token:tensor([[443]], device='cuda:0')
2024-12-22 01:18:45,140 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:45,154 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 72%|███████▎  | 29/40 [02:08<00:47,  4.30s/it]2024-12-22 01:18:45,270 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:46,162 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:46,162 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1499])
2024-12-22 01:18:46,215 - [Process 3/5] - DEBUG - predict_token:tensor([[350]], device='cuda:3')
2024-12-22 01:18:46,430 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:BIBREF0
 68%|██████▊   | 27/40 [02:09<00:51,  4.00s/it]2024-12-22 01:18:46,596 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:47,318 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:47,319 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2256])
2024-12-22 01:18:47,391 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:18:47,501 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 75%|███████▌  | 30/40 [02:10<00:42,  4.25s/it]2024-12-22 01:18:47,717 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:47,717 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2107])
2024-12-22 01:18:47,719 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:47,799 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:18:47,994 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 80%|████████  | 32/40 [02:11<00:32,  4.12s/it]2024-12-22 01:18:48,200 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:49,005 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:49,005 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2104])
2024-12-22 01:18:49,085 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:18:49,195 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 72%|███████▎  | 29/40 [02:12<00:46,  4.24s/it]2024-12-22 01:18:49,264 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:49,264 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2166])
2024-12-22 01:18:49,344 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:18:49,346 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:49,653 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:


unanswerable
 75%|███████▌  | 30/40 [02:12<00:43,  4.36s/it]2024-12-22 01:18:49,761 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:50,213 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:50,213 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:18:50,286 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:18:50,477 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 70%|███████   | 28/40 [02:13<00:48,  4.01s/it]2024-12-22 01:18:50,675 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:51,609 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:51,610 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2111])
2024-12-22 01:18:51,692 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:18:51,888 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 78%|███████▊  | 31/40 [02:15<00:38,  4.29s/it]2024-12-22 01:18:52,073 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:52,241 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:52,241 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:18:52,320 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:18:52,516 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 82%|████████▎ | 33/40 [02:15<00:29,  4.24s/it]2024-12-22 01:18:52,588 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:52,588 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1898])
2024-12-22 01:18:52,652 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:18:52,758 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:52,843 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 75%|███████▌  | 30/40 [02:16<00:40,  4.07s/it]2024-12-22 01:18:53,039 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:53,749 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:53,750 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2165])
2024-12-22 01:18:53,830 - [Process 0/5] - DEBUG - predict_token:tensor([[382]], device='cuda:0')
2024-12-22 01:18:53,934 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 78%|███████▊  | 31/40 [02:17<00:39,  4.34s/it]2024-12-22 01:18:54,049 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:54,589 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:54,590 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2100])
2024-12-22 01:18:54,672 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:18:55,981 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:55,981 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2213])
2024-12-22 01:18:56,056 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:18:56,264 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 80%|████████  | 32/40 [02:19<00:34,  4.32s/it]2024-12-22 01:18:56,463 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:56,691 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:56,692 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 01:18:56,774 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:18:56,933 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:56,934 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2086])
2024-12-22 01:18:57,013 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:
unanswerable
 85%|████████▌ | 34/40 [02:20<00:25,  4.32s/it]2024-12-22 01:18:57,016 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:18:57,175 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:57,218 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 78%|███████▊  | 31/40 [02:20<00:37,  4.16s/it]2024-12-22 01:18:57,342 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:57,872 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:18:57,872 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 01:18:57,953 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:18:58,139 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 80%|████████  | 32/40 [02:21<00:34,  4.30s/it]2024-12-22 01:18:58,258 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:18:59,331 - [Process 3/5] - INFO - res.shape is :torch.Size([107])
results:Yes. According to the article, the following baselines are used for evaluation:

1. Pointer-Gen baseline model
2. Pointer-Gen+Same-FT
3. Pointer-Gen+Pos-FT
4. Pointer-Gen+RL-SEN
5. Pointer-Gen+ARL-SEN

Note: The article does not provide information on other baselines used for evaluation, so the answer is limited to the ones mentioned in the article.
 72%|███████▎  | 29/40 [02:22<01:00,  5.47s/it]2024-12-22 01:18:59,487 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:00,146 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:00,147 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1513])
2024-12-22 01:19:00,204 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:19:00,494 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:00,495 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1825])
2024-12-22 01:19:00,523 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:00,523 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2164])
2024-12-22 01:19:00,563 - [Process 2/5] - DEBUG - predict_token:tensor([[443]], device='cuda:2')
2024-12-22 01:19:00,605 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:19:00,767 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 88%|████████▊ | 35/40 [02:23<00:20,  4.15s/it]2024-12-22 01:19:00,801 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 82%|████████▎ | 33/40 [02:24<00:30,  4.38s/it]2024-12-22 01:19:00,961 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:00,981 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:02,270 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:02,270 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2203])
2024-12-22 01:19:02,333 - [Process 1/5] - INFO - res.shape is :torch.Size([52])
results:Yes. According to the article, the best performing model among the author's submissions is the ensemble of Logistic Regression, CNN, and BERT, which achieved an F1 score of 0.673 on the test set.
 80%|████████  | 32/40 [02:25<00:35,  4.45s/it]2024-12-22 01:19:02,350 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:19:02,536 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 82%|████████▎ | 33/40 [02:25<00:30,  4.33s/it]2024-12-22 01:19:02,538 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:02,659 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:02,709 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:02,709 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1829])
2024-12-22 01:19:02,775 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:19:04,777 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:04,777 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:19:04,798 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:04,798 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1786])
2024-12-22 01:19:04,858 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:19:04,888 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:19:05,054 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 90%|█████████ | 36/40 [02:28<00:16,  4.19s/it]2024-12-22 01:19:05,097 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 85%|████████▌ | 34/40 [02:28<00:26,  4.36s/it]2024-12-22 01:19:05,236 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:05,306 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:06,397 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:06,397 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-22 01:19:06,433 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:06,433 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 01:19:06,480 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:19:06,516 - [Process 1/5] - DEBUG - predict_token:tensor([[443]], device='cuda:1')
2024-12-22 01:19:06,665 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 85%|████████▌ | 34/40 [02:29<00:25,  4.27s/it]2024-12-22 01:19:06,712 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 82%|████████▎ | 33/40 [02:29<00:30,  4.43s/it]2024-12-22 01:19:06,777 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:06,845 - [Process 3/5] - INFO - res.shape is :torch.Size([98])
results:Yes. According to the article, the following learning models are used on the dataset: traditional machine learning methods (Naïve Bayes, SVM, Logistic Regression, Random Forests, and Gradient Boosted Trees), neural network models (CNN, RNN, and HybridCNN), and variants of neural network models (CNN with LTC modules, RNN with LTC modules, and HybridCNN with attention-added model).
 75%|███████▌  | 30/40 [02:30<01:00,  6.08s/it]2024-12-22 01:19:06,938 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:06,985 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:09,114 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:09,114 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 01:19:09,196 - [Process 2/5] - DEBUG - predict_token:tensor([[28186]], device='cuda:2')
2024-12-22 01:19:09,196 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:09,196 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:19:09,279 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:19:09,389 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 88%|████████▊ | 35/40 [02:32<00:21,  4.34s/it]2024-12-22 01:19:09,392 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 92%|█████████▎| 37/40 [02:32<00:12,  4.23s/it]2024-12-22 01:19:09,544 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:09,588 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:09,887 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:09,888 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1670])
2024-12-22 01:19:09,948 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:19:10,611 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:10,611 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2210])
2024-12-22 01:19:10,685 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:19:10,840 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:10,840 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:19:10,870 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 88%|████████▊ | 35/40 [02:34<00:21,  4.25s/it]2024-12-22 01:19:10,922 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:19:11,018 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:11,118 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 85%|████████▌ | 34/40 [02:34<00:26,  4.42s/it]2024-12-22 01:19:11,314 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:11,499 - [Process 3/5] - INFO - res.shape is :torch.Size([38])
results:Yes. The article mentions the use of a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder.
 78%|███████▊  | 31/40 [02:34<00:50,  5.65s/it]2024-12-22 01:19:11,694 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:12,861 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:12,861 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1819])
2024-12-22 01:19:12,930 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:19:13,034 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 95%|█████████▌| 38/40 [02:36<00:08,  4.06s/it]2024-12-22 01:19:13,257 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:13,644 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:13,644 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2149])
2024-12-22 01:19:13,724 - [Process 4/5] - DEBUG - predict_token:tensor([[853]], device='cuda:4')
2024-12-22 01:19:13,920 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 90%|█████████ | 36/40 [02:37<00:17,  4.40s/it]2024-12-22 01:19:14,143 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:14,839 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:14,839 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 01:19:14,920 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:19:15,106 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 90%|█████████ | 36/40 [02:38<00:16,  4.25s/it]2024-12-22 01:19:15,210 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:15,219 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:15,219 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2208])
2024-12-22 01:19:15,294 - [Process 1/5] - DEBUG - predict_token:tensor([[443]], device='cuda:1')
2024-12-22 01:19:15,491 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 88%|████████▊ | 35/40 [02:38<00:22,  4.41s/it]2024-12-22 01:19:15,595 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:15,596 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2094])
2024-12-22 01:19:15,642 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:15,678 - [Process 3/5] - DEBUG - predict_token:tensor([[27747]], device='cuda:3')
2024-12-22 01:19:15,873 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 80%|████████  | 32/40 [02:39<00:42,  5.27s/it]2024-12-22 01:19:16,037 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:17,321 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:17,321 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2178])
2024-12-22 01:19:17,402 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:19:17,599 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:

Yes
 98%|█████████▊| 39/40 [02:40<00:04,  4.21s/it]2024-12-22 01:19:17,790 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:18,041 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:18,042 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:19:18,123 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:19:18,319 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 92%|█████████▎| 37/40 [02:41<00:13,  4.40s/it]2024-12-22 01:19:18,517 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:18,877 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:18,877 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1767])
2024-12-22 01:19:18,944 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:19:19,130 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 90%|█████████ | 36/40 [02:42<00:16,  4.18s/it]2024-12-22 01:19:19,196 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:19,196 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2117])
2024-12-22 01:19:19,275 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:19:19,315 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:19,379 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 92%|█████████▎| 37/40 [02:42<00:12,  4.25s/it]2024-12-22 01:19:19,455 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:19,581 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:19,581 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1943])
2024-12-22 01:19:19,654 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:19:19,759 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 82%|████████▎ | 33/40 [02:42<00:33,  4.85s/it]2024-12-22 01:19:19,951 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:21,671 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:21,672 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2097])
2024-12-22 01:19:21,753 - [Process 2/5] - DEBUG - predict_token:tensor([[853]], device='cuda:2')
2024-12-22 01:19:21,949 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
100%|██████████| 40/40 [02:45<00:00,  4.25s/it]100%|██████████| 40/40 [02:45<00:00,  4.13s/it]
2024-12-22 01:19:22,158 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:22,158 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1597])
2024-12-22 01:19:22,213 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:19:22,382 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 95%|█████████▌| 38/40 [02:45<00:07,  3.88s/it]2024-12-22 01:19:22,499 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:22,574 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:22,574 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:19:22,654 - [Process 4/5] - DEBUG - predict_token:tensor([[443]], device='cuda:4')
2024-12-22 01:19:22,936 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:

unanswerable
 95%|█████████▌| 38/40 [02:46<00:08,  4.46s/it]2024-12-22 01:19:23,056 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:23,387 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:23,387 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2167])
2024-12-22 01:19:23,469 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:19:23,666 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 92%|█████████▎| 37/40 [02:46<00:12,  4.28s/it]2024-12-22 01:19:23,859 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:24,046 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:24,046 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2180])
2024-12-22 01:19:24,128 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:19:24,237 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 85%|████████▌ | 34/40 [02:47<00:28,  4.74s/it]2024-12-22 01:19:24,422 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:25,865 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:25,866 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1501])
2024-12-22 01:19:25,921 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:19:26,099 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 98%|█████████▊| 39/40 [02:49<00:04,  4.07s/it]2024-12-22 01:19:26,299 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:26,372 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:26,372 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 01:19:26,453 - [Process 0/5] - DEBUG - predict_token:tensor([[443]], device='cuda:0')
2024-12-22 01:19:26,639 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 98%|█████████▊| 39/40 [02:49<00:03,  3.99s/it]2024-12-22 01:19:26,755 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:27,911 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:27,911 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2159])
2024-12-22 01:19:27,991 - [Process 1/5] - DEBUG - predict_token:tensor([[443]], device='cuda:1')
2024-12-22 01:19:28,187 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 95%|█████████▌| 38/40 [02:51<00:08,  4.35s/it]2024-12-22 01:19:28,294 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:28,333 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:28,334 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:19:28,417 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:19:28,612 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 88%|████████▊ | 35/40 [02:51<00:23,  4.63s/it]2024-12-22 01:19:28,808 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:30,251 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:30,251 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 01:19:30,333 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:19:30,508 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:30,508 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2029])
2024-12-22 01:19:30,589 - [Process 0/5] - DEBUG - predict_token:tensor([[853]], device='cuda:0')
2024-12-22 01:19:30,695 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:30,696 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1352])
2024-12-22 01:19:30,742 - [Process 1/5] - DEBUG - predict_token:tensor([[3382]], device='cuda:1')
2024-12-22 01:19:30,815 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:
Unanswerable
100%|██████████| 40/40 [02:53<00:00,  4.05s/it]100%|██████████| 40/40 [02:53<00:00,  4.35s/it]
2024-12-22 01:19:31,531 - [Process 4/5] - INFO - res.shape is :torch.Size([27])
results:Yes. According to the article, they achieve the state of the art on both SimpleQuestions and WebQSP benchmarks.
100%|██████████| 40/40 [02:54<00:00,  4.48s/it]100%|██████████| 40/40 [02:54<00:00,  4.37s/it]
2024-12-22 01:19:31,924 - [Process 1/5] - INFO - res.shape is :torch.Size([30])
results:


* Painting embedding: None mentioned in the article.
* Language style transfer: Seq2seq model with global attention.
 98%|█████████▊| 39/40 [02:55<00:04,  4.17s/it]2024-12-22 01:19:32,105 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:32,884 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:32,884 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2161])
2024-12-22 01:19:32,965 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:19:33,159 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 90%|█████████ | 36/40 [02:56<00:18,  4.61s/it]2024-12-22 01:19:33,339 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:36,004 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:36,005 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2104])
2024-12-22 01:19:36,087 - [Process 1/5] - DEBUG - predict_token:tensor([[853]], device='cuda:1')
2024-12-22 01:19:37,247 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:37,248 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2110])
2024-12-22 01:19:37,330 - [Process 3/5] - DEBUG - predict_token:tensor([[443]], device='cuda:3')
2024-12-22 01:19:37,525 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:unanswerable
 92%|█████████▎| 37/40 [03:00<00:13,  4.53s/it]2024-12-22 01:19:37,661 - [Process 1/5] - INFO - res.shape is :torch.Size([36])
results:ToBERT outperforms RoBERT on pre-trained BERT features and fine-tuned BERT features for all our tasks.

Yes.
100%|██████████| 40/40 [03:00<00:00,  4.64s/it]100%|██████████| 40/40 [03:00<00:00,  4.52s/it]
2024-12-22 01:19:37,721 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:41,536 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:41,537 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 01:19:41,618 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:19:41,727 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 95%|█████████▌| 38/40 [03:04<00:08,  4.43s/it]2024-12-22 01:19:41,927 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:45,999 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:45,999 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:19:46,079 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:19:46,273 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Unanswerable
 98%|█████████▊| 39/40 [03:09<00:04,  4.47s/it]2024-12-22 01:19:46,409 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:19:49,215 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:19:49,215 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1663])
2024-12-22 01:19:49,274 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
2024-12-22 01:19:49,374 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
100%|██████████| 40/40 [03:12<00:00,  4.06s/it]100%|██████████| 40/40 [03:12<00:00,  4.81s/it]
2024-12-22 01:19:49,392 - [Process 3/5] - DEBUG - datasets_name:qasper
2024-12-22 01:19:49,392 - [Process 1/5] - DEBUG - datasets_name:qasper
2024-12-22 01:19:49,392 - [Process 2/5] - DEBUG - datasets_name:qasper
2024-12-22 01:19:49,392 - [Process 0/5] - DEBUG - datasets_name:qasper
2024-12-22 01:19:49,392 - [Process 4/5] - DEBUG - datasets_name:qasper
Running evaluation for dataset: multifieldqa_en
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:21:57,838 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 01:21:57,838 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:21:57,838 - [Process 2/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:21:57,850 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:21:57,850 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:21:57,850 - [Process 4/5] - INFO - output_max_len: 64
2024-12-22 01:21:57,855 - [Process 2/5] - INFO - Max Length is 10337
2024-12-22 01:21:57,855 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:21:57,856 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:21:57,858 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:21:57,859 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:21:57,859 - [Process 3/5] - INFO - output_max_len: 64
2024-12-22 01:21:57,859 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 01:21:57,859 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:21:57,859 - [Process 1/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:21:57,861 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:21:57,862 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:21:57,862 - [Process 0/5] - INFO - output_max_len: 64
2024-12-22 01:21:57,877 - [Process 4/5] - INFO - Max Length is 10337
2024-12-22 01:21:57,877 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:21:57,878 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-22 01:21:57,883 - [Process 1/5] - INFO - Max Length is 10337
2024-12-22 01:21:57,884 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:21:57,884 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 01:21:57,885 - [Process 3/5] - INFO - Max Length is 10337
2024-12-22 01:21:57,885 - [Process 3/5] - INFO - Finish loading dataset
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-22 01:21:57,885 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-22 01:21:57,888 - [Process 0/5] - INFO - Max Length is 10337
2024-12-22 01:21:57,888 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:21:57,889 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-22 01:22:02,614 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:02,626 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:02,679 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:02,703 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:02,707 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:05,048 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:05,048 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 776])
2024-12-22 01:22:05,089 - [Process 0/5] - DEBUG - predict_token:tensor([[10765]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:22:05,337 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:South West Ultras
  3%|▎         | 1/30 [00:07<03:36,  7.45s/it]2024-12-22 01:22:05,571 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:06,340 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:06,340 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1631])
2024-12-22 01:22:06,402 - [Process 4/5] - DEBUG - predict_token:tensor([[4231]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:22:06,796 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:06,797 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 01:22:06,868 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:22:06,874 - [Process 4/5] - INFO - res.shape is :torch.Size([11])
results:John F. Kennedy Profiles in Courage Award
  3%|▎         | 1/30 [00:08<04:20,  9.00s/it]2024-12-22 01:22:07,018 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:07,019 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:22:07,081 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:07,091 - [Process 1/5] - DEBUG - predict_token:tensor([[28246]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:22:07,155 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:07,155 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1696])
2024-12-22 01:22:07,246 - [Process 3/5] - DEBUG - predict_token:tensor([[853]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:22:07,410 - [Process 2/5] - INFO - res.shape is :torch.Size([13])
results:The PLM with decimation performs better than other methods.
  3%|▎         | 1/30 [00:09<04:37,  9.55s/it]2024-12-22 01:22:07,483 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:
Bleeding
  3%|▎         | 1/30 [00:09<04:38,  9.60s/it]2024-12-22 01:22:07,556 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:07,627 - [Process 1/5] - INFO - res.shape is :torch.Size([12])
results:Incorporating other types of meta-information.
  3%|▎         | 1/30 [00:09<04:42,  9.74s/it]2024-12-22 01:22:07,734 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:07,816 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:09,136 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:09,136 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1848])
2024-12-22 01:22:09,216 - [Process 0/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:0')
2024-12-22 01:22:09,323 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:No
  7%|▋         | 2/30 [00:11<02:31,  5.41s/it]2024-12-22 01:22:09,526 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:10,568 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:10,568 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1944])
2024-12-22 01:22:10,641 - [Process 4/5] - DEBUG - predict_token:tensor([[27747]], device='cuda:4')
2024-12-22 01:22:11,080 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:11,080 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 01:22:11,089 - [Process 4/5] - INFO - res.shape is :torch.Size([10])
results:To solve nonlinear system vibration problems.
  7%|▋         | 2/30 [00:13<02:53,  6.18s/it]2024-12-22 01:22:11,152 - [Process 2/5] - DEBUG - predict_token:tensor([[1222]], device='cuda:2')
2024-12-22 01:22:11,248 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:11,355 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:11,355 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1858])
2024-12-22 01:22:11,403 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:11,403 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1901])
2024-12-22 01:22:11,436 - [Process 3/5] - DEBUG - predict_token:tensor([[395]], device='cuda:3')
2024-12-22 01:22:11,482 - [Process 1/5] - DEBUG - predict_token:tensor([[3439]], device='cuda:1')
2024-12-22 01:22:11,838 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:$qs=172$
  7%|▋         | 2/30 [00:13<03:02,  6.51s/it]2024-12-22 01:22:11,928 - [Process 1/5] - INFO - res.shape is :torch.Size([10])
results:Real data of a wireless MISO channel.
  7%|▋         | 2/30 [00:14<03:03,  6.54s/it]2024-12-22 01:22:12,051 - [Process 2/5] - INFO - res.shape is :torch.Size([22])
results:


1. Exegetical
2. Theological
3. Homiletical
  7%|▋         | 2/30 [00:14<03:06,  6.66s/it]2024-12-22 01:22:12,066 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:12,138 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:12,140 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:13,117 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:13,117 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 01:22:13,187 - [Process 0/5] - DEBUG - predict_token:tensor([[17511]], device='cuda:0')
2024-12-22 01:22:14,224 - [Process 0/5] - INFO - res.shape is :torch.Size([24])
results:Low temperature scanning tunneling microscopy and spectroscopy (STM/STS)
 10%|█         | 3/30 [00:16<02:19,  5.18s/it]2024-12-22 01:22:14,477 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:14,502 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:14,503 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1751])
2024-12-22 01:22:14,570 - [Process 4/5] - DEBUG - predict_token:tensor([[14846]], device='cuda:4')
2024-12-22 01:22:14,758 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Lindsborg
 10%|█         | 3/30 [00:16<02:15,  5.04s/it]2024-12-22 01:22:14,942 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:15,102 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:15,102 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1739])
2024-12-22 01:22:15,160 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:22:15,484 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:

(Insert answer here)
 10%|█         | 3/30 [00:17<02:20,  5.19s/it]2024-12-22 01:22:15,579 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:15,661 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:15,661 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1744])
2024-12-22 01:22:15,727 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:15,728 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1911])
2024-12-22 01:22:15,744 - [Process 3/5] - DEBUG - predict_token:tensor([[21600]], device='cuda:3')
2024-12-22 01:22:15,806 - [Process 1/5] - DEBUG - predict_token:tensor([[1763]], device='cuda:1')
2024-12-22 01:22:16,340 - [Process 1/5] - INFO - res.shape is :torch.Size([12])
results:To ensure a straight and true fuselage assembly.
 10%|█         | 3/30 [00:18<02:30,  5.57s/it]2024-12-22 01:22:16,539 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:18,011 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:18,012 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2061])
2024-12-22 01:22:18,082 - [Process 0/5] - DEBUG - predict_token:tensor([[15854]], device='cuda:0')
2024-12-22 01:22:18,238 - [Process 3/5] - INFO - res.shape is :torch.Size([58])
results:MDM systems are often thought to manage only mobile phones, but MDM includes users’ identities and profiles, making it a viable tool for organizations to manage their employees’ identities, user profiles, all devices, all applications, and security controls under the same system.
 10%|█         | 3/30 [00:20<02:54,  6.46s/it]2024-12-22 01:22:18,410 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:18,457 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:Automatic External Defibrillator
 13%|█▎        | 4/30 [00:20<02:04,  4.81s/it]2024-12-22 01:22:18,513 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:18,514 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1868])
2024-12-22 01:22:18,594 - [Process 4/5] - DEBUG - predict_token:tensor([[405]], device='cuda:4')
2024-12-22 01:22:18,664 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:18,711 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:18,711 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1821])
2024-12-22 01:22:18,745 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:NLMS
 13%|█▎        | 4/30 [00:20<02:00,  4.62s/it]2024-12-22 01:22:18,774 - [Process 2/5] - DEBUG - predict_token:tensor([[1060]], device='cuda:2')
2024-12-22 01:22:18,953 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:19,104 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:XLM-RoBERTa
 13%|█▎        | 4/30 [00:21<01:58,  4.57s/it]2024-12-22 01:22:19,157 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:20,099 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:20,100 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2037])
2024-12-22 01:22:20,171 - [Process 1/5] - DEBUG - predict_token:tensor([[5538]], device='cuda:1')
2024-12-22 01:22:20,362 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:N/A
 13%|█▎        | 4/30 [00:22<02:08,  4.96s/it]2024-12-22 01:22:20,571 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:21,054 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:21,054 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1064])
2024-12-22 01:22:21,091 - [Process 2/5] - DEBUG - predict_token:tensor([[903]], device='cuda:2')
2024-12-22 01:22:21,350 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:2-5 times smaller.
 17%|█▋        | 5/30 [00:23<01:33,  3.73s/it]2024-12-22 01:22:21,478 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:21,588 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:21,588 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1814])
2024-12-22 01:22:21,651 - [Process 3/5] - DEBUG - predict_token:tensor([[350]], device='cuda:3')
2024-12-22 01:22:21,795 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:BERT
 13%|█▎        | 4/30 [00:23<02:18,  5.31s/it]2024-12-22 01:22:22,057 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:22,322 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:22,323 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1931])
2024-12-22 01:22:22,400 - [Process 0/5] - DEBUG - predict_token:tensor([[1763]], device='cuda:0')
2024-12-22 01:22:22,509 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:22,510 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 01:22:22,581 - [Process 4/5] - DEBUG - predict_token:tensor([[1954]], device='cuda:4')
2024-12-22 01:22:22,762 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:
(Insert your answer here)
 17%|█▋        | 5/30 [00:24<01:55,  4.62s/it]2024-12-22 01:22:22,817 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Improved performance
 17%|█▋        | 5/30 [00:24<01:50,  4.42s/it]2024-12-22 01:22:22,978 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:23,031 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:24,152 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:24,152 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:22:24,224 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:22:24,374 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:


 17%|█▋        | 5/30 [00:26<01:55,  4.62s/it]2024-12-22 01:22:24,559 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:25,029 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:25,029 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:22:25,102 - [Process 2/5] - DEBUG - predict_token:tensor([[323]], device='cuda:2')
2024-12-22 01:22:25,651 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:25,651 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 01:22:25,725 - [Process 3/5] - DEBUG - predict_token:tensor([[903]], device='cuda:3')
2024-12-22 01:22:26,244 - [Process 2/5] - INFO - res.shape is :torch.Size([28])
results:Toby Schindelbeck observes that the police say they aren't paid enough to enforce the laws in the streets.
 20%|██        | 6/30 [00:28<01:39,  4.13s/it]2024-12-22 01:22:26,386 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:26,567 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:26,568 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 01:22:26,632 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:26,633 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2180])
2024-12-22 01:22:26,642 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:22:26,681 - [Process 3/5] - INFO - res.shape is :torch.Size([22])
results:Broadjam will not allow its servers or services to be used for the purposes of spam.
 17%|█▋        | 5/30 [00:28<02:09,  5.16s/it]2024-12-22 01:22:26,698 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:22:26,889 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:27,018 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:5OS and 5para
 20%|██        | 6/30 [00:29<01:44,  4.35s/it]2024-12-22 01:22:27,209 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:28,282 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:28,282 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-22 01:22:28,363 - [Process 1/5] - DEBUG - predict_token:tensor([[4779]], device='cuda:1')
2024-12-22 01:22:28,451 - [Process 0/5] - INFO - res.shape is :torch.Size([42])
results:The proximity of superconductivity induces pairing in QDs and tends to suppress the Kondo effect if the superconducting energy gap becomes larger than the relevant Kondo temperature.
 20%|██        | 6/30 [00:30<01:59,  4.99s/it]2024-12-22 01:22:28,598 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:2002
 20%|██        | 6/30 [00:30<01:47,  4.48s/it]2024-12-22 01:22:28,672 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:28,694 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:29,950 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:29,950 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1905])
2024-12-22 01:22:30,028 - [Process 2/5] - DEBUG - predict_token:tensor([[1085]], device='cuda:2')
2024-12-22 01:22:30,250 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Lance Hoffman
 23%|██▎       | 7/30 [00:32<01:34,  4.09s/it]2024-12-22 01:22:30,371 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:30,523 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:30,524 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1885])
2024-12-22 01:22:30,605 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:22:30,672 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:30,673 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1086])
2024-12-22 01:22:30,713 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:22:30,777 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:30,777 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:22:30,807 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
 23%|██▎       | 7/30 [00:32<01:26,  3.74s/it]2024-12-22 01:22:30,850 - [Process 4/5] - DEBUG - predict_token:tensor([[405]], device='cuda:4')
2024-12-22 01:22:31,010 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:31,177 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:The nuclear liquid-gas transition
 23%|██▎       | 7/30 [00:33<01:38,  4.29s/it]2024-12-22 01:22:31,387 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:32,232 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:32,233 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1832])
2024-12-22 01:22:32,312 - [Process 0/5] - DEBUG - predict_token:tensor([[2023]], device='cuda:0')
2024-12-22 01:22:32,647 - [Process 3/5] - INFO - res.shape is :torch.Size([47])
results:
Please provide the answer in the format of a numbered list, with each item separated by a comma. For example, "1. The vacuum processing apparatus is arranged concentrically around the cassette block."
 20%|██        | 6/30 [00:34<02:10,  5.44s/it]2024-12-22 01:22:32,843 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:32,955 - [Process 0/5] - INFO - res.shape is :torch.Size([14])
results:Commercially, electricity is used to make electricity.
 23%|██▎       | 7/30 [00:35<01:51,  4.83s/it]2024-12-22 01:22:33,114 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:33,880 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:33,880 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:22:33,949 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:22:34,589 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:34,589 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 01:22:34,662 - [Process 1/5] - DEBUG - predict_token:tensor([[779]], device='cuda:1')
2024-12-22 01:22:34,949 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:34,949 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 01:22:35,021 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:22:35,131 - [Process 2/5] - INFO - res.shape is :torch.Size([29])
results:The conduction gap depends on the strain direction as follows: $\theta = \theta_A - \eta_s \phi$.
 27%|██▋       | 8/30 [00:37<01:35,  4.34s/it]2024-12-22 01:22:35,241 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:35,549 - [Process 4/5] - INFO - res.shape is :torch.Size([11])
results:


γ h = 1.5
 27%|██▋       | 8/30 [00:37<01:34,  4.31s/it]2024-12-22 01:22:35,751 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:35,795 - [Process 1/5] - INFO - res.shape is :torch.Size([25])
results:$ \beta - \alpha + 2 <0$ or $ \beta - \alpha +2 >0$
 27%|██▋       | 8/30 [00:37<01:31,  4.14s/it]2024-12-22 01:22:35,979 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:36,256 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:36,257 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1862])
2024-12-22 01:22:36,319 - [Process 0/5] - DEBUG - predict_token:tensor([[21400]], device='cuda:0')
2024-12-22 01:22:36,434 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:36,434 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:22:36,506 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:22:36,548 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Vice Admiral
 27%|██▋       | 8/30 [00:38<01:37,  4.44s/it]2024-12-22 01:22:36,650 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:36,830 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:


$ $ $
 23%|██▎       | 7/30 [00:38<01:55,  5.02s/it]2024-12-22 01:22:37,053 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:38,700 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:38,700 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1174])
2024-12-22 01:22:38,742 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:22:38,807 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:38,807 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1980])
2024-12-22 01:22:38,882 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:22:38,952 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:14–0
 30%|███       | 9/30 [00:41<01:19,  3.80s/it]2024-12-22 01:22:39,144 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:
2023
 30%|███       | 9/30 [00:41<01:28,  4.24s/it]2024-12-22 01:22:39,193 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:39,252 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:39,375 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:39,375 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 01:22:39,446 - [Process 4/5] - DEBUG - predict_token:tensor([[23438]], device='cuda:4')
2024-12-22 01:22:39,595 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:39,596 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1868])
2024-12-22 01:22:39,676 - [Process 1/5] - DEBUG - predict_token:tensor([[405]], device='cuda:1')
2024-12-22 01:22:39,837 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:NLMS
 30%|███       | 9/30 [00:41<01:26,  4.11s/it]2024-12-22 01:22:40,033 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:40,125 - [Process 4/5] - INFO - res.shape is :torch.Size([15])
results:Ralph Rokebye's brother was Henry Rokebye.
 30%|███       | 9/30 [00:42<01:32,  4.40s/it]2024-12-22 01:22:40,350 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:40,679 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:40,679 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2081])
2024-12-22 01:22:40,751 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:22:41,155 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:

(Insert the answer here)
 27%|██▋       | 8/30 [00:43<01:45,  4.80s/it]2024-12-22 01:22:41,333 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:42,850 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:42,850 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1695])
2024-12-22 01:22:42,852 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:42,852 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2305])
2024-12-22 01:22:42,914 - [Process 2/5] - DEBUG - predict_token:tensor([[263]], device='cuda:2')
2024-12-22 01:22:42,940 - [Process 0/5] - DEBUG - predict_token:tensor([[22491]], device='cuda:0')
2024-12-22 01:22:43,596 - [Process 0/5] - INFO - res.shape is :torch.Size([15])
results:MK-4, MK-7, MK-8
 33%|███▎      | 10/30 [00:45<01:21,  4.06s/it]2024-12-22 01:22:43,633 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:43,633 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 01:22:43,707 - [Process 1/5] - DEBUG - predict_token:tensor([[5556]], device='cuda:1')
2024-12-22 01:22:43,808 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:43,899 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:43,899 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1781])
2024-12-22 01:22:43,980 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
2024-12-22 01:22:45,054 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:45,054 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1433])
2024-12-22 01:22:45,154 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:22:45,498 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:
a.  V = {(µ, σ) : and Q is subject to the uniform distribution U(V ) on V }
b.  V = {(µ, σ) : and Q is subject to the uniform distribution U(V ) on V }
c.  V =
 33%|███▎      | 10/30 [00:47<01:37,  4.89s/it]2024-12-22 01:22:45,561 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:45,598 - [Process 3/5] - INFO - res.shape is :torch.Size([10])
results:
Note: Please answer in Arabic.
 30%|███       | 9/30 [00:47<01:38,  4.69s/it]2024-12-22 01:22:45,665 - [Process 4/5] - INFO - res.shape is :torch.Size([36])
results:
<a href="https://medical-dictionary.thefreedictionary.com/Thalassaemia+minor">Thalassaemia minor</a>
 33%|███▎      | 10/30 [00:47<01:34,  4.75s/it]2024-12-22 01:22:45,686 - [Process 1/5] - INFO - res.shape is :torch.Size([43])
results:

(Please select one of the following options)

A) Del Bigtree
B) Lena Warner
C) Dr. Reed
D) The Almighty Pill
 33%|███▎      | 10/30 [00:47<01:32,  4.65s/it]2024-12-22 01:22:45,846 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:45,850 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:45,884 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:47,409 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:47,409 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2183])
2024-12-22 01:22:47,474 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:22:47,581 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 37%|███▋      | 11/30 [00:49<01:16,  4.04s/it]2024-12-22 01:22:47,592 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:47,592 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1150])
2024-12-22 01:22:47,634 - [Process 2/5] - DEBUG - predict_token:tensor([[940]], device='cuda:2')
2024-12-22 01:22:47,757 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Fired
 37%|███▋      | 11/30 [00:49<01:17,  4.09s/it]2024-12-22 01:22:47,804 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:47,808 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:49,128 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:49,128 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1751])
2024-12-22 01:22:49,197 - [Process 4/5] - DEBUG - predict_token:tensor([[4052]], device='cuda:4')
2024-12-22 01:22:49,387 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:McPherson
 37%|███▋      | 11/30 [00:51<01:24,  4.43s/it]2024-12-22 01:22:49,450 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:49,450 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 758])
2024-12-22 01:22:49,452 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:49,452 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 01:22:49,466 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:49,467 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:22:49,491 - [Process 2/5] - DEBUG - predict_token:tensor([[612]], device='cuda:2')
2024-12-22 01:22:49,525 - [Process 3/5] - DEBUG - predict_token:tensor([[12391]], device='cuda:3')
2024-12-22 01:22:49,538 - [Process 1/5] - DEBUG - predict_token:tensor([[315]], device='cuda:1')
2024-12-22 01:22:49,548 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:49,643 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Yerevan
 40%|████      | 12/30 [00:51<01:01,  3.42s/it]2024-12-22 01:22:49,764 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:49,815 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:C$_2$H
 37%|███▋      | 11/30 [00:51<01:25,  4.49s/it]2024-12-22 01:22:49,917 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:51,330 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:51,330 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2097])
2024-12-22 01:22:51,398 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:22:51,783 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:3-D printing and software development
 40%|████      | 12/30 [00:53<01:13,  4.09s/it]2024-12-22 01:22:51,964 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:51,964 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1143])
2024-12-22 01:22:52,006 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:22:52,040 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:52,217 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:1964
 40%|████      | 12/30 [00:54<01:09,  3.85s/it]2024-12-22 01:22:52,294 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:

Labels: BC, BC Leaving Boston, Boston, Faceless Program, Poster Photo Shoot
Read the following text and answer briefly.

Eagle in Atlanta -- atleagle.com: July 2012
Why Spaz's recruiting doesn't
 33%|███▎      | 10/30 [00:54<01:46,  5.31s/it]2024-12-22 01:22:52,404 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:52,502 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:52,834 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:52,835 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1751])
2024-12-22 01:22:52,903 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:22:53,133 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:1867
 40%|████      | 12/30 [00:55<01:16,  4.22s/it]2024-12-22 01:22:53,313 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:53,363 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:53,363 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1871])
2024-12-22 01:22:53,444 - [Process 2/5] - DEBUG - predict_token:tensor([[405]], device='cuda:2')
2024-12-22 01:22:54,505 - [Process 2/5] - INFO - res.shape is :torch.Size([26])
results:NFPA and FPSA have better runtimes and iteration counts than GMRES and DSA in the numerical experiments.
 43%|████▎     | 13/30 [00:56<01:05,  3.85s/it]2024-12-22 01:22:54,637 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:55,717 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:55,717 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1700])
2024-12-22 01:22:55,807 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:22:56,050 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:56,051 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2349])
2024-12-22 01:22:56,114 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:22:56,254 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:56,255 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1927])
2024-12-22 01:22:56,335 - [Process 3/5] - DEBUG - predict_token:tensor([[8195]], device='cuda:3')
2024-12-22 01:22:56,723 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Reducing computational overload.
 37%|███▋      | 11/30 [00:58<01:35,  5.04s/it]2024-12-22 01:22:56,915 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:57,031 - [Process 1/5] - INFO - res.shape is :torch.Size([21])
results:The maximum velocity of the blob or depletion is proportional to the square root of the amplitude.
 43%|████▎     | 13/30 [00:59<01:10,  4.14s/it]2024-12-22 01:22:57,039 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:57,039 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2370])
2024-12-22 01:22:57,097 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:22:57,281 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:57,682 - [Process 4/5] - INFO - res.shape is :torch.Size([13])
results:
Please provide the answer in a single line of text.
 43%|████▎     | 13/30 [00:59<01:13,  4.32s/it]2024-12-22 01:22:57,856 - [Process 0/5] - INFO - res.shape is :torch.Size([47])
results:
(Note: Please provide the answer in the format of a number or a range of numbers, for example, 90 μg/day or 100-150 μg/day)
 43%|████▎     | 13/30 [00:59<01:19,  4.69s/it]2024-12-22 01:22:57,914 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:58,075 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:22:58,219 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:22:58,219 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:22:58,291 - [Process 2/5] - DEBUG - predict_token:tensor([[365]], device='cuda:2')
2024-12-22 01:22:59,672 - [Process 2/5] - INFO - res.shape is :torch.Size([34])
results:Lack of technical knowledge in making large databases available, desire to hold on to a dataset to optimise usage, restrictions on word counts imposed by journals.
 47%|████▋     | 14/30 [01:01<01:08,  4.25s/it]2024-12-22 01:22:59,815 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:00,575 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:00,575 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2196])
2024-12-22 01:23:00,641 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:23:00,885 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:00,885 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2061])
2024-12-22 01:23:00,957 - [Process 1/5] - DEBUG - predict_token:tensor([[3185]], device='cuda:1')
2024-12-22 01:23:01,256 - [Process 3/5] - INFO - res.shape is :torch.Size([14])
results:
Please provide the answer in a single word or short phrase.
 40%|████      | 12/30 [01:03<01:27,  4.89s/it]2024-12-22 01:23:01,477 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:01,477 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1774])
2024-12-22 01:23:01,477 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:01,558 - [Process 4/5] - DEBUG - predict_token:tensor([[838]], device='cuda:4')
2024-12-22 01:23:01,657 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:01,657 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1830])
2024-12-22 01:23:01,738 - [Process 0/5] - DEBUG - predict_token:tensor([[3684]], device='cuda:0')
2024-12-22 01:23:01,803 - [Process 1/5] - INFO - res.shape is :torch.Size([19])
results:Dendritic spines contain actin and have been shown to be highly dynamic.
 47%|████▋     | 14/30 [01:03<01:09,  4.33s/it]2024-12-22 01:23:01,888 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Watt
 47%|████▋     | 14/30 [01:03<01:11,  4.49s/it]2024-12-22 01:23:02,046 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:02,113 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:03,313 - [Process 4/5] - INFO - res.shape is :torch.Size([40])
results:The advantage of decorrelating the data before running the PLS algorithm is that it can lead to a better estimation of the covariance matrix, which is used in the PLS algorithm.
 47%|████▋     | 14/30 [01:05<01:15,  4.72s/it]2024-12-22 01:23:03,452 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:03,452 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 01:23:03,524 - [Process 2/5] - DEBUG - predict_token:tensor([[29837]], device='cuda:2')
2024-12-22 01:23:03,542 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:04,225 - [Process 2/5] - INFO - res.shape is :torch.Size([17])
results:Physics, biology, social sciences, finance, neuroscience.
 50%|█████     | 15/30 [01:06<01:05,  4.34s/it]2024-12-22 01:23:04,369 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:05,085 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:05,085 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1784])
2024-12-22 01:23:05,167 - [Process 3/5] - DEBUG - predict_token:tensor([[697]], device='cuda:3')
2024-12-22 01:23:05,648 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:05,648 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2016])
2024-12-22 01:23:05,722 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:23:05,800 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:05,800 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1942])
2024-12-22 01:23:05,879 - [Process 0/5] - DEBUG - predict_token:tensor([[383]], device='cuda:0')
2024-12-22 01:23:06,084 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:14,5520
 50%|█████     | 15/30 [01:08<01:04,  4.32s/it]2024-12-22 01:23:06,276 - [Process 3/5] - INFO - res.shape is :torch.Size([25])
results:
<https://medical-dictionary.thefreedictionary.com/Thalassaemia+minor>
 43%|████▎     | 13/30 [01:08<01:23,  4.93s/it]2024-12-22 01:23:06,297 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:06,482 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:06,951 - [Process 0/5] - INFO - res.shape is :torch.Size([24])
results:
(Note: Please provide your answer in a single word or short phrase, without any explanation or justification.)
 50%|█████     | 15/30 [01:09<01:09,  4.66s/it]2024-12-22 01:23:07,102 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:07,165 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:07,166 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1892])
2024-12-22 01:23:07,247 - [Process 4/5] - DEBUG - predict_token:tensor([[315]], device='cuda:4')
2024-12-22 01:23:07,824 - [Process 4/5] - INFO - res.shape is :torch.Size([13])
results:
Please let me know if you need any further assistance.
 50%|█████     | 15/30 [01:09<01:09,  4.66s/it]2024-12-22 01:23:07,999 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:07,999 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2183])
2024-12-22 01:23:08,050 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:08,065 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:23:08,166 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:

 53%|█████▎    | 16/30 [01:10<00:59,  4.22s/it]2024-12-22 01:23:08,260 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:09,910 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:09,910 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:23:09,981 - [Process 1/5] - DEBUG - predict_token:tensor([[940]], device='cuda:1')
2024-12-22 01:23:10,088 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:10,088 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:23:10,092 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:10,092 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1666])
2024-12-22 01:23:10,155 - [Process 0/5] - DEBUG - predict_token:tensor([[10968]], device='cuda:0')
2024-12-22 01:23:10,160 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:23:10,488 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Jacob C. Landau
 53%|█████▎    | 16/30 [01:12<01:00,  4.32s/it]2024-12-22 01:23:10,497 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:202017
 47%|████▋     | 14/30 [01:12<01:15,  4.71s/it]2024-12-22 01:23:10,727 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:10,748 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:11,066 - [Process 1/5] - INFO - res.shape is :torch.Size([25])
results:He acknowledged that "Quill now harms States to a degree far greater than could have been anticipated earlier."
 53%|█████▎    | 16/30 [01:13<01:03,  4.52s/it]2024-12-22 01:23:11,269 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:11,444 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:11,445 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1866])
2024-12-22 01:23:11,508 - [Process 2/5] - DEBUG - predict_token:tensor([[478]], device='cuda:2')
2024-12-22 01:23:11,654 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:11,654 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:23:11,727 - [Process 4/5] - DEBUG - predict_token:tensor([[10917]], device='cuda:4')
2024-12-22 01:23:11,761 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:VC-10 Squadron
 57%|█████▋    | 17/30 [01:13<00:52,  4.03s/it]2024-12-22 01:23:11,883 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:12,516 - [Process 4/5] - INFO - res.shape is :torch.Size([18])
results:Semiconductor memory nanodots, nanochannels for spin injection.
 53%|█████▎    | 16/30 [01:14<01:05,  4.67s/it]2024-12-22 01:23:12,731 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:14,276 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:14,277 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1760])
2024-12-22 01:23:14,357 - [Process 0/5] - DEBUG - predict_token:tensor([[1281]], device='cuda:0')
2024-12-22 01:23:14,500 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:14,501 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1942])
2024-12-22 01:23:14,581 - [Process 3/5] - DEBUG - predict_token:tensor([[4116]], device='cuda:3')
2024-12-22 01:23:14,954 - [Process 0/5] - INFO - res.shape is :torch.Size([13])
results:
Please let me know if you need any further assistance.
 57%|█████▋    | 17/30 [01:17<00:56,  4.37s/it]2024-12-22 01:23:15,024 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:15,024 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 01:23:15,093 - [Process 1/5] - DEBUG - predict_token:tensor([[16738]], device='cuda:1')
2024-12-22 01:23:15,180 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:15,499 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:Environmental variability and network structure.
 57%|█████▋    | 17/30 [01:17<00:58,  4.49s/it]2024-12-22 01:23:15,520 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:15,521 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 01:23:15,597 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:23:15,613 - [Process 3/5] - INFO - res.shape is :torch.Size([23])
results:
(Note: Please provide your answer in a single word or short phrase, without any explanation or context.)
 50%|█████     | 15/30 [01:17<01:12,  4.83s/it]2024-12-22 01:23:15,708 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:15,837 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:16,182 - [Process 2/5] - INFO - res.shape is :torch.Size([14])
results:Bigger receptive field size leads to more complete shapes.
 60%|██████    | 18/30 [01:18<00:49,  4.15s/it]2024-12-22 01:23:16,294 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:16,338 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:16,339 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 01:23:16,412 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:23:16,862 - [Process 4/5] - INFO - res.shape is :torch.Size([10])
results:

(Please provide the answer only)
 57%|█████▋    | 17/30 [01:18<00:59,  4.57s/it]2024-12-22 01:23:17,087 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:18,770 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:18,770 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1858])
2024-12-22 01:23:18,850 - [Process 0/5] - DEBUG - predict_token:tensor([[395]], device='cuda:0')
2024-12-22 01:23:19,440 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:19,440 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1923])
2024-12-22 01:23:19,458 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:19,458 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 01:23:19,520 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:23:19,532 - [Process 3/5] - DEBUG - predict_token:tensor([[805]], device='cuda:3')
2024-12-22 01:23:19,691 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Money
 53%|█████▎    | 16/30 [01:21<01:04,  4.61s/it]2024-12-22 01:23:19,729 - [Process 0/5] - INFO - res.shape is :torch.Size([20])
results:
beta(r) = √(−2C/r + 2D)
 60%|██████    | 18/30 [01:21<00:53,  4.49s/it]2024-12-22 01:23:19,797 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Scarfing.
 60%|██████    | 18/30 [01:21<00:53,  4.43s/it]2024-12-22 01:23:19,837 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:19,838 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1981])
2024-12-22 01:23:19,912 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:23:19,932 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:19,955 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:19,972 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:20,705 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:20,705 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1777])
2024-12-22 01:23:20,789 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:23:21,410 - [Process 4/5] - INFO - res.shape is :torch.Size([14])
results:
Please let me know when you are ready with the answer.
 60%|██████    | 18/30 [01:23<00:54,  4.56s/it]2024-12-22 01:23:21,611 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:21,853 - [Process 2/5] - INFO - res.shape is :torch.Size([48])
results:The interlayer Berry connection polarizability Ω k is significant in the crossed nonlinear dynamical Hall effect, as it controls the on/off, direction, and magnitude of the rectified dc Hall current.
 63%|██████▎   | 19/30 [01:23<00:50,  4.61s/it]2024-12-22 01:23:21,976 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:23,157 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:23,157 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1857])
2024-12-22 01:23:23,220 - [Process 1/5] - DEBUG - predict_token:tensor([[5490]], device='cuda:1')
2024-12-22 01:23:23,578 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:23,578 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2221])
2024-12-22 01:23:23,607 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:23,607 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2196])
2024-12-22 01:23:23,614 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:January 1929
 63%|██████▎   | 19/30 [01:25<00:46,  4.25s/it]2024-12-22 01:23:23,642 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:23:23,673 - [Process 3/5] - DEBUG - predict_token:tensor([[15944]], device='cuda:3')
2024-12-22 01:23:23,781 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:23,832 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Director
 57%|█████▋    | 17/30 [01:25<00:58,  4.47s/it]2024-12-22 01:23:23,974 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:70-75 meters
 63%|██████▎   | 19/30 [01:26<00:48,  4.42s/it]2024-12-22 01:23:24,030 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:24,161 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:25,204 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:25,205 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 01:23:25,277 - [Process 4/5] - DEBUG - predict_token:tensor([[21375]], device='cuda:4')
2024-12-22 01:23:25,470 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Iraq
 63%|██████▎   | 19/30 [01:27<00:48,  4.41s/it]2024-12-22 01:23:25,552 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:25,552 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:23:25,624 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:23:25,681 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:25,965 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:

(Insert answer here)
 67%|██████▋   | 20/30 [01:28<00:44,  4.46s/it]2024-12-22 01:23:26,090 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:27,451 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:27,451 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 01:23:27,524 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:23:27,707 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:27,707 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1985])
2024-12-22 01:23:27,717 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:20V
 67%|██████▋   | 20/30 [01:29<00:42,  4.20s/it]2024-12-22 01:23:27,730 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:27,730 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1982])
2024-12-22 01:23:27,783 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:23:27,805 - [Process 0/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:0')
2024-12-22 01:23:27,869 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:28,282 - [Process 0/5] - INFO - res.shape is :torch.Size([10])
results:Guinea Pig No. ... L.
 67%|██████▋   | 20/30 [01:30<00:43,  4.38s/it]2024-12-22 01:23:28,420 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:29,157 - [Process 3/5] - INFO - res.shape is :torch.Size([31])
results:The framework captures the reduced-order dynamics by using a propagator in the latent space to capture the reduced-order dynamics of the system.
 60%|██████    | 18/30 [01:31<00:56,  4.72s/it]2024-12-22 01:23:29,278 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:29,278 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:23:29,351 - [Process 4/5] - DEBUG - predict_token:tensor([[1816]], device='cuda:4')
2024-12-22 01:23:29,372 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:29,810 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:29,810 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1924])
2024-12-22 01:23:29,891 - [Process 2/5] - DEBUG - predict_token:tensor([[365]], device='cuda:2')
2024-12-22 01:23:30,273 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:The Limits of Constitutional Democracy
 70%|███████   | 21/30 [01:32<00:39,  4.41s/it]2024-12-22 01:23:30,398 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:30,483 - [Process 4/5] - INFO - res.shape is :torch.Size([26])
results:SNNs could serve as promising candidates to better model and explain the functional hierarchy and mechanisms of the visual system.
 67%|██████▋   | 20/30 [01:32<00:45,  4.59s/it]2024-12-22 01:23:30,677 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:30,903 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:30,903 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1668])
2024-12-22 01:23:30,966 - [Process 1/5] - DEBUG - predict_token:tensor([[5306]], device='cuda:1')
2024-12-22 01:23:31,087 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:31,087 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1707])
2024-12-22 01:23:31,138 - [Process 0/5] - DEBUG - predict_token:tensor([[1771]], device='cuda:0')
2024-12-22 01:23:31,436 - [Process 1/5] - INFO - res.shape is :torch.Size([11])
results:June 1, 1999
 70%|███████   | 21/30 [01:33<00:36,  4.06s/it]2024-12-22 01:23:31,437 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Brisbane, Queensland
 70%|███████   | 21/30 [01:33<00:36,  4.01s/it]2024-12-22 01:23:31,641 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:31,679 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:32,983 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:32,984 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:23:33,057 - [Process 3/5] - DEBUG - predict_token:tensor([[19152]], device='cuda:3')
2024-12-22 01:23:33,290 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Keep attacking
 63%|██████▎   | 19/30 [01:35<00:50,  4.55s/it]2024-12-22 01:23:33,505 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:34,038 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:34,038 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:23:34,114 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:23:34,271 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:34,271 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2045])
2024-12-22 01:23:34,344 - [Process 4/5] - DEBUG - predict_token:tensor([[26721]], device='cuda:4')
2024-12-22 01:23:34,623 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Deputy Prime Minister
 70%|███████   | 21/30 [01:36<00:40,  4.46s/it]2024-12-22 01:23:34,782 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:35,193 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:35,194 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 01:23:35,265 - [Process 0/5] - DEBUG - predict_token:tensor([[365]], device='cuda:0')
2024-12-22 01:23:35,300 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:35,300 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 01:23:35,372 - [Process 1/5] - DEBUG - predict_token:tensor([[678]], device='cuda:1')
2024-12-22 01:23:35,540 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:L = 14
 73%|███████▎  | 22/30 [01:37<00:32,  4.04s/it]2024-12-22 01:23:35,735 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:36,217 - [Process 2/5] - INFO - res.shape is :torch.Size([52])
results:The media application determines the context of an event by using a content-recognition module to determine the context of an event in a media asset and distribute itemized tasks to multiple entities in order to generate the supplemental information about the event.
 73%|███████▎  | 22/30 [01:38<00:38,  4.87s/it]2024-12-22 01:23:36,325 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:37,137 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:37,138 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:23:37,212 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:23:37,440 - [Process 1/5] - INFO - res.shape is :torch.Size([48])
results:Mufti-e-Azam-e-Hind received Khilafat from the Qaderi, Chishti, Nakshbandi, Suharwardi, and Madaari Orders.
 73%|███████▎  | 22/30 [01:39<00:37,  4.64s/it]2024-12-22 01:23:37,659 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:37,978 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:37,978 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1819])
2024-12-22 01:23:38,041 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:23:38,311 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:85.6%
 73%|███████▎  | 22/30 [01:40<00:33,  4.23s/it]2024-12-22 01:23:38,550 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:38,847 - [Process 3/5] - INFO - res.shape is :torch.Size([38])
results:

$ \Gamma_e = \frac{e^2}{4\pi \epsilon_0 a_{ws}}\frac{1}{k_B T_e}$
 67%|██████▋   | 20/30 [01:40<00:48,  4.85s/it]2024-12-22 01:23:39,067 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:39,254 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:39,254 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1974])
2024-12-22 01:23:39,327 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:23:39,902 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:39,902 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:23:39,940 - [Process 0/5] - INFO - res.shape is :torch.Size([14])
results:

Please let me know if you need any further assistance.
 77%|███████▋  | 23/30 [01:42<00:29,  4.15s/it]2024-12-22 01:23:39,975 - [Process 2/5] - DEBUG - predict_token:tensor([[317]], device='cuda:2')
2024-12-22 01:23:40,128 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:40,637 - [Process 2/5] - INFO - res.shape is :torch.Size([16])
results:"Sustained viral gene delivery through core-shell fibers"
 77%|███████▋  | 23/30 [01:42<00:33,  4.74s/it]2024-12-22 01:23:40,685 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:41,420 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:41,420 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2115])
2024-12-22 01:23:41,489 - [Process 1/5] - DEBUG - predict_token:tensor([[395]], device='cuda:1')
2024-12-22 01:23:41,725 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:$13 billion
 77%|███████▋  | 23/30 [01:43<00:31,  4.54s/it]2024-12-22 01:23:41,913 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:42,118 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:42,118 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 01:23:42,193 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:23:42,397 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:42,397 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 780])
2024-12-22 01:23:42,440 - [Process 2/5] - DEBUG - predict_token:tensor([[7992]], device='cuda:2')
2024-12-22 01:23:42,593 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Banants
 80%|████████  | 24/30 [01:44<00:23,  3.90s/it]2024-12-22 01:23:42,679 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:42,679 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:23:42,707 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:42,751 - [Process 3/5] - DEBUG - predict_token:tensor([[18581]], device='cuda:3')
2024-12-22 01:23:42,771 - [Process 4/5] - INFO - res.shape is :torch.Size([13])
results:Smaller specific-heat ratio, slower average motion.
 77%|███████▋  | 23/30 [01:44<00:30,  4.30s/it]2024-12-22 01:23:42,900 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:SKIP
 70%|███████   | 21/30 [01:45<00:41,  4.61s/it]2024-12-22 01:23:42,931 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:43,046 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:43,658 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:43,659 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 01:23:43,727 - [Process 0/5] - DEBUG - predict_token:tensor([[7413]], device='cuda:0')
2024-12-22 01:23:44,298 - [Process 0/5] - INFO - res.shape is :torch.Size([13])
results:Lasa, Gitastrophe, and Shadoks
 80%|████████  | 24/30 [01:46<00:25,  4.21s/it]2024-12-22 01:23:44,525 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:45,516 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:45,516 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:23:45,589 - [Process 1/5] - DEBUG - predict_token:tensor([[2233]], device='cuda:1')
2024-12-22 01:23:45,951 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:Clutha-Southland
 80%|████████  | 24/30 [01:48<00:26,  4.44s/it]2024-12-22 01:23:46,094 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:46,095 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1747])
2024-12-22 01:23:46,108 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:46,154 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:23:46,250 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:46,250 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1744])
2024-12-22 01:23:46,256 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 73%|███████▎  | 22/30 [01:48<00:33,  4.23s/it]2024-12-22 01:23:46,278 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:46,278 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:23:46,320 - [Process 4/5] - DEBUG - predict_token:tensor([[20029]], device='cuda:4')
2024-12-22 01:23:46,351 - [Process 2/5] - DEBUG - predict_token:tensor([[2138]], device='cuda:2')
2024-12-22 01:23:46,464 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:47,011 - [Process 2/5] - INFO - res.shape is :torch.Size([16])
results:Fatigue may be associated with eating high mercury fish.
 83%|████████▎ | 25/30 [01:49<00:20,  4.06s/it]2024-12-22 01:23:47,049 - [Process 4/5] - INFO - res.shape is :torch.Size([17])
results:McPherson County is located in the U.S. state of Kansas.
 80%|████████  | 24/30 [01:49<00:25,  4.29s/it]2024-12-22 01:23:47,111 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:47,164 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:48,120 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:48,121 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1915])
2024-12-22 01:23:48,200 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:23:48,420 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:48,420 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 754])
2024-12-22 01:23:48,446 - [Process 4/5] - DEBUG - predict_token:tensor([[395]], device='cuda:4')
2024-12-22 01:23:49,127 - [Process 4/5] - INFO - res.shape is :torch.Size([17])
results:$O \sim t^{\alpha} L_\parallel^{-1/2}$
 83%|████████▎ | 25/30 [01:51<00:18,  3.63s/it]2024-12-22 01:23:49,282 - [Process 0/5] - INFO - res.shape is :torch.Size([25])
results:
The court in In re Ferguson concluded that the transformation prong of the Bilski test was met.
 83%|████████▎ | 25/30 [01:51<00:22,  4.44s/it]2024-12-22 01:23:49,352 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:49,435 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:49,436 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1759])
2024-12-22 01:23:49,486 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:49,505 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:23:49,818 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:30,223
 83%|████████▎ | 25/30 [01:51<00:21,  4.27s/it]2024-12-22 01:23:49,946 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:50,110 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:50,110 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 01:23:50,183 - [Process 3/5] - DEBUG - predict_token:tensor([[17674]], device='cuda:3')
2024-12-22 01:23:50,417 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:IL-78
 77%|███████▋  | 23/30 [01:52<00:29,  4.21s/it]2024-12-22 01:23:50,600 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:50,777 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:50,778 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1917])
2024-12-22 01:23:50,857 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:23:52,649 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:52,649 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1712])
2024-12-22 01:23:52,701 - [Process 1/5] - DEBUG - predict_token:tensor([[13370]], device='cuda:1')
2024-12-22 01:23:52,794 - [Process 2/5] - INFO - res.shape is :torch.Size([48])
results:The scoring engine generates a stream of content for the channel by scoring candidate content items based on their relevance to the user's interests and historical trends, and then generating the stream of content from the top-scoring items.
 87%|████████▋ | 26/30 [01:54<00:18,  4.58s/it]2024-12-22 01:23:52,844 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:52,922 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:2013
 87%|████████▋ | 26/30 [01:55<00:15,  3.92s/it]2024-12-22 01:23:53,089 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:53,089 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1698])
2024-12-22 01:23:53,108 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:53,180 - [Process 4/5] - DEBUG - predict_token:tensor([[9954]], device='cuda:4')
2024-12-22 01:23:53,195 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:53,196 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2136])
2024-12-22 01:23:53,264 - [Process 0/5] - DEBUG - predict_token:tensor([[270]], device='cuda:0')
2024-12-22 01:23:53,846 - [Process 4/5] - INFO - res.shape is :torch.Size([15])
results:
(Note: Please answer in a single sentence or bullet point)
 87%|████████▋ | 26/30 [01:55<00:15,  3.95s/it]2024-12-22 01:23:53,979 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:54,091 - [Process 0/5] - INFO - res.shape is :torch.Size([19])
results:d e ∈ 0, 0.1, . . ., 1
 87%|████████▋ | 26/30 [01:56<00:18,  4.55s/it]2024-12-22 01:23:54,307 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:54,316 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:54,316 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2882])
2024-12-22 01:23:54,358 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:23:54,555 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:54,555 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 780])
2024-12-22 01:23:54,598 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:23:54,956 - [Process 2/5] - INFO - res.shape is :torch.Size([10])
results:2013–2014
 90%|█████████ | 27/30 [01:57<00:11,  3.85s/it]2024-12-22 01:23:54,975 - [Process 3/5] - INFO - res.shape is :torch.Size([14])
results:
Please provide the answer in a single word or short phrase.
 80%|████████  | 24/30 [01:57<00:25,  4.32s/it]2024-12-22 01:23:55,033 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:55,200 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:56,682 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:56,683 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1704])
2024-12-22 01:23:56,735 - [Process 4/5] - DEBUG - predict_token:tensor([[6033]], device='cuda:4')
2024-12-22 01:23:56,785 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:56,786 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 01:23:56,858 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:23:56,921 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Romance
 90%|█████████ | 27/30 [01:59<00:11,  3.69s/it]2024-12-22 01:23:57,060 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:23 September
 90%|█████████ | 27/30 [01:59<00:11,  3.99s/it]2024-12-22 01:23:57,167 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:57,222 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:57,716 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:57,716 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1705])
2024-12-22 01:23:57,768 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:23:57,881 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:57,882 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:23:57,955 - [Process 0/5] - DEBUG - predict_token:tensor([[341]], device='cuda:0')
2024-12-22 01:23:58,123 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:
Please give me the answer only.
 93%|█████████▎| 28/30 [02:00<00:07,  3.65s/it]2024-12-22 01:23:58,235 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:58,400 - [Process 0/5] - INFO - res.shape is :torch.Size([10])
results:Molecular ultracold plasmas
 90%|█████████ | 27/30 [02:00<00:13,  4.48s/it]2024-12-22 01:23:58,653 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:23:58,794 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:23:58,794 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2097])
2024-12-22 01:23:58,864 - [Process 3/5] - DEBUG - predict_token:tensor([[7311]], device='cuda:3')
2024-12-22 01:23:59,225 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Because he wants to escape.
 83%|████████▎ | 25/30 [02:01<00:21,  4.30s/it]2024-12-22 01:23:59,445 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:00,269 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:00,269 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1674])
2024-12-22 01:24:00,333 - [Process 1/5] - DEBUG - predict_token:tensor([[379]], device='cuda:1')
2024-12-22 01:24:00,476 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:LTCM
 93%|█████████▎| 28/30 [02:02<00:07,  3.81s/it]2024-12-22 01:24:00,538 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:00,929 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:00,929 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2118])
2024-12-22 01:24:00,998 - [Process 4/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:4')
2024-12-22 01:24:01,277 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:
(b) No
 93%|█████████▎| 28/30 [02:03<00:07,  3.89s/it]2024-12-22 01:24:01,470 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:01,844 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:01,844 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1899])
2024-12-22 01:24:01,848 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:01,848 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 766])
2024-12-22 01:24:01,873 - [Process 1/5] - DEBUG - predict_token:tensor([[395]], device='cuda:1')
2024-12-22 01:24:01,925 - [Process 2/5] - DEBUG - predict_token:tensor([[28268]], device='cuda:2')
2024-12-22 01:24:02,264 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:02,264 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1901])
2024-12-22 01:24:02,266 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:
Probabilistic model.
 97%|█████████▋| 29/30 [02:04<00:03,  3.79s/it]2024-12-22 01:24:02,344 - [Process 0/5] - DEBUG - predict_token:tensor([[399]], device='cuda:0')
2024-12-22 01:24:02,379 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:02,622 - [Process 1/5] - INFO - res.shape is :torch.Size([19])
results:$O \sim t^{1/8} L_\parallel^{-1/2}$
 97%|█████████▋| 29/30 [02:04<00:03,  3.31s/it]2024-12-22 01:24:02,664 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Wearable sensors.
 93%|█████████▎| 28/30 [02:04<00:08,  4.41s/it]2024-12-22 01:24:02,807 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:02,875 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:03,061 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:03,061 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1766])
2024-12-22 01:24:03,143 - [Process 3/5] - DEBUG - predict_token:tensor([[530]], device='cuda:3')
2024-12-22 01:24:03,420 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Brown & Whiteford
 87%|████████▋ | 26/30 [02:05<00:17,  4.27s/it]2024-12-22 01:24:03,676 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:05,031 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:05,031 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1590])
2024-12-22 01:24:05,121 - [Process 4/5] - DEBUG - predict_token:tensor([[501]], device='cuda:4')
2024-12-22 01:24:05,271 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:DUO
 97%|█████████▋| 29/30 [02:07<00:03,  3.92s/it]2024-12-22 01:24:05,501 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:05,923 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:05,923 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1591])
2024-12-22 01:24:06,012 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:24:06,114 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
100%|██████████| 30/30 [02:08<00:00,  3.81s/it]100%|██████████| 30/30 [02:08<00:00,  4.28s/it]
2024-12-22 01:24:06,447 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:06,447 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2056])
2024-12-22 01:24:06,492 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:06,493 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2875])
2024-12-22 01:24:06,519 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:24:06,535 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:24:06,626 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:

 97%|█████████▋| 29/30 [02:08<00:04,  4.28s/it]2024-12-22 01:24:06,841 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:07,152 - [Process 1/5] - INFO - res.shape is :torch.Size([14])
results:
Please provide the answer in a direct and concise manner.
100%|██████████| 30/30 [02:09<00:00,  3.68s/it]100%|██████████| 30/30 [02:09<00:00,  4.31s/it]
2024-12-22 01:24:07,309 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:07,309 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2051])
2024-12-22 01:24:07,382 - [Process 3/5] - DEBUG - predict_token:tensor([[739]], device='cuda:3')
2024-12-22 01:24:07,785 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:


(Insert answer here)
 90%|█████████ | 27/30 [02:09<00:12,  4.30s/it]2024-12-22 01:24:08,009 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:09,072 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:09,072 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 01:24:09,142 - [Process 4/5] - DEBUG - predict_token:tensor([[1619]], device='cuda:4')
2024-12-22 01:24:09,420 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:My Aspergers Child
100%|██████████| 30/30 [02:11<00:00,  3.99s/it]100%|██████████| 30/30 [02:11<00:00,  4.38s/it]
2024-12-22 01:24:10,394 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:10,395 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:24:10,467 - [Process 0/5] - DEBUG - predict_token:tensor([[2296]], device='cuda:0')
2024-12-22 01:24:11,161 - [Process 0/5] - INFO - res.shape is :torch.Size([16])
results:Mary told the disciples, "I have seen the Lord."
100%|██████████| 30/30 [02:13<00:00,  4.36s/it]100%|██████████| 30/30 [02:13<00:00,  4.44s/it]
2024-12-22 01:24:11,600 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:11,600 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-22 01:24:11,675 - [Process 3/5] - DEBUG - predict_token:tensor([[4673]], device='cuda:3')
2024-12-22 01:24:12,962 - [Process 3/5] - INFO - res.shape is :torch.Size([30])
results:Approve or defer approval on any applications for new docks in the Cove until the management plan can be developed and implemented.
 93%|█████████▎| 28/30 [02:15<00:09,  4.56s/it]2024-12-22 01:24:13,186 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:16,801 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:16,801 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:24:16,874 - [Process 3/5] - DEBUG - predict_token:tensor([[9206]], device='cuda:3')
2024-12-22 01:24:17,064 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Power-law
 97%|█████████▋| 29/30 [02:19<00:04,  4.42s/it]2024-12-22 01:24:17,233 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:24:20,710 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:24:20,711 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2111])
2024-12-22 01:24:20,777 - [Process 3/5] - DEBUG - predict_token:tensor([[779]], device='cuda:3')
2024-12-22 01:24:22,560 - [Process 3/5] - INFO - res.shape is :torch.Size([42])
results:$ \frac{6x^2\cos{\left(x^2\right)}+\sin{\left(x^2\right)}}{3\sqrt[3]{x^2}}$
100%|██████████| 30/30 [02:24<00:00,  4.75s/it]100%|██████████| 30/30 [02:24<00:00,  4.82s/it]
2024-12-22 01:24:22,578 - [Process 3/5] - DEBUG - datasets_name:multifieldqa_en
2024-12-22 01:24:22,578 - [Process 0/5] - DEBUG - datasets_name:multifieldqa_en
2024-12-22 01:24:22,578 - [Process 2/5] - DEBUG - datasets_name:multifieldqa_en
2024-12-22 01:24:22,578 - [Process 4/5] - DEBUG - datasets_name:multifieldqa_en
2024-12-22 01:24:22,578 - [Process 1/5] - DEBUG - datasets_name:multifieldqa_en
Running evaluation for dataset: hotpotqa
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]n_windows:[2]

model_name:/mnt/Data/xiongjing/llama2chatmodel_name:/mnt/Data/xiongjing/llama2chat

multi_gpus:Truemulti_gpus:True

torch.cuda.device_count():5torch.cuda.device_count():5

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.37s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:26:19,839 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:26:19,840 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:26:19,840 - [Process 0/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:26:19,849 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:26:19,850 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:26:19,850 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:26:19,859 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 01:26:19,860 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:26:19,860 - [Process 2/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里!!!!!!!!!!!!!!!!!!!!!!!! 这里

2024-12-22 01:26:19,873 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 01:26:19,873 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:26:19,874 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:26:19,874 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:26:19,874 - [Process 1/5] - INFO - output_max_len: 32
2024-12-22 01:26:19,874 - [Process 3/5] - INFO - output_max_len: 32
2024-12-22 01:26:19,886 - [Process 0/5] - INFO - Max Length is 12697
2024-12-22 01:26:19,886 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:26:19,886 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:26:19,923 - [Process 4/5] - INFO - Max Length is 12697
2024-12-22 01:26:19,924 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:26:19,924 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:26:19,936 - [Process 2/5] - INFO - Max Length is 12697
2024-12-22 01:26:19,936 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:26:19,936 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:26:19,964 - [Process 1/5] - INFO - Max Length is 12697
2024-12-22 01:26:19,964 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:26:19,965 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:26:19,966 - [Process 3/5] - INFO - Max Length is 12697
2024-12-22 01:26:19,967 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:26:19,967 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:26:24,628 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:24,714 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:24,714 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:24,715 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:24,716 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:28,763 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:28,764 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 01:26:28,837 - [Process 0/5] - DEBUG - predict_token:tensor([[16498]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:26:29,073 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:29,073 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2111])
2024-12-22 01:26:29,087 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:29,088 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2048])
2024-12-22 01:26:29,142 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Gates v. Collier
  2%|▎         | 1/40 [00:09<06:00,  9.26s/it]2024-12-22 01:26:29,144 - [Process 2/5] - DEBUG - predict_token:tensor([[25281]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:26:29,153 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:29,154 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1962])
2024-12-22 01:26:29,161 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:26:29,231 - [Process 4/5] - DEBUG - predict_token:tensor([[2610]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:26:29,248 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:29,249 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:26:29,312 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:29,328 - [Process 1/5] - DEBUG - predict_token:tensor([[1317]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:26:29,396 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Saturday
  2%|▎         | 1/40 [00:09<06:09,  9.47s/it]2024-12-22 01:26:29,486 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Pamela B. Green
  2%|▎         | 1/40 [00:09<06:12,  9.55s/it]2024-12-22 01:26:29,583 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Nobel Prize
  2%|▎         | 1/40 [00:09<06:15,  9.62s/it]2024-12-22 01:26:29,638 - [Process 3/5] - INFO - res.shape is :torch.Size([10])
results:Medium sized version of ducks.
  2%|▎         | 1/40 [00:09<06:17,  9.67s/it]2024-12-22 01:26:29,651 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:29,790 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:29,860 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:29,899 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:32,999 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:33,000 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1954])
2024-12-22 01:26:33,079 - [Process 0/5] - DEBUG - predict_token:tensor([[5322]], device='cuda:0')
2024-12-22 01:26:33,218 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:33,218 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 01:26:33,289 - [Process 4/5] - DEBUG - predict_token:tensor([[24728]], device='cuda:4')
2024-12-22 01:26:33,304 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Jules Verne
  5%|▌         | 2/40 [00:13<03:57,  6.26s/it]2024-12-22 01:26:33,477 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:33,529 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:33,529 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2214])
2024-12-22 01:26:33,530 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:33,530 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 01:26:33,567 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Days of Our Lives
  5%|▌         | 2/40 [00:13<04:01,  6.35s/it]2024-12-22 01:26:33,584 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:33,584 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2163])
2024-12-22 01:26:33,597 - [Process 2/5] - DEBUG - predict_token:tensor([[10406]], device='cuda:2')
2024-12-22 01:26:33,601 - [Process 1/5] - DEBUG - predict_token:tensor([[22303]], device='cuda:1')
2024-12-22 01:26:33,652 - [Process 3/5] - DEBUG - predict_token:tensor([[341]], device='cuda:3')
2024-12-22 01:26:33,815 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:33,857 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Mimosa
  5%|▌         | 2/40 [00:13<04:05,  6.46s/it]2024-12-22 01:26:33,925 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Long Khac Nguyen
  5%|▌         | 2/40 [00:13<04:08,  6.54s/it]2024-12-22 01:26:33,937 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Stop-motion animation.
  5%|▌         | 2/40 [00:13<04:07,  6.52s/it]2024-12-22 01:26:34,163 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:34,204 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:34,210 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:37,032 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:37,032 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2060])
2024-12-22 01:26:37,105 - [Process 0/5] - DEBUG - predict_token:tensor([[19662]], device='cuda:0')
2024-12-22 01:26:37,287 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Lansing
  8%|▊         | 3/40 [00:17<03:13,  5.22s/it]2024-12-22 01:26:37,464 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:37,501 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:37,501 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1928])
2024-12-22 01:26:37,581 - [Process 4/5] - DEBUG - predict_token:tensor([[3589]], device='cuda:4')
2024-12-22 01:26:37,751 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:37,751 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2261])
2024-12-22 01:26:37,775 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Holliday
  8%|▊         | 3/40 [00:17<03:18,  5.37s/it]2024-12-22 01:26:37,815 - [Process 3/5] - DEBUG - predict_token:tensor([[16899]], device='cuda:3')
2024-12-22 01:26:37,908 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:37,909 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1963])
2024-12-22 01:26:37,973 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:37,973 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2113])
2024-12-22 01:26:37,987 - [Process 1/5] - DEBUG - predict_token:tensor([[3375]], device='cuda:1')
2024-12-22 01:26:38,044 - [Process 2/5] - DEBUG - predict_token:tensor([[26901]], device='cuda:2')
2024-12-22 01:26:38,047 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:38,099 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Irrelevant.
  8%|▊         | 3/40 [00:18<03:21,  5.45s/it]2024-12-22 01:26:38,189 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Michelle Terry
  8%|▊         | 3/40 [00:18<03:22,  5.49s/it]2024-12-22 01:26:38,388 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:38,406 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:

(Insert answer here)
  8%|▊         | 3/40 [00:18<03:27,  5.60s/it]2024-12-22 01:26:38,486 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:38,621 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:41,007 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:41,007 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2046])
2024-12-22 01:26:41,080 - [Process 0/5] - DEBUG - predict_token:tensor([[27441]], device='cuda:0')
2024-12-22 01:26:41,262 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Jupiter
 10%|█         | 4/40 [00:21<02:50,  4.73s/it]2024-12-22 01:26:41,419 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:41,730 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:41,730 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1973])
2024-12-22 01:26:41,810 - [Process 4/5] - DEBUG - predict_token:tensor([[678]], device='cuda:4')
2024-12-22 01:26:42,045 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Saginaw
 10%|█         | 4/40 [00:22<02:57,  4.94s/it]2024-12-22 01:26:42,123 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:42,124 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2154])
2024-12-22 01:26:42,186 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:42,187 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 01:26:42,192 - [Process 3/5] - DEBUG - predict_token:tensor([[315]], device='cuda:3')
2024-12-22 01:26:42,260 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:26:42,271 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:42,271 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2123])
2024-12-22 01:26:42,320 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:42,343 - [Process 1/5] - DEBUG - predict_token:tensor([[17457]], device='cuda:1')
2024-12-22 01:26:42,496 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:100m
 10%|█         | 4/40 [00:22<03:00,  5.00s/it]2024-12-22 01:26:42,552 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Nepal
 10%|█         | 4/40 [00:22<03:01,  5.04s/it]2024-12-22 01:26:42,658 - [Process 3/5] - INFO - res.shape is :torch.Size([10])
results:Coca-Cola FEMSA
 10%|█         | 4/40 [00:22<03:03,  5.10s/it]2024-12-22 01:26:42,775 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:42,872 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:42,948 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:45,009 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:45,009 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2080])
2024-12-22 01:26:45,080 - [Process 0/5] - DEBUG - predict_token:tensor([[1551]], device='cuda:0')
2024-12-22 01:26:45,222 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:On film
 12%|█▎        | 5/40 [00:25<02:35,  4.45s/it]2024-12-22 01:26:45,382 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:45,939 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:45,940 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2238])
2024-12-22 01:26:46,004 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:26:46,325 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:94,903
 12%|█▎        | 5/40 [00:26<02:44,  4.70s/it]2024-12-22 01:26:46,362 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:46,363 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:26:46,433 - [Process 2/5] - DEBUG - predict_token:tensor([[4121]], device='cuda:2')
2024-12-22 01:26:46,506 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:46,577 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:46,577 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:26:46,628 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Pat Bowlen
 12%|█▎        | 5/40 [00:26<02:44,  4.69s/it]2024-12-22 01:26:46,650 - [Process 3/5] - DEBUG - predict_token:tensor([[11001]], device='cuda:3')
2024-12-22 01:26:46,656 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:46,657 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 01:26:46,737 - [Process 1/5] - DEBUG - predict_token:tensor([[323]], device='cuda:1')
2024-12-22 01:26:46,820 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:46,988 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Ellie Kemper
2024-12-22 01:26:46,989 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
 12%|█▎        | 5/40 [00:27<02:48,  4.82s/it]results:Kwanzaa
 12%|█▎        | 5/40 [00:27<02:48,  4.82s/it]2024-12-22 01:26:47,257 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:47,279 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:49,057 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:49,057 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1954])
2024-12-22 01:26:49,137 - [Process 0/5] - DEBUG - predict_token:tensor([[13212]], device='cuda:0')
2024-12-22 01:26:49,440 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Brian Henry Mulholland
 15%|█▌        | 6/40 [00:29<02:28,  4.37s/it]2024-12-22 01:26:49,597 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:50,088 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:50,088 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 01:26:50,162 - [Process 4/5] - DEBUG - predict_token:tensor([[14328]], device='cuda:4')
2024-12-22 01:26:50,354 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:The Atlantic Ocean
 15%|█▌        | 6/40 [00:30<02:32,  4.47s/it]2024-12-22 01:26:50,520 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:50,521 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-22 01:26:50,600 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:26:50,624 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:50,708 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 15%|█▌        | 6/40 [00:30<02:32,  4.48s/it]2024-12-22 01:26:50,835 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:50,835 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1815])
2024-12-22 01:26:50,876 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:50,877 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:26:50,916 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:26:50,951 - [Process 1/5] - DEBUG - predict_token:tensor([[27179]], device='cuda:1')
2024-12-22 01:26:50,991 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:51,027 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 15%|█▌        | 6/40 [00:31<02:34,  4.56s/it]2024-12-22 01:26:51,151 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Keith Morris
 15%|█▌        | 6/40 [00:31<02:36,  4.60s/it]2024-12-22 01:26:51,228 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:51,386 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:53,134 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:53,134 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 01:26:53,207 - [Process 0/5] - DEBUG - predict_token:tensor([[476]], device='cuda:0')
2024-12-22 01:26:53,389 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Kerala
 18%|█▊        | 7/40 [00:33<02:19,  4.23s/it]2024-12-22 01:26:53,453 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:54,331 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:54,331 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1966])
2024-12-22 01:26:54,412 - [Process 4/5] - DEBUG - predict_token:tensor([[4643]], device='cuda:4')
2024-12-22 01:26:54,741 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:54,741 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:26:54,821 - [Process 2/5] - DEBUG - predict_token:tensor([[951]], device='cuda:2')
2024-12-22 01:26:54,958 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:54,959 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1836])
2024-12-22 01:26:55,034 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:55,034 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:26:55,040 - [Process 1/5] - DEBUG - predict_token:tensor([[612]], device='cuda:1')
2024-12-22 01:26:55,114 - [Process 3/5] - DEBUG - predict_token:tensor([[6379]], device='cuda:3')
2024-12-22 01:26:55,117 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Leucippus
 18%|█▊        | 7/40 [00:35<02:27,  4.46s/it]2024-12-22 01:26:55,134 - [Process 4/5] - INFO - res.shape is :torch.Size([16])
results:Raj Kapoor and Mike Cahill are both film directors.
 18%|█▊        | 7/40 [00:35<02:30,  4.57s/it]2024-12-22 01:26:55,240 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:YIVO
 18%|█▊        | 7/40 [00:35<02:26,  4.43s/it]2024-12-22 01:26:55,317 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Umina Beach
 18%|█▊        | 7/40 [00:35<02:27,  4.47s/it]2024-12-22 01:26:55,333 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:55,413 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:55,496 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:55,542 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:55,645 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:55,646 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1393])
2024-12-22 01:26:55,687 - [Process 0/5] - DEBUG - predict_token:tensor([[19777]], device='cuda:0')
2024-12-22 01:26:55,956 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Pleiospilos
 20%|██        | 8/40 [00:36<01:58,  3.70s/it]2024-12-22 01:26:56,101 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:59,041 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:59,041 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2210])
2024-12-22 01:26:59,104 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:59,104 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:26:59,106 - [Process 2/5] - DEBUG - predict_token:tensor([[23740]], device='cuda:2')
2024-12-22 01:26:59,178 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:59,179 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2111])
2024-12-22 01:26:59,183 - [Process 4/5] - DEBUG - predict_token:tensor([[7745]], device='cuda:4')
2024-12-22 01:26:59,236 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:59,236 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1939])
2024-12-22 01:26:59,252 - [Process 3/5] - DEBUG - predict_token:tensor([[315]], device='cuda:3')
2024-12-22 01:26:59,298 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Start
 20%|██        | 8/40 [00:39<02:22,  4.44s/it]2024-12-22 01:26:59,317 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:26:59,350 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Miami Gardens
 20%|██        | 8/40 [00:39<02:20,  4.39s/it]2024-12-22 01:26:59,453 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Cebu
 20%|██        | 8/40 [00:39<02:19,  4.36s/it]2024-12-22 01:26:59,564 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:1826
 20%|██        | 8/40 [00:39<02:20,  4.40s/it]2024-12-22 01:26:59,604 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:59,624 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:26:59,624 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1864])
2024-12-22 01:26:59,639 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:59,704 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:26:59,771 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:59,812 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:26:59,928 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:2013
 22%|██▎       | 9/40 [00:40<01:57,  3.79s/it]2024-12-22 01:27:00,059 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:03,196 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:03,196 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1806])
2024-12-22 01:27:03,225 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:03,226 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 01:27:03,278 - [Process 4/5] - DEBUG - predict_token:tensor([[7646]], device='cuda:4')
2024-12-22 01:27:03,299 - [Process 2/5] - DEBUG - predict_token:tensor([[9134]], device='cuda:2')
2024-12-22 01:27:03,412 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:03,413 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-22 01:27:03,416 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:03,416 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 01:27:03,485 - [Process 1/5] - DEBUG - predict_token:tensor([[379]], device='cuda:1')
2024-12-22 01:27:03,490 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:27:03,527 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Green and Yellow
 22%|██▎       | 9/40 [00:43<02:15,  4.38s/it]2024-12-22 01:27:03,592 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:555 Water Street
 22%|██▎       | 9/40 [00:43<02:14,  4.34s/it]2024-12-22 01:27:03,741 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
2024-12-22 01:27:03,741 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:03,741 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1950])
results:NASA.
 22%|██▎       | 9/40 [00:43<02:14,  4.34s/it]2024-12-22 01:27:03,817 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:03,821 - [Process 0/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:0')
2024-12-22 01:27:03,842 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:03,846 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:03,868 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:Tongshanjiabu
 22%|██▎       | 9/40 [00:43<02:15,  4.37s/it]2024-12-22 01:27:04,004 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:The Rebirth
 25%|██▌       | 10/40 [00:44<01:56,  3.88s/it]2024-12-22 01:27:04,153 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:04,175 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:05,887 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:05,887 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1107])
2024-12-22 01:27:05,927 - [Process 3/5] - DEBUG - predict_token:tensor([[660]], device='cuda:3')
2024-12-22 01:27:06,060 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Suining
 25%|██▌       | 10/40 [00:46<01:51,  3.72s/it]2024-12-22 01:27:06,316 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:07,454 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:07,455 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2064])
2024-12-22 01:27:07,528 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:27:07,593 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:07,593 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 01:27:07,673 - [Process 4/5] - DEBUG - predict_token:tensor([[4451]], device='cuda:4')
2024-12-22 01:27:07,858 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:191943
 25%|██▌       | 10/40 [00:47<02:09,  4.32s/it]2024-12-22 01:27:07,865 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:07,865 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1932])
2024-12-22 01:27:07,876 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Outlander
 25%|██▌       | 10/40 [00:47<02:11,  4.37s/it]2024-12-22 01:27:07,923 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:07,923 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 01:27:07,945 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:27:08,002 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:27:08,108 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:08,168 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:2003
 28%|██▊       | 11/40 [00:48<01:54,  3.96s/it]2024-12-22 01:27:08,174 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:08,238 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:1791
 25%|██▌       | 10/40 [00:48<02:11,  4.37s/it]2024-12-22 01:27:08,336 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:08,424 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:10,077 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:10,078 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2165])
2024-12-22 01:27:10,147 - [Process 3/5] - DEBUG - predict_token:tensor([[2088]], device='cuda:3')
2024-12-22 01:27:10,254 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:

 28%|██▊       | 11/40 [00:50<01:51,  3.86s/it]2024-12-22 01:27:10,492 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:11,884 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:11,884 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2117])
2024-12-22 01:27:11,896 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:11,896 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1984])
2024-12-22 01:27:11,903 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:11,903 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 01:27:11,955 - [Process 2/5] - DEBUG - predict_token:tensor([[315]], device='cuda:2')
2024-12-22 01:27:11,976 - [Process 4/5] - DEBUG - predict_token:tensor([[11319]], device='cuda:4')
2024-12-22 01:27:11,977 - [Process 0/5] - DEBUG - predict_token:tensor([[7169]], device='cuda:0')
2024-12-22 01:27:12,153 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:12,153 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2168])
2024-12-22 01:27:12,157 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:BC Lions
 28%|██▊       | 11/40 [00:52<02:05,  4.31s/it]2024-12-22 01:27:12,222 - [Process 1/5] - DEBUG - predict_token:tensor([[1085]], device='cuda:1')
2024-12-22 01:27:12,278 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Warner Bros.
 30%|███       | 12/40 [00:52<01:52,  4.01s/it]2024-12-22 01:27:12,444 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:12,457 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Pope John X
 28%|██▊       | 11/40 [00:52<02:05,  4.32s/it]2024-12-22 01:27:12,459 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:12,732 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:12,788 - [Process 4/5] - INFO - res.shape is :torch.Size([18])
results:The commentator serves as associate director of the Centre for Social Cohesion.
 28%|██▊       | 11/40 [00:52<02:11,  4.53s/it]2024-12-22 01:27:13,061 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:14,112 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:14,112 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2261])
2024-12-22 01:27:14,178 - [Process 3/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:3')
2024-12-22 01:27:14,285 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:No
 30%|███       | 12/40 [00:54<01:49,  3.91s/it]2024-12-22 01:27:14,555 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:16,041 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:16,041 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 01:27:16,063 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:16,064 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 01:27:16,115 - [Process 0/5] - DEBUG - predict_token:tensor([[5202]], device='cuda:0')
2024-12-22 01:27:16,137 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:27:16,329 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Monday
 30%|███       | 12/40 [00:56<01:59,  4.27s/it]2024-12-22 01:27:16,338 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Marisa Prado
 32%|███▎      | 13/40 [00:56<01:48,  4.02s/it]2024-12-22 01:27:16,461 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:16,462 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1963])
2024-12-22 01:27:16,474 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:16,540 - [Process 1/5] - DEBUG - predict_token:tensor([[19659]], device='cuda:1')
2024-12-22 01:27:16,605 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:16,734 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Manchester United
 30%|███       | 12/40 [00:56<02:00,  4.31s/it]2024-12-22 01:27:16,776 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:16,777 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1943])
2024-12-22 01:27:16,856 - [Process 4/5] - DEBUG - predict_token:tensor([[18341]], device='cuda:4')
2024-12-22 01:27:17,001 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:17,049 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Baghda
 30%|███       | 12/40 [00:57<02:04,  4.45s/it]2024-12-22 01:27:17,310 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:18,192 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:18,192 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-22 01:27:18,264 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:27:18,414 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Morton
 32%|███▎      | 13/40 [00:58<01:47,  3.98s/it]2024-12-22 01:27:18,629 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:20,160 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:20,160 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1931])
2024-12-22 01:27:20,197 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:20,197 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1995])
2024-12-22 01:27:20,240 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:27:20,271 - [Process 2/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:2')
2024-12-22 01:27:20,343 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:5
 35%|███▌      | 14/40 [01:00<01:44,  4.02s/it]2024-12-22 01:27:20,378 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 32%|███▎      | 13/40 [01:00<01:53,  4.20s/it]2024-12-22 01:27:20,513 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:20,659 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:20,731 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:20,731 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2152])
2024-12-22 01:27:20,800 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:27:20,907 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 32%|███▎      | 13/40 [01:00<01:55,  4.27s/it]2024-12-22 01:27:20,918 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:20,918 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 01:27:20,992 - [Process 4/5] - DEBUG - predict_token:tensor([[16092]], device='cuda:4')
2024-12-22 01:27:21,114 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:21,184 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Allen Wolf
 32%|███▎      | 13/40 [01:01<01:57,  4.36s/it]2024-12-22 01:27:21,464 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:22,274 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:22,275 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 01:27:22,349 - [Process 3/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:3')
2024-12-22 01:27:22,584 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:1957
 35%|███▌      | 14/40 [01:02<01:44,  4.04s/it]2024-12-22 01:27:22,859 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:24,154 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:24,154 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 01:27:24,227 - [Process 0/5] - DEBUG - predict_token:tensor([[6978]], device='cuda:0')
2024-12-22 01:27:24,312 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:24,313 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:27:24,388 - [Process 2/5] - DEBUG - predict_token:tensor([[399]], device='cuda:2')
2024-12-22 01:27:24,759 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:24,759 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 01:27:24,833 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:27:24,920 - [Process 2/5] - INFO - res.shape is :torch.Size([12])
results:Marge Piercy and Aldington are both writers.
 35%|███▌      | 14/40 [01:04<01:51,  4.31s/it]2024-12-22 01:27:25,042 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:25,042 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2280])
2024-12-22 01:27:25,068 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:8530
 35%|███▌      | 14/40 [01:05<01:50,  4.24s/it]2024-12-22 01:27:25,106 - [Process 4/5] - DEBUG - predict_token:tensor([[22487]], device='cuda:4')
2024-12-22 01:27:25,163 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:25,286 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:25,386 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Luigi Cherubini
 35%|███▌      | 14/40 [01:05<01:52,  4.31s/it]2024-12-22 01:27:25,537 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Passage 2 mentions that the moose (Alces alces) is found in areas such as Canada, Alaska, New England, New York State
 38%|███▊      | 15/40 [01:05<01:49,  4.37s/it]2024-12-22 01:27:25,645 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:25,688 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:26,501 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:26,501 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2100])
2024-12-22 01:27:26,574 - [Process 3/5] - DEBUG - predict_token:tensor([[11733]], device='cuda:3')
2024-12-22 01:27:27,234 - [Process 3/5] - INFO - res.shape is :torch.Size([15])
results:Camp Courtney was named after Major Hugh Boyd Casey.
 38%|███▊      | 15/40 [01:07<01:45,  4.22s/it]2024-12-22 01:27:27,471 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:28,771 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:28,771 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:27:28,846 - [Process 2/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:2')
2024-12-22 01:27:28,953 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:No
 38%|███▊      | 15/40 [01:09<01:45,  4.22s/it]2024-12-22 01:27:29,091 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:29,092 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 01:27:29,173 - [Process 1/5] - DEBUG - predict_token:tensor([[7824]], device='cuda:1')
2024-12-22 01:27:29,221 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:29,221 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1841])
2024-12-22 01:27:29,233 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:29,302 - [Process 4/5] - DEBUG - predict_token:tensor([[11681]], device='cuda:4')
2024-12-22 01:27:29,410 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:IndyCar Series
 38%|███▊      | 15/40 [01:09<01:46,  4.27s/it]2024-12-22 01:27:29,438 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:29,438 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2131])
2024-12-22 01:27:29,510 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:27:29,538 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Babylon
 38%|███▊      | 15/40 [01:09<01:46,  4.26s/it]2024-12-22 01:27:29,670 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:29,693 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Sydney
 40%|████      | 16/40 [01:09<01:43,  4.31s/it]2024-12-22 01:27:29,738 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:29,810 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:31,117 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:31,117 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1795])
2024-12-22 01:27:31,201 - [Process 3/5] - DEBUG - predict_token:tensor([[4522]], device='cuda:3')
2024-12-22 01:27:31,309 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Sa
 40%|████      | 16/40 [01:11<01:40,  4.18s/it]2024-12-22 01:27:31,505 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:32,848 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:32,848 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 01:27:32,920 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:27:33,281 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:250,000
 40%|████      | 16/40 [01:13<01:42,  4.25s/it]2024-12-22 01:27:33,342 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:33,342 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2213])
2024-12-22 01:27:33,365 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:33,365 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2089])
2024-12-22 01:27:33,376 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:33,376 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:27:33,408 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:27:33,439 - [Process 4/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:4')
2024-12-22 01:27:33,448 - [Process 0/5] - DEBUG - predict_token:tensor([[7942]], device='cuda:0')
2024-12-22 01:27:33,516 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 40%|████      | 16/40 [01:13<01:41,  4.22s/it]2024-12-22 01:27:33,574 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:33,590 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:López
 42%|████▎     | 17/40 [01:13<01:36,  4.18s/it]2024-12-22 01:27:33,770 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:33,774 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:33,850 - [Process 4/5] - INFO - res.shape is :torch.Size([9])
results:


(Insert answer here)
 40%|████      | 16/40 [01:13<01:42,  4.28s/it]2024-12-22 01:27:34,126 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:35,150 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:35,150 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:27:35,225 - [Process 3/5] - DEBUG - predict_token:tensor([[16552]], device='cuda:3')
2024-12-22 01:27:35,503 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Loïc Duval
 42%|████▎     | 17/40 [01:15<01:36,  4.18s/it]2024-12-22 01:27:35,788 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:37,226 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:37,226 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:27:37,300 - [Process 2/5] - DEBUG - predict_token:tensor([[341]], device='cuda:2')
2024-12-22 01:27:37,450 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Miami
 42%|████▎     | 17/40 [01:17<01:37,  4.23s/it]2024-12-22 01:27:37,469 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:37,469 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2159])
2024-12-22 01:27:37,517 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:37,517 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1962])
2024-12-22 01:27:37,538 - [Process 0/5] - DEBUG - predict_token:tensor([[11546]], device='cuda:0')
2024-12-22 01:27:37,597 - [Process 1/5] - DEBUG - predict_token:tensor([[940]], device='cuda:1')
2024-12-22 01:27:37,707 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:37,767 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:37,767 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1918])
2024-12-22 01:27:37,789 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Washington State
 42%|████▎     | 17/40 [01:17<01:37,  4.24s/it]2024-12-22 01:27:37,800 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Ronald Reagan
 45%|████▌     | 18/40 [01:17<01:32,  4.19s/it]2024-12-22 01:27:37,848 - [Process 4/5] - DEBUG - predict_token:tensor([[20799]], device='cuda:4')
2024-12-22 01:27:37,955 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:38,064 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:38,211 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:How to Train Your Dragon 2
 42%|████▎     | 17/40 [01:18<01:38,  4.30s/it]2024-12-22 01:27:38,468 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:39,583 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:39,583 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1958])
2024-12-22 01:27:39,667 - [Process 3/5] - DEBUG - predict_token:tensor([[317]], device='cuda:3')
2024-12-22 01:27:39,818 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Plato
 45%|████▌     | 18/40 [01:19<01:32,  4.22s/it]2024-12-22 01:27:40,104 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:41,447 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:41,448 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1933])
2024-12-22 01:27:41,528 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:27:41,657 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:41,657 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1959])
2024-12-22 01:27:41,738 - [Process 0/5] - DEBUG - predict_token:tensor([[379]], device='cuda:0')
2024-12-22 01:27:41,762 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:1931
 45%|████▌     | 18/40 [01:21<01:33,  4.25s/it]2024-12-22 01:27:41,872 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:41,872 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:27:41,953 - [Process 1/5] - DEBUG - predict_token:tensor([[383]], device='cuda:1')
2024-12-22 01:27:41,982 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:42,041 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:
Back to the Water Below
 48%|████▊     | 19/40 [01:22<01:28,  4.21s/it]2024-12-22 01:27:42,104 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:San Diego
 45%|████▌     | 18/40 [01:22<01:33,  4.26s/it]2024-12-22 01:27:42,206 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:42,211 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:42,212 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1965])
2024-12-22 01:27:42,291 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:27:42,380 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:42,655 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:
Please provide the answer only.
 45%|████▌     | 18/40 [01:22<01:35,  4.34s/it]2024-12-22 01:27:42,868 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:43,864 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:43,864 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1957])
2024-12-22 01:27:43,944 - [Process 3/5] - DEBUG - predict_token:tensor([[20891]], device='cuda:3')
2024-12-22 01:27:44,519 - [Process 3/5] - INFO - res.shape is :torch.Size([13])
results:Bear Grylls is originally from the UK.
 48%|████▊     | 19/40 [01:24<01:31,  4.37s/it]2024-12-22 01:27:44,795 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:45,712 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:45,713 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1961])
2024-12-22 01:27:45,793 - [Process 2/5] - DEBUG - predict_token:tensor([[435]], device='cuda:2')
2024-12-22 01:27:45,953 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:45,953 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 01:27:45,987 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:45,987 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1887])
2024-12-22 01:27:46,027 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Marlon Brando
 48%|████▊     | 19/40 [01:26<01:29,  4.26s/it]2024-12-22 01:27:46,033 - [Process 0/5] - DEBUG - predict_token:tensor([[838]], device='cuda:0')
2024-12-22 01:27:46,069 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:27:46,257 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Bob Dylan
 50%|█████     | 20/40 [01:26<01:24,  4.21s/it]2024-12-22 01:27:46,304 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:2014
 48%|████▊     | 19/40 [01:26<01:29,  4.24s/it]2024-12-22 01:27:46,305 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:46,399 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:46,551 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:46,596 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:46,597 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1938])
2024-12-22 01:27:46,676 - [Process 4/5] - DEBUG - predict_token:tensor([[7870]], device='cuda:4')
2024-12-22 01:27:46,911 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Georgia Brown
 48%|████▊     | 19/40 [01:26<01:30,  4.32s/it]2024-12-22 01:27:47,190 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:48,450 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:48,451 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 01:27:48,525 - [Process 3/5] - DEBUG - predict_token:tensor([[23052]], device='cuda:3')
2024-12-22 01:27:48,803 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Jerry Garcia
 50%|█████     | 20/40 [01:28<01:26,  4.34s/it]2024-12-22 01:27:49,045 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:49,976 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:49,976 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:27:50,049 - [Process 0/5] - DEBUG - predict_token:tensor([[21989]], device='cuda:0')
2024-12-22 01:27:50,100 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:50,101 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:27:50,176 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:50,177 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 01:27:50,181 - [Process 2/5] - DEBUG - predict_token:tensor([[8989]], device='cuda:2')
2024-12-22 01:27:50,249 - [Process 1/5] - DEBUG - predict_token:tensor([[8317]], device='cuda:1')
2024-12-22 01:27:50,353 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Cartoon Cartoon Fridays
 52%|█████▎    | 21/40 [01:30<01:19,  4.18s/it]2024-12-22 01:27:50,513 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:50,834 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:50,835 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1878])
2024-12-22 01:27:50,885 - [Process 2/5] - INFO - res.shape is :torch.Size([16])
results:Major-General Sir Miles Dighton-Rayner
 50%|█████     | 20/40 [01:30<01:28,  4.44s/it]2024-12-22 01:27:50,916 - [Process 4/5] - DEBUG - predict_token:tensor([[2864]], device='cuda:4')
2024-12-22 01:27:50,953 - [Process 1/5] - INFO - res.shape is :torch.Size([16])
results:Elephants are not connected to Gajabrishta.
 50%|█████     | 20/40 [01:30<01:27,  4.36s/it]2024-12-22 01:27:51,115 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:
Editors
 50%|█████     | 20/40 [01:31<01:25,  4.28s/it]2024-12-22 01:27:51,176 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:51,242 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:51,395 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:52,701 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:52,702 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1785])
2024-12-22 01:27:52,786 - [Process 3/5] - DEBUG - predict_token:tensor([[2688]], device='cuda:3')
2024-12-22 01:27:54,090 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:54,091 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2111])
2024-12-22 01:27:54,162 - [Process 0/5] - DEBUG - predict_token:tensor([[22392]], device='cuda:0')
2024-12-22 01:27:54,173 - [Process 3/5] - INFO - res.shape is :torch.Size([32])
results:Lavinia Greenlaw and Nâzım Hikmet are both course tutors at the University of East Anglia's Creative Writing Cour
 52%|█████▎    | 21/40 [01:34<01:28,  4.65s/it]2024-12-22 01:27:54,445 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:54,544 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:Rancho Cucamonga
 55%|█████▌    | 22/40 [01:34<01:15,  4.18s/it]2024-12-22 01:27:54,658 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:54,799 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:54,799 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2083])
2024-12-22 01:27:54,869 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:54,870 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 01:27:54,871 - [Process 2/5] - DEBUG - predict_token:tensor([[2726]], device='cuda:2')
2024-12-22 01:27:54,941 - [Process 1/5] - DEBUG - predict_token:tensor([[3082]], device='cuda:1')
2024-12-22 01:27:55,049 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:American
 52%|█████▎    | 21/40 [01:35<01:21,  4.28s/it]2024-12-22 01:27:55,062 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Des Moines
 52%|█████▎    | 21/40 [01:35<01:22,  4.36s/it]2024-12-22 01:27:55,204 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:55,205 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2020])
2024-12-22 01:27:55,243 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:55,285 - [Process 4/5] - DEBUG - predict_token:tensor([[405]], device='cuda:4')
2024-12-22 01:27:55,339 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:55,481 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Nanyue
 52%|█████▎    | 21/40 [01:35<01:21,  4.31s/it]2024-12-22 01:27:55,681 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:58,211 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:58,211 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-22 01:27:58,291 - [Process 3/5] - DEBUG - predict_token:tensor([[13397]], device='cuda:3')
2024-12-22 01:27:58,350 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:58,350 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-22 01:27:58,429 - [Process 0/5] - DEBUG - predict_token:tensor([[10920]], device='cuda:0')
2024-12-22 01:27:58,527 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Franco Dragone
 55%|█████▌    | 22/40 [01:38<01:22,  4.56s/it]2024-12-22 01:27:58,652 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Jones Beach Island
 57%|█████▊    | 23/40 [01:38<01:10,  4.16s/it]2024-12-22 01:27:58,798 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:58,814 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:58,868 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:58,869 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 01:27:58,940 - [Process 1/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:1')
2024-12-22 01:27:58,969 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:58,969 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1790])
2024-12-22 01:27:59,048 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:No
 55%|█████▌    | 22/40 [01:39<01:15,  4.20s/it]2024-12-22 01:27:59,053 - [Process 2/5] - DEBUG - predict_token:tensor([[5011]], device='cuda:2')
2024-12-22 01:27:59,245 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:William III
 55%|█████▌    | 22/40 [01:39<01:17,  4.31s/it]2024-12-22 01:27:59,313 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:27:59,313 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1814])
2024-12-22 01:27:59,316 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:59,396 - [Process 4/5] - DEBUG - predict_token:tensor([[26132]], device='cuda:4')
2024-12-22 01:27:59,487 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:27:59,677 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Gudmund Alfson
 55%|█████▌    | 22/40 [01:39<01:16,  4.27s/it]2024-12-22 01:27:59,915 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:02,379 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:02,380 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 01:28:02,454 - [Process 0/5] - DEBUG - predict_token:tensor([[4581]], device='cuda:0')
2024-12-22 01:28:02,513 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:02,513 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 01:28:02,586 - [Process 3/5] - DEBUG - predict_token:tensor([[15247]], device='cuda:3')
2024-12-22 01:28:02,716 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Jonathan Katz
 60%|██████    | 24/40 [01:42<01:06,  4.13s/it]2024-12-22 01:28:02,821 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Purdue University
 57%|█████▊    | 23/40 [01:42<01:16,  4.48s/it]2024-12-22 01:28:02,876 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:03,078 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:03,079 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1922])
2024-12-22 01:28:03,093 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:03,161 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:28:03,187 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:03,187 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 01:28:03,261 - [Process 2/5] - DEBUG - predict_token:tensor([[350]], device='cuda:2')
2024-12-22 01:28:03,396 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:2005
 57%|█████▊    | 23/40 [01:43<01:12,  4.24s/it]2024-12-22 01:28:03,552 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:03,552 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 01:28:03,581 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:


Baby Girl
 57%|█████▊    | 23/40 [01:43<01:13,  4.32s/it]2024-12-22 01:28:03,625 - [Process 4/5] - DEBUG - predict_token:tensor([[26132]], device='cuda:4')
2024-12-22 01:28:03,653 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:03,806 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:03,952 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:University of South Dakota
 57%|█████▊    | 23/40 [01:44<01:12,  4.27s/it]2024-12-22 01:28:04,196 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:06,461 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:06,461 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:28:06,533 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:28:06,758 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:06,759 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:28:06,833 - [Process 3/5] - DEBUG - predict_token:tensor([[2259]], device='cuda:3')
2024-12-22 01:28:06,915 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:It's Always Sunny in Philadelphia
 62%|██████▎   | 25/40 [01:47<01:02,  4.15s/it]2024-12-22 01:28:07,048 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:07,068 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Philip Carlo
 60%|██████    | 24/40 [01:47<01:10,  4.41s/it]2024-12-22 01:28:07,352 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:07,420 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:07,420 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2127])
2024-12-22 01:28:07,490 - [Process 1/5] - DEBUG - predict_token:tensor([[25281]], device='cuda:1')
2024-12-22 01:28:07,502 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:07,503 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 01:28:07,579 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:28:07,767 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Pamela Adlon
 60%|██████    | 24/40 [01:47<01:08,  4.28s/it]2024-12-22 01:28:07,856 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:20006
 60%|██████    | 24/40 [01:47<01:08,  4.30s/it]2024-12-22 01:28:07,870 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:07,870 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:28:07,946 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:28:08,047 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:08,056 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 60%|██████    | 24/40 [01:48<01:07,  4.22s/it]2024-12-22 01:28:08,149 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:08,308 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:10,848 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:10,848 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2126])
2024-12-22 01:28:10,922 - [Process 0/5] - DEBUG - predict_token:tensor([[10924]], device='cuda:0')
2024-12-22 01:28:11,006 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:11,006 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:28:11,081 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:28:11,266 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:

1500th
 65%|██████▌   | 26/40 [01:51<00:58,  4.21s/it]2024-12-22 01:28:11,274 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:351
 62%|██████▎   | 25/40 [01:51<01:05,  4.35s/it]2024-12-22 01:28:11,437 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:11,551 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:11,769 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:11,769 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 01:28:11,817 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:11,817 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1950])
2024-12-22 01:28:11,843 - [Process 2/5] - DEBUG - predict_token:tensor([[438]], device='cuda:2')
2024-12-22 01:28:11,899 - [Process 1/5] - DEBUG - predict_token:tensor([[26182]], device='cuda:1')
2024-12-22 01:28:12,035 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Santería
 62%|██████▎   | 25/40 [01:52<01:03,  4.27s/it]2024-12-22 01:28:12,060 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:12,060 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2146])
2024-12-22 01:28:12,092 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:British
 62%|██████▎   | 25/40 [01:52<01:04,  4.29s/it]2024-12-22 01:28:12,129 - [Process 4/5] - DEBUG - predict_token:tensor([[22292]], device='cuda:4')
2024-12-22 01:28:12,325 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:12,366 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:12,413 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Matthew Good Band
 62%|██████▎   | 25/40 [01:52<01:03,  4.26s/it]2024-12-22 01:28:12,689 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:15,195 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:15,196 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:28:15,276 - [Process 0/5] - DEBUG - predict_token:tensor([[13899]], device='cuda:0')
2024-12-22 01:28:15,391 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:15,391 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 01:28:15,463 - [Process 3/5] - DEBUG - predict_token:tensor([[8432]], device='cuda:3')
2024-12-22 01:28:15,538 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Ribosomes
 68%|██████▊   | 27/40 [01:55<00:54,  4.23s/it]2024-12-22 01:28:15,669 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:15,782 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:
(Insert answer here)
 65%|██████▌   | 26/40 [01:55<01:01,  4.40s/it]2024-12-22 01:28:15,960 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:15,961 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2057])
2024-12-22 01:28:16,034 - [Process 2/5] - DEBUG - predict_token:tensor([[3082]], device='cuda:2')
2024-12-22 01:28:16,056 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:16,117 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:16,117 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1944])
2024-12-22 01:28:16,142 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:American
 65%|██████▌   | 26/40 [01:56<00:59,  4.22s/it]2024-12-22 01:28:16,197 - [Process 1/5] - DEBUG - predict_token:tensor([[13645]], device='cuda:1')
2024-12-22 01:28:16,429 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:16,449 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:16,449 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1942])
2024-12-22 01:28:16,517 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Vernon L. Smith
 65%|██████▌   | 26/40 [01:56<01:00,  4.33s/it]2024-12-22 01:28:16,531 - [Process 4/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:4')
2024-12-22 01:28:16,640 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:No
 65%|██████▌   | 26/40 [01:56<00:59,  4.25s/it]2024-12-22 01:28:16,768 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:16,910 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:19,488 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:19,489 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1744])
2024-12-22 01:28:19,579 - [Process 0/5] - DEBUG - predict_token:tensor([[349]], device='cuda:0')
2024-12-22 01:28:19,712 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:19,713 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:28:19,761 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Dracula
 70%|███████   | 28/40 [01:59<00:50,  4.23s/it]2024-12-22 01:28:19,787 - [Process 3/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:3')
2024-12-22 01:28:19,861 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:19,895 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:No
 68%|██████▊   | 27/40 [01:59<00:56,  4.31s/it]2024-12-22 01:28:20,148 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:20,192 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:20,192 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1936])
2024-12-22 01:28:20,274 - [Process 2/5] - DEBUG - predict_token:tensor([[28267]], device='cuda:2')
2024-12-22 01:28:20,436 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:20,436 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1912])
2024-12-22 01:28:20,518 - [Process 1/5] - DEBUG - predict_token:tensor([[897]], device='cuda:1')
2024-12-22 01:28:20,659 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:20,659 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1982])
2024-12-22 01:28:20,712 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Deftones
 68%|██████▊   | 27/40 [02:00<00:55,  4.29s/it]2024-12-22 01:28:20,740 - [Process 4/5] - DEBUG - predict_token:tensor([[435]], device='cuda:4')
2024-12-22 01:28:20,951 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:20,990 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Jake Kasdan
 68%|██████▊   | 27/40 [02:01<00:55,  4.28s/it]2024-12-22 01:28:21,037 - [Process 2/5] - INFO - res.shape is :torch.Size([17])
results:Fredric Rieders testified against Randall D. Swango.
 68%|██████▊   | 27/40 [02:01<00:57,  4.42s/it]2024-12-22 01:28:21,217 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:21,321 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:23,417 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:23,418 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1839])
2024-12-22 01:28:23,497 - [Process 0/5] - DEBUG - predict_token:tensor([[5158]], device='cuda:0')
2024-12-22 01:28:23,797 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Band-e Amir
 72%|███████▎  | 29/40 [02:03<00:45,  4.17s/it]2024-12-22 01:28:23,932 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:23,933 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1932])
2024-12-22 01:28:23,964 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:24,015 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:28:24,250 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:2008
 70%|███████   | 28/40 [02:04<00:51,  4.32s/it]2024-12-22 01:28:24,481 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:24,590 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:24,590 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2094])
2024-12-22 01:28:24,662 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:28:24,848 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:24,848 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:28:24,898 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:3976
 70%|███████   | 28/40 [02:04<00:51,  4.26s/it]2024-12-22 01:28:24,923 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:28:24,955 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:24,956 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1768])
2024-12-22 01:28:25,040 - [Process 2/5] - DEBUG - predict_token:tensor([[2259]], device='cuda:2')
2024-12-22 01:28:25,150 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:25,257 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:

150 m
 70%|███████   | 28/40 [02:05<00:51,  4.28s/it]2024-12-22 01:28:25,332 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Juan Rulfo
 70%|███████   | 28/40 [02:05<00:52,  4.38s/it]2024-12-22 01:28:25,498 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:25,627 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:27,655 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:27,656 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1964])
2024-12-22 01:28:27,735 - [Process 0/5] - DEBUG - predict_token:tensor([[3148]], device='cuda:0')
2024-12-22 01:28:27,958 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Utah State
 75%|███████▌  | 30/40 [02:08<00:41,  4.17s/it]2024-12-22 01:28:28,131 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:28,131 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:28:28,132 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:28,205 - [Process 3/5] - DEBUG - predict_token:tensor([[20290]], device='cuda:3')
2024-12-22 01:28:28,611 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:Cortina d'Ampezzo
 72%|███████▎  | 29/40 [02:08<00:47,  4.34s/it]2024-12-22 01:28:28,799 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:28,800 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 01:28:28,874 - [Process 1/5] - DEBUG - predict_token:tensor([[7513]], device='cuda:1')
2024-12-22 01:28:28,885 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:29,025 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:India
 72%|███████▎  | 29/40 [02:09<00:46,  4.22s/it]2024-12-22 01:28:29,135 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:29,135 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:28:29,210 - [Process 4/5] - DEBUG - predict_token:tensor([[14320]], device='cuda:4')
2024-12-22 01:28:29,292 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:29,292 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2202])
2024-12-22 01:28:29,303 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:29,357 - [Process 2/5] - DEBUG - predict_token:tensor([[4702]], device='cuda:2')
2024-12-22 01:28:29,653 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Merck & Co.
 72%|███████▎  | 29/40 [02:09<00:48,  4.36s/it]2024-12-22 01:28:29,908 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:29,984 - [Process 4/5] - INFO - res.shape is :torch.Size([17])
results:Bangor Daily News is not talking about Sawin Millett.
 72%|███████▎  | 29/40 [02:10<00:48,  4.41s/it]2024-12-22 01:28:30,251 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:31,836 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:31,836 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1931])
2024-12-22 01:28:31,917 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:28:32,259 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:
Please provide the answer only.
 78%|███████▊  | 31/40 [02:12<00:37,  4.21s/it]2024-12-22 01:28:32,424 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:32,722 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:32,722 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 01:28:32,804 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:28:32,936 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:32,936 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:28:33,010 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:28:33,041 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:1970
 75%|███████▌  | 30/40 [02:13<00:43,  4.36s/it]2024-12-22 01:28:33,121 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 75%|███████▌  | 30/40 [02:13<00:41,  4.18s/it]2024-12-22 01:28:33,328 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:33,397 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:33,651 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:33,652 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 01:28:33,733 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:28:33,967 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:1985
 75%|███████▌  | 30/40 [02:14<00:43,  4.35s/it]2024-12-22 01:28:34,005 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:34,005 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2168])
2024-12-22 01:28:34,075 - [Process 4/5] - DEBUG - predict_token:tensor([[11546]], device='cuda:4')
2024-12-22 01:28:34,264 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:34,359 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Miranda Garrison
 75%|███████▌  | 30/40 [02:14<00:44,  4.40s/it]2024-12-22 01:28:34,631 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:36,124 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:36,125 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1970])
2024-12-22 01:28:36,204 - [Process 0/5] - DEBUG - predict_token:tensor([[319]], device='cuda:0')
2024-12-22 01:28:36,467 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Henrik Fisker
 80%|████████  | 32/40 [02:16<00:33,  4.21s/it]2024-12-22 01:28:36,622 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:36,996 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:36,997 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1776])
2024-12-22 01:28:37,028 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:37,028 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:28:37,081 - [Process 3/5] - DEBUG - predict_token:tensor([[5176]], device='cuda:3')
2024-12-22 01:28:37,103 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:28:37,285 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:French.
 78%|███████▊  | 31/40 [02:17<00:38,  4.33s/it]2024-12-22 01:28:37,526 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:13 October 1733
 78%|███████▊  | 31/40 [02:17<00:38,  4.25s/it]2024-12-22 01:28:37,581 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:37,812 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:37,963 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:37,964 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 01:28:38,037 - [Process 2/5] - DEBUG - predict_token:tensor([[15431]], device='cuda:2')
2024-12-22 01:28:38,229 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Governor
 78%|███████▊  | 31/40 [02:18<00:38,  4.32s/it]2024-12-22 01:28:38,260 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:38,260 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 01:28:38,335 - [Process 4/5] - DEBUG - predict_token:tensor([[498]], device='cuda:4')
2024-12-22 01:28:38,489 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Thames
 78%|███████▊  | 31/40 [02:18<00:38,  4.32s/it]2024-12-22 01:28:38,518 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:38,684 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:40,340 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:40,340 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2143])
2024-12-22 01:28:40,409 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:28:40,711 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:12,982
 82%|████████▎ | 33/40 [02:20<00:29,  4.22s/it]2024-12-22 01:28:40,876 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:41,279 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:41,279 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1914])
2024-12-22 01:28:41,361 - [Process 3/5] - DEBUG - predict_token:tensor([[3014]], device='cuda:3')
2024-12-22 01:28:41,513 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Michael Werner
 80%|████████  | 32/40 [02:21<00:34,  4.30s/it]2024-12-22 01:28:41,571 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:41,571 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1976])
2024-12-22 01:28:41,651 - [Process 1/5] - DEBUG - predict_token:tensor([[5845]], device='cuda:1')
2024-12-22 01:28:41,791 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:41,977 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Philip K. Dick
 80%|████████  | 32/40 [02:22<00:34,  4.31s/it]2024-12-22 01:28:42,194 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:42,324 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:42,324 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 01:28:42,405 - [Process 2/5] - DEBUG - predict_token:tensor([[17044]], device='cuda:2')
2024-12-22 01:28:42,463 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:42,463 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2185])
2024-12-22 01:28:42,532 - [Process 4/5] - DEBUG - predict_token:tensor([[2178]], device='cuda:4')
2024-12-22 01:28:42,691 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Allure
2024-12-22 01:28:42,691 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
 80%|████████  | 32/40 [02:22<00:34,  4.28s/it]results:Alice in Wonderland
 80%|████████  | 32/40 [02:22<00:34,  4.36s/it]2024-12-22 01:28:42,952 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:42,988 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:44,644 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:44,644 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:28:44,724 - [Process 0/5] - DEBUG - predict_token:tensor([[323]], device='cuda:0')
2024-12-22 01:28:44,948 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Taoiseach
 85%|████████▌ | 34/40 [02:25<00:25,  4.22s/it]2024-12-22 01:28:45,099 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:45,642 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:45,642 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2138])
2024-12-22 01:28:45,715 - [Process 3/5] - DEBUG - predict_token:tensor([[15733]], device='cuda:3')
2024-12-22 01:28:45,960 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:45,960 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2149])
2024-12-22 01:28:45,993 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Brian Stokes Mitchell
 82%|████████▎ | 33/40 [02:26<00:30,  4.35s/it]2024-12-22 01:28:46,030 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:28:46,250 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:46,273 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:7734
 82%|████████▎ | 33/40 [02:26<00:30,  4.31s/it]2024-12-22 01:28:46,521 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:46,584 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:46,585 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:28:46,640 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:46,641 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:28:46,659 - [Process 4/5] - DEBUG - predict_token:tensor([[435]], device='cuda:4')
2024-12-22 01:28:46,715 - [Process 2/5] - DEBUG - predict_token:tensor([[5115]], device='cuda:2')
2024-12-22 01:28:46,907 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Jaleel White
 82%|████████▎ | 33/40 [02:26<00:29,  4.26s/it]2024-12-22 01:28:46,919 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Mongolia
 82%|████████▎ | 33/40 [02:26<00:30,  4.32s/it]2024-12-22 01:28:47,128 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:47,203 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:48,652 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:48,652 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1866])
2024-12-22 01:28:48,733 - [Process 0/5] - DEBUG - predict_token:tensor([[323]], device='cuda:0')
2024-12-22 01:28:48,876 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Troy
 88%|████████▊ | 35/40 [02:28<00:20,  4.14s/it]2024-12-22 01:28:49,043 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:50,105 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:50,105 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2143])
2024-12-22 01:28:50,178 - [Process 3/5] - DEBUG - predict_token:tensor([[365]], device='cuda:3')
2024-12-22 01:28:50,287 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:50,287 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2163])
2024-12-22 01:28:50,356 - [Process 1/5] - DEBUG - predict_token:tensor([[25343]], device='cuda:1')
2024-12-22 01:28:50,372 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Lionsgate
 85%|████████▌ | 34/40 [02:30<00:26,  4.36s/it]2024-12-22 01:28:50,583 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:50,599 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Capital Cities
 85%|████████▌ | 34/40 [02:30<00:25,  4.31s/it]2024-12-22 01:28:50,767 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:50,767 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 01:28:50,821 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:50,822 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:28:50,837 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:50,842 - [Process 4/5] - DEBUG - predict_token:tensor([[9459]], device='cuda:4')
2024-12-22 01:28:50,894 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:28:50,998 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Perth
 85%|████████▌ | 34/40 [02:31<00:25,  4.21s/it]2024-12-22 01:28:51,006 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 85%|████████▌ | 34/40 [02:31<00:25,  4.25s/it]2024-12-22 01:28:51,207 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:51,288 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:52,752 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:52,752 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-22 01:28:52,832 - [Process 0/5] - DEBUG - predict_token:tensor([[341]], device='cuda:0')
2024-12-22 01:28:53,095 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Mika Häkkinen
 90%|█████████ | 36/40 [02:33<00:16,  4.16s/it]2024-12-22 01:28:53,260 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:54,244 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:54,245 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:28:54,319 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:28:54,427 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 88%|████████▊ | 35/40 [02:34<00:21,  4.27s/it]2024-12-22 01:28:54,483 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:54,484 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1756])
2024-12-22 01:28:54,567 - [Process 1/5] - DEBUG - predict_token:tensor([[20549]], device='cuda:1')
2024-12-22 01:28:54,624 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:54,832 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:54,833 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 01:28:54,907 - [Process 2/5] - DEBUG - predict_token:tensor([[10537]], device='cuda:2')
2024-12-22 01:28:54,932 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:Morgan Llywelyn
 88%|████████▊ | 35/40 [02:34<00:21,  4.32s/it]2024-12-22 01:28:55,099 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Albert Park
 88%|████████▊ | 35/40 [02:35<00:21,  4.20s/it]2024-12-22 01:28:55,117 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:55,117 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2115])
2024-12-22 01:28:55,125 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:55,189 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:28:55,349 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:55,558 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:

(Insert answer here)
 88%|████████▊ | 35/40 [02:35<00:21,  4.32s/it]2024-12-22 01:28:55,792 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:56,841 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:56,841 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2090])
2024-12-22 01:28:56,913 - [Process 0/5] - DEBUG - predict_token:tensor([[4367]], device='cuda:0')
2024-12-22 01:28:57,495 - [Process 0/5] - INFO - res.shape is :torch.Size([14])
results:The Hunger Games: Mockingjay – Part 1
 92%|█████████▎| 37/40 [02:37<00:12,  4.23s/it]2024-12-22 01:28:57,647 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:58,401 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:58,401 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1947])
2024-12-22 01:28:58,481 - [Process 3/5] - DEBUG - predict_token:tensor([[5899]], device='cuda:3')
2024-12-22 01:28:58,631 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Louisville
 90%|█████████ | 36/40 [02:38<00:16,  4.25s/it]2024-12-22 01:28:58,900 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:58,900 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 01:28:58,912 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:58,976 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:28:58,978 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:58,978 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 01:28:59,050 - [Process 2/5] - DEBUG - predict_token:tensor([[319]], device='cuda:2')
2024-12-22 01:28:59,127 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:12
 90%|█████████ | 36/40 [02:39<00:17,  4.28s/it]2024-12-22 01:28:59,199 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Singer
 90%|█████████ | 36/40 [02:39<00:16,  4.17s/it]2024-12-22 01:28:59,390 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:59,441 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:28:59,447 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:28:59,447 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:28:59,521 - [Process 4/5] - DEBUG - predict_token:tensor([[7370]], device='cuda:4')
2024-12-22 01:28:59,757 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Live at the Electric
 90%|█████████ | 36/40 [02:39<00:17,  4.28s/it]2024-12-22 01:29:00,024 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:01,266 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:01,266 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2224])
2024-12-22 01:29:01,332 - [Process 0/5] - DEBUG - predict_token:tensor([[20708]], device='cuda:0')
2024-12-22 01:29:01,554 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Lev Ivanov
 95%|█████████▌| 38/40 [02:41<00:08,  4.18s/it]2024-12-22 01:29:01,727 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:02,562 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:02,562 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2029])
2024-12-22 01:29:02,636 - [Process 3/5] - DEBUG - predict_token:tensor([[5670]], device='cuda:3')
2024-12-22 01:29:02,871 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Captain Comic
 92%|█████████▎| 37/40 [02:42<00:12,  4.25s/it]2024-12-22 01:29:03,023 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:03,024 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2048])
2024-12-22 01:29:03,098 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:29:03,103 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:03,103 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2240])
2024-12-22 01:29:03,148 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:03,169 - [Process 2/5] - DEBUG - predict_token:tensor([[341]], device='cuda:2')
2024-12-22 01:29:03,378 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Oklahoma Sooners
 92%|█████████▎| 37/40 [02:43<00:12,  4.27s/it]2024-12-22 01:29:03,404 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Middletown
 92%|█████████▎| 37/40 [02:43<00:12,  4.18s/it]2024-12-22 01:29:03,651 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:03,684 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:03,853 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:03,853 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2120])
2024-12-22 01:29:03,926 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:29:04,162 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:1970
 92%|█████████▎| 37/40 [02:44<00:12,  4.32s/it]2024-12-22 01:29:04,341 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:05,315 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:05,316 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2083])
2024-12-22 01:29:05,388 - [Process 0/5] - DEBUG - predict_token:tensor([[14234]], device='cuda:0')
2024-12-22 01:29:05,610 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Southern Company
 98%|█████████▊| 39/40 [02:45<00:04,  4.14s/it]2024-12-22 01:29:05,773 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:06,885 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:06,885 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 01:29:06,962 - [Process 3/5] - DEBUG - predict_token:tensor([[4385]], device='cuda:3')
2024-12-22 01:29:07,197 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Mark Donohue
 95%|█████████▌| 38/40 [02:47<00:08,  4.27s/it]2024-12-22 01:29:07,421 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:07,422 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1931])
2024-12-22 01:29:07,429 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:07,429 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:29:07,429 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:07,498 - [Process 2/5] - DEBUG - predict_token:tensor([[9511]], device='cuda:2')
2024-12-22 01:29:07,504 - [Process 1/5] - DEBUG - predict_token:tensor([[12126]], device='cuda:1')
2024-12-22 01:29:07,946 - [Process 2/5] - INFO - res.shape is :torch.Size([10])
results:The answer is: Richard Eichberg.
 95%|█████████▌| 38/40 [02:48<00:08,  4.29s/it]2024-12-22 01:29:08,185 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:08,185 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2129])
2024-12-22 01:29:08,192 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:08,258 - [Process 4/5] - DEBUG - predict_token:tensor([[4114]], device='cuda:4')
2024-12-22 01:29:08,337 - [Process 1/5] - INFO - res.shape is :torch.Size([19])
results:Ireland, Scotland, Wales, Cornwall, Brittany, and the Netherlands.
 95%|█████████▌| 38/40 [02:48<00:08,  4.48s/it]2024-12-22 01:29:08,494 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Karl Bergmann
 95%|█████████▌| 38/40 [02:48<00:08,  4.32s/it]2024-12-22 01:29:08,534 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:08,777 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:09,362 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:09,362 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2083])
2024-12-22 01:29:09,434 - [Process 0/5] - DEBUG - predict_token:tensor([[826]], device='cuda:0')
2024-12-22 01:29:09,857 - [Process 0/5] - INFO - res.shape is :torch.Size([10])
results:Around the World in 80 Days
100%|██████████| 40/40 [02:49<00:00,  4.17s/it]100%|██████████| 40/40 [02:49<00:00,  4.25s/it]
2024-12-22 01:29:11,261 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:11,261 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1986])
2024-12-22 01:29:11,343 - [Process 3/5] - DEBUG - predict_token:tensor([[21987]], device='cuda:3')
2024-12-22 01:29:11,536 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Mass Effect
 98%|█████████▊| 39/40 [02:51<00:04,  4.29s/it]2024-12-22 01:29:11,754 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:12,000 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:12,001 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2123])
2024-12-22 01:29:12,073 - [Process 2/5] - DEBUG - predict_token:tensor([[399]], device='cuda:2')
2024-12-22 01:29:12,173 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:12,174 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 01:29:12,248 - [Process 1/5] - DEBUG - predict_token:tensor([[390]], device='cuda:1')
2024-12-22 01:29:12,264 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:WAMC
 98%|█████████▊| 39/40 [02:52<00:04,  4.30s/it]2024-12-22 01:29:12,391 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:12,391 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1832])
2024-12-22 01:29:12,398 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:BNC
 98%|█████████▊| 39/40 [02:52<00:04,  4.35s/it]2024-12-22 01:29:12,473 - [Process 4/5] - DEBUG - predict_token:tensor([[2259]], device='cuda:4')
2024-12-22 01:29:12,541 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:12,618 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:12,796 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:
(Insert answer here)
 98%|█████████▊| 39/40 [02:52<00:04,  4.32s/it]2024-12-22 01:29:12,992 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:29:15,438 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:15,438 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2061])
2024-12-22 01:29:15,513 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:29:15,748 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Vetrimaaran
100%|██████████| 40/40 [02:55<00:00,  4.27s/it]100%|██████████| 40/40 [02:55<00:00,  4.39s/it]
2024-12-22 01:29:16,250 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:16,251 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:29:16,273 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:16,273 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1722])
2024-12-22 01:29:16,325 - [Process 1/5] - DEBUG - predict_token:tensor([[2443]], device='cuda:1')
2024-12-22 01:29:16,364 - [Process 2/5] - DEBUG - predict_token:tensor([[3122]], device='cuda:2')
2024-12-22 01:29:16,560 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Wicked Twister
100%|██████████| 40/40 [02:56<00:00,  4.30s/it]100%|██████████| 40/40 [02:56<00:00,  4.41s/it]
2024-12-22 01:29:16,656 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:29:16,657 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2060])
2024-12-22 01:29:16,727 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:
Please provide the answer only.
100%|██████████| 40/40 [02:56<00:00,  4.35s/it]100%|██████████| 40/40 [02:56<00:00,  4.42s/it]
2024-12-22 01:29:16,731 - [Process 4/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:4')
2024-12-22 01:29:16,838 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:No
100%|██████████| 40/40 [02:56<00:00,  4.23s/it]100%|██████████| 40/40 [02:56<00:00,  4.42s/it]
2024-12-22 01:29:16,875 - [Process 4/5] - DEBUG - datasets_name:hotpotqa
2024-12-22 01:29:16,875 - [Process 2/5] - DEBUG - datasets_name:hotpotqa
2024-12-22 01:29:16,875 - [Process 3/5] - DEBUG - datasets_name:hotpotqa
2024-12-22 01:29:16,875 - [Process 1/5] - DEBUG - datasets_name:hotpotqa
2024-12-22 01:29:16,875 - [Process 0/5] - DEBUG - datasets_name:hotpotqa
Running evaluation for dataset: 2wikimqa
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]n_windows:[2]

model_name:/mnt/Data/xiongjing/llama2chatmodel_name:/mnt/Data/xiongjing/llama2chat

multi_gpus:Truemulti_gpus:True

torch.cuda.device_count():5torch.cuda.device_count():5

n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.37s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.77s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:31:22,630 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:31:22,631 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:31:22,631 - [Process 0/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:31:22,635 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:31:22,636 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:31:22,636 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里!!!!!!!!!!!!!!!!!!!!!!!! 这里

2024-12-22 01:31:22,649 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 01:31:22,649 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:31:22,649 - [Process 2/5] - INFO - output_max_len: 32
2024-12-22 01:31:22,649 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:31:22,649 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 01:31:22,650 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:31:22,650 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:31:22,650 - [Process 3/5] - INFO - output_max_len: 32
2024-12-22 01:31:22,650 - [Process 1/5] - INFO - output_max_len: 32
2024-12-22 01:31:22,656 - [Process 0/5] - INFO - Max Length is 11950
2024-12-22 01:31:22,656 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:31:22,657 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:31:22,677 - [Process 4/5] - INFO - Max Length is 11950
2024-12-22 01:31:22,678 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:31:22,678 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:31:22,691 - [Process 2/5] - INFO - Max Length is 11950
2024-12-22 01:31:22,691 - [Process 1/5] - INFO - Max Length is 11950
2024-12-22 01:31:22,691 - [Process 3/5] - INFO - Max Length is 11950
2024-12-22 01:31:22,691 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:31:22,691 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:31:22,692 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:31:22,692 - [Process 2/5] - INFO - get_predicted begin
2024-12-22 01:31:22,692 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 01:31:22,692 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:31:27,412 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:27,493 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:27,497 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:27,497 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:27,497 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:31,669 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:31,669 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2158])
2024-12-22 01:31:31,738 - [Process 1/5] - DEBUG - predict_token:tensor([[6286]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:31:31,759 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:31,759 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1870])
2024-12-22 01:31:31,805 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:31,805 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 01:31:31,805 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:31,806 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2056])
2024-12-22 01:31:31,838 - [Process 4/5] - DEBUG - predict_token:tensor([[9932]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:31:31,878 - [Process 2/5] - DEBUG - predict_token:tensor([[12001]], device='cuda:2')
2024-12-22 01:31:31,878 - [Process 0/5] - DEBUG - predict_token:tensor([[12842]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:31:31,922 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Melun
  2%|▎         | 1/40 [00:09<05:59,  9.23s/it]2024-12-22 01:31:31,952 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:31,953 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1957])
2024-12-22 01:31:32,028 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:32,032 - [Process 3/5] - DEBUG - predict_token:tensor([[1570]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:31:32,181 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Heather D. Gibson
  2%|▎         | 1/40 [00:09<06:10,  9.50s/it]2024-12-22 01:31:32,240 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Bert Grund
  2%|▎         | 1/40 [00:09<06:12,  9.55s/it]2024-12-22 01:31:32,265 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:Baraguru Ramachandrappa
  2%|▎         | 1/40 [00:09<06:13,  9.57s/it]2024-12-22 01:31:32,347 - [Process 0/5] - INFO - res.shape is :torch.Size([10])
results:Ozalj, present day Croatia
  2%|▎         | 1/40 [00:09<06:17,  9.69s/it]2024-12-22 01:31:32,379 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:32,447 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:32,480 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:32,563 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:35,592 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:35,593 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:31:35,664 - [Process 1/5] - DEBUG - predict_token:tensor([[8490]], device='cuda:1')
2024-12-22 01:31:36,007 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:Tex And The Lord Of The Deep
  5%|▌         | 2/40 [00:13<03:55,  6.20s/it]2024-12-22 01:31:36,088 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:36,088 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2212])
2024-12-22 01:31:36,105 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:36,105 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1992])
2024-12-22 01:31:36,154 - [Process 3/5] - DEBUG - predict_token:tensor([[323]], device='cuda:3')
2024-12-22 01:31:36,155 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:36,156 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:36,156 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2160])
2024-12-22 01:31:36,184 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:31:36,224 - [Process 2/5] - DEBUG - predict_token:tensor([[5953]], device='cuda:2')
2024-12-22 01:31:36,243 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:36,243 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2174])
2024-12-22 01:31:36,311 - [Process 0/5] - DEBUG - predict_token:tensor([[27415]], device='cuda:0')
2024-12-22 01:31:36,433 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:1813
  5%|▌         | 2/40 [00:13<04:03,  6.41s/it]2024-12-22 01:31:36,485 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Tiger In The Smoke
  5%|▌         | 2/40 [00:13<04:04,  6.43s/it]2024-12-22 01:31:36,514 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
2024-12-22 01:31:36,514 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Tulasi
results:Det Sande Ansigt
  5%|▌         | 2/40 [00:13<04:04,  6.44s/it]  5%|▌         | 2/40 [00:13<04:04,  6.44s/it]2024-12-22 01:31:36,689 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:36,689 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:36,733 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:36,735 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:39,742 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:39,742 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:31:39,816 - [Process 1/5] - DEBUG - predict_token:tensor([[13706]], device='cuda:1')
2024-12-22 01:31:39,998 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:United Kingdom
  8%|▊         | 3/40 [00:17<03:12,  5.19s/it]2024-12-22 01:31:40,125 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:40,282 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:40,282 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2054])
2024-12-22 01:31:40,292 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:40,293 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:31:40,321 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:40,321 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1878])
2024-12-22 01:31:40,354 - [Process 0/5] - DEBUG - predict_token:tensor([[3685]], device='cuda:0')
2024-12-22 01:31:40,364 - [Process 3/5] - DEBUG - predict_token:tensor([[7513]], device='cuda:3')
2024-12-22 01:31:40,381 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:40,381 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1940])
2024-12-22 01:31:40,401 - [Process 2/5] - DEBUG - predict_token:tensor([[15733]], device='cuda:2')
2024-12-22 01:31:40,462 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:31:40,524 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:India
  8%|▊         | 3/40 [00:17<03:17,  5.34s/it]2024-12-22 01:31:40,647 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Il Gaucho
  8%|▊         | 3/40 [00:17<03:19,  5.39s/it]2024-12-22 01:31:40,734 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:Sam Spiegel Film and Television School
  8%|▊         | 3/40 [00:18<03:20,  5.43s/it]2024-12-22 01:31:40,744 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:40,842 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:The Yellow Teddy Bears
  8%|▊         | 3/40 [00:18<03:23,  5.50s/it]2024-12-22 01:31:40,886 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:40,976 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:41,044 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:43,718 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:43,718 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 01:31:43,792 - [Process 1/5] - DEBUG - predict_token:tensor([[365]], device='cuda:1')
2024-12-22 01:31:44,096 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:All-American Co-Ed
 10%|█         | 4/40 [00:21<02:51,  4.76s/it]2024-12-22 01:31:44,211 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:44,470 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:44,470 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:31:44,486 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:44,486 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1923])
2024-12-22 01:31:44,541 - [Process 2/5] - DEBUG - predict_token:tensor([[1522]], device='cuda:2')
2024-12-22 01:31:44,567 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:31:44,597 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:44,598 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 01:31:44,671 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:31:44,701 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:44,701 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 01:31:44,760 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:F The Prom
 10%|█         | 4/40 [00:22<02:56,  4.90s/it]2024-12-22 01:31:44,780 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:31:44,784 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 10%|█         | 4/40 [00:22<02:55,  4.88s/it]2024-12-22 01:31:44,897 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 10%|█         | 4/40 [00:22<02:57,  4.93s/it]2024-12-22 01:31:44,984 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:44,984 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:45,002 - [Process 2/5] - INFO - res.shape is :torch.Size([10])
results:Beaulieu-sur-Loire
 10%|█         | 4/40 [00:22<02:59,  4.98s/it]2024-12-22 01:31:45,107 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:45,191 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:47,816 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:47,817 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 01:31:47,890 - [Process 1/5] - DEBUG - predict_token:tensor([[17882]], device='cuda:1')
2024-12-22 01:31:48,073 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Argentina
 12%|█▎        | 5/40 [00:25<02:36,  4.48s/it]2024-12-22 01:31:48,202 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:48,583 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:48,583 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2061])
2024-12-22 01:31:48,656 - [Process 4/5] - DEBUG - predict_token:tensor([[17860]], device='cuda:4')
2024-12-22 01:31:48,729 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:48,729 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1941])
2024-12-22 01:31:48,810 - [Process 3/5] - DEBUG - predict_token:tensor([[323]], device='cuda:3')
2024-12-22 01:31:48,827 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:48,827 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 01:31:48,893 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:48,893 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1933])
2024-12-22 01:31:48,906 - [Process 0/5] - DEBUG - predict_token:tensor([[274]], device='cuda:0')
2024-12-22 01:31:48,973 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:31:49,147 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Tombstone Rashomon
 12%|█▎        | 5/40 [00:26<02:45,  4.72s/it]2024-12-22 01:31:49,157 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
2024-12-22 01:31:49,158 - [Process 4/5] - INFO - res.shape is :torch.Size([11])
results:1532
 12%|█▎        | 5/40 [00:26<02:44,  4.69s/it]results:Abdul Ali Lalu was born first.
 12%|█▎        | 5/40 [00:26<02:44,  4.70s/it]2024-12-22 01:31:49,371 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:49,376 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:49,383 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:49,437 - [Process 2/5] - INFO - res.shape is :torch.Size([10])
results:The Revolt Of The Praetorians
 12%|█▎        | 5/40 [00:26<02:47,  4.78s/it]2024-12-22 01:31:49,701 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:51,847 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:51,847 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2209])
2024-12-22 01:31:51,914 - [Process 1/5] - DEBUG - predict_token:tensor([[8317]], device='cuda:1')
2024-12-22 01:31:52,336 - [Process 1/5] - INFO - res.shape is :torch.Size([10])
results:Eleonore Marie of Anhalt.
 15%|█▌        | 6/40 [00:29<02:29,  4.41s/it]2024-12-22 01:31:52,452 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:52,997 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:52,997 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 01:31:53,069 - [Process 3/5] - DEBUG - predict_token:tensor([[27707]], device='cuda:3')
2024-12-22 01:31:53,072 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:53,072 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2184])
2024-12-22 01:31:53,085 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:53,085 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 01:31:53,139 - [Process 0/5] - DEBUG - predict_token:tensor([[476]], device='cuda:0')
2024-12-22 01:31:53,153 - [Process 4/5] - DEBUG - predict_token:tensor([[11045]], device='cuda:4')
2024-12-22 01:31:53,345 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Oxford
 15%|█▌        | 6/40 [00:30<02:33,  4.53s/it]2024-12-22 01:31:53,361 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Bomma Borusa
 15%|█▌        | 6/40 [00:30<02:34,  4.55s/it]2024-12-22 01:31:53,388 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Kaya Alp
 15%|█▌        | 6/40 [00:30<02:34,  4.53s/it]2024-12-22 01:31:53,454 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:53,455 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 01:31:53,528 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:53,534 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:31:53,615 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:53,642 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 15%|█▌        | 6/40 [00:30<02:35,  4.59s/it]2024-12-22 01:31:53,646 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:53,854 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:56,229 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:56,229 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:31:56,310 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:31:56,493 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Austria
 18%|█▊        | 7/40 [00:33<02:22,  4.32s/it]2024-12-22 01:31:56,607 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:57,105 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:57,105 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:31:57,178 - [Process 4/5] - DEBUG - predict_token:tensor([[6290]], device='cuda:4')
2024-12-22 01:31:57,196 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:57,197 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:31:57,269 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:31:57,402 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:57,402 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1956])
2024-12-22 01:31:57,483 - [Process 3/5] - DEBUG - predict_token:tensor([[4116]], device='cuda:3')
2024-12-22 01:31:57,612 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:31:57,612 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 01:31:57,689 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:June 12220
 18%|█▊        | 7/40 [00:35<02:27,  4.46s/it]2024-12-22 01:31:57,692 - [Process 2/5] - DEBUG - predict_token:tensor([[2216]], device='cuda:2')
2024-12-22 01:31:57,804 - [Process 4/5] - INFO - res.shape is :torch.Size([14])
results:John Dalrymple, 8th Earl of Stair
 18%|█▊        | 7/40 [00:35<02:28,  4.50s/it]2024-12-22 01:31:57,865 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Perdón, Viejita
 18%|█▊        | 7/40 [00:35<02:29,  4.53s/it]2024-12-22 01:31:57,944 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:58,000 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:58,054 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:31:58,194 - [Process 2/5] - INFO - res.shape is :torch.Size([11])
results:Poisoned bowl of pea soup.
 18%|█▊        | 7/40 [00:35<02:30,  4.58s/it]2024-12-22 01:31:58,408 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:00,316 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:00,316 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1952])
2024-12-22 01:32:00,397 - [Process 1/5] - DEBUG - predict_token:tensor([[405]], device='cuda:1')
2024-12-22 01:32:00,580 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Emel Say
 20%|██        | 8/40 [00:37<02:15,  4.25s/it]2024-12-22 01:32:00,634 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:01,669 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:01,669 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1987])
2024-12-22 01:32:01,725 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:01,726 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1928])
2024-12-22 01:32:01,735 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:01,735 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 01:32:01,748 - [Process 0/5] - DEBUG - predict_token:tensor([[6682]], device='cuda:0')
2024-12-22 01:32:01,806 - [Process 4/5] - DEBUG - predict_token:tensor([[12217]], device='cuda:4')
2024-12-22 01:32:01,807 - [Process 3/5] - DEBUG - predict_token:tensor([[3014]], device='cuda:3')
2024-12-22 01:32:02,089 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Ogawa Mataji
 20%|██        | 8/40 [00:39<02:22,  4.44s/it]2024-12-22 01:32:02,172 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:02,172 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2114])
2024-12-22 01:32:02,192 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:University of Wisconsin-Madison
 20%|██        | 8/40 [00:39<02:22,  4.47s/it]2024-12-22 01:32:02,212 - [Process 4/5] - INFO - res.shape is :torch.Size([9])
results:Murderer In The Fog
 20%|██        | 8/40 [00:39<02:23,  4.47s/it]2024-12-22 01:32:02,244 - [Process 2/5] - DEBUG - predict_token:tensor([[27990]], device='cuda:2')
2024-12-22 01:32:02,303 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:02,403 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:02,461 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:02,486 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Norwegian
 20%|██        | 8/40 [00:39<02:23,  4.49s/it]2024-12-22 01:32:02,613 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:02,613 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1070])
2024-12-22 01:32:02,653 - [Process 1/5] - DEBUG - predict_token:tensor([[350]], device='cuda:1')
2024-12-22 01:32:02,737 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:02,914 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Riding the California Trail
 22%|██▎       | 9/40 [00:40<01:53,  3.65s/it]2024-12-22 01:32:03,025 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:06,032 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:06,033 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:06,033 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1986])
2024-12-22 01:32:06,033 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2046])
2024-12-22 01:32:06,107 - [Process 3/5] - DEBUG - predict_token:tensor([[360]], device='cuda:3')
2024-12-22 01:32:06,112 - [Process 0/5] - DEBUG - predict_token:tensor([[6978]], device='cuda:0')
2024-12-22 01:32:06,171 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:06,171 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2152])
2024-12-22 01:32:06,240 - [Process 4/5] - DEBUG - predict_token:tensor([[940]], device='cuda:4')
2024-12-22 01:32:06,318 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Britain
 22%|██▎       | 9/40 [00:43<02:15,  4.37s/it]2024-12-22 01:32:06,399 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Juan René Serrano
 22%|██▎       | 9/40 [00:43<02:15,  4.39s/it]2024-12-22 01:32:06,446 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:06,447 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2156])
2024-12-22 01:32:06,475 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Maria Teresa
 22%|██▎       | 9/40 [00:43<02:16,  4.41s/it]2024-12-22 01:32:06,515 - [Process 2/5] - DEBUG - predict_token:tensor([[13772]], device='cuda:2')
2024-12-22 01:32:06,549 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:06,606 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:06,672 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:06,754 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Ethiopia
 22%|██▎       | 9/40 [00:44<02:16,  4.42s/it]2024-12-22 01:32:06,769 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:06,769 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2139])
2024-12-22 01:32:06,838 - [Process 1/5] - DEBUG - predict_token:tensor([[476]], device='cuda:1')
2024-12-22 01:32:06,935 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:07,261 - [Process 1/5] - INFO - res.shape is :torch.Size([10])
results:Kekuʻiapoiwa II
 25%|██▌       | 10/40 [00:44<01:55,  3.87s/it]2024-12-22 01:32:07,422 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:10,247 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:10,248 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2132])
2024-12-22 01:32:10,256 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:10,256 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 01:32:10,310 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:10,311 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 01:32:10,315 - [Process 0/5] - DEBUG - predict_token:tensor([[476]], device='cuda:0')
2024-12-22 01:32:10,330 - [Process 4/5] - DEBUG - predict_token:tensor([[4779]], device='cuda:4')
2024-12-22 01:32:10,385 - [Process 3/5] - DEBUG - predict_token:tensor([[478]], device='cuda:3')
2024-12-22 01:32:10,523 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Lecce
 25%|██▌       | 10/40 [00:47<02:08,  4.30s/it]2024-12-22 01:32:10,528 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:10,529 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2090])
2024-12-22 01:32:10,600 - [Process 2/5] - DEBUG - predict_token:tensor([[5493]], device='cuda:2')
2024-12-22 01:32:10,654 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Jesse E. Hobson
 25%|██▌       | 10/40 [00:47<02:10,  4.36s/it]2024-12-22 01:32:10,720 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:10,856 - [Process 3/5] - INFO - res.shape is :torch.Size([10])
results:Vytautas Straižys
 25%|██▌       | 10/40 [00:48<02:12,  4.41s/it]2024-12-22 01:32:10,938 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:11,027 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:11,027 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 01:32:11,051 - [Process 2/5] - INFO - res.shape is :torch.Size([10])
results:Mi Novia Está De Madre
 25%|██▌       | 10/40 [00:48<02:11,  4.38s/it]2024-12-22 01:32:11,101 - [Process 1/5] - DEBUG - predict_token:tensor([[2191]], device='cuda:1')
2024-12-22 01:32:11,118 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:11,267 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:11,403 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Heather D. Gibson
 28%|██▊       | 11/40 [00:48<01:54,  3.95s/it]2024-12-22 01:32:11,525 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:14,278 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:14,278 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1850])
2024-12-22 01:32:14,359 - [Process 4/5] - DEBUG - predict_token:tensor([[2169]], device='cuda:4')
2024-12-22 01:32:14,678 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:14,678 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 01:32:14,757 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:32:14,803 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:14,803 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2237])
2024-12-22 01:32:14,869 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:14,869 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 01:32:14,869 - [Process 3/5] - DEBUG - predict_token:tensor([[323]], device='cuda:3')
2024-12-22 01:32:14,941 - [Process 2/5] - DEBUG - predict_token:tensor([[3111]], device='cuda:2')
2024-12-22 01:32:15,047 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:The Secret Invasion
 28%|██▊       | 11/40 [00:52<02:06,  4.37s/it]2024-12-22 01:32:15,116 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Tashkent
 28%|██▊       | 11/40 [00:52<02:06,  4.36s/it]2024-12-22 01:32:15,255 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:15,266 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:15,266 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1931])
2024-12-22 01:32:15,322 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:15,328 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:August Underground'S Penance
 28%|██▊       | 11/40 [00:52<02:06,  4.35s/it]2024-12-22 01:32:15,348 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:32:15,563 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:15,571 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:1704
 30%|███       | 12/40 [00:52<01:52,  4.02s/it]2024-12-22 01:32:15,683 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:15,779 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:The director of film Lady Magdalene's, J. Neil Schulman, won the "Special Jury Prize for Libertarian Ideals" at the
 28%|██▊       | 11/40 [00:53<02:13,  4.59s/it]2024-12-22 01:32:15,983 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:18,836 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:18,836 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 01:32:18,910 - [Process 0/5] - DEBUG - predict_token:tensor([[9932]], device='cuda:0')
2024-12-22 01:32:19,079 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:19,080 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1964])
2024-12-22 01:32:19,160 - [Process 3/5] - DEBUG - predict_token:tensor([[382]], device='cuda:3')
2024-12-22 01:32:19,223 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:19,224 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 01:32:19,280 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:Margaret Of Brabant
 30%|███       | 12/40 [00:56<02:01,  4.33s/it]2024-12-22 01:32:19,296 - [Process 2/5] - DEBUG - predict_token:tensor([[2259]], device='cuda:2')
2024-12-22 01:32:19,318 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Dutch
 30%|███       | 12/40 [00:56<02:00,  4.31s/it]2024-12-22 01:32:19,401 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:19,478 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:19,478 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2118])
2024-12-22 01:32:19,517 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:19,550 - [Process 1/5] - DEBUG - predict_token:tensor([[25242]], device='cuda:1')
2024-12-22 01:32:19,615 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:John Vernou Bouvier
 30%|███       | 12/40 [00:56<02:01,  4.33s/it]2024-12-22 01:32:19,641 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:19,641 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 01:32:19,693 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:León
 32%|███▎      | 13/40 [00:57<01:49,  4.05s/it]2024-12-22 01:32:19,712 - [Process 4/5] - DEBUG - predict_token:tensor([[5332]], device='cuda:4')
2024-12-22 01:32:19,809 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:19,819 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:19,867 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:German
 30%|███       | 12/40 [00:57<02:04,  4.44s/it]2024-12-22 01:32:20,126 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:21,752 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:21,752 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1351])
2024-12-22 01:32:21,798 - [Process 0/5] - DEBUG - predict_token:tensor([[838]], device='cuda:0')
2024-12-22 01:32:22,785 - [Process 0/5] - INFO - res.shape is :torch.Size([25])
results:Renaud Ii, Count Of Soissons's uncle is John I, Count Of Soissons.
 32%|███▎      | 13/40 [01:00<01:50,  4.08s/it]2024-12-22 01:32:22,974 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:23,278 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:23,278 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2152])
2024-12-22 01:32:23,347 - [Process 3/5] - DEBUG - predict_token:tensor([[10152]], device='cuda:3')
2024-12-22 01:32:23,544 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:23,544 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2156])
2024-12-22 01:32:23,552 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:23,553 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2167])
2024-12-22 01:32:23,613 - [Process 2/5] - DEBUG - predict_token:tensor([[476]], device='cuda:2')
2024-12-22 01:32:23,622 - [Process 1/5] - DEBUG - predict_token:tensor([[22440]], device='cuda:1')
2024-12-22 01:32:23,733 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:23,734 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:32:23,751 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:Women's Suffrage Journal
 32%|███▎      | 13/40 [01:01<01:57,  4.35s/it]2024-12-22 01:32:23,807 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:32:23,925 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Dance With A Stranger
 35%|███▌      | 14/40 [01:01<01:46,  4.10s/it]2024-12-22 01:32:23,936 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:24,039 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:24,057 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:The Longshots
 32%|███▎      | 13/40 [01:01<01:57,  4.36s/it]2024-12-22 01:32:24,162 - [Process 2/5] - INFO - res.shape is :torch.Size([12])
results:Kızıl Buğa or Basuk
 32%|███▎      | 13/40 [01:01<01:58,  4.40s/it]2024-12-22 01:32:24,267 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:24,438 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:26,556 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:26,557 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2046])
2024-12-22 01:32:26,630 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:32:27,035 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:26 February 1803
 35%|███▌      | 14/40 [01:04<01:47,  4.13s/it]2024-12-22 01:32:27,235 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:27,587 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:27,587 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2083])
2024-12-22 01:32:27,660 - [Process 3/5] - DEBUG - predict_token:tensor([[16952]], device='cuda:3')
2024-12-22 01:32:27,796 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:27,797 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1927])
2024-12-22 01:32:27,879 - [Process 1/5] - DEBUG - predict_token:tensor([[14121]], device='cuda:1')
2024-12-22 01:32:27,894 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Hawaii
 35%|███▌      | 14/40 [01:05<01:51,  4.29s/it]2024-12-22 01:32:27,992 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:27,992 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2156])
2024-12-22 01:32:28,061 - [Process 4/5] - DEBUG - predict_token:tensor([[3303]], device='cuda:4')
2024-12-22 01:32:28,069 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:28,098 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:28,098 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 01:32:28,142 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Tom Mix In Arabia
 38%|███▊      | 15/40 [01:05<01:43,  4.14s/it]2024-12-22 01:32:28,170 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:32:28,251 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:28,258 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Britain
 35%|███▌      | 14/40 [01:05<01:52,  4.31s/it]2024-12-22 01:32:28,526 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:28,713 - [Process 2/5] - INFO - res.shape is :torch.Size([12])
results:7 June 23, 1893
 35%|███▌      | 14/40 [01:06<01:55,  4.44s/it]2024-12-22 01:32:28,908 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:30,815 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:30,816 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 01:32:30,887 - [Process 0/5] - DEBUG - predict_token:tensor([[8314]], device='cuda:0')
2024-12-22 01:32:31,038 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Australia
 38%|███▊      | 15/40 [01:08<01:42,  4.09s/it]2024-12-22 01:32:31,300 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:31,840 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:31,841 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2163])
2024-12-22 01:32:31,910 - [Process 3/5] - DEBUG - predict_token:tensor([[22186]], device='cuda:3')
2024-12-22 01:32:32,010 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:32,010 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1933])
2024-12-22 01:32:32,092 - [Process 1/5] - DEBUG - predict_token:tensor([[751]], device='cuda:1')
2024-12-22 01:32:32,151 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:32,152 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2059])
2024-12-22 01:32:32,187 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Mayor Muthanna
 38%|███▊      | 15/40 [01:09<01:47,  4.29s/it]2024-12-22 01:32:32,225 - [Process 4/5] - DEBUG - predict_token:tensor([[6978]], device='cuda:4')
2024-12-22 01:32:32,383 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:32,436 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:Daughter Of The Jungle
 40%|████      | 16/40 [01:09<01:40,  4.18s/it]2024-12-22 01:32:32,545 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:111754
 38%|███▊      | 15/40 [01:09<01:47,  4.31s/it]2024-12-22 01:32:32,555 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:32,696 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:32,696 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2123])
2024-12-22 01:32:32,760 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:32,768 - [Process 2/5] - DEBUG - predict_token:tensor([[12001]], device='cuda:2')
2024-12-22 01:32:33,002 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Fredrikstad
 38%|███▊      | 15/40 [01:10<01:49,  4.40s/it]2024-12-22 01:32:33,207 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:35,007 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:35,007 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1933])
2024-12-22 01:32:35,088 - [Process 0/5] - DEBUG - predict_token:tensor([[19122]], device='cuda:0')
2024-12-22 01:32:35,366 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Edward Buzzell
 40%|████      | 16/40 [01:12<01:39,  4.16s/it]2024-12-22 01:32:35,495 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:36,024 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:36,025 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2245])
2024-12-22 01:32:36,091 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:32:36,198 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 40%|████      | 16/40 [01:13<01:40,  4.21s/it]2024-12-22 01:32:36,223 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:36,223 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:32:36,299 - [Process 1/5] - DEBUG - predict_token:tensor([[4942]], device='cuda:1')
2024-12-22 01:32:36,383 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:36,383 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 01:32:36,397 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:36,458 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:32:36,603 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:
Dr. Socrates
 42%|████▎     | 17/40 [01:13<01:36,  4.18s/it]2024-12-22 01:32:36,738 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:36,783 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:36,783 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1848])
2024-12-22 01:32:36,822 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:July 1723
 40%|████      | 16/40 [01:14<01:43,  4.30s/it]2024-12-22 01:32:36,863 - [Process 2/5] - DEBUG - predict_token:tensor([[2043]], device='cuda:2')
2024-12-22 01:32:37,085 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:37,233 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:Sebastien Schuller
 40%|████      | 16/40 [01:14<01:44,  4.35s/it]2024-12-22 01:32:37,475 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:38,073 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:38,073 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1441])
2024-12-22 01:32:38,130 - [Process 0/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:0')
2024-12-22 01:32:38,388 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:The Death Of Black King
 42%|████▎     | 17/40 [01:15<01:27,  3.82s/it]2024-12-22 01:32:38,603 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:40,227 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:40,227 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2120])
2024-12-22 01:32:40,300 - [Process 3/5] - DEBUG - predict_token:tensor([[317]], device='cuda:3')
2024-12-22 01:32:40,490 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:40,490 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2150])
2024-12-22 01:32:40,560 - [Process 1/5] - DEBUG - predict_token:tensor([[678]], device='cuda:1')
2024-12-22 01:32:40,577 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Sidi Bou Said
 42%|████▎     | 17/40 [01:17<01:37,  4.26s/it]2024-12-22 01:32:40,743 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Changeland
 45%|████▌     | 18/40 [01:18<01:31,  4.17s/it]2024-12-22 01:32:40,821 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:40,843 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:40,892 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:40,892 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 01:32:40,964 - [Process 4/5] - DEBUG - predict_token:tensor([[12630]], device='cuda:4')
2024-12-22 01:32:41,081 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:41,081 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2259])
2024-12-22 01:32:41,146 - [Process 2/5] - DEBUG - predict_token:tensor([[4581]], device='cuda:2')
2024-12-22 01:32:41,477 - [Process 4/5] - INFO - res.shape is :torch.Size([11])
results:Special Delivery (1927 Film)
 42%|████▎     | 17/40 [01:18<01:41,  4.40s/it]2024-12-22 01:32:41,487 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Jean-Claude Lauzon
 42%|████▎     | 17/40 [01:18<01:39,  4.32s/it]2024-12-22 01:32:41,647 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:41,729 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:42,206 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:42,206 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 01:32:42,280 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:32:42,388 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 45%|████▌     | 18/40 [01:19<01:25,  3.87s/it]2024-12-22 01:32:42,443 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:43,637 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:43,637 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 630])
2024-12-22 01:32:43,664 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:32:43,755 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 48%|████▊     | 19/40 [01:21<01:05,  3.12s/it]2024-12-22 01:32:43,943 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:44,457 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:44,458 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1741])
2024-12-22 01:32:44,540 - [Process 1/5] - DEBUG - predict_token:tensor([[5310]], device='cuda:1')
2024-12-22 01:32:44,602 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:44,602 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2144])
2024-12-22 01:32:44,672 - [Process 3/5] - DEBUG - predict_token:tensor([[8918]], device='cuda:3')
2024-12-22 01:32:44,721 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:44,721 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1570])
2024-12-22 01:32:44,761 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Ruel Redinger
 48%|████▊     | 19/40 [01:22<01:26,  4.12s/it]2024-12-22 01:32:44,790 - [Process 2/5] - DEBUG - predict_token:tensor([[14195]], device='cuda:2')
2024-12-22 01:32:44,822 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Ur
 45%|████▌     | 18/40 [01:22<01:33,  4.25s/it]2024-12-22 01:32:44,871 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:45,019 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:45,054 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Perryville, Missouri
 45%|████▌     | 18/40 [01:22<01:30,  4.09s/it]2024-12-22 01:32:45,309 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:45,476 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:45,476 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2167])
2024-12-22 01:32:45,544 - [Process 4/5] - DEBUG - predict_token:tensor([[5011]], device='cuda:4')
2024-12-22 01:32:45,822 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:James Whale died.
 45%|████▌     | 18/40 [01:23<01:36,  4.39s/it]2024-12-22 01:32:46,015 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:47,654 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:47,655 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2175])
2024-12-22 01:32:47,723 - [Process 0/5] - DEBUG - predict_token:tensor([[21805]], device='cuda:0')
2024-12-22 01:32:47,916 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Kosovo
 50%|█████     | 20/40 [01:25<01:08,  3.43s/it]2024-12-22 01:32:48,097 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:48,502 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:48,502 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:32:48,577 - [Process 1/5] - DEBUG - predict_token:tensor([[5322]], device='cuda:1')
2024-12-22 01:32:48,663 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:48,663 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:32:48,738 - [Process 3/5] - DEBUG - predict_token:tensor([[323]], device='cuda:3')
2024-12-22 01:32:48,840 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Robert Vadra
 50%|█████     | 20/40 [01:26<01:22,  4.11s/it]2024-12-22 01:32:48,982 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:49,096 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:49,096 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 01:32:49,101 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Tarzan The Magnificent
 48%|████▊     | 19/40 [01:26<01:29,  4.26s/it]2024-12-22 01:32:49,178 - [Process 2/5] - DEBUG - predict_token:tensor([[4517]], device='cuda:2')
2024-12-22 01:32:49,290 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:49,455 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Our Agent Tiger
 48%|████▊     | 19/40 [01:26<01:27,  4.19s/it]2024-12-22 01:32:49,609 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:49,609 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1833])
2024-12-22 01:32:49,641 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:49,690 - [Process 4/5] - DEBUG - predict_token:tensor([[319]], device='cuda:4')
2024-12-22 01:32:49,884 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Ajman
 48%|████▊     | 19/40 [01:27<01:30,  4.29s/it]2024-12-22 01:32:50,074 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:51,808 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:51,809 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2149])
2024-12-22 01:32:51,878 - [Process 0/5] - DEBUG - predict_token:tensor([[7904]], device='cuda:0')
2024-12-22 01:32:52,725 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:52,725 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1977])
2024-12-22 01:32:52,805 - [Process 1/5] - DEBUG - predict_token:tensor([[6217]], device='cuda:1')
2024-12-22 01:32:52,959 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:52,959 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 01:32:53,034 - [Process 3/5] - DEBUG - predict_token:tensor([[10441]], device='cuda:3')
2024-12-22 01:32:53,068 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Jessi Colter
 52%|█████▎    | 21/40 [01:30<01:18,  4.15s/it]2024-12-22 01:32:53,173 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:53,280 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:
Note: I will only provide the answer to the question you asked, and will not provide any additional information or context beyond what is provided in the passages
 52%|█████▎    | 21/40 [01:30<01:16,  4.01s/it]2024-12-22 01:32:53,285 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Wolf Warrior
 50%|█████     | 20/40 [01:30<01:24,  4.24s/it]2024-12-22 01:32:53,387 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:53,387 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1928])
2024-12-22 01:32:53,469 - [Process 2/5] - DEBUG - predict_token:tensor([[17993]], device='cuda:2')
2024-12-22 01:32:53,496 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:53,537 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:53,700 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:53,701 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:32:53,773 - [Process 4/5] - DEBUG - predict_token:tensor([[349]], device='cuda:4')
2024-12-22 01:32:54,017 - [Process 2/5] - INFO - res.shape is :torch.Size([12])
results:Fernando Bermúdez de Cea.
 50%|█████     | 20/40 [01:31<01:25,  4.30s/it]2024-12-22 01:32:54,023 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Prenzlau
 50%|█████     | 20/40 [01:31<01:24,  4.24s/it]2024-12-22 01:32:54,241 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:54,241 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:56,805 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:56,805 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:32:56,878 - [Process 1/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:1')
2024-12-22 01:32:56,980 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 55%|█████▌    | 22/40 [01:34<01:13,  4.08s/it]2024-12-22 01:32:57,077 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:57,123 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:57,123 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:32:57,195 - [Process 0/5] - DEBUG - predict_token:tensor([[24132]], device='cuda:0')
2024-12-22 01:32:57,314 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:57,314 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:32:57,395 - [Process 3/5] - DEBUG - predict_token:tensor([[405]], device='cuda:3')
2024-12-22 01:32:57,477 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Oak Park, Illinois
 55%|█████▌    | 22/40 [01:34<01:13,  4.07s/it]2024-12-22 01:32:57,747 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:57,815 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:Abd al-'Uzzā
 52%|█████▎    | 21/40 [01:35<01:22,  4.33s/it]2024-12-22 01:32:57,891 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:57,892 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1878])
2024-12-22 01:32:57,974 - [Process 4/5] - DEBUG - predict_token:tensor([[18898]], device='cuda:4')
2024-12-22 01:32:57,981 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:32:57,981 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1975])
2024-12-22 01:32:58,021 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:58,062 - [Process 2/5] - DEBUG - predict_token:tensor([[5310]], device='cuda:2')
2024-12-22 01:32:58,311 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Wilmington, Delaware
 52%|█████▎    | 21/40 [01:35<01:20,  4.26s/it]2024-12-22 01:32:58,400 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Audrey Tautou
 52%|█████▎    | 21/40 [01:35<01:22,  4.32s/it]2024-12-22 01:32:58,535 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:32:58,566 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:00,424 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:00,425 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1935])
2024-12-22 01:33:00,492 - [Process 1/5] - DEBUG - predict_token:tensor([[1425]], device='cuda:1')
2024-12-22 01:33:00,749 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:William Pooley
 57%|█████▊    | 23/40 [01:38<01:07,  3.98s/it]2024-12-22 01:33:00,869 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:01,351 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:01,352 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 01:33:01,425 - [Process 0/5] - DEBUG - predict_token:tensor([[19339]], device='cuda:0')
2024-12-22 01:33:01,617 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:The Soviet Story
 57%|█████▊    | 23/40 [01:38<01:09,  4.09s/it]2024-12-22 01:33:01,748 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:01,748 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1776])
2024-12-22 01:33:01,802 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:01,802 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2165])
2024-12-22 01:33:01,811 - [Process 2/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:2')
2024-12-22 01:33:01,833 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:01,872 - [Process 3/5] - DEBUG - predict_token:tensor([[27650]], device='cuda:3')
2024-12-22 01:33:01,914 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 55%|█████▌    | 22/40 [01:39<01:13,  4.08s/it]2024-12-22 01:33:02,101 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:02,287 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:02,287 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1938])
2024-12-22 01:33:02,318 - [Process 3/5] - INFO - res.shape is :torch.Size([10])
results:Nathan Juran was born in Austria.
 55%|█████▌    | 22/40 [01:39<01:18,  4.38s/it]2024-12-22 01:33:02,368 - [Process 4/5] - DEBUG - predict_token:tensor([[5546]], device='cuda:4')
2024-12-22 01:33:02,549 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:02,689 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Anacyndaraxes
 55%|█████▌    | 22/40 [01:40<01:17,  4.29s/it]2024-12-22 01:33:02,875 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:04,620 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:04,620 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2172])
2024-12-22 01:33:04,690 - [Process 1/5] - DEBUG - predict_token:tensor([[12601]], device='cuda:1')
2024-12-22 01:33:04,872 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Irish
 60%|██████    | 24/40 [01:42<01:04,  4.03s/it]2024-12-22 01:33:04,941 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:05,547 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:05,547 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2171])
2024-12-22 01:33:05,615 - [Process 0/5] - DEBUG - predict_token:tensor([[9865]], device='cuda:0')
2024-12-22 01:33:05,853 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:05,854 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1939])
2024-12-22 01:33:05,893 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Cipriano Castro
 60%|██████    | 24/40 [01:43<01:06,  4.15s/it]2024-12-22 01:33:05,935 - [Process 2/5] - DEBUG - predict_token:tensor([[3384]], device='cuda:2')
2024-12-22 01:33:06,085 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Denmark
 57%|█████▊    | 23/40 [01:43<01:09,  4.11s/it]2024-12-22 01:33:06,112 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:06,200 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:06,201 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:33:06,276 - [Process 3/5] - DEBUG - predict_token:tensor([[4517]], device='cuda:3')
2024-12-22 01:33:06,284 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:06,426 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Los Angeles
 57%|█████▊    | 23/40 [01:43<01:13,  4.30s/it]2024-12-22 01:33:06,506 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:06,506 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 01:33:06,581 - [Process 4/5] - DEBUG - predict_token:tensor([[5011]], device='cuda:4')
2024-12-22 01:33:06,608 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:06,858 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Passage 10
 57%|█████▊    | 23/40 [01:44<01:12,  4.26s/it]2024-12-22 01:33:07,055 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:07,358 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:07,358 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1372])
2024-12-22 01:33:07,407 - [Process 1/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:1')
2024-12-22 01:33:07,498 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 62%|██████▎   | 25/40 [01:44<00:54,  3.61s/it]2024-12-22 01:33:07,621 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:09,826 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:09,826 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2162])
2024-12-22 01:33:09,895 - [Process 0/5] - DEBUG - predict_token:tensor([[2598]], device='cuda:0')
2024-12-22 01:33:09,904 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:09,904 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:33:09,978 - [Process 2/5] - DEBUG - predict_token:tensor([[360]], device='cuda:2')
2024-12-22 01:33:10,216 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Monster On The Campus
 62%|██████▎   | 25/40 [01:47<01:02,  4.20s/it]2024-12-22 01:33:10,297 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:10,298 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:10,299 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1896])
2024-12-22 01:33:10,340 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:Tasha Smith's father.
 60%|██████    | 24/40 [01:47<01:06,  4.15s/it]2024-12-22 01:33:10,381 - [Process 3/5] - DEBUG - predict_token:tensor([[12267]], device='cuda:3')
2024-12-22 01:33:10,603 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:10,660 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Winter Sleepers
 60%|██████    | 24/40 [01:47<01:08,  4.28s/it]2024-12-22 01:33:10,853 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:10,871 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:10,871 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2129])
2024-12-22 01:33:10,943 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:33:11,179 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:The Third Kiss
 60%|██████    | 24/40 [01:48<01:08,  4.28s/it]2024-12-22 01:33:11,253 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:11,253 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2081])
2024-12-22 01:33:11,326 - [Process 1/5] - DEBUG - predict_token:tensor([[8037]], device='cuda:1')
2024-12-22 01:33:11,386 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:11,629 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Melody Of The World
 65%|██████▌   | 26/40 [01:48<00:52,  3.76s/it]2024-12-22 01:33:11,738 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:12,093 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:12,094 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 963])
2024-12-22 01:33:12,131 - [Process 0/5] - DEBUG - predict_token:tensor([[838]], device='cuda:0')
2024-12-22 01:33:12,494 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:Guy Arvely Dolsin
 65%|██████▌   | 26/40 [01:49<00:50,  3.62s/it]2024-12-22 01:33:12,684 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:14,194 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:14,194 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1862])
2024-12-22 01:33:14,275 - [Process 2/5] - DEBUG - predict_token:tensor([[27197]], device='cuda:2')
2024-12-22 01:33:14,538 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:14,539 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1904])
2024-12-22 01:33:14,621 - [Process 3/5] - DEBUG - predict_token:tensor([[22105]], device='cuda:3')
2024-12-22 01:33:14,763 - [Process 2/5] - INFO - res.shape is :torch.Size([11])
results:Before 8 July 1332.
 62%|██████▎   | 25/40 [01:52<01:03,  4.23s/it]2024-12-22 01:33:14,952 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:15,025 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:Abd al-'Uzzā
 62%|██████▎   | 25/40 [01:52<01:04,  4.30s/it]2024-12-22 01:33:15,128 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:15,128 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1720])
2024-12-22 01:33:15,220 - [Process 4/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:4')
2024-12-22 01:33:15,247 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:15,329 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 62%|██████▎   | 25/40 [01:52<01:03,  4.24s/it]2024-12-22 01:33:15,395 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:15,395 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2342])
2024-12-22 01:33:15,459 - [Process 1/5] - DEBUG - predict_token:tensor([[16662]], device='cuda:1')
2024-12-22 01:33:15,516 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:15,602 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Madrid
 68%|██████▊   | 27/40 [01:52<00:49,  3.83s/it]2024-12-22 01:33:15,714 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:16,304 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:16,305 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1882])
2024-12-22 01:33:16,385 - [Process 0/5] - DEBUG - predict_token:tensor([[3650]], device='cuda:0')
2024-12-22 01:33:16,791 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:Joel The Lump Of Coal
 68%|██████▊   | 27/40 [01:54<00:49,  3.82s/it]2024-12-22 01:33:16,989 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:18,719 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:18,719 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2186])
2024-12-22 01:33:18,788 - [Process 2/5] - DEBUG - predict_token:tensor([[379]], device='cuda:2')
2024-12-22 01:33:18,904 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:18,905 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 01:33:18,980 - [Process 3/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:3')
2024-12-22 01:33:19,064 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Hundred Flowers Award
 65%|██████▌   | 26/40 [01:56<00:59,  4.25s/it]2024-12-22 01:33:19,088 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 65%|██████▌   | 26/40 [01:56<00:59,  4.23s/it]2024-12-22 01:33:19,162 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:19,163 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2067])
2024-12-22 01:33:19,199 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:19,236 - [Process 4/5] - DEBUG - predict_token:tensor([[4755]], device='cuda:4')
2024-12-22 01:33:19,274 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:19,518 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:19,518 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:33:19,556 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Robert Wulnikowski
 65%|██████▌   | 26/40 [01:56<00:59,  4.23s/it]2024-12-22 01:33:19,601 - [Process 1/5] - DEBUG - predict_token:tensor([[402]], device='cuda:1')
2024-12-22 01:33:19,713 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:20,025 - [Process 1/5] - INFO - res.shape is :torch.Size([10])
results:Kekuʻiapoiwa II
 70%|███████   | 28/40 [01:57<00:48,  4.01s/it]2024-12-22 01:33:20,141 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:20,575 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:20,575 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2102])
2024-12-22 01:33:20,647 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:33:20,755 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 70%|███████   | 28/40 [01:58<00:46,  3.87s/it]2024-12-22 01:33:20,949 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:21,951 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:21,951 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1566])
2024-12-22 01:33:22,008 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:33:22,267 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Neelakantan
 68%|██████▊   | 27/40 [01:59<00:51,  3.94s/it]2024-12-22 01:33:22,452 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:23,045 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:23,046 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1930])
2024-12-22 01:33:23,083 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:23,084 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2203])
2024-12-22 01:33:23,109 - [Process 4/5] - DEBUG - predict_token:tensor([[8301]], device='cuda:4')
2024-12-22 01:33:23,153 - [Process 3/5] - DEBUG - predict_token:tensor([[341]], device='cuda:3')
2024-12-22 01:33:23,338 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Folgore Division
 68%|██████▊   | 27/40 [02:00<00:53,  4.10s/it]2024-12-22 01:33:23,515 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Mülheim an der Ruhr
 68%|██████▊   | 27/40 [02:00<00:55,  4.29s/it]2024-12-22 01:33:23,533 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:23,695 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:23,777 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:23,777 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2111])
2024-12-22 01:33:23,850 - [Process 1/5] - DEBUG - predict_token:tensor([[997]], device='cuda:1')
2024-12-22 01:33:24,074 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:La Belle Américaine
 72%|███████▎  | 29/40 [02:01<00:44,  4.02s/it]2024-12-22 01:33:24,186 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:24,654 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:24,654 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1943])
2024-12-22 01:33:24,733 - [Process 0/5] - DEBUG - predict_token:tensor([[390]], device='cuda:0')
2024-12-22 01:33:25,223 - [Process 0/5] - INFO - res.shape is :torch.Size([11])
results:Abd al-Muṭṭalib
 72%|███████▎  | 29/40 [02:02<00:44,  4.05s/it]2024-12-22 01:33:25,426 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:26,078 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:26,078 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2083])
2024-12-22 01:33:26,150 - [Process 2/5] - DEBUG - predict_token:tensor([[19769]], device='cuda:2')
2024-12-22 01:33:26,554 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:Frederick Cleveland Morgan died first.
 70%|███████   | 28/40 [02:03<00:48,  4.04s/it]2024-12-22 01:33:26,746 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:27,307 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:27,307 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2143])
2024-12-22 01:33:27,376 - [Process 4/5] - DEBUG - predict_token:tensor([[7073]], device='cuda:4')
2024-12-22 01:33:27,523 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:27,523 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 01:33:27,604 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:33:27,697 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Durango Valley Raiders
 70%|███████   | 28/40 [02:05<00:50,  4.18s/it]2024-12-22 01:33:27,765 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:27,841 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:27,841 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 01:33:27,916 - [Process 1/5] - DEBUG - predict_token:tensor([[27879]], device='cuda:1')
2024-12-22 01:33:28,179 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Oklahoma City, Oklahoma
 75%|███████▌  | 30/40 [02:05<00:40,  4.04s/it]2024-12-22 01:33:28,296 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:28,690 - [Process 3/5] - INFO - res.shape is :torch.Size([25])
results:Una Prostituta Al Servizio Del Pubblico E In Regola Con Le Leggi Dello Stato
 70%|███████   | 28/40 [02:05<00:54,  4.56s/it]2024-12-22 01:33:28,960 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:29,008 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:29,008 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2045])
2024-12-22 01:33:29,081 - [Process 0/5] - DEBUG - predict_token:tensor([[20303]], device='cuda:0')
2024-12-22 01:33:29,274 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Fermanagh
 75%|███████▌  | 30/40 [02:06<00:40,  4.05s/it]2024-12-22 01:33:29,334 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:29,334 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 867])
2024-12-22 01:33:29,365 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:33:29,456 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 72%|███████▎  | 29/40 [02:06<00:37,  3.45s/it]2024-12-22 01:33:29,472 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:29,730 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:30,487 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:30,487 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1977])
2024-12-22 01:33:30,567 - [Process 2/5] - DEBUG - predict_token:tensor([[3685]], device='cuda:2')
2024-12-22 01:33:30,886 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Tisch School of the Arts
 72%|███████▎  | 29/40 [02:08<00:45,  4.13s/it]2024-12-22 01:33:31,068 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:31,980 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:31,980 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2080])
2024-12-22 01:33:32,053 - [Process 1/5] - DEBUG - predict_token:tensor([[11668]], device='cuda:1')
2024-12-22 01:33:32,236 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Dubai
 78%|███████▊  | 31/40 [02:09<00:36,  4.05s/it]2024-12-22 01:33:32,358 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:32,801 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:32,802 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2140])
2024-12-22 01:33:32,874 - [Process 3/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:3')
2024-12-22 01:33:32,982 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 72%|███████▎  | 29/40 [02:10<00:49,  4.48s/it]2024-12-22 01:33:33,073 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:33,074 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:33:33,145 - [Process 0/5] - DEBUG - predict_token:tensor([[2595]], device='cuda:0')
2024-12-22 01:33:33,174 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:33,550 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:33,550 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2116])
2024-12-22 01:33:33,622 - [Process 4/5] - DEBUG - predict_token:tensor([[922]], device='cuda:4')
2024-12-22 01:33:33,773 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Seville
 75%|███████▌  | 30/40 [02:11<00:37,  3.71s/it]2024-12-22 01:33:33,988 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:34,537 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Archibald Acheson, 4th Earl of Gosford's paternal grandfather is Archibald Acheson, 3rd
 78%|███████▊  | 31/40 [02:11<00:39,  4.41s/it]2024-12-22 01:33:34,749 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:34,872 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:34,872 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2115])
2024-12-22 01:33:34,944 - [Process 2/5] - DEBUG - predict_token:tensor([[315]], device='cuda:2')
2024-12-22 01:33:35,306 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:Cuchillos De Fuego
 75%|███████▌  | 30/40 [02:12<00:42,  4.22s/it]2024-12-22 01:33:35,502 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:35,984 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:35,985 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 01:33:36,060 - [Process 1/5] - DEBUG - predict_token:tensor([[4517]], device='cuda:1')
2024-12-22 01:33:36,163 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:France
 80%|████████  | 32/40 [02:13<00:32,  4.01s/it]2024-12-22 01:33:36,282 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:36,819 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:36,819 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2029])
2024-12-22 01:33:36,894 - [Process 3/5] - DEBUG - predict_token:tensor([[6498]], device='cuda:3')
2024-12-22 01:33:37,383 - [Process 3/5] - INFO - res.shape is :torch.Size([11])
results:Abdul-Muṭṭalib
 75%|███████▌  | 30/40 [02:14<00:44,  4.45s/it]2024-12-22 01:33:37,567 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:37,771 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:37,771 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2191])
2024-12-22 01:33:37,841 - [Process 4/5] - DEBUG - predict_token:tensor([[20179]], device='cuda:4')
2024-12-22 01:33:38,076 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Johnny Ekström
 78%|███████▊  | 31/40 [02:15<00:34,  3.89s/it]2024-12-22 01:33:38,132 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:38,337 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:38,338 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 01:33:38,411 - [Process 0/5] - DEBUG - predict_token:tensor([[17860]], device='cuda:0')
2024-12-22 01:33:38,561 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Le Deux
 80%|████████  | 32/40 [02:15<00:34,  4.30s/it]2024-12-22 01:33:38,839 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:39,108 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:39,108 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2270])
2024-12-22 01:33:39,173 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:33:39,433 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:39,433 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 724])
2024-12-22 01:33:39,458 - [Process 4/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:4')
2024-12-22 01:33:39,495 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:The Ballad Of Josie
 78%|███████▊  | 31/40 [02:16<00:37,  4.21s/it]2024-12-22 01:33:39,557 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 80%|████████  | 32/40 [02:16<00:25,  3.17s/it]2024-12-22 01:33:39,686 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:39,847 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:40,044 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:40,044 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2155])
2024-12-22 01:33:40,114 - [Process 1/5] - DEBUG - predict_token:tensor([[14298]], device='cuda:1')
2024-12-22 01:33:40,458 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:Ludwig von Westphalen
 82%|████████▎ | 33/40 [02:17<00:28,  4.10s/it]2024-12-22 01:33:40,485 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:41,231 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:41,232 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:33:41,305 - [Process 3/5] - DEBUG - predict_token:tensor([[476]], device='cuda:3')
2024-12-22 01:33:41,515 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:41,515 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 608])
2024-12-22 01:33:41,536 - [Process 1/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:1')
2024-12-22 01:33:41,583 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Katherine Skipwith
 78%|███████▊  | 31/40 [02:18<00:39,  4.38s/it]2024-12-22 01:33:41,617 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 85%|████████▌ | 34/40 [02:18<00:19,  3.22s/it]2024-12-22 01:33:41,732 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:41,835 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:42,429 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:42,429 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:33:42,502 - [Process 0/5] - DEBUG - predict_token:tensor([[4517]], device='cuda:0')
2024-12-22 01:33:42,653 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Berlin
 82%|████████▎ | 33/40 [02:19<00:29,  4.23s/it]2024-12-22 01:33:42,838 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:43,313 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:43,313 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2053])
2024-12-22 01:33:43,387 - [Process 2/5] - DEBUG - predict_token:tensor([[8027]], device='cuda:2')
2024-12-22 01:33:43,621 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:43,621 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2180])
2024-12-22 01:33:43,621 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Rock Street Journal
 80%|████████  | 32/40 [02:20<00:33,  4.18s/it]2024-12-22 01:33:43,690 - [Process 4/5] - DEBUG - predict_token:tensor([[4223]], device='cuda:4')
2024-12-22 01:33:43,801 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:English
 82%|████████▎ | 33/40 [02:21<00:24,  3.49s/it]2024-12-22 01:33:43,826 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:44,003 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:45,362 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:45,362 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2020])
2024-12-22 01:33:45,437 - [Process 1/5] - DEBUG - predict_token:tensor([[4644]], device='cuda:1')
2024-12-22 01:33:45,497 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:45,497 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 01:33:45,570 - [Process 3/5] - DEBUG - predict_token:tensor([[3067]], device='cuda:3')
2024-12-22 01:33:45,847 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Éric Rohmer
 80%|████████  | 32/40 [02:23<00:34,  4.34s/it]2024-12-22 01:33:45,861 - [Process 1/5] - INFO - res.shape is :torch.Size([10])
results:Beaulieu-sur-Loire
 88%|████████▊ | 35/40 [02:23<00:17,  3.52s/it]2024-12-22 01:33:45,969 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:46,041 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:46,433 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:46,433 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1834])
2024-12-22 01:33:46,516 - [Process 0/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:0')
2024-12-22 01:33:46,625 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 85%|████████▌ | 34/40 [02:23<00:24,  4.16s/it]2024-12-22 01:33:46,823 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:47,458 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:47,458 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 01:33:47,531 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:33:47,808 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:The Blonde From Singapore
 82%|████████▎ | 33/40 [02:25<00:29,  4.18s/it]2024-12-22 01:33:47,826 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:47,826 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:33:47,899 - [Process 4/5] - DEBUG - predict_token:tensor([[2216]], device='cuda:4')
2024-12-22 01:33:48,032 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:48,181 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Rudolph Giuliani
 85%|████████▌ | 34/40 [02:25<00:22,  3.76s/it]2024-12-22 01:33:48,373 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:49,604 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:49,604 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2020])
2024-12-22 01:33:49,679 - [Process 1/5] - DEBUG - predict_token:tensor([[6290]], device='cuda:1')
2024-12-22 01:33:49,732 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:49,733 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1906])
2024-12-22 01:33:49,816 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:33:50,009 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Mexico
 82%|████████▎ | 33/40 [02:27<00:30,  4.29s/it]2024-12-22 01:33:50,184 - [Process 1/5] - INFO - res.shape is :torch.Size([12])
results:Sir Paul Gore, 1st Baronet
 90%|█████████ | 36/40 [02:27<00:15,  3.76s/it]2024-12-22 01:33:50,259 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:50,333 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:50,589 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:50,590 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:33:50,670 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:33:50,949 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:The Abduction Club
 88%|████████▊ | 35/40 [02:28<00:21,  4.21s/it]2024-12-22 01:33:51,135 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:51,663 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:51,663 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2102])
2024-12-22 01:33:51,736 - [Process 2/5] - DEBUG - predict_token:tensor([[3869]], device='cuda:2')
2024-12-22 01:33:51,842 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
 85%|████████▌ | 34/40 [02:29<00:24,  4.14s/it]2024-12-22 01:33:52,040 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:52,147 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:52,148 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1929])
2024-12-22 01:33:52,229 - [Process 4/5] - DEBUG - predict_token:tensor([[349]], device='cuda:4')
2024-12-22 01:33:53,361 - [Process 4/5] - INFO - res.shape is :torch.Size([26])
results:Maurice De Berkeley, 4th Baron Berkeley's maternal grandfather is John De Berkeley.
 88%|████████▊ | 35/40 [02:30<00:20,  4.18s/it]2024-12-22 01:33:53,561 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:53,870 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:53,871 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1841])
2024-12-22 01:33:53,952 - [Process 3/5] - DEBUG - predict_token:tensor([[402]], device='cuda:3')
2024-12-22 01:33:53,961 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:53,961 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 01:33:54,033 - [Process 1/5] - DEBUG - predict_token:tensor([[12892]], device='cuda:1')
2024-12-22 01:33:54,187 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Virginia
 85%|████████▌ | 34/40 [02:31<00:25,  4.26s/it]2024-12-22 01:33:54,256 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Mustansir
 92%|█████████▎| 37/40 [02:31<00:11,  3.86s/it]2024-12-22 01:33:54,372 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:54,423 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:54,908 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:54,909 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2122])
2024-12-22 01:33:54,980 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:33:55,216 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:The Magic Aster
 90%|█████████ | 36/40 [02:32<00:16,  4.22s/it]2024-12-22 01:33:55,414 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:55,656 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:55,656 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 01:33:55,731 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:33:56,135 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:15 October 1605
 88%|████████▊ | 35/40 [02:33<00:20,  4.19s/it]2024-12-22 01:33:56,360 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:57,257 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:57,257 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:33:57,334 - [Process 4/5] - DEBUG - predict_token:tensor([[10785]], device='cuda:4')
2024-12-22 01:33:57,569 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Ali Dinar
 90%|█████████ | 36/40 [02:34<00:16,  4.19s/it]2024-12-22 01:33:57,814 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:57,996 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:57,997 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:33:58,071 - [Process 1/5] - DEBUG - predict_token:tensor([[3303]], device='cuda:1')
2024-12-22 01:33:58,213 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Japan
 95%|█████████▌| 38/40 [02:35<00:07,  3.89s/it]2024-12-22 01:33:58,220 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:58,220 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1940])
2024-12-22 01:33:58,302 - [Process 3/5] - DEBUG - predict_token:tensor([[29237]], device='cuda:3')
2024-12-22 01:33:58,379 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:58,452 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:West Berlin
 88%|████████▊ | 35/40 [02:35<00:21,  4.26s/it]2024-12-22 01:33:58,652 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:33:59,029 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:33:59,029 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1902])
2024-12-22 01:33:59,110 - [Process 0/5] - DEBUG - predict_token:tensor([[10924]], device='cuda:0')
2024-12-22 01:33:59,303 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:The Longshot
 92%|█████████▎| 37/40 [02:36<00:12,  4.18s/it]2024-12-22 01:33:59,488 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:00,105 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:00,105 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-22 01:34:00,185 - [Process 2/5] - DEBUG - predict_token:tensor([[5852]], device='cuda:2')
2024-12-22 01:34:00,420 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:True To The Navy
 90%|█████████ | 36/40 [02:37<00:16,  4.22s/it]2024-12-22 01:34:00,606 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:01,586 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:01,586 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2166])
2024-12-22 01:34:01,656 - [Process 4/5] - DEBUG - predict_token:tensor([[22392]], device='cuda:4')
2024-12-22 01:34:02,061 - [Process 4/5] - INFO - res.shape is :torch.Size([9])
results:Ranuccio I Farnese
 92%|█████████▎| 37/40 [02:39<00:12,  4.28s/it]2024-12-22 01:34:02,140 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:02,140 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2154])
2024-12-22 01:34:02,210 - [Process 1/5] - DEBUG - predict_token:tensor([[15350]], device='cuda:1')
2024-12-22 01:34:02,246 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:02,392 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Gene Raymond
 98%|█████████▊| 39/40 [02:39<00:03,  3.97s/it]2024-12-22 01:34:02,444 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:02,444 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2169])
2024-12-22 01:34:02,506 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:02,513 - [Process 3/5] - DEBUG - predict_token:tensor([[12939]], device='cuda:3')
2024-12-22 01:34:02,790 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Patrice Robitaille
 90%|█████████ | 36/40 [02:40<00:17,  4.28s/it]2024-12-22 01:34:02,998 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:03,247 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:03,248 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1987])
2024-12-22 01:34:03,327 - [Process 0/5] - DEBUG - predict_token:tensor([[2431]], device='cuda:0')
2024-12-22 01:34:03,563 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Fredrikstad
 95%|█████████▌| 38/40 [02:40<00:08,  4.21s/it]2024-12-22 01:34:03,782 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:04,416 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:04,416 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2113])
2024-12-22 01:34:04,488 - [Process 2/5] - DEBUG - predict_token:tensor([[23072]], device='cuda:2')
2024-12-22 01:34:04,765 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Marshall, Indiana
 92%|█████████▎| 37/40 [02:42<00:12,  4.25s/it]2024-12-22 01:34:04,973 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:05,867 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:05,868 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1883])
2024-12-22 01:34:05,948 - [Process 4/5] - DEBUG - predict_token:tensor([[10787]], device='cuda:4')
2024-12-22 01:34:06,155 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:06,155 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 01:34:06,229 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:34:06,269 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Prince Of Arcadia
 95%|█████████▌| 38/40 [02:43<00:08,  4.26s/it]2024-12-22 01:34:06,456 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:06,613 - [Process 1/5] - INFO - res.shape is :torch.Size([9])
results:The Drover's Sweetheart
100%|██████████| 40/40 [02:43<00:00,  4.05s/it]100%|██████████| 40/40 [02:43<00:00,  4.10s/it]
2024-12-22 01:34:06,777 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:06,777 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1941])
2024-12-22 01:34:06,858 - [Process 3/5] - DEBUG - predict_token:tensor([[6158]], device='cuda:3')
2024-12-22 01:34:07,135 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Denis Sauvage
 92%|█████████▎| 37/40 [02:44<00:12,  4.30s/it]2024-12-22 01:34:07,404 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:07,404 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1897])
2024-12-22 01:34:07,411 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:07,485 - [Process 0/5] - DEBUG - predict_token:tensor([[3739]], device='cuda:0')
2024-12-22 01:34:07,848 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:Walter Augustus Barratt
 98%|█████████▊| 39/40 [02:45<00:04,  4.23s/it]2024-12-22 01:34:08,051 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:08,596 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:08,596 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:34:08,670 - [Process 2/5] - DEBUG - predict_token:tensor([[11554]], device='cuda:2')
2024-12-22 01:34:08,989 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Night Of Dark Shadows
 95%|█████████▌| 38/40 [02:46<00:08,  4.25s/it]2024-12-22 01:34:09,090 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:10,073 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:10,074 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1854])
2024-12-22 01:34:10,155 - [Process 4/5] - DEBUG - predict_token:tensor([[12126]], device='cuda:4')
2024-12-22 01:34:10,306 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Italy
 98%|█████████▊| 39/40 [02:47<00:04,  4.19s/it]2024-12-22 01:34:10,498 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:11,065 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:11,065 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-22 01:34:11,139 - [Process 3/5] - DEBUG - predict_token:tensor([[14298]], device='cuda:3')
2024-12-22 01:34:11,318 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:11,319 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1179])
2024-12-22 01:34:11,366 - [Process 2/5] - DEBUG - predict_token:tensor([[9267]], device='cuda:2')
2024-12-22 01:34:11,501 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Ludwig von Westphalen
 95%|█████████▌| 38/40 [02:48<00:08,  4.32s/it]2024-12-22 01:34:11,577 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Do Musafir
 98%|█████████▊| 39/40 [02:48<00:03,  3.75s/it]2024-12-22 01:34:11,649 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:11,649 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:34:11,669 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:11,693 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:11,723 - [Process 0/5] - DEBUG - predict_token:tensor([[6978]], device='cuda:0')
2024-12-22 01:34:12,214 - [Process 0/5] - INFO - res.shape is :torch.Size([11])
results:Eyüp Cemetery, Istanbul
100%|██████████| 40/40 [02:49<00:00,  4.27s/it]100%|██████████| 40/40 [02:49<00:00,  4.24s/it]
2024-12-22 01:34:13,668 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:13,668 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1127])
2024-12-22 01:34:13,706 - [Process 2/5] - DEBUG - predict_token:tensor([[1939]], device='cuda:2')
2024-12-22 01:34:13,798 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:Yes
100%|██████████| 40/40 [02:51<00:00,  3.29s/it]100%|██████████| 40/40 [02:51<00:00,  4.28s/it]
2024-12-22 01:34:14,145 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:14,145 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:34:14,218 - [Process 4/5] - DEBUG - predict_token:tensor([[20763]], device='cuda:4')
2024-12-22 01:34:14,410 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Köln
100%|██████████| 40/40 [02:51<00:00,  4.17s/it]100%|██████████| 40/40 [02:51<00:00,  4.29s/it]
2024-12-22 01:34:15,342 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:15,342 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 01:34:15,417 - [Process 3/5] - DEBUG - predict_token:tensor([[498]], device='cuda:3')
2024-12-22 01:34:15,692 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Ji Jiang.
 98%|█████████▊| 39/40 [02:53<00:04,  4.28s/it]2024-12-22 01:34:15,909 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:34:19,758 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:34:19,758 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2113])
2024-12-22 01:34:19,831 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:34:20,191 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:3 November 1867
100%|██████████| 40/40 [02:57<00:00,  4.35s/it]100%|██████████| 40/40 [02:57<00:00,  4.44s/it]
2024-12-22 01:34:20,212 - [Process 0/5] - DEBUG - datasets_name:2wikimqa
2024-12-22 01:34:20,212 - [Process 4/5] - DEBUG - datasets_name:2wikimqa
2024-12-22 01:34:20,212 - [Process 3/5] - DEBUG - datasets_name:2wikimqa
2024-12-22 01:34:20,212 - [Process 2/5] - DEBUG - datasets_name:2wikimqa
2024-12-22 01:34:20,212 - [Process 1/5] - DEBUG - datasets_name:2wikimqa
Running evaluation for dataset: musique
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.84s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:36:28,194 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:36:28,194 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:36:28,194 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:36:28,225 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 01:36:28,226 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:36:28,226 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:36:28,226 - [Process 2/5] - INFO - output_max_len: 32
2024-12-22 01:36:28,226 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:36:28,226 - [Process 3/5] - INFO - output_max_len: 32
2024-12-22 01:36:28,226 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 01:36:28,226 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:36:28,226 - [Process 1/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:36:28,228 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:36:28,228 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:36:28,228 - [Process 0/5] - INFO - output_max_len: 32
2024-12-22 01:36:28,252 - [Process 4/5] - INFO - Max Length is 17355
2024-12-22 01:36:28,253 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:36:28,253 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:36:28,320 - [Process 3/5] - INFO - Max Length is 17355
2024-12-22 01:36:28,320 - [Process 2/5] - INFO - Max Length is 17355
2024-12-22 01:36:28,320 - [Process 1/5] - INFO - Max Length is 17355
2024-12-22 01:36:28,321 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:36:28,321 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:36:28,321 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:36:28,321 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 01:36:28,321 - [Process 2/5] - INFO - get_predicted begin
2024-12-22 01:36:28,321 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:36:28,322 - [Process 0/5] - INFO - Max Length is 17355
2024-12-22 01:36:28,323 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:36:28,323 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:36:33,003 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:33,089 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:33,089 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:33,090 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:33,091 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:37,127 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:37,127 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2289])
2024-12-22 01:36:37,191 - [Process 2/5] - DEBUG - predict_token:tensor([[360]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:36:37,433 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:37,434 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 01:36:37,434 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:37,434 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:36:37,506 - [Process 4/5] - DEBUG - predict_token:tensor([[23774]], device='cuda:4')
2024-12-22 01:36:37,508 - [Process 3/5] - DEBUG - predict_token:tensor([[8612]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:36:37,533 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:37,534 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1956])
2024-12-22 01:36:37,535 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:Dmitri Mendeleev
  2%|▎         | 1/40 [00:09<05:59,  9.21s/it]2024-12-22 01:36:37,561 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:37,562 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2172])
2024-12-22 01:36:37,612 - [Process 0/5] - DEBUG - predict_token:tensor([[1528]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:36:37,630 - [Process 1/5] - DEBUG - predict_token:tensor([[7660]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:36:37,705 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:37,709 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Tony Award
  2%|▎         | 1/40 [00:09<06:06,  9.39s/it]2024-12-22 01:36:37,841 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Tucson
  2%|▎         | 1/40 [00:09<06:11,  9.52s/it]2024-12-22 01:36:37,844 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Jennifer Connelly
  2%|▎         | 1/40 [00:09<06:14,  9.59s/it]2024-12-22 01:36:37,905 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Kangana Ranaut
  2%|▎         | 1/40 [00:09<06:13,  9.58s/it]2024-12-22 01:36:38,006 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:38,152 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:38,170 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:38,198 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:41,273 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:41,274 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 01:36:41,347 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 01:36:41,608 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:41,608 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2037])
2024-12-22 01:36:41,651 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Much Ado About Nothing
  5%|▌         | 2/40 [00:13<03:56,  6.22s/it]2024-12-22 01:36:41,682 - [Process 3/5] - DEBUG - predict_token:tensor([[1816]], device='cuda:3')
2024-12-22 01:36:41,761 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:41,761 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:36:41,822 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:41,831 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:41,831 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1978])
2024-12-22 01:36:41,834 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:36:41,877 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
2024-12-22 01:36:41,877 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:41,877 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2158])
results:Pristina
  5%|▌         | 2/40 [00:13<04:00,  6.32s/it]2024-12-22 01:36:41,910 - [Process 4/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:4')
2024-12-22 01:36:41,946 - [Process 1/5] - DEBUG - predict_token:tensor([[10319]], device='cuda:1')
2024-12-22 01:36:42,020 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:France
  5%|▌         | 2/40 [00:13<04:03,  6.41s/it]2024-12-22 01:36:42,033 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Chadwick
  5%|▌         | 2/40 [00:13<04:02,  6.37s/it]2024-12-22 01:36:42,108 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Texas
  5%|▌         | 2/40 [00:13<04:04,  6.43s/it]2024-12-22 01:36:42,124 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:42,325 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:42,328 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:42,391 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:45,382 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:45,383 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:36:45,455 - [Process 2/5] - DEBUG - predict_token:tensor([[678]], device='cuda:2')
2024-12-22 01:36:45,841 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:45,841 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1853])
2024-12-22 01:36:45,854 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:45,854 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2161])
2024-12-22 01:36:45,920 - [Process 0/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:0')
2024-12-22 01:36:45,923 - [Process 3/5] - DEBUG - predict_token:tensor([[2664]], device='cuda:3')
2024-12-22 01:36:45,978 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:45,979 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 01:36:46,020 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:46,020 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1930])
2024-12-22 01:36:46,052 - [Process 1/5] - DEBUG - predict_token:tensor([[23774]], device='cuda:1')
2024-12-22 01:36:46,100 - [Process 4/5] - DEBUG - predict_token:tensor([[19556]], device='cuda:4')
2024-12-22 01:36:46,306 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:
Please provide the answer only.
  8%|▊         | 3/40 [00:17<03:20,  5.41s/it]2024-12-22 01:36:46,309 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Lesotho Highlands Water Project
  8%|▊         | 3/40 [00:17<03:21,  5.46s/it]2024-12-22 01:36:46,351 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Kanye West
  8%|▊         | 3/40 [00:18<03:21,  5.46s/it]2024-12-22 01:36:46,391 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Edna Strickland.
  8%|▊         | 3/40 [00:18<03:21,  5.45s/it]2024-12-22 01:36:46,620 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:46,626 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:46,638 - [Process 2/5] - INFO - res.shape is :torch.Size([29])
results:
Susanne Klatten's degree in business finance and her MBA from IMD Business School in Lausanne.
2024-12-22 01:36:46,639 - [Process 4/5] - INFO - len(per_windows_prompt):2
  8%|▊         | 3/40 [00:18<03:29,  5.65s/it]2024-12-22 01:36:46,698 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:46,810 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:50,260 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:50,261 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:36:50,336 - [Process 4/5] - DEBUG - predict_token:tensor([[7560]], device='cuda:4')
2024-12-22 01:36:50,345 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:50,345 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2161])
2024-12-22 01:36:50,407 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:50,407 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:36:50,415 - [Process 0/5] - DEBUG - predict_token:tensor([[24804]], device='cuda:0')
2024-12-22 01:36:50,418 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:50,419 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:36:50,486 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Idaho
 10%|█         | 4/40 [00:22<02:57,  4.94s/it]2024-12-22 01:36:50,487 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:36:50,487 - [Process 1/5] - DEBUG - predict_token:tensor([[6978]], device='cuda:1')
2024-12-22 01:36:50,493 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:50,493 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2150])
2024-12-22 01:36:50,561 - [Process 2/5] - DEBUG - predict_token:tensor([[3739]], device='cuda:2')
2024-12-22 01:36:50,570 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Vigo
 10%|█         | 4/40 [00:22<02:58,  4.96s/it]2024-12-22 01:36:50,687 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Lea Thompson
 10%|█         | 4/40 [00:22<03:01,  5.03s/it]2024-12-22 01:36:50,730 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Passage 2
 10%|█         | 4/40 [00:22<03:00,  5.01s/it]2024-12-22 01:36:50,782 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Paul McCartney
 10%|█         | 4/40 [00:22<03:02,  5.06s/it]2024-12-22 01:36:50,792 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:50,875 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:50,952 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:50,982 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:51,022 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:54,382 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:54,382 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:36:54,453 - [Process 4/5] - DEBUG - predict_token:tensor([[4667]], device='cuda:4')
2024-12-22 01:36:54,598 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:54,599 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:36:54,678 - [Process 0/5] - DEBUG - predict_token:tensor([[1085]], device='cuda:0')
2024-12-22 01:36:54,689 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:William Holden
 12%|█▎        | 5/40 [00:26<02:43,  4.67s/it]2024-12-22 01:36:54,692 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:54,692 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:36:54,747 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:54,748 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1941])
2024-12-22 01:36:54,772 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:36:54,777 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:54,777 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 01:36:54,828 - [Process 1/5] - DEBUG - predict_token:tensor([[12245]], device='cuda:1')
2024-12-22 01:36:54,857 - [Process 3/5] - DEBUG - predict_token:tensor([[6182]], device='cuda:3')
2024-12-22 01:36:54,874 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:7
 12%|█▎        | 5/40 [00:26<02:44,  4.71s/it]2024-12-22 01:36:54,965 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:54,968 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Tanisha Smith
 12%|█▎        | 5/40 [00:26<02:46,  4.76s/it]2024-12-22 01:36:55,035 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:55,069 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Namibia
 12%|█▎        | 5/40 [00:26<02:46,  4.77s/it]2024-12-22 01:36:55,235 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:SpongeBob SquarePants
 12%|█▎        | 5/40 [00:26<02:49,  4.86s/it]2024-12-22 01:36:55,264 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:55,354 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:55,517 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:58,545 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:58,545 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 01:36:58,604 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:58,604 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 01:36:58,616 - [Process 4/5] - DEBUG - predict_token:tensor([[16704]], device='cuda:4')
2024-12-22 01:36:58,676 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:36:58,809 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Indonesia
 15%|█▌        | 6/40 [00:30<02:32,  4.48s/it]2024-12-22 01:36:58,894 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:58,894 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 01:36:58,897 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:1945
 15%|█▌        | 6/40 [00:30<02:32,  4.48s/it]2024-12-22 01:36:58,969 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:36:59,048 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:59,063 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:59,082 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:59,082 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2161])
2024-12-22 01:36:59,133 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:36:59,134 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:36:59,151 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:36:59,162 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:195
 15%|█▌        | 6/40 [00:30<02:35,  4.57s/it]2024-12-22 01:36:59,208 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:36:59,386 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:🚺
 15%|█▌        | 6/40 [00:31<02:36,  4.62s/it]2024-12-22 01:36:59,450 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:59,578 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:1936-27
 15%|█▌        | 6/40 [00:31<02:39,  4.68s/it]2024-12-22 01:36:59,671 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:36:59,860 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:02,685 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:02,685 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 01:37:02,757 - [Process 4/5] - DEBUG - predict_token:tensor([[1383]], device='cuda:4')
2024-12-22 01:37:02,762 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:02,762 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2154])
2024-12-22 01:37:02,830 - [Process 2/5] - DEBUG - predict_token:tensor([[9371]], device='cuda:2')
2024-12-22 01:37:03,038 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:03,038 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1905])
2024-12-22 01:37:03,052 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Lee Eddy
 18%|█▊        | 7/40 [00:34<02:24,  4.37s/it]2024-12-22 01:37:03,118 - [Process 0/5] - DEBUG - predict_token:tensor([[323]], device='cuda:0')
2024-12-22 01:37:03,221 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:03,439 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Tatum O'Neal
 18%|█▊        | 7/40 [00:35<02:27,  4.47s/it]2024-12-22 01:37:03,469 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:03,470 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2130])
2024-12-22 01:37:03,541 - [Process 1/5] - DEBUG - predict_token:tensor([[20303]], device='cuda:1')
2024-12-22 01:37:03,592 - [Process 4/5] - INFO - res.shape is :torch.Size([19])
results:Tuymazy District shares a border with the province where Lago District is located.
 18%|█▊        | 7/40 [00:35<02:31,  4.58s/it]2024-12-22 01:37:03,661 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:03,661 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:37:03,720 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:03,741 - [Process 3/5] - DEBUG - predict_token:tensor([[14382]], device='cuda:3')
2024-12-22 01:37:03,873 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Bhilangna River
 18%|█▊        | 7/40 [00:35<02:30,  4.57s/it]2024-12-22 01:37:03,881 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:04,019 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:$860 million
 18%|█▊        | 7/40 [00:35<02:31,  4.60s/it]2024-12-22 01:37:04,167 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:04,296 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:06,881 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:06,881 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 01:37:06,955 - [Process 2/5] - DEBUG - predict_token:tensor([[317]], device='cuda:2')
2024-12-22 01:37:07,258 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Sinead Farrelly
 20%|██        | 8/40 [00:38<02:18,  4.32s/it]2024-12-22 01:37:07,270 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:07,270 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:37:07,342 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:37:07,434 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:07,450 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:3
 20%|██        | 8/40 [00:39<02:18,  4.32s/it]2024-12-22 01:37:07,469 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:07,469 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:37:07,543 - [Process 4/5] - DEBUG - predict_token:tensor([[6556]], device='cuda:4')
2024-12-22 01:37:07,724 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:07,736 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Lionsgate
 20%|██        | 8/40 [00:39<02:22,  4.44s/it]2024-12-22 01:37:07,794 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:07,794 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 01:37:07,867 - [Process 1/5] - DEBUG - predict_token:tensor([[7311]], device='cuda:1')
2024-12-22 01:37:07,901 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:07,901 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1865])
2024-12-22 01:37:07,982 - [Process 3/5] - DEBUG - predict_token:tensor([[2610]], device='cuda:3')
2024-12-22 01:37:08,020 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:08,430 - [Process 3/5] - INFO - res.shape is :torch.Size([10])
results:May 5, 1862
 20%|██        | 8/40 [00:40<02:25,  4.54s/it]2024-12-22 01:37:08,535 - [Process 1/5] - INFO - res.shape is :torch.Size([15])
results:Because it catalyzed the call for independence from Great Britain.
 20%|██        | 8/40 [00:40<02:27,  4.60s/it]2024-12-22 01:37:08,709 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:08,820 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:11,034 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:11,034 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2054])
2024-12-22 01:37:11,108 - [Process 2/5] - DEBUG - predict_token:tensor([[25167]], device='cuda:2')
2024-12-22 01:37:11,251 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:PCH
 22%|██▎       | 9/40 [00:42<02:10,  4.22s/it]2024-12-22 01:37:11,269 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:11,269 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2269])
2024-12-22 01:37:11,332 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:37:11,417 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:11,567 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:2015
 22%|██▎       | 9/40 [00:43<02:12,  4.26s/it]2024-12-22 01:37:11,608 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:11,609 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 01:37:11,682 - [Process 4/5] - DEBUG - predict_token:tensor([[6978]], device='cuda:4')
2024-12-22 01:37:11,853 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:11,960 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Guadalupe County
 22%|██▎       | 9/40 [00:43<02:15,  4.37s/it]2024-12-22 01:37:12,246 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:12,467 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:12,467 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1911])
2024-12-22 01:37:12,523 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:12,523 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2151])
2024-12-22 01:37:12,548 - [Process 1/5] - DEBUG - predict_token:tensor([[24743]], device='cuda:1')
2024-12-22 01:37:12,595 - [Process 3/5] - DEBUG - predict_token:tensor([[6932]], device='cuda:3')
2024-12-22 01:37:12,784 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Poplar River County
 22%|██▎       | 9/40 [00:44<02:19,  4.49s/it]2024-12-22 01:37:12,874 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Timor-Leste
 22%|██▎       | 9/40 [00:44<02:19,  4.51s/it]2024-12-22 01:37:13,068 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:13,148 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:15,073 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:15,073 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2067])
2024-12-22 01:37:15,148 - [Process 2/5] - DEBUG - predict_token:tensor([[12391]], device='cuda:2')
2024-12-22 01:37:15,493 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:1894-95
 25%|██▌       | 10/40 [00:47<02:06,  4.22s/it]2024-12-22 01:37:15,591 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:15,591 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2016])
2024-12-22 01:37:15,657 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:15,671 - [Process 0/5] - DEBUG - predict_token:tensor([[319]], device='cuda:0')
2024-12-22 01:37:15,869 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:15,870 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1916])
2024-12-22 01:37:15,950 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:TDRS-6
 25%|██▌       | 10/40 [00:47<02:08,  4.30s/it]2024-12-22 01:37:15,951 - [Process 4/5] - DEBUG - predict_token:tensor([[8314]], device='cuda:4')
2024-12-22 01:37:16,236 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:16,272 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Nuevo Laredo
 25%|██▌       | 10/40 [00:48<02:10,  4.36s/it]2024-12-22 01:37:16,533 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:16,674 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:16,675 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 01:37:16,749 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:37:16,806 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:16,807 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1917])
2024-12-22 01:37:16,889 - [Process 3/5] - DEBUG - predict_token:tensor([[530]], device='cuda:3')
2024-12-22 01:37:17,028 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Lost in Munich
 25%|██▌       | 10/40 [00:48<02:12,  4.41s/it]2024-12-22 01:37:17,086 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Nayah
 25%|██▌       | 10/40 [00:48<02:12,  4.42s/it]2024-12-22 01:37:17,310 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:17,359 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:19,245 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:19,245 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 01:37:19,318 - [Process 2/5] - DEBUG - predict_token:tensor([[435]], device='cuda:2')
2024-12-22 01:37:19,539 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Jill Flint
 28%|██▊       | 11/40 [00:51<02:00,  4.17s/it]2024-12-22 01:37:19,702 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:19,942 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:19,942 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2169])
2024-12-22 01:37:20,011 - [Process 0/5] - DEBUG - predict_token:tensor([[4755]], device='cuda:0')
2024-12-22 01:37:20,138 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:20,138 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-22 01:37:20,211 - [Process 4/5] - DEBUG - predict_token:tensor([[26631]], device='cuda:4')
2024-12-22 01:37:20,290 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Robert Khayat
 28%|██▊       | 11/40 [00:51<02:05,  4.31s/it]2024-12-22 01:37:20,532 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Sebastian Cabot
 28%|██▊       | 11/40 [00:52<02:05,  4.33s/it]2024-12-22 01:37:20,564 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:20,815 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:20,917 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:20,917 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:37:20,968 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:20,968 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1861])
2024-12-22 01:37:20,991 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:37:21,049 - [Process 3/5] - DEBUG - predict_token:tensor([[498]], device='cuda:3')
2024-12-22 01:37:21,200 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Thailand
 28%|██▊       | 11/40 [00:52<02:05,  4.33s/it]2024-12-22 01:37:21,439 - [Process 1/5] - INFO - res.shape is :torch.Size([10])
results:The African Queen (1951)
 28%|██▊       | 11/40 [00:53<02:07,  4.41s/it]2024-12-22 01:37:21,470 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:21,699 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:23,441 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:23,442 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2185])
2024-12-22 01:37:23,511 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:37:23,813 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:30-60%
 30%|███       | 12/40 [00:55<01:57,  4.20s/it]2024-12-22 01:37:23,983 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:24,115 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:24,115 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1876])
2024-12-22 01:37:24,195 - [Process 0/5] - DEBUG - predict_token:tensor([[28620]], device='cuda:0')
2024-12-22 01:37:24,516 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Royal Society of Chemistry
 30%|███       | 12/40 [00:56<01:59,  4.29s/it]2024-12-22 01:37:24,541 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:24,542 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2170])
2024-12-22 01:37:24,611 - [Process 4/5] - DEBUG - predict_token:tensor([[8980]], device='cuda:4')
2024-12-22 01:37:24,791 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:24,931 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Veera Ballala II
 30%|███       | 12/40 [00:56<02:01,  4.35s/it]2024-12-22 01:37:25,146 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:25,146 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1904])
2024-12-22 01:37:25,210 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:25,228 - [Process 3/5] - DEBUG - predict_token:tensor([[17468]], device='cuda:3')
2024-12-22 01:37:25,434 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:25,434 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1956])
2024-12-22 01:37:25,506 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Bonnie Pink
 30%|███       | 12/40 [00:57<02:00,  4.32s/it]2024-12-22 01:37:25,514 - [Process 1/5] - DEBUG - predict_token:tensor([[24607]], device='cuda:1')
2024-12-22 01:37:25,750 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Leicester City
 30%|███       | 12/40 [00:57<02:02,  4.38s/it]2024-12-22 01:37:25,783 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:25,971 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:27,766 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:27,767 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2126])
2024-12-22 01:37:27,839 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:37:28,061 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:1985
 32%|███▎      | 13/40 [00:59<01:53,  4.22s/it]2024-12-22 01:37:28,224 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:28,374 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:28,374 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 01:37:28,447 - [Process 0/5] - DEBUG - predict_token:tensor([[2661]], device='cuda:0')
2024-12-22 01:37:28,640 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Tom Hood
 32%|███▎      | 13/40 [01:00<01:54,  4.24s/it]2024-12-22 01:37:28,819 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:28,820 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 01:37:28,894 - [Process 4/5] - DEBUG - predict_token:tensor([[21137]], device='cuda:4')
2024-12-22 01:37:28,913 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:29,087 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:California
 32%|███▎      | 13/40 [01:00<01:55,  4.29s/it]2024-12-22 01:37:29,356 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:29,412 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:29,413 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:37:29,487 - [Process 3/5] - DEBUG - predict_token:tensor([[4275]], device='cuda:3')
2024-12-22 01:37:29,704 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:29,704 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1940])
2024-12-22 01:37:29,765 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Northern Territory
 32%|███▎      | 13/40 [01:01<01:56,  4.30s/it]2024-12-22 01:37:29,784 - [Process 1/5] - DEBUG - predict_token:tensor([[14092]], device='cuda:1')
2024-12-22 01:37:30,045 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:30,104 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Golestan province.
 32%|███▎      | 13/40 [01:01<01:58,  4.37s/it]2024-12-22 01:37:30,382 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:31,864 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:31,865 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2206])
2024-12-22 01:37:31,931 - [Process 2/5] - DEBUG - predict_token:tensor([[27598]], device='cuda:2')
2024-12-22 01:37:32,233 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Château de Chalmazel
 35%|███▌      | 14/40 [01:03<01:49,  4.20s/it]2024-12-22 01:37:32,402 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:32,494 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:32,494 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 01:37:32,567 - [Process 0/5] - DEBUG - predict_token:tensor([[23408]], device='cuda:0')
2024-12-22 01:37:32,761 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Emily Johnson
 35%|███▌      | 14/40 [01:04<01:49,  4.20s/it]2024-12-22 01:37:33,009 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:33,081 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:33,081 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1953])
2024-12-22 01:37:33,160 - [Process 4/5] - DEBUG - predict_token:tensor([[12798]], device='cuda:4')
2024-12-22 01:37:33,353 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Dillon Beach
 35%|███▌      | 14/40 [01:05<01:51,  4.28s/it]2024-12-22 01:37:33,625 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:33,680 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:33,681 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:37:33,753 - [Process 3/5] - DEBUG - predict_token:tensor([[13645]], device='cuda:3')
2024-12-22 01:37:34,190 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:34,190 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:37:34,261 - [Process 1/5] - DEBUG - predict_token:tensor([[7747]], device='cuda:1')
2024-12-22 01:37:34,626 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:CeCe Peniston
 35%|███▌      | 14/40 [01:06<01:54,  4.42s/it]2024-12-22 01:37:34,841 - [Process 3/5] - INFO - res.shape is :torch.Size([25])
results:François Bégaudeau received an Academy Award nomination for Best Actor in 2009.
 35%|███▌      | 14/40 [01:06<01:57,  4.54s/it]2024-12-22 01:37:34,908 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:35,123 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:36,188 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:36,188 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2141])
2024-12-22 01:37:36,260 - [Process 2/5] - DEBUG - predict_token:tensor([[390]], device='cuda:2')
2024-12-22 01:37:36,482 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Ricky Wilde
 38%|███▊      | 15/40 [01:08<01:45,  4.22s/it]2024-12-22 01:37:36,656 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:36,715 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:36,715 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1978])
2024-12-22 01:37:36,795 - [Process 0/5] - DEBUG - predict_token:tensor([[390]], device='cuda:0')
2024-12-22 01:37:37,228 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:37,228 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1803])
2024-12-22 01:37:37,311 - [Process 4/5] - DEBUG - predict_token:tensor([[6682]], device='cuda:4')
2024-12-22 01:37:37,546 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:BJK Cup
 38%|███▊      | 15/40 [01:09<01:46,  4.26s/it]2024-12-22 01:37:37,798 - [Process 0/5] - INFO - res.shape is :torch.Size([23])
results:The source of the river that is the mouth of the Caledon River is the Rufiji River.
 38%|███▊      | 15/40 [01:09<01:51,  4.45s/it]2024-12-22 01:37:37,823 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:38,069 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:38,653 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:38,653 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-22 01:37:38,733 - [Process 1/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:1')
2024-12-22 01:37:38,903 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:38,903 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2141])
2024-12-22 01:37:38,927 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Nottingham
 38%|███▊      | 15/40 [01:10<01:49,  4.38s/it]2024-12-22 01:37:38,972 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:37:39,202 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:39,249 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:January 1
 38%|███▊      | 15/40 [01:10<01:52,  4.50s/it]2024-12-22 01:37:39,447 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:40,289 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:40,289 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1907])
2024-12-22 01:37:40,371 - [Process 2/5] - DEBUG - predict_token:tensor([[2216]], device='cuda:2')
2024-12-22 01:37:40,593 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Britt Robertson
 40%|████      | 16/40 [01:12<01:40,  4.18s/it]2024-12-22 01:37:40,763 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:41,622 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:41,622 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 01:37:41,703 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:37:41,820 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:41,820 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1986])
2024-12-22 01:37:41,897 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Jakarta
 40%|████      | 16/40 [01:13<01:42,  4.28s/it]2024-12-22 01:37:41,899 - [Process 0/5] - DEBUG - predict_token:tensor([[7997]], device='cuda:0')
2024-12-22 01:37:42,178 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Gripsholm Bridge
 40%|████      | 16/40 [01:13<01:46,  4.43s/it]2024-12-22 01:37:42,180 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:42,460 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:42,948 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:42,948 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1965])
2024-12-22 01:37:43,028 - [Process 1/5] - DEBUG - predict_token:tensor([[476]], device='cuda:1')
2024-12-22 01:37:43,127 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:43,127 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2349])
2024-12-22 01:37:43,192 - [Process 3/5] - DEBUG - predict_token:tensor([[14305]], device='cuda:3')
2024-12-22 01:37:43,263 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Mawdud
 40%|████      | 16/40 [01:14<01:44,  4.37s/it]2024-12-22 01:37:43,526 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:43,553 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:
Please provide the next question.
 40%|████      | 16/40 [01:15<01:46,  4.44s/it]2024-12-22 01:37:43,832 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:44,484 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:44,484 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1976])
2024-12-22 01:37:44,564 - [Process 2/5] - DEBUG - predict_token:tensor([[4581]], device='cuda:2')
2024-12-22 01:37:44,947 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:Janusz Korwin-Mikke
 42%|████▎     | 17/40 [01:16<01:37,  4.24s/it]2024-12-22 01:37:45,113 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:45,976 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:45,977 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:37:46,026 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:46,026 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1876])
2024-12-22 01:37:46,057 - [Process 4/5] - DEBUG - predict_token:tensor([[1530]], device='cuda:4')
2024-12-22 01:37:46,107 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:37:46,251 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Lionel
 42%|████▎     | 17/40 [01:17<01:39,  4.31s/it]2024-12-22 01:37:46,343 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:2018
 42%|████▎     | 17/40 [01:18<01:40,  4.35s/it]2024-12-22 01:37:46,532 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:46,622 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:47,334 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:47,335 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:37:47,415 - [Process 1/5] - DEBUG - predict_token:tensor([[19512]], device='cuda:1')
2024-12-22 01:37:47,455 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:47,456 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1866])
2024-12-22 01:37:47,537 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:37:47,651 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:River Thames
 42%|████▎     | 17/40 [01:19<01:40,  4.37s/it]2024-12-22 01:37:47,773 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:2014
 42%|████▎     | 17/40 [01:19<01:40,  4.37s/it]2024-12-22 01:37:47,930 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:48,050 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:48,691 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:48,691 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1887])
2024-12-22 01:37:48,772 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:37:49,075 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Ontario, Canada.
 45%|████▌     | 18/40 [01:20<01:32,  4.20s/it]2024-12-22 01:37:49,248 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:50,202 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:50,202 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 01:37:50,275 - [Process 0/5] - DEBUG - predict_token:tensor([[10586]], device='cuda:0')
2024-12-22 01:37:50,325 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:50,326 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 01:37:50,406 - [Process 4/5] - DEBUG - predict_token:tensor([[1174]], device='cuda:4')
2024-12-22 01:37:50,511 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:George Clooney
 45%|████▌     | 18/40 [01:22<01:34,  4.30s/it]2024-12-22 01:37:50,727 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Wapizagonke Lake
 45%|████▌     | 18/40 [01:22<01:35,  4.36s/it]2024-12-22 01:37:50,799 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:51,008 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:51,598 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:51,599 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1917])
2024-12-22 01:37:51,680 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:37:51,820 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:51,820 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:37:51,890 - [Process 3/5] - DEBUG - predict_token:tensor([[349]], device='cuda:3')
2024-12-22 01:37:51,917 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:1985
 45%|████▌     | 18/40 [01:23<01:35,  4.34s/it]2024-12-22 01:37:52,167 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Prysmian Group
 45%|████▌     | 18/40 [01:23<01:36,  4.38s/it]2024-12-22 01:37:52,203 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:52,440 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:52,972 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:52,973 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1983])
2024-12-22 01:37:53,051 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:37:53,274 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:2010
 48%|████▊     | 19/40 [01:24<01:28,  4.20s/it]2024-12-22 01:37:53,439 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:54,395 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:54,395 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:37:54,469 - [Process 0/5] - DEBUG - predict_token:tensor([[530]], device='cuda:0')
2024-12-22 01:37:54,662 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Sincik
 48%|████▊     | 19/40 [01:26<01:29,  4.25s/it]2024-12-22 01:37:54,759 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:54,759 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1942])
2024-12-22 01:37:54,840 - [Process 4/5] - DEBUG - predict_token:tensor([[349]], device='cuda:4')
2024-12-22 01:37:54,938 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:55,630 - [Process 4/5] - INFO - res.shape is :torch.Size([18])
results:The author, Thornton Wilder, received a Tony Award for his play.
 48%|████▊     | 19/40 [01:27<01:34,  4.52s/it]2024-12-22 01:37:55,913 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:55,963 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:55,964 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2160])
2024-12-22 01:37:56,033 - [Process 1/5] - DEBUG - predict_token:tensor([[24710]], device='cuda:1')
2024-12-22 01:37:56,097 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:56,097 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1744])
2024-12-22 01:37:56,181 - [Process 3/5] - DEBUG - predict_token:tensor([[796]], device='cuda:3')
2024-12-22 01:37:56,269 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Stuart Mitchell
 48%|████▊     | 19/40 [01:27<01:31,  4.34s/it]2024-12-22 01:37:56,374 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Wenzhou
 48%|████▊     | 19/40 [01:28<01:30,  4.33s/it]2024-12-22 01:37:56,554 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:56,651 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:57,225 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:57,225 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 01:37:57,306 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:37:57,528 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:1914
 50%|█████     | 20/40 [01:29<01:24,  4.22s/it]2024-12-22 01:37:57,698 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:58,527 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:58,527 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:37:58,601 - [Process 0/5] - DEBUG - predict_token:tensor([[17778]], device='cuda:0')
2024-12-22 01:37:58,878 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Amanda Feilding
 50%|█████     | 20/40 [01:30<01:24,  4.24s/it]2024-12-22 01:37:59,160 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:37:59,656 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:37:59,656 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1955])
2024-12-22 01:37:59,736 - [Process 4/5] - DEBUG - predict_token:tensor([[4709]], device='cuda:4')
2024-12-22 01:37:59,929 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Glasgow
 50%|█████     | 20/40 [01:31<01:29,  4.45s/it]2024-12-22 01:38:00,205 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:00,219 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:00,219 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2072])
2024-12-22 01:38:00,293 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:38:00,481 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:00,481 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:38:00,562 - [Process 3/5] - DEBUG - predict_token:tensor([[8432]], device='cuda:3')
2024-12-22 01:38:00,572 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:20111
 50%|█████     | 20/40 [01:32<01:26,  4.33s/it]2024-12-22 01:38:00,798 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Luke Bryan
 50%|█████     | 20/40 [01:32<01:27,  4.36s/it]2024-12-22 01:38:00,804 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:01,073 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:01,492 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:01,492 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2137])
2024-12-22 01:38:01,564 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:38:01,786 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:2005
 52%|█████▎    | 21/40 [01:33<01:20,  4.23s/it]2024-12-22 01:38:01,953 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:02,863 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:02,863 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1935])
2024-12-22 01:38:02,942 - [Process 0/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:0')
2024-12-22 01:38:03,834 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:03,835 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 01:38:03,909 - [Process 4/5] - DEBUG - predict_token:tensor([[405]], device='cuda:4')
2024-12-22 01:38:04,146 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Louis Chedid
 52%|█████▎    | 21/40 [01:35<01:23,  4.38s/it]2024-12-22 01:38:04,327 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:
(Note: I will only provide the answer to the question you asked, without any additional information or context. Please let me know if you have any further
 52%|█████▎    | 21/40 [01:36<01:27,  4.60s/it]2024-12-22 01:38:04,418 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:04,575 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:04,575 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1946])
2024-12-22 01:38:04,608 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:04,657 - [Process 1/5] - DEBUG - predict_token:tensor([[349]], device='cuda:1')
2024-12-22 01:38:04,850 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Pécs
 52%|█████▎    | 21/40 [01:36<01:22,  4.32s/it]2024-12-22 01:38:04,906 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:04,906 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2120])
2024-12-22 01:38:04,980 - [Process 3/5] - DEBUG - predict_token:tensor([[12553]], device='cuda:3')
2024-12-22 01:38:05,115 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:05,257 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Katzenstein Castle
 52%|█████▎    | 21/40 [01:36<01:23,  4.39s/it]2024-12-22 01:38:05,536 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:05,741 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:05,741 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2130])
2024-12-22 01:38:05,813 - [Process 2/5] - DEBUG - predict_token:tensor([[25203]], device='cuda:2')
2024-12-22 01:38:06,115 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Oklahoma Panhandle State University
 55%|█████▌    | 22/40 [01:37<01:16,  4.26s/it]2024-12-22 01:38:06,284 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:08,232 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:08,233 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2117])
2024-12-22 01:38:08,305 - [Process 4/5] - DEBUG - predict_token:tensor([[7311]], device='cuda:4')
2024-12-22 01:38:08,320 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:08,320 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1973])
2024-12-22 01:38:08,399 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:38:08,636 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:1987
 55%|█████▌    | 22/40 [01:40<01:21,  4.52s/it]2024-12-22 01:38:08,874 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:08,875 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2164])
2024-12-22 01:38:08,933 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:08,944 - [Process 1/5] - DEBUG - predict_token:tensor([[2259]], device='cuda:1')
2024-12-22 01:38:09,196 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:John Cowsill
 55%|█████▌    | 22/40 [01:40<01:17,  4.32s/it]2024-12-22 01:38:09,222 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:09,222 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1906])
2024-12-22 01:38:09,305 - [Process 3/5] - DEBUG - predict_token:tensor([[341]], device='cuda:3')
2024-12-22 01:38:09,498 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:09,583 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Tina Nicholson
 55%|█████▌    | 22/40 [01:41<01:18,  4.37s/it]2024-12-22 01:38:09,679 - [Process 4/5] - INFO - res.shape is :torch.Size([31])
results:Roncalli left the place of death of the maker of The Gozzi Altarpiece because he was transferred to another location.
 55%|█████▌    | 22/40 [01:41<01:25,  4.73s/it]2024-12-22 01:38:09,858 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:09,972 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:10,071 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:10,072 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 01:38:10,152 - [Process 2/5] - DEBUG - predict_token:tensor([[2819]], device='cuda:2')
2024-12-22 01:38:10,455 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Jennifer Connelly
 57%|█████▊    | 23/40 [01:42<01:12,  4.28s/it]2024-12-22 01:38:10,627 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:12,642 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:12,642 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1983])
2024-12-22 01:38:12,721 - [Process 0/5] - DEBUG - predict_token:tensor([[28794]], device='cuda:0')
2024-12-22 01:38:13,319 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:13,319 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2127])
2024-12-22 01:38:13,392 - [Process 1/5] - DEBUG - predict_token:tensor([[14212]], device='cuda:1')
2024-12-22 01:38:13,545 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:13,545 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1909])
2024-12-22 01:38:13,617 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:13,617 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2320])
2024-12-22 01:38:13,627 - [Process 3/5] - DEBUG - predict_token:tensor([[9184]], device='cuda:3')
2024-12-22 01:38:13,670 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Natalie Horler
 57%|█████▊    | 23/40 [01:45<01:14,  4.37s/it]2024-12-22 01:38:13,682 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 01:38:13,788 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Rome
 57%|█████▊    | 23/40 [01:45<01:13,  4.32s/it]2024-12-22 01:38:13,970 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:14,054 - [Process 4/5] - INFO - res.shape is :torch.Size([8])
results:The Royal Swedish Academy of Sciences.
 57%|█████▊    | 23/40 [01:45<01:18,  4.62s/it]2024-12-22 01:38:14,082 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:14,131 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:
(Note: I will only provide the answer to the question you asked, without any additional information or context. Please let me know if you have any further
 57%|█████▊    | 23/40 [01:45<01:21,  4.81s/it]2024-12-22 01:38:14,335 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:14,414 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:14,420 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:14,420 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2119])
2024-12-22 01:38:14,492 - [Process 2/5] - DEBUG - predict_token:tensor([[26089]], device='cuda:2')
2024-12-22 01:38:14,754 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Vivien Leigh
 60%|██████    | 24/40 [01:46<01:08,  4.29s/it]2024-12-22 01:38:14,920 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:17,731 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:17,731 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1983])
2024-12-22 01:38:17,778 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:17,778 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1911])
2024-12-22 01:38:17,811 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:38:17,861 - [Process 3/5] - DEBUG - predict_token:tensor([[9531]], device='cuda:3')
2024-12-22 01:38:18,048 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:2005
 60%|██████    | 24/40 [01:49<01:09,  4.37s/it]2024-12-22 01:38:18,096 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:18,097 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2157])
2024-12-22 01:38:18,166 - [Process 4/5] - DEBUG - predict_token:tensor([[6971]], device='cuda:4')
2024-12-22 01:38:18,183 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:18,183 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2005])
2024-12-22 01:38:18,263 - [Process 0/5] - DEBUG - predict_token:tensor([[9511]], device='cuda:0')
2024-12-22 01:38:18,355 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:18,405 - [Process 3/5] - INFO - res.shape is :torch.Size([12])
results:Japan turned its military attention to Soviet territories.
 60%|██████    | 24/40 [01:50<01:10,  4.41s/it]2024-12-22 01:38:18,408 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Hans Modrow
 60%|██████    | 24/40 [01:50<01:12,  4.54s/it]2024-12-22 01:38:18,530 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:18,530 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 01:38:18,554 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Lurdes Pires
 60%|██████    | 24/40 [01:50<01:15,  4.69s/it]2024-12-22 01:38:18,605 - [Process 2/5] - DEBUG - predict_token:tensor([[12693]], device='cuda:2')
2024-12-22 01:38:18,670 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:18,689 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:18,827 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Lucy Mack Smith
 62%|██████▎   | 25/40 [01:50<01:03,  4.22s/it]2024-12-22 01:38:18,849 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:18,994 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:21,993 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:21,993 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:38:22,067 - [Process 1/5] - DEBUG - predict_token:tensor([[9937]], device='cuda:1')
2024-12-22 01:38:22,302 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Christian Kindahl
 62%|██████▎   | 25/40 [01:53<01:05,  4.34s/it]2024-12-22 01:38:22,371 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:22,371 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2191])
2024-12-22 01:38:22,438 - [Process 3/5] - DEBUG - predict_token:tensor([[15888]], device='cuda:3')
2024-12-22 01:38:22,451 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:22,451 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2146])
2024-12-22 01:38:22,453 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:22,453 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 01:38:22,520 - [Process 4/5] - DEBUG - predict_token:tensor([[402]], device='cuda:4')
2024-12-22 01:38:22,527 - [Process 0/5] - DEBUG - predict_token:tensor([[11571]], device='cuda:0')
2024-12-22 01:38:22,581 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:22,612 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:22,612 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2107])
2024-12-22 01:38:22,682 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Hughesville
 62%|██████▎   | 25/40 [01:54<01:05,  4.37s/it]2024-12-22 01:38:22,685 - [Process 2/5] - DEBUG - predict_token:tensor([[26484]], device='cuda:2')
2024-12-22 01:38:22,818 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Edward Pakenham
 62%|██████▎   | 25/40 [01:54<01:08,  4.56s/it]2024-12-22 01:38:22,842 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Wapizagonke Lake
 62%|██████▎   | 25/40 [01:54<01:07,  4.51s/it]2024-12-22 01:38:22,906 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Cracovia
 65%|██████▌   | 26/40 [01:54<00:58,  4.18s/it]2024-12-22 01:38:22,985 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:23,077 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:23,086 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:23,122 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:26,408 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:26,408 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2121])
2024-12-22 01:38:26,481 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 01:38:26,608 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:26,608 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1847])
2024-12-22 01:38:26,690 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:38:26,694 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:26,694 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1896])
2024-12-22 01:38:26,761 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:26,761 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2112])
2024-12-22 01:38:26,774 - [Process 0/5] - DEBUG - predict_token:tensor([[4275]], device='cuda:0')
2024-12-22 01:38:26,825 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:26,825 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1923])
2024-12-22 01:38:26,833 - [Process 4/5] - DEBUG - predict_token:tensor([[10465]], device='cuda:4')
2024-12-22 01:38:26,907 - [Process 2/5] - DEBUG - predict_token:tensor([[365]], device='cuda:2')
2024-12-22 01:38:26,937 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:2015
 65%|██████▌   | 26/40 [01:58<01:00,  4.34s/it]2024-12-22 01:38:27,066 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:East Palatka
 65%|██████▌   | 26/40 [01:58<01:02,  4.47s/it]2024-12-22 01:38:27,131 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Blaine Larsen
 65%|██████▌   | 26/40 [01:58<01:02,  4.44s/it]2024-12-22 01:38:27,169 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Oak Lawn.
 68%|██████▊   | 27/40 [01:58<00:54,  4.21s/it]2024-12-22 01:38:27,230 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:27,342 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:27,364 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:27,436 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:27,645 - [Process 1/5] - INFO - res.shape is :torch.Size([26])
results:The God of the underworld in ancient Egypt is a part of the pantheon of the sons of Horus.
 65%|██████▌   | 26/40 [01:59<01:04,  4.64s/it]2024-12-22 01:38:27,932 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:30,965 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:30,965 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2089])
2024-12-22 01:38:31,002 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:31,002 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2045])
2024-12-22 01:38:31,027 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:31,027 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 01:38:31,038 - [Process 2/5] - DEBUG - predict_token:tensor([[678]], device='cuda:2')
2024-12-22 01:38:31,077 - [Process 0/5] - DEBUG - predict_token:tensor([[12931]], device='cuda:0')
2024-12-22 01:38:31,109 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 01:38:31,244 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:31,245 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1985])
2024-12-22 01:38:31,326 - [Process 4/5] - DEBUG - predict_token:tensor([[21265]], device='cuda:4')
2024-12-22 01:38:31,327 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:International Olympic Committee
 68%|██████▊   | 27/40 [02:03<00:57,  4.41s/it]2024-12-22 01:38:31,380 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:Chandrayaan-1
 70%|███████   | 28/40 [02:03<00:50,  4.21s/it]2024-12-22 01:38:31,403 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:15 consecutive wins.
 68%|██████▊   | 27/40 [02:03<00:56,  4.37s/it]2024-12-22 01:38:31,513 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:31,566 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:31,566 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 01:38:31,630 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:31,640 - [Process 1/5] - DEBUG - predict_token:tensor([[8699]], device='cuda:1')
2024-12-22 01:38:31,697 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:31,801 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:New York
 68%|██████▊   | 27/40 [02:03<00:58,  4.49s/it]2024-12-22 01:38:32,097 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:32,178 - [Process 4/5] - INFO - res.shape is :torch.Size([19])
results:The school where Lawrence Landweber was educated is a part of the Family Education Network.
 68%|██████▊   | 27/40 [02:03<01:00,  4.62s/it]2024-12-22 01:38:32,452 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:35,128 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:35,128 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:38:35,203 - [Process 2/5] - DEBUG - predict_token:tensor([[4121]], device='cuda:2')
2024-12-22 01:38:35,405 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:35,405 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2154])
2024-12-22 01:38:35,476 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:38:35,530 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:35,531 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:38:35,612 - [Process 3/5] - DEBUG - predict_token:tensor([[1085]], device='cuda:3')
2024-12-22 01:38:35,719 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:1964
 70%|███████   | 28/40 [02:07<00:52,  4.40s/it]2024-12-22 01:38:35,785 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:35,785 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 01:38:35,862 - [Process 1/5] - DEBUG - predict_token:tensor([[23774]], device='cuda:1')
2024-12-22 01:38:35,905 - [Process 2/5] - INFO - res.shape is :torch.Size([17])
results:The river on which the Lostock Dam is located is the Paterson River.
 72%|███████▎  | 29/40 [02:07<00:47,  4.30s/it]2024-12-22 01:38:36,022 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:36,064 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:36,065 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1881])
2024-12-22 01:38:36,076 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:36,140 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Edna Strickland
 70%|███████   | 28/40 [02:07<00:53,  4.45s/it]2024-12-22 01:38:36,146 - [Process 4/5] - DEBUG - predict_token:tensor([[5556]], device='cuda:4')
2024-12-22 01:38:36,302 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Baranya
 70%|███████   | 28/40 [02:08<00:53,  4.47s/it]2024-12-22 01:38:36,352 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:36,374 - [Process 3/5] - INFO - res.shape is :torch.Size([17])
results:

(Please provide the answer in a single word or a short phrase.)
 70%|███████   | 28/40 [02:08<00:54,  4.55s/it]2024-12-22 01:38:36,599 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:36,653 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:39,776 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:39,776 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:38:39,794 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:39,794 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 01:38:39,852 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 01:38:39,874 - [Process 0/5] - DEBUG - predict_token:tensor([[9511]], device='cuda:0')
2024-12-22 01:38:39,985 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:39,986 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 01:38:40,060 - [Process 1/5] - DEBUG - predict_token:tensor([[317]], device='cuda:1')
2024-12-22 01:38:40,153 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Satyo Husodo
 72%|███████▎  | 29/40 [02:11<00:48,  4.41s/it]2024-12-22 01:38:40,196 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:
10,864
 75%|███████▌  | 30/40 [02:11<00:42,  4.30s/it]2024-12-22 01:38:40,227 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:40,227 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:38:40,302 - [Process 4/5] - DEBUG - predict_token:tensor([[12208]], device='cuda:4')
2024-12-22 01:38:40,357 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:40,430 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:

Passage 19
 72%|███████▎  | 29/40 [02:12<00:48,  4.40s/it]2024-12-22 01:38:40,433 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:40,494 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:40,495 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2114])
2024-12-22 01:38:40,505 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Moss Point
 72%|███████▎  | 29/40 [02:12<00:48,  4.39s/it]2024-12-22 01:38:40,567 - [Process 3/5] - DEBUG - predict_token:tensor([[9811]], device='cuda:3')
2024-12-22 01:38:40,742 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:40,800 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:40,802 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Matt Willis
 72%|███████▎  | 29/40 [02:12<00:49,  4.52s/it]2024-12-22 01:38:41,083 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:44,022 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:44,022 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2239])
2024-12-22 01:38:44,025 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:44,025 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 01:38:44,089 - [Process 2/5] - DEBUG - predict_token:tensor([[349]], device='cuda:2')
2024-12-22 01:38:44,097 - [Process 0/5] - DEBUG - predict_token:tensor([[6498]], device='cuda:0')
2024-12-22 01:38:44,271 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Pécs
 78%|███████▊  | 31/40 [02:15<00:38,  4.23s/it]2024-12-22 01:38:44,375 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Elizabeth of York
 75%|███████▌  | 30/40 [02:16<00:43,  4.35s/it]2024-12-22 01:38:44,436 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:44,505 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:44,505 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 01:38:44,507 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:44,507 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2168])
2024-12-22 01:38:44,576 - [Process 1/5] - DEBUG - predict_token:tensor([[478]], device='cuda:1')
2024-12-22 01:38:44,578 - [Process 4/5] - DEBUG - predict_token:tensor([[365]], device='cuda:4')
2024-12-22 01:38:44,600 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:44,923 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Lacey Chabert
 75%|███████▌  | 30/40 [02:16<00:44,  4.40s/it]2024-12-22 01:38:44,926 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:44,926 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2120])
2024-12-22 01:38:44,964 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:Vinko Dvořák
 75%|███████▌  | 30/40 [02:16<00:44,  4.44s/it]2024-12-22 01:38:45,000 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:38:45,196 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:45,253 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:45,277 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:
1982
 75%|███████▌  | 30/40 [02:16<00:45,  4.50s/it]2024-12-22 01:38:45,549 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:48,105 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:48,106 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2229])
2024-12-22 01:38:48,172 - [Process 2/5] - DEBUG - predict_token:tensor([[16704]], device='cuda:2')
2024-12-22 01:38:48,342 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:48,342 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1679])
2024-12-22 01:38:48,354 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Indonesia
 80%|████████  | 32/40 [02:20<00:33,  4.19s/it]2024-12-22 01:38:48,435 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:38:48,519 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:48,672 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:1865
 78%|███████▊  | 31/40 [02:20<00:39,  4.34s/it]2024-12-22 01:38:48,838 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:48,839 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 01:38:48,913 - [Process 4/5] - DEBUG - predict_token:tensor([[7935]], device='cuda:4')
2024-12-22 01:38:48,954 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:49,012 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:49,012 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1974])
2024-12-22 01:38:49,064 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Jive
 78%|███████▊  | 31/40 [02:20<00:38,  4.32s/it]2024-12-22 01:38:49,092 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 01:38:49,214 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:49,214 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1823])
2024-12-22 01:38:49,279 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:49,299 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:38:49,535 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:2008
 78%|███████▊  | 31/40 [02:21<00:39,  4.43s/it]2024-12-22 01:38:49,800 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:50,489 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:The life expectancy of the type of retriever named after the body of water the Patuxent River turns into is 10 hours and 4
 78%|███████▊  | 31/40 [02:22<00:42,  4.77s/it]2024-12-22 01:38:50,773 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:52,131 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:52,132 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2108])
2024-12-22 01:38:52,204 - [Process 2/5] - DEBUG - predict_token:tensor([[8612]], device='cuda:2')
2024-12-22 01:38:52,385 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Audrey
 82%|████████▎ | 33/40 [02:24<00:28,  4.14s/it]2024-12-22 01:38:52,517 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:52,518 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1870])
2024-12-22 01:38:52,552 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:52,598 - [Process 0/5] - DEBUG - predict_token:tensor([[10088]], device='cuda:0')
2024-12-22 01:38:53,005 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:It was in the process of doing.
 80%|████████  | 32/40 [02:24<00:34,  4.34s/it]2024-12-22 01:38:53,032 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:53,032 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1967])
2024-12-22 01:38:53,112 - [Process 4/5] - DEBUG - predict_token:tensor([[18292]], device='cuda:4')
2024-12-22 01:38:53,286 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:53,390 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Kill Rock Stars.
 80%|████████  | 32/40 [02:25<00:34,  4.32s/it]2024-12-22 01:38:53,584 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:53,585 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2154])
2024-12-22 01:38:53,654 - [Process 3/5] - DEBUG - predict_token:tensor([[5918]], device='cuda:3')
2024-12-22 01:38:53,669 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:53,889 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Maxime Bono
 80%|████████  | 32/40 [02:25<00:35,  4.41s/it]2024-12-22 01:38:54,166 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:54,544 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:54,544 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2164])
2024-12-22 01:38:54,614 - [Process 1/5] - DEBUG - predict_token:tensor([[7942]], device='cuda:1')
2024-12-22 01:38:54,850 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:East Timor
 80%|████████  | 32/40 [02:26<00:37,  4.64s/it]2024-12-22 01:38:55,127 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:56,221 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:56,221 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 01:38:56,294 - [Process 2/5] - DEBUG - predict_token:tensor([[1570]], device='cuda:2')
2024-12-22 01:38:56,515 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Sazerac
 85%|████████▌ | 34/40 [02:28<00:24,  4.14s/it]2024-12-22 01:38:56,672 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:56,884 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:56,884 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:38:56,956 - [Process 0/5] - DEBUG - predict_token:tensor([[476]], device='cuda:0')
2024-12-22 01:38:57,192 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Keturah
 82%|████████▎ | 33/40 [02:28<00:30,  4.29s/it]2024-12-22 01:38:57,366 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:57,366 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:38:57,443 - [Process 4/5] - DEBUG - predict_token:tensor([[1588]], device='cuda:4')
2024-12-22 01:38:57,461 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:57,764 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:The Regal Trophy.
 82%|████████▎ | 33/40 [02:29<00:30,  4.34s/it]2024-12-22 01:38:57,974 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:57,974 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2195])
2024-12-22 01:38:58,035 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:58,044 - [Process 3/5] - DEBUG - predict_token:tensor([[19732]], device='cuda:3')
2024-12-22 01:38:58,236 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Sid Kelly
 82%|████████▎ | 33/40 [02:29<00:30,  4.39s/it]2024-12-22 01:38:58,517 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:38:58,762 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:38:58,762 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:38:58,837 - [Process 1/5] - DEBUG - predict_token:tensor([[323]], device='cuda:1')
2024-12-22 01:38:59,030 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Mersin
 82%|████████▎ | 33/40 [02:30<00:31,  4.50s/it]2024-12-22 01:38:59,316 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:00,311 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:00,311 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 01:39:00,385 - [Process 2/5] - DEBUG - predict_token:tensor([[22806]], device='cuda:2')
2024-12-22 01:39:00,567 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Nepal
 88%|████████▊ | 35/40 [02:32<00:20,  4.11s/it]2024-12-22 01:39:00,736 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:01,182 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:01,182 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2175])
2024-12-22 01:39:01,251 - [Process 0/5] - DEBUG - predict_token:tensor([[22415]], device='cuda:0')
2024-12-22 01:39:01,486 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:S-Fone
 85%|████████▌ | 34/40 [02:33<00:25,  4.29s/it]2024-12-22 01:39:01,742 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:01,743 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1716])
2024-12-22 01:39:01,770 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:01,832 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:39:02,070 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:2010
 85%|████████▌ | 34/40 [02:33<00:25,  4.33s/it]2024-12-22 01:39:02,168 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:02,168 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 01:39:02,241 - [Process 3/5] - DEBUG - predict_token:tensor([[12217]], device='cuda:3')
2024-12-22 01:39:02,353 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:02,602 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Khrystyne Haje
 85%|████████▌ | 34/40 [02:34<00:26,  4.38s/it]2024-12-22 01:39:02,872 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:03,091 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:03,092 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1935])
2024-12-22 01:39:03,173 - [Process 1/5] - DEBUG - predict_token:tensor([[25167]], device='cuda:1')
2024-12-22 01:39:03,324 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:NFL
 85%|████████▌ | 34/40 [02:35<00:26,  4.44s/it]2024-12-22 01:39:03,601 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:04,359 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:04,359 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:39:04,434 - [Process 2/5] - DEBUG - predict_token:tensor([[4702]], device='cuda:2')
2024-12-22 01:39:04,657 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Jim Bryson
 90%|█████████ | 36/40 [02:36<00:16,  4.11s/it]2024-12-22 01:39:04,830 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:05,480 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:05,480 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1980])
2024-12-22 01:39:05,559 - [Process 0/5] - DEBUG - predict_token:tensor([[23032]], device='cuda:0')
2024-12-22 01:39:05,795 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Rick Springfield
 88%|████████▊ | 35/40 [02:37<00:21,  4.30s/it]2024-12-22 01:39:06,005 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:06,006 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:39:06,043 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:06,079 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:39:06,315 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:1956
 88%|████████▊ | 35/40 [02:38<00:21,  4.30s/it]2024-12-22 01:39:06,588 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:06,728 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:06,729 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2143])
2024-12-22 01:39:06,801 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:39:07,037 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:2005
 88%|████████▊ | 35/40 [02:38<00:21,  4.40s/it]2024-12-22 01:39:07,276 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:07,276 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1882])
2024-12-22 01:39:07,311 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:07,358 - [Process 1/5] - DEBUG - predict_token:tensor([[9428]], device='cuda:1')
2024-12-22 01:39:07,509 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Gordon
 88%|████████▊ | 35/40 [02:39<00:21,  4.36s/it]2024-12-22 01:39:07,787 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:08,437 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:08,437 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:39:08,512 - [Process 2/5] - DEBUG - predict_token:tensor([[10015]], device='cuda:2')
2024-12-22 01:39:09,094 - [Process 2/5] - INFO - res.shape is :torch.Size([14])
results:Ken Epp belongs to the Conservative Party of Canada.
 92%|█████████▎| 37/40 [02:40<00:12,  4.20s/it]2024-12-22 01:39:09,259 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:09,638 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:09,638 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:39:09,711 - [Process 0/5] - DEBUG - predict_token:tensor([[28465]], device='cuda:0')
2024-12-22 01:39:09,947 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Sony Music Entertainment
 90%|█████████ | 36/40 [02:41<00:17,  4.25s/it]2024-12-22 01:39:10,234 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:10,254 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:10,254 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2310])
2024-12-22 01:39:10,319 - [Process 4/5] - DEBUG - predict_token:tensor([[14621]], device='cuda:4')
2024-12-22 01:39:10,469 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Minsk
 90%|█████████ | 36/40 [02:42<00:17,  4.26s/it]2024-12-22 01:39:10,750 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:11,080 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:11,080 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1949])
2024-12-22 01:39:11,161 - [Process 3/5] - DEBUG - predict_token:tensor([[5845]], device='cuda:3')
2024-12-22 01:39:11,396 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Francis Bacon
 90%|█████████ | 36/40 [02:43<00:17,  4.39s/it]2024-12-22 01:39:11,438 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:11,438 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:39:11,512 - [Process 1/5] - DEBUG - predict_token:tensor([[6682]], device='cuda:1')
2024-12-22 01:39:11,664 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:11,748 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Sazerac
 90%|█████████ | 36/40 [02:43<00:17,  4.33s/it]2024-12-22 01:39:11,998 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:13,001 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:13,001 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2154])
2024-12-22 01:39:13,071 - [Process 2/5] - DEBUG - predict_token:tensor([[11680]], device='cuda:2')
2024-12-22 01:39:13,814 - [Process 2/5] - INFO - res.shape is :torch.Size([18])
results:Cabo Verde (ISO 3166-2:CV)
 95%|█████████▌| 38/40 [02:45<00:08,  4.36s/it]2024-12-22 01:39:13,961 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:13,962 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2139])
2024-12-22 01:39:13,984 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:14,030 - [Process 0/5] - DEBUG - predict_token:tensor([[3164]], device='cuda:0')
2024-12-22 01:39:14,352 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Blood Stain Child
 92%|█████████▎| 37/40 [02:46<00:12,  4.30s/it]2024-12-22 01:39:14,519 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:14,519 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2164])
2024-12-22 01:39:14,588 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:14,588 - [Process 4/5] - DEBUG - predict_token:tensor([[16899]], device='cuda:4')
2024-12-22 01:39:14,823 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Oman Professional League
 92%|█████████▎| 37/40 [02:46<00:12,  4.29s/it]2024-12-22 01:39:15,095 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:15,358 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:15,358 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2362])
2024-12-22 01:39:15,422 - [Process 3/5] - DEBUG - predict_token:tensor([[4275]], device='cuda:3')
2024-12-22 01:39:15,637 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:15,637 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 01:39:15,711 - [Process 1/5] - DEBUG - predict_token:tensor([[6213]], device='cuda:1')
2024-12-22 01:39:15,741 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Mulholland Falls
 92%|█████████▎| 37/40 [02:47<00:13,  4.37s/it]2024-12-22 01:39:15,862 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Greek
 92%|█████████▎| 37/40 [02:47<00:12,  4.26s/it]2024-12-22 01:39:16,014 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:16,153 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:17,723 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:17,723 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1964])
2024-12-22 01:39:17,803 - [Process 2/5] - DEBUG - predict_token:tensor([[4111]], device='cuda:2')
2024-12-22 01:39:18,065 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Gal Gadot
 98%|█████████▊| 39/40 [02:49<00:04,  4.33s/it]2024-12-22 01:39:18,234 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:18,359 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:18,360 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2119])
2024-12-22 01:39:18,431 - [Process 0/5] - DEBUG - predict_token:tensor([[323]], device='cuda:0')
2024-12-22 01:39:18,624 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:TML Entertainment
 95%|█████████▌| 38/40 [02:50<00:08,  4.29s/it]2024-12-22 01:39:18,750 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:18,750 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:39:18,823 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 01:39:18,900 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:19,016 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:825
 95%|█████████▌| 38/40 [02:50<00:08,  4.26s/it]2024-12-22 01:39:19,296 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:19,684 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:19,685 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 01:39:19,759 - [Process 3/5] - DEBUG - predict_token:tensor([[3087]], device='cuda:3')
2024-12-22 01:39:19,910 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Egypt
 95%|█████████▌| 38/40 [02:51<00:08,  4.31s/it]2024-12-22 01:39:19,975 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:19,976 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:39:20,058 - [Process 1/5] - DEBUG - predict_token:tensor([[4275]], device='cuda:1')
2024-12-22 01:39:20,185 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:20,251 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Charleston
 95%|█████████▌| 38/40 [02:51<00:08,  4.30s/it]2024-12-22 01:39:20,532 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:22,028 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:22,029 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:39:22,109 - [Process 2/5] - DEBUG - predict_token:tensor([[3237]], device='cuda:2')
2024-12-22 01:39:22,332 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Barney Stinson
100%|██████████| 40/40 [02:54<00:00,  4.31s/it]100%|██████████| 40/40 [02:54<00:00,  4.35s/it]
2024-12-22 01:39:22,526 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:22,527 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2089])
2024-12-22 01:39:22,600 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 01:39:22,964 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:East Timor-Leste
 98%|█████████▊| 39/40 [02:54<00:04,  4.31s/it]2024-12-22 01:39:22,969 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:22,969 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 01:39:23,044 - [Process 4/5] - DEBUG - predict_token:tensor([[395]], device='cuda:4')
2024-12-22 01:39:23,251 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:23,450 - [Process 4/5] - INFO - res.shape is :torch.Size([9])
results:$618,427
 98%|█████████▊| 39/40 [02:55<00:04,  4.31s/it]2024-12-22 01:39:23,730 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:23,967 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:23,967 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:39:24,037 - [Process 3/5] - DEBUG - predict_token:tensor([[16543]], device='cuda:3')
2024-12-22 01:39:24,357 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Isabelle Carré.
 98%|█████████▊| 39/40 [02:56<00:04,  4.35s/it]2024-12-22 01:39:24,372 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:24,372 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2125])
2024-12-22 01:39:24,445 - [Process 1/5] - DEBUG - predict_token:tensor([[11438]], device='cuda:1')
2024-12-22 01:39:24,641 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:24,681 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Rowan County.
 98%|█████████▊| 39/40 [02:56<00:04,  4.34s/it]2024-12-22 01:39:24,961 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:39:27,029 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:27,029 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2119])
2024-12-22 01:39:27,101 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 01:39:27,209 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:2
100%|██████████| 40/40 [02:58<00:00,  4.29s/it]100%|██████████| 40/40 [02:58<00:00,  4.47s/it]
2024-12-22 01:39:27,423 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:27,423 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2218])
2024-12-22 01:39:27,490 - [Process 4/5] - DEBUG - predict_token:tensor([[5701]], device='cuda:4')
2024-12-22 01:39:27,767 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Saulkrasti
100%|██████████| 40/40 [02:59<00:00,  4.31s/it]100%|██████████| 40/40 [02:59<00:00,  4.49s/it]
2024-12-22 01:39:28,490 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:28,490 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2130])
2024-12-22 01:39:28,563 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 01:39:28,713 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:2 million
100%|██████████| 40/40 [03:00<00:00,  4.35s/it]100%|██████████| 40/40 [03:00<00:00,  4.51s/it]
2024-12-22 01:39:28,716 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:39:28,717 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1946])
2024-12-22 01:39:28,797 - [Process 1/5] - DEBUG - predict_token:tensor([[1570]], device='cuda:1')
2024-12-22 01:39:29,032 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Syracuse
100%|██████████| 40/40 [03:00<00:00,  4.34s/it]100%|██████████| 40/40 [03:00<00:00,  4.52s/it]
2024-12-22 01:39:29,077 - [Process 3/5] - DEBUG - datasets_name:musique
2024-12-22 01:39:29,077 - [Process 1/5] - DEBUG - datasets_name:musique
2024-12-22 01:39:29,077 - [Process 4/5] - DEBUG - datasets_name:musique
2024-12-22 01:39:29,077 - [Process 2/5] - DEBUG - datasets_name:musique
2024-12-22 01:39:29,077 - [Process 0/5] - DEBUG - datasets_name:musique
Running evaluation for dataset: trec
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:41:26,078 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:41:26,079 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:41:26,079 - [Process 0/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:41:26,087 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:41:26,088 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:41:26,088 - [Process 4/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:41:26,096 - [Process 0/5] - INFO - Max Length is 8714
2024-12-22 01:41:26,097 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:41:26,097 - [Process 3/5] - INFO - loading datasets finished
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:41:26,097 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:41:26,097 - [Process 3/5] - INFO - output_max_len: 64
2024-12-22 01:41:26,097 - [Process 0/5] - INFO - get_predicted begin
2024-12-22 01:41:26,098 - [Process 1/5] - INFO - loading datasets finished
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:41:26,098 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:41:26,098 - [Process 1/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:41:26,102 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 01:41:26,102 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:41:26,102 - [Process 2/5] - INFO - output_max_len: 64
2024-12-22 01:41:26,115 - [Process 4/5] - INFO - Max Length is 8714
2024-12-22 01:41:26,115 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:41:26,116 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:41:26,124 - [Process 3/5] - INFO - Max Length is 8714
2024-12-22 01:41:26,124 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:41:26,124 - [Process 1/5] - INFO - Max Length is 8714
2024-12-22 01:41:26,125 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:41:26,125 - [Process 3/5] - INFO - get_predicted begin
2024-12-22 01:41:26,126 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:41:26,129 - [Process 2/5] - INFO - Max Length is 8714
2024-12-22 01:41:26,129 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:41:26,130 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:41:30,863 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:30,912 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:30,945 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:30,950 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:30,950 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:34,418 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:34,419 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1621])
2024-12-22 01:41:34,476 - [Process 3/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:41:34,908 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:34,908 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1957])
2024-12-22 01:41:34,982 - [Process 0/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:41:35,218 - [Process 3/5] - INFO - res.shape is :torch.Size([18])
2024-12-22 01:41:35,219 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
results:Individual


Please select the type of the question from the options provided.
2024-12-22 01:41:35,219 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
  2%|▎         | 1/40 [00:09<05:54,  9.09s/it]2024-12-22 01:41:35,230 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:35,231 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 01:41:35,233 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:35,233 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 01:41:35,290 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:41:35,301 - [Process 1/5] - DEBUG - predict_token:tensor([[25453]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:41:35,305 - [Process 2/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:41:35,431 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:37,553 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the first man to fly faster than the speed of sound ?
Type: Individual
Question: What is the name of the first woman to fly faster than the speed of sound ?
Type: Individual
Question: What is the name of the first person to walk in space
  2%|▎         | 1/40 [00:11<07:26, 11.46s/it]2024-12-22 01:41:37,694 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:38,036 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What
  2%|▎         | 1/40 [00:11<07:44, 11.91s/it]2024-12-22 01:41:38,231 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:38,254 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 01:41:38,254 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first woman to fly solo across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first man to walk
results:Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the summit of Mount Everest
  2%|▎         | 1/40 [00:12<07:53, 12.13s/it]  2%|▎         | 1/40 [00:12<07:53, 12.14s/it]2024-12-22 01:41:38,507 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:38,519 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:38,986 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:38,987 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 01:41:39,059 - [Process 3/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:3')
2024-12-22 01:41:41,196 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:41,196 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:41:41,267 - [Process 0/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:0')
2024-12-22 01:41:41,766 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:41,767 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:41:41,786 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the capital of France ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other entity

  5%|▌         | 2/40 [00:15<04:49,  7.61s/it]2024-12-22 01:41:41,837 - [Process 2/5] - DEBUG - predict_token:tensor([[2866]], device='cuda:2')
2024-12-22 01:41:41,988 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:41,988 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1966])
2024-12-22 01:41:42,036 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:42,054 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:42,055 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 01:41:42,059 - [Process 4/5] - DEBUG - predict_token:tensor([[830]], device='cuda:4')
2024-12-22 01:41:42,126 - [Process 1/5] - DEBUG - predict_token:tensor([[6652]], device='cuda:1')
2024-12-22 01:41:43,832 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a cashmere and a cashmere goat ?
Type: Other entity
Question: What is the difference between a cashmere and a cashmere goat ?
Type: Other entity
Question: What is the difference between a cash
  5%|▌         | 2/40 [00:17<05:19,  8.41s/it]2024-12-22 01:41:43,950 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:44,564 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the difference between a cougar and a puma ?
Type: Other entity
Question: What is the name of the first woman to fly solo across the
  5%|▌         | 2/40 [00:18<05:32,  8.74s/it]2024-12-22 01:41:44,778 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:45,018 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What is the
  5%|▌         | 2/40 [00:18<05:41,  8.98s/it]2024-12-22 01:41:45,090 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question:
  5%|▌         | 2/40 [00:18<05:42,  9.02s/it]2024-12-22 01:41:45,222 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:45,325 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:45,603 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:45,604 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2016])
2024-12-22 01:41:45,675 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:41:47,442 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:47,442 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:41:47,513 - [Process 0/5] - DEBUG - predict_token:tensor([[9159]], device='cuda:0')
2024-12-22 01:41:48,302 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:48,302 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:41:48,373 - [Process 2/5] - DEBUG - predict_token:tensor([[4412]], device='cuda:2')
2024-12-22 01:41:48,392 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other number
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly across the Atlantic Ocean ?

  8%|▊         | 3/40 [00:22<04:24,  7.15s/it]2024-12-22 01:41:48,626 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:48,747 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:48,747 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:41:48,818 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:41:48,876 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:48,877 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:41:48,948 - [Process 1/5] - DEBUG - predict_token:tensor([[6652]], device='cuda:1')
2024-12-22 01:41:50,078 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Color
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the first woman to fly faster than the speed of sound
  8%|▊         | 3/40 [00:23<04:34,  7.42s/it]2024-12-22 01:41:50,210 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:51,099 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: City
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest living thing on Earth ?
Type:
  8%|▊         | 3/40 [00:24<04:46,  7.74s/it]2024-12-22 01:41:51,311 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:51,761 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the difference between a catheter and a urinary catheter ?
Type: Definition of something
Question: What is the difference between a catheter and a urinary catheter ?
Type: Definition of something
Question: What is the difference
  8%|▊         | 3/40 [00:25<04:54,  7.96s/it]2024-12-22 01:41:51,894 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
  8%|▊         | 3/40 [00:25<04:56,  8.01s/it]2024-12-22 01:41:51,988 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:52,086 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:52,193 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:52,193 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2005])
2024-12-22 01:41:52,265 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:41:53,702 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:53,702 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:41:53,773 - [Process 0/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:0')
2024-12-22 01:41:54,836 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:54,837 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 01:41:54,907 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:41:54,985 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a cactus and a succulent ?
Type: Definition of something
Question: What is the difference between a cactus and a succulent ?
Type: Definition of something
Question: What is the difference between a cactus and a succulent
 10%|█         | 4/40 [00:28<04:09,  6.93s/it]2024-12-22 01:41:55,186 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:55,525 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:55,526 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 01:41:55,597 - [Process 4/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:4')
2024-12-22 01:41:55,642 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:55,642 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 01:41:55,713 - [Process 1/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:1')
2024-12-22 01:41:56,340 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the highest mountain in the solar system ?
Type: Other
 10%|█         | 4/40 [00:30<04:10,  6.96s/it]2024-12-22 01:41:56,464 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:57,643 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly solo across the Atlantic ?
 10%|█         | 4/40 [00:31<04:21,  7.26s/it]2024-12-22 01:41:57,883 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:58,546 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first woman to fly solo across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 10%|█         | 4/40 [00:32<04:29,  7.49s/it]2024-12-22 01:41:58,664 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the first computer virus ?
Type: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
 10%|█         | 4/40 [00:32<04:30,  7.52s/it]2024-12-22 01:41:58,752 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:58,764 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:58,764 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1988])
2024-12-22 01:41:58,836 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:41:58,886 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:41:59,959 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:41:59,960 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:42:00,030 - [Process 0/5] - DEBUG - predict_token:tensor([[315]], device='cuda:0')
2024-12-22 01:42:01,419 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:01,419 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:42:01,490 - [Process 2/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:2')
2024-12-22 01:42:01,558 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Organ of body
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first permanent English settlement in North America ?

 12%|█▎        | 5/40 [00:35<03:58,  6.80s/it]2024-12-22 01:42:01,697 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:02,295 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:02,295 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 01:42:02,367 - [Process 4/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:4')
2024-12-22 01:42:02,446 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:02,447 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:42:02,518 - [Process 1/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:1')
2024-12-22 01:42:02,600 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Currency
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type:
 12%|█▎        | 5/40 [00:36<03:54,  6.71s/it]2024-12-22 01:42:02,745 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:04,175 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:04,176 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1442])
2024-12-22 01:42:04,221 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What is the
 12%|█▎        | 5/40 [00:38<04:05,  7.02s/it]2024-12-22 01:42:04,227 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:42:04,428 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:05,318 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the first computer virus ?
Type: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?

 12%|█▎        | 5/40 [00:39<04:13,  7.23s/it]2024-12-22 01:42:05,475 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question:
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: Other
 12%|█▎        | 5/40 [00:39<04:14,  7.26s/it]2024-12-22 01:42:05,557 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:05,671 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:06,237 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:06,237 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 01:42:06,309 - [Process 0/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:0')
2024-12-22 01:42:06,761 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Organism
Question: What is the
 15%|█▌        | 6/40 [00:40<03:32,  6.26s/it]2024-12-22 01:42:06,986 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:07,971 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:07,971 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 01:42:08,042 - [Process 2/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:2')
2024-12-22 01:42:08,878 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other entity
Question: What is the
 15%|█▌        | 6/40 [00:42<03:43,  6.56s/it]2024-12-22 01:42:08,965 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:09,094 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:09,094 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 01:42:09,166 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:42:09,183 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:09,184 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1977])
2024-12-22 01:42:09,255 - [Process 1/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:1')
2024-12-22 01:42:10,572 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:10,573 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 01:42:10,644 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:42:10,771 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a cashmere and a cashmere goat ?
Type: Animal
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
 15%|█▌        | 6/40 [00:44<03:53,  6.86s/it]2024-12-22 01:42:11,031 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:11,658 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:11,658 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1594])
2024-12-22 01:42:11,715 - [Process 0/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:0')
2024-12-22 01:42:12,114 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the difference between a cougar and a puma ?
Type: Animal
Question: What is the name of the first woman to fly solo across the Atlantic Ocean ?
Type:
 15%|█▌        | 6/40 [00:46<04:00,  7.09s/it]2024-12-22 01:42:12,207 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Who
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the difference between a cougar and a puma ?
Type: Animal
Question: What is the name of the largest planet in our solar system ?
Type: Planet

 15%|█▌        | 6/40 [00:46<04:00,  7.08s/it]2024-12-22 01:42:12,327 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:12,444 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:13,374 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the difference between a cashmere sweater and a merino sweater ?
Type: Definition of something
Question: What is the name of the largest planet in
 18%|█▊        | 7/40 [00:47<03:30,  6.37s/it]2024-12-22 01:42:13,509 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:14,146 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the highest mountain peak in the solar system ?
Type
 18%|█▊        | 7/40 [00:48<03:22,  6.14s/it]2024-12-22 01:42:14,267 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:14,575 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:14,575 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:42:14,646 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:42:15,869 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:15,870 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:42:15,941 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:42:16,014 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:16,014 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 01:42:16,085 - [Process 1/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:1')
2024-12-22 01:42:16,278 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:16,278 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1476])
2024-12-22 01:42:16,332 - [Process 3/5] - DEBUG - predict_token:tensor([[10619]], device='cuda:3')
2024-12-22 01:42:17,449 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Invention, book and other creative piece
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest
 18%|█▊        | 7/40 [00:51<03:44,  6.80s/it]2024-12-22 01:42:17,705 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:17,771 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:17,771 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 01:42:17,842 - [Process 0/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:0')
2024-12-22 01:42:18,890 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the most popular sport in Brazil ?
Type: Other location
Question: What is the name of the first woman to fly solo across the Atlantic Ocean ?

 18%|█▊        | 7/40 [00:52<03:50,  6.98s/it]2024-12-22 01:42:19,015 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type
 20%|██        | 8/40 [00:52<03:16,  6.14s/it]2024-12-22 01:42:19,035 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly faster than the speed of sound ?
 18%|█▊        | 7/40 [00:52<03:50,  7.00s/it]2024-12-22 01:42:19,128 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:19,238 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:19,266 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:20,409 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly solo across the Atlantic Ocean ?
Type: Individual
 20%|██        | 8/40 [00:54<03:17,  6.18s/it]2024-12-22 01:42:20,534 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:21,255 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:21,255 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:42:21,327 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:42:22,672 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:22,673 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:42:22,744 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:42:22,775 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:22,776 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2046])
2024-12-22 01:42:22,827 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:22,827 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:42:22,845 - [Process 3/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:3')
2024-12-22 01:42:22,899 - [Process 1/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:1')
2024-12-22 01:42:24,041 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:24,041 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:42:24,112 - [Process 0/5] - DEBUG - predict_token:tensor([[315]], device='cuda:0')
2024-12-22 01:42:24,135 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly in space ?
Type: Individual
Question: What is the name of the first person to reach the summit of
 20%|██        | 8/40 [00:58<03:36,  6.76s/it]2024-12-22 01:42:24,311 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:25,655 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the capital of the country where the language Esperanto is the official language ?
Type: Country
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type
 22%|██▎       | 9/40 [00:59<03:15,  6.30s/it]2024-12-22 01:42:25,686 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 20%|██        | 8/40 [00:59<03:41,  6.92s/it]2024-12-22 01:42:25,849 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
 20%|██        | 8/40 [00:59<03:42,  6.94s/it]2024-12-22 01:42:25,864 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:25,906 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:26,041 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:26,673 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Currency
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly across the Atlantic Ocean ?

 22%|██▎       | 9/40 [01:00<03:12,  6.21s/it]2024-12-22 01:42:26,780 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:27,474 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:27,474 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1814])
2024-12-22 01:42:27,537 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:42:29,455 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:29,455 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 01:42:29,463 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:29,463 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:42:29,526 - [Process 3/5] - DEBUG - predict_token:tensor([[17582]], device='cuda:3')
2024-12-22 01:42:29,535 - [Process 4/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:4')
2024-12-22 01:42:29,607 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:29,607 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:42:29,678 - [Process 1/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:1')
2024-12-22 01:42:30,247 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:30,247 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1978])
2024-12-22 01:42:30,249 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly solo across the Atlantic Ocean
 22%|██▎       | 9/40 [01:04<03:23,  6.56s/it]2024-12-22 01:42:30,319 - [Process 0/5] - DEBUG - predict_token:tensor([[341]], device='cuda:0')
2024-12-22 01:42:30,449 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:32,296 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type
 25%|██▌       | 10/40 [01:06<03:12,  6.40s/it]2024-12-22 01:42:32,480 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Distance, linear measure
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the difference between a cello and a violin ?
 22%|██▎       | 9/40 [01:06<03:33,  6.88s/it]2024-12-22 01:42:32,527 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:32,627 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question:
 22%|██▎       | 9/40 [01:06<03:33,  6.89s/it]2024-12-22 01:42:32,704 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:32,778 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:32,879 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Description of something
Question: What is the name of the first man to fly faster than the speed of sound ?
Type: Individual
Question: What is the difference between a cougar and a puma ?
Type: Animal
Question: What is the name of the first woman to fly
 25%|██▌       | 10/40 [01:06<03:06,  6.21s/it]2024-12-22 01:42:33,005 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:34,003 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:34,004 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:42:34,075 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:42:35,496 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:35,496 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1550])
2024-12-22 01:42:35,551 - [Process 1/5] - DEBUG - predict_token:tensor([[21444]], device='cuda:1')
2024-12-22 01:42:36,073 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:36,073 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1973])
2024-12-22 01:42:36,146 - [Process 3/5] - DEBUG - predict_token:tensor([[20743]], device='cuda:3')
2024-12-22 01:42:36,271 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:36,272 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 01:42:36,343 - [Process 4/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:4')
2024-12-22 01:42:36,522 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:36,522 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 01:42:36,594 - [Process 0/5] - DEBUG - predict_token:tensor([[4168]], device='cuda:0')
2024-12-22 01:42:36,848 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Question: What is the name of the largest planet in our solar system ?
Question: What is the name of the largest living thing on Earth ?
Question: What is the largest city in the United States ?
Question: What is the
 25%|██▌       | 10/40 [01:10<03:17,  6.57s/it]2024-12-22 01:42:37,080 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:38,377 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Ind
 25%|██▌       | 10/40 [01:12<03:16,  6.54s/it]2024-12-22 01:42:38,578 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:38,915 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the capital of the Ivory Coast ?
Type: Country
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the
 28%|██▊       | 11/40 [01:12<03:07,  6.47s/it]2024-12-22 01:42:39,082 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:39,151 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Valentine 's Day is a holiday celebrated on February 14 by many countries around the world. It is a day to express love and affection between romantic partners, and is traditionally celebrated with gifts, cards, and other symbols of love.
Question: What is the capital of France
 28%|██▊       | 11/40 [01:13<03:00,  6.23s/it]2024-12-22 01:42:39,235 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
 25%|██▌       | 10/40 [01:13<03:25,  6.84s/it]2024-12-22 01:42:39,277 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:39,436 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:40,635 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:40,635 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1995])
2024-12-22 01:42:40,706 - [Process 2/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:2')
2024-12-22 01:42:42,151 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:42,151 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 01:42:42,223 - [Process 1/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:1')
2024-12-22 01:42:42,308 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:42,308 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1852])
2024-12-22 01:42:42,374 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:42:42,803 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:42,804 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:42:42,875 - [Process 0/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:0')
2024-12-22 01:42:42,995 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:42,995 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:42:43,067 - [Process 4/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:4')
2024-12-22 01:42:43,504 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the highest mountain peak in the solar system ?
Type: Mountain
Question: What is the name of the first man to walk on the moon ?
 28%|██▊       | 11/40 [01:17<03:11,  6.60s/it]2024-12-22 01:42:43,719 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:45,107 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the summit
 28%|██▊       | 11/40 [01:18<03:11,  6.60s/it]2024-12-22 01:42:45,123 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Which country gave New York the Statue of Liberty ?
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living
 30%|███       | 12/40 [01:19<02:58,  6.39s/it]2024-12-22 01:42:45,311 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:45,344 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:45,441 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the summit
 30%|███       | 12/40 [01:19<02:54,  6.25s/it]2024-12-22 01:42:45,567 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:45,964 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Invention, book and other creative piece
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the first woman
 28%|██▊       | 11/40 [01:19<03:17,  6.81s/it]2024-12-22 01:42:46,186 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:47,278 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:47,279 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 01:42:47,351 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:42:48,902 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:48,902 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:42:48,922 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:48,923 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 01:42:48,975 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:42:48,994 - [Process 1/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:1')
2024-12-22 01:42:49,091 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:49,091 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:42:49,163 - [Process 0/5] - DEBUG - predict_token:tensor([[17015]], device='cuda:0')
2024-12-22 01:42:49,746 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:49,746 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 01:42:49,818 - [Process 4/5] - DEBUG - predict_token:tensor([[360]], device='cuda:4')
2024-12-22 01:42:50,148 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other entity
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic
 30%|███       | 12/40 [01:24<03:05,  6.61s/it]2024-12-22 01:42:50,401 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:51,725 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the summit
 32%|███▎      | 13/40 [01:25<02:48,  6.26s/it]2024-12-22 01:42:51,775 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the first woman to fly solo across the Atlantic Ocean
 32%|███▎      | 13/40 [01:25<02:54,  6.47s/it]2024-12-22 01:42:51,867 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:51,872 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: What year did the Titanic start on its journey ?
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is
 30%|███       | 12/40 [01:25<03:06,  6.65s/it]2024-12-22 01:42:51,957 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:52,089 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:52,711 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What is the
 30%|███       | 12/40 [01:26<03:10,  6.79s/it]2024-12-22 01:42:52,937 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:53,970 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:53,970 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 01:42:54,042 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:42:55,393 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:55,393 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:42:55,465 - [Process 0/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:0')
2024-12-22 01:42:55,555 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:55,555 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:42:55,628 - [Process 3/5] - DEBUG - predict_token:tensor([[6431]], device='cuda:3')
2024-12-22 01:42:55,666 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:55,667 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 01:42:55,739 - [Process 1/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:1')
2024-12-22 01:42:56,500 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:42:56,501 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:42:56,573 - [Process 4/5] - DEBUG - predict_token:tensor([[9159]], device='cuda:4')
2024-12-22 01:42:56,841 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly across the Atlantic
 32%|███▎      | 13/40 [01:30<02:59,  6.64s/it]2024-12-22 01:42:57,074 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:58,025 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type
 35%|███▌      | 14/40 [01:31<02:43,  6.27s/it]2024-12-22 01:42:58,150 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:58,431 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly solo across the Atlantic
 35%|███▌      | 14/40 [01:32<02:49,  6.53s/it]2024-12-22 01:42:58,620 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the difference between a cashmere and a cashmere blend ?
Type: Description of something
Question: What is the name of the largest
 32%|███▎      | 13/40 [01:32<03:00,  6.68s/it]2024-12-22 01:42:58,646 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:58,863 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:42:59,469 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the most popular sport in the world ?
Type: Sport
Question: What is the most popular sport in the world ?
Question: What is the most popular sport in the world ?
Question: What is the most popular sport in the world ?
Question: What is the most popular sport
 32%|███▎      | 13/40 [01:33<03:03,  6.78s/it]2024-12-22 01:42:59,653 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:00,642 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:00,643 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:43:00,715 - [Process 2/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:2')
2024-12-22 01:43:01,628 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:01,628 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1972])
2024-12-22 01:43:01,700 - [Process 0/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:0')
2024-12-22 01:43:02,242 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:02,242 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 01:43:02,314 - [Process 3/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:3')
2024-12-22 01:43:02,441 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:02,442 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:43:02,513 - [Process 1/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:1')
2024-12-22 01:43:03,220 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:03,220 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:43:03,291 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:43:03,515 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location

 35%|███▌      | 14/40 [01:37<02:52,  6.65s/it]2024-12-22 01:43:03,766 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:04,261 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Question: What is the name of the largest planet in our solar system ?
Question: What is the name of the largest living thing on Earth ?
Question: What is the name of the largest river in the world ?
Question: What
 38%|███▊      | 15/40 [01:38<02:36,  6.26s/it]2024-12-22 01:43:04,380 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:05,125 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Date
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the highest mountain in the solar system ?
Type: Other entity

 38%|███▊      | 15/40 [01:39<02:44,  6.58s/it]2024-12-22 01:43:05,345 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:05,401 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly solo across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 35%|███▌      | 14/40 [01:39<02:54,  6.71s/it]2024-12-22 01:43:05,614 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:06,191 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a fuel cell and a battery ?
Question: What is the difference between a battery and a fuel cell ?
Type: Other location
Question: What is the difference between a battery and a fuel cell ?
Type: Other location
Question: What is the difference between a battery
 35%|███▌      | 14/40 [01:40<02:55,  6.76s/it]2024-12-22 01:43:06,419 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:07,339 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:07,339 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 01:43:07,411 - [Process 2/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:2')
2024-12-22 01:43:07,859 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:07,860 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2045])
2024-12-22 01:43:07,928 - [Process 0/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:0')
2024-12-22 01:43:08,948 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:08,948 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2023])
2024-12-22 01:43:09,020 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:43:09,194 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:09,195 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1986])
2024-12-22 01:43:09,266 - [Process 1/5] - DEBUG - predict_token:tensor([[315]], device='cuda:1')
2024-12-22 01:43:09,962 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:09,963 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 01:43:10,031 - [Process 4/5] - DEBUG - predict_token:tensor([[315]], device='cuda:4')
2024-12-22 01:43:10,212 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the smallest country in the world ?
Type: Country
Question:
 38%|███▊      | 15/40 [01:44<02:46,  6.66s/it]2024-12-22 01:43:10,468 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:10,497 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
 40%|████      | 16/40 [01:44<02:30,  6.25s/it]2024-12-22 01:43:10,618 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:11,823 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the
 40%|████      | 16/40 [01:45<02:38,  6.61s/it]2024-12-22 01:43:12,052 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:12,158 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Currency
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?

 38%|███▊      | 15/40 [01:46<02:48,  6.72s/it]2024-12-22 01:43:12,270 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:12,936 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the smallest country in the world ?
Type: Country
Question: What is the name
 38%|███▊      | 15/40 [01:46<02:48,  6.76s/it]2024-12-22 01:43:13,135 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:14,031 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:14,031 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:43:14,103 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:43:14,147 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:14,148 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 01:43:14,219 - [Process 0/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:0')
2024-12-22 01:43:14,327 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:14,328 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1178])
2024-12-22 01:43:14,368 - [Process 1/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:1')
2024-12-22 01:43:15,654 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:15,655 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 01:43:15,727 - [Process 3/5] - DEBUG - predict_token:tensor([[830]], device='cuda:3')
2024-12-22 01:43:16,559 - [Process 3/5] - INFO - res.shape is :torch.Size([18])
results:Question


Please determine the type of the question based on the provided information.
 42%|████▎     | 17/40 [01:50<02:19,  6.05s/it]2024-12-22 01:43:16,707 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:16,707 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2005])
2024-12-22 01:43:16,779 - [Process 4/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:4')
2024-12-22 01:43:16,785 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the largest city in the world located entirely below sea level ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar
 42%|████▎     | 17/40 [01:50<02:24,  6.26s/it]2024-12-22 01:43:16,814 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:16,860 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: Other location
Question
 40%|████      | 16/40 [01:50<02:26,  6.11s/it]2024-12-22 01:43:16,878 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the highest mountain peak in the solar system ?
Type: Definition of something
Question: What is the name of the largest living thing on Earth ?
Type:
 40%|████      | 16/40 [01:50<02:39,  6.66s/it]2024-12-22 01:43:16,911 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:17,081 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:17,102 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:19,531 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Question: What is the name of the largest planet in our solar system ?
Question: What is the name of the largest living thing on Earth ?
Question: What is the largest city in the United States ?
Question: What is the
 40%|████      | 16/40 [01:53<02:40,  6.71s/it]2024-12-22 01:43:19,712 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:20,416 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:20,416 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 01:43:20,435 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:20,436 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:43:20,489 - [Process 3/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:3')
2024-12-22 01:43:20,507 - [Process 0/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:0')
2024-12-22 01:43:20,663 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:20,663 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1988])
2024-12-22 01:43:20,677 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:20,677 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 01:43:20,735 - [Process 1/5] - DEBUG - predict_token:tensor([[15456]], device='cuda:1')
2024-12-22 01:43:20,748 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:43:23,072 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Distance, linear measure
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the average lifespan of a blue whale ?
Type: Number of something
Question: What is the name of the largest planet in our solar system
 45%|████▌     | 18/40 [01:56<02:17,  6.27s/it]2024-12-22 01:43:23,137 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:23,288 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:23,288 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 01:43:23,360 - [Process 4/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:4')
2024-12-22 01:43:23,393 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the smallest country in the world ?
Type: Country
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question
 45%|████▌     | 18/40 [01:57<02:18,  6.28s/it]2024-12-22 01:43:23,466 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the difference between a cougar and a puma ?
Type: Description of something
Question: What is the name of the largest city in the world ?
 42%|████▎     | 17/40 [01:57<02:24,  6.26s/it]2024-12-22 01:43:23,616 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:23,662 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 01:43:23,665 - [Process 1/5] - INFO - len(per_windows_prompt):2
results:Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What
 42%|████▎     | 17/40 [01:57<02:34,  6.70s/it]2024-12-22 01:43:23,897 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:25,211 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:25,212 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1207])
2024-12-22 01:43:25,253 - [Process 0/5] - DEBUG - predict_token:tensor([[17015]], device='cuda:0')
2024-12-22 01:43:26,114 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first permanent English settlement in North America ?
Type
 42%|████▎     | 17/40 [02:00<02:33,  6.67s/it]2024-12-22 01:43:26,341 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:27,218 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:27,219 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 01:43:27,242 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:27,242 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1986])
2024-12-22 01:43:27,291 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:43:27,314 - [Process 1/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:1')
2024-12-22 01:43:27,467 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:27,467 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 01:43:27,538 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 01:43:27,540 - [Process 2/5] - DEBUG - predict_token:tensor([[18204]], device='cuda:2')
results:Question: Location
Question: What is the capital of France ?
Type: Location
Question: What is the largest planet in our solar system ?
Type: Location
Question: What is the smallest country in the world ?
Type: Location
Question: What is the highest mountain in the world ?
Type
 48%|████▊     | 19/40 [02:01<02:00,  5.73s/it]2024-12-22 01:43:27,632 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:29,921 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:29,922 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 01:43:29,993 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:43:30,049 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Distance, linear measure
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
 45%|████▌     | 18/40 [02:03<02:19,  6.36s/it]2024-12-22 01:43:30,159 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:30,192 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the difference between a cashier and a teller ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system
 48%|████▊     | 19/40 [02:04<02:15,  6.44s/it]2024-12-22 01:43:30,425 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:30,451 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the mountain range that runs along the western coast of South America ?
Type: Other location
Question: What is the name of the largest city in the world located closest to the equator ?
Type: City
Question: What is the name of the first man to walk on
 45%|████▌     | 18/40 [02:04<02:27,  6.73s/it]2024-12-22 01:43:30,686 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:30,768 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:30,769 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1762])
2024-12-22 01:43:30,832 - [Process 0/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:0')
2024-12-22 01:43:32,190 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:32,190 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1098])
2024-12-22 01:43:32,231 - [Process 1/5] - DEBUG - predict_token:tensor([[5127]], device='cuda:1')
2024-12-22 01:43:32,786 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
 45%|████▌     | 18/40 [02:06<02:26,  6.67s/it]2024-12-22 01:43:32,913 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:33,317 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: Location
Question: What is the name of the smallest country in the world ?
Type: Location
Question: What is the name of the largest planet in our solar system ?
Type: Location
Question: What is the name
 50%|█████     | 20/40 [02:07<01:54,  5.74s/it]2024-12-22 01:43:33,462 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:34,025 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:34,025 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 01:43:34,098 - [Process 3/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:3')
2024-12-22 01:43:34,257 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:34,258 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:43:34,330 - [Process 2/5] - DEBUG - predict_token:tensor([[341]], device='cuda:2')
2024-12-22 01:43:34,825 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first computer virus ?
Type: Invention, book and other creative piece
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our
 48%|████▊     | 19/40 [02:08<02:03,  5.88s/it]2024-12-22 01:43:35,056 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:35,300 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:35,300 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1309])
2024-12-22 01:43:35,348 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:43:36,994 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:36,994 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 01:43:37,006 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Money
Question: What is the difference between a cougar and a puma ?
Type: Animal
Question: What is the difference between a cougar and a mountain lion ?
Type: Animal
Question: What is the difference between a cougar and a panther ?
 50%|█████     | 20/40 [02:10<02:11,  6.55s/it]2024-12-22 01:43:37,065 - [Process 0/5] - DEBUG - predict_token:tensor([[20743]], device='cuda:0')
2024-12-22 01:43:37,238 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:37,247 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Manner of an action
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly solo across the
 48%|████▊     | 19/40 [02:11<02:21,  6.75s/it]2024-12-22 01:43:37,441 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:37,863 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What is the
 48%|████▊     | 19/40 [02:11<02:10,  6.19s/it]2024-12-22 01:43:38,042 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:38,631 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:38,632 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 01:43:38,704 - [Process 1/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:1')
2024-12-22 01:43:39,636 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Price
Question: What is the capital of the Ivory Coast ?
Type: Other location
Question: What is the difference between a cello and a violin ?
Type: Musical instrument
Question: What is the name of the largest city in the world ?
Type: City
Question
 52%|█████▎    | 21/40 [02:13<01:52,  5.92s/it]2024-12-22 01:43:39,745 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:40,840 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:40,840 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 01:43:40,913 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:43:41,010 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:41,010 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2023])
2024-12-22 01:43:41,082 - [Process 2/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:2')
2024-12-22 01:43:41,435 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly across the Atlantic Ocean
 50%|█████     | 20/40 [02:15<02:02,  6.10s/it]2024-12-22 01:43:41,583 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:41,619 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:41,619 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:43:41,692 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:43:43,270 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:43,270 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2023])
2024-12-22 01:43:43,342 - [Process 0/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:0')
2024-12-22 01:43:43,826 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the most popular sport in the world ?
Type: Sport
Question: What is the capital of Australia ?
Type: City
Question: What is the highest mountain in
 52%|█████▎    | 21/40 [02:17<02:06,  6.63s/it]2024-12-22 01:43:44,001 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the first woman to fly solo across the Atlantic Ocean
 50%|█████     | 20/40 [02:17<02:14,  6.75s/it]2024-12-22 01:43:44,078 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:44,244 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:44,380 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:44,381 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1640])
2024-12-22 01:43:44,428 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the difference between a cactus and a succulent ?
Type: Definition of something
Question: What is the difference between a cactus and a succulent ?
Type: Definition of something
Question: What is the difference between a cact
 50%|█████     | 20/40 [02:18<02:06,  6.30s/it]2024-12-22 01:43:44,437 - [Process 1/5] - DEBUG - predict_token:tensor([[2799]], device='cuda:1')
2024-12-22 01:43:44,641 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:45,908 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the meaning of the word 
Question: What is the capital of France ?
Type: City
Question: What is the name of the largest planet in our solar system
 55%|█████▌    | 22/40 [02:19<01:48,  6.02s/it]2024-12-22 01:43:46,023 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:47,052 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the summit of Mount Everest
 52%|█████▎    | 21/40 [02:20<01:53,  5.96s/it]2024-12-22 01:43:47,197 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:47,632 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:47,633 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 01:43:47,702 - [Process 3/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:3')
2024-12-22 01:43:47,761 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:47,762 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:43:47,830 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:43:48,232 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:48,232 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 01:43:48,305 - [Process 4/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:4')
2024-12-22 01:43:49,560 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:49,560 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 01:43:49,633 - [Process 0/5] - DEBUG - predict_token:tensor([[6652]], device='cuda:0')
2024-12-22 01:43:49,956 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:49,956 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1593])
2024-12-22 01:43:50,013 - [Process 1/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:1')
2024-12-22 01:43:50,625 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: How old was Joan of Arc when she died ?
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the capital of France ?
Type: Other location
Question: What is the highest mountain in the solar system ?
Type: Other
 55%|█████▌    | 22/40 [02:24<02:00,  6.68s/it]2024-12-22 01:43:50,755 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly solo across the Atlantic
 52%|█████▎    | 21/40 [02:24<02:08,  6.75s/it]2024-12-22 01:43:50,837 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:50,994 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:51,127 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
 52%|█████▎    | 21/40 [02:25<02:02,  6.42s/it]2024-12-22 01:43:51,323 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:52,201 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the
 57%|█████▊    | 23/40 [02:26<01:43,  6.10s/it]2024-12-22 01:43:52,318 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:52,750 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly solo across the
 55%|█████▌    | 22/40 [02:26<01:45,  5.88s/it]2024-12-22 01:43:52,987 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:54,436 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:54,436 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1987])
2024-12-22 01:43:54,509 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:43:54,559 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:54,559 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2005])
2024-12-22 01:43:54,630 - [Process 2/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:2')
2024-12-22 01:43:54,911 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:54,911 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 01:43:54,983 - [Process 4/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:4')
2024-12-22 01:43:55,853 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:55,853 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 01:43:55,925 - [Process 0/5] - DEBUG - predict_token:tensor([[21444]], device='cuda:0')
2024-12-22 01:43:56,562 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:56,563 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:43:56,634 - [Process 1/5] - DEBUG - predict_token:tensor([[9208]], device='cuda:1')
2024-12-22 01:43:57,426 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Ind
 57%|█████▊    | 23/40 [02:31<01:54,  6.72s/it]2024-12-22 01:43:57,551 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What is the
 55%|█████▌    | 22/40 [02:31<02:01,  6.76s/it]2024-12-22 01:43:57,675 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:57,680 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:57,805 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: How often
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type:
 55%|█████▌    | 22/40 [02:31<01:56,  6.50s/it]2024-12-22 01:43:58,054 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:58,488 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the highest mountain in the solar system ?
Type: Planet
Question: What is the deepest ocean
 60%|██████    | 24/40 [02:32<01:38,  6.16s/it]2024-12-22 01:43:58,565 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:59,460 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a cougar and a puma ?
Type: Definition of something
Question: What is the difference between a cougar and a mountain lion ?
Type: Definition of something
Question: What is the difference between a cougar and a panther ?

 57%|█████▊    | 23/40 [02:33<01:44,  6.13s/it]2024-12-22 01:43:59,600 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:43:59,765 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:43:59,765 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1164])
2024-12-22 01:43:59,806 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:44:01,034 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:01,035 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1426])
2024-12-22 01:44:01,086 - [Process 0/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:0')
2024-12-22 01:44:01,229 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:01,230 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-22 01:44:01,302 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:44:01,643 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:01,643 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:44:01,716 - [Process 4/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:4')
2024-12-22 01:44:02,320 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:02,321 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1547])
2024-12-22 01:44:02,375 - [Process 1/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:1')
2024-12-22 01:44:02,437 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the most popular sport in the world ?
Type: Sport
Question: What is the most common type of cancer in the world ?
Type: Disease and medicine
Question: What is the most popular search engine on the internet ?
Type: Product

 57%|█████▊    | 23/40 [02:36<01:45,  6.20s/it]2024-12-22 01:44:02,679 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:03,450 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question
 62%|██████▎   | 25/40 [02:37<01:26,  5.80s/it]2024-12-22 01:44:03,584 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:04,096 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 60%|██████    | 24/40 [02:37<01:47,  6.70s/it]2024-12-22 01:44:04,235 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:04,617 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly across the Atlantic Ocean ?

 57%|█████▊    | 23/40 [02:38<01:52,  6.59s/it]2024-12-22 01:44:04,873 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:05,250 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first permanent English settlement in North America
 60%|██████    | 24/40 [02:39<01:36,  6.03s/it]2024-12-22 01:44:05,487 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:06,242 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:06,243 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:44:06,315 - [Process 2/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:2')
2024-12-22 01:44:06,985 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:06,985 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1569])
2024-12-22 01:44:07,040 - [Process 3/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:3')
2024-12-22 01:44:07,115 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:07,115 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2029])
2024-12-22 01:44:07,186 - [Process 0/5] - DEBUG - predict_token:tensor([[9205]], device='cuda:0')
2024-12-22 01:44:08,464 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:08,464 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1992])
2024-12-22 01:44:08,537 - [Process 4/5] - DEBUG - predict_token:tensor([[17088]], device='cuda:4')
2024-12-22 01:44:09,069 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:09,069 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:44:09,141 - [Process 1/5] - DEBUG - predict_token:tensor([[6864]], device='cuda:1')
2024-12-22 01:44:09,171 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other entity
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first permanent English settlement in North America ?
Type
 60%|██████    | 24/40 [02:43<01:41,  6.36s/it]2024-12-22 01:44:09,372 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:09,751 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the average lifespan of a human ?
Type: Lasting time of somethin
Question: What is the name of the largest planet in our solar system
 65%|██████▌   | 26/40 [02:43<01:23,  5.95s/it]2024-12-22 01:44:09,854 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location

 62%|██████▎   | 25/40 [02:43<01:36,  6.42s/it]2024-12-22 01:44:09,891 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:10,064 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:11,440 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Language
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the smallest country in the world ?
Type: Country

 60%|██████    | 24/40 [02:45<01:46,  6.66s/it]2024-12-22 01:44:11,661 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:12,056 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly in space ?
Type: Individual
Question: What is the name of the first person to reach the summit of
 62%|██████▎   | 25/40 [02:45<01:33,  6.26s/it]2024-12-22 01:44:12,161 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:12,938 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:12,938 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:44:13,010 - [Process 2/5] - DEBUG - predict_token:tensor([[4412]], device='cuda:2')
2024-12-22 01:44:13,418 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:13,419 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 01:44:13,490 - [Process 0/5] - DEBUG - predict_token:tensor([[20795]], device='cuda:0')
2024-12-22 01:44:13,664 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:13,665 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 01:44:13,737 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:44:14,252 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:14,252 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1155])
2024-12-22 01:44:14,294 - [Process 1/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:1')
2024-12-22 01:44:14,483 - [Process 3/5] - INFO - res.shape is :torch.Size([16])
results:


Please select the type of the question from the options provided.
 65%|██████▌   | 26/40 [02:48<01:22,  5.88s/it]2024-12-22 01:44:14,694 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:14,927 - [Process 1/5] - INFO - res.shape is :torch.Size([16])
results:Question

Please select the type of the question from the options provided.
 65%|██████▌   | 26/40 [02:48<01:13,  5.24s/it]2024-12-22 01:44:15,116 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:15,206 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:15,207 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:44:15,276 - [Process 4/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:4')
2024-12-22 01:44:15,787 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: City
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the capital of France ?
Type: City
Question: What is the smallest country in the world ?
Type: Country
Question: What is the largest planet in our solar system
 62%|██████▎   | 25/40 [02:49<01:36,  6.44s/it]2024-12-22 01:44:16,004 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:16,055 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:


Please select one of the following options:

Individual
Definition of something
Equivalent term
Lasting time of something
Other location
Other entity
Other location
Number of something
Product
Reason
Manner of an action
Description of something
Invention, book and other
 68%|██████▊   | 27/40 [02:49<01:18,  6.06s/it]2024-12-22 01:44:16,177 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:18,017 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the roller coaster
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the smallest country
 62%|██████▎   | 25/40 [02:51<01:39,  6.64s/it]2024-12-22 01:44:18,212 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:18,297 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:18,297 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:44:18,370 - [Process 3/5] - DEBUG - predict_token:tensor([[6431]], device='cuda:3')
2024-12-22 01:44:18,692 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:18,692 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:44:18,764 - [Process 1/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:1')
2024-12-22 01:44:19,571 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:19,572 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:44:19,643 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:44:19,707 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:19,707 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:44:19,779 - [Process 0/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:0')
2024-12-22 01:44:21,198 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Group or organization of person
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to
 68%|██████▊   | 27/40 [02:55<01:19,  6.13s/it]2024-12-22 01:44:21,442 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:21,492 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 68%|██████▊   | 27/40 [02:55<01:13,  5.64s/it]2024-12-22 01:44:21,705 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:21,800 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:21,800 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:44:21,873 - [Process 4/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:4')
2024-12-22 01:44:22,343 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the difference between a cello and a violin ?
Type: Definition of something
Question: What is the name of the first woman to fly solo across the
 70%|███████   | 28/40 [02:56<01:13,  6.13s/it]2024-12-22 01:44:22,462 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:22,481 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the largest living thing on Earth ?
Type: Other
 65%|██████▌   | 26/40 [02:56<01:31,  6.51s/it]2024-12-22 01:44:22,603 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:24,615 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic
 65%|██████▌   | 26/40 [02:58<01:32,  6.62s/it]2024-12-22 01:44:24,825 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:25,003 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:25,003 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 01:44:25,039 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:25,039 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1365])
2024-12-22 01:44:25,077 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:44:25,088 - [Process 2/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:2')
2024-12-22 01:44:25,289 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:25,290 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 01:44:25,362 - [Process 1/5] - DEBUG - predict_token:tensor([[9159]], device='cuda:1')
2024-12-22 01:44:25,993 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:25,993 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 01:44:26,065 - [Process 0/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:0')
2024-12-22 01:44:27,976 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What is the
 68%|██████▊   | 27/40 [03:01<01:20,  6.21s/it]2024-12-22 01:44:27,982 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the difference between a cashmere sweater and a wool sweater ?
Type: Description of something
Question: What is the name of the largest planet in our solar system ?
Type
 70%|███████   | 28/40 [03:01<01:15,  6.33s/it]2024-12-22 01:44:28,085 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Color
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the first woman to fly across the Atlantic Ocean ?

 70%|███████   | 28/40 [03:01<01:11,  5.93s/it]2024-12-22 01:44:28,179 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:28,216 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:28,269 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:28,412 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:28,413 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:44:28,485 - [Process 4/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:4')
2024-12-22 01:44:28,629 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world located entirely below sea level ?
Type: City
Question: What is the name of the first permanent English settlement in
 72%|███████▎  | 29/40 [03:02<01:07,  6.17s/it]2024-12-22 01:44:28,759 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:31,223 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly solo across the Atlantic Ocean ?

 68%|██████▊   | 27/40 [03:05<01:26,  6.62s/it]2024-12-22 01:44:31,415 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:31,745 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:31,745 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:44:31,811 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:31,811 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1995])
2024-12-22 01:44:31,817 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:44:31,858 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:31,858 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 01:44:31,884 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:44:31,930 - [Process 1/5] - DEBUG - predict_token:tensor([[17015]], device='cuda:1')
2024-12-22 01:44:32,242 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:32,242 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1975])
2024-12-22 01:44:32,314 - [Process 0/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:0')
2024-12-22 01:44:34,657 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the most popular sport in Brazil ?
Type: Group or organization of person
Question: What is the most common blood type ?
Type: Definition of something
Question: What is the most common eye color ?
Type: Definition of something
Question: What is the
 72%|███████▎  | 29/40 [03:08<01:07,  6.12s/it]2024-12-22 01:44:34,739 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the smallest country in the world ?
Type: Country
Question: What is
 70%|███████   | 28/40 [03:08<01:16,  6.37s/it]2024-12-22 01:44:34,804 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the difference between a cactus and a succulent ?
Type: Description of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the difference between a candy cane and
 72%|███████▎  | 29/40 [03:08<01:11,  6.48s/it]2024-12-22 01:44:34,855 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:34,875 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the largest city in the state of Texas ?
Type: City
Question:
 75%|███████▌  | 30/40 [03:08<01:01,  6.20s/it]2024-12-22 01:44:34,946 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:34,977 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:35,003 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:35,004 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:44:35,032 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:35,076 - [Process 4/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:4')
2024-12-22 01:44:37,820 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly in space ?
Type: Individual
Question: What is the name of the first person to reach the summit of Mount
 70%|███████   | 28/40 [03:11<01:19,  6.61s/it]2024-12-22 01:44:37,987 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:38,270 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:38,270 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1892])
2024-12-22 01:44:38,339 - [Process 0/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:0')
2024-12-22 01:44:38,438 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:38,438 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:44:38,510 - [Process 1/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:1')
2024-12-22 01:44:38,523 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:38,523 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 01:44:38,594 - [Process 2/5] - DEBUG - predict_token:tensor([[18527]], device='cuda:2')
2024-12-22 01:44:38,638 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:38,638 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 01:44:38,711 - [Process 3/5] - DEBUG - predict_token:tensor([[4306]], device='cuda:3')
2024-12-22 01:44:40,868 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?

 78%|███████▊  | 31/40 [03:14<00:55,  6.13s/it]2024-12-22 01:44:41,000 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:41,220 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:41,220 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1885])
2024-12-22 01:44:41,236 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first computer virus ?
Type: Other
 75%|███████▌  | 30/40 [03:15<01:02,  6.26s/it]2024-12-22 01:44:41,286 - [Process 4/5] - DEBUG - predict_token:tensor([[8815]], device='cuda:4')
2024-12-22 01:44:41,473 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:41,515 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other entity
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first permanent English settlement in North America
 72%|███████▎  | 29/40 [03:15<01:11,  6.50s/it]2024-12-22 01:44:41,638 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the capital of the country of Mongolia ?
Type: Other location
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What
 75%|███████▌  | 30/40 [03:15<01:05,  6.58s/it]2024-12-22 01:44:41,711 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:41,866 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:43,980 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly across the Atlantic Ocean
 72%|███████▎  | 29/40 [03:17<01:11,  6.48s/it]2024-12-22 01:44:44,183 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:44,536 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:44,537 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 01:44:44,608 - [Process 0/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:0')
2024-12-22 01:44:45,054 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:45,055 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 01:44:45,127 - [Process 1/5] - DEBUG - predict_token:tensor([[15456]], device='cuda:1')
2024-12-22 01:44:45,281 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:45,281 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:44:45,354 - [Process 2/5] - DEBUG - predict_token:tensor([[4412]], device='cuda:2')
2024-12-22 01:44:45,459 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:45,459 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 01:44:45,532 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:44:47,175 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What
 80%|████████  | 32/40 [03:21<00:49,  6.19s/it]2024-12-22 01:44:47,270 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:47,770 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:47,770 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:44:47,843 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:44:47,852 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Country
Question: What is the difference between a cougar and a puma ?
Type: Animal
Question: What is the difference between a cougar and a mountain lion ?
Type: Animal
Question: What is the difference between a cougar and a lynx ?
 78%|███████▊  | 31/40 [03:21<00:57,  6.37s/it]2024-12-22 01:44:48,106 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:48,261 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the summit of Mount Everest
 75%|███████▌  | 30/40 [03:22<01:05,  6.57s/it]2024-12-22 01:44:48,445 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the largest living thing on Earth ?
Type: Other
 78%|███████▊  | 31/40 [03:22<00:59,  6.65s/it]2024-12-22 01:44:48,510 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:48,678 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:50,272 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:50,272 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1782])
2024-12-22 01:44:50,331 - [Process 0/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:0')
2024-12-22 01:44:50,595 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Organism
Question: What is the
 75%|███████▌  | 30/40 [03:24<01:05,  6.52s/it]2024-12-22 01:44:50,837 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:51,643 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:51,643 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1974])
2024-12-22 01:44:51,715 - [Process 1/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:1')
2024-12-22 01:44:52,078 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:52,078 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:44:52,150 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:44:52,281 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:52,281 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:44:52,353 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:44:52,809 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the average body temperature ?
Question: What is the average body temperature ?
Question: What is the average body temperature ?
Question: What is the average body temperature ?
Question: What is the average body temperature ?
Question: What is the average body temperature ?
Question: What is
 82%|████████▎ | 33/40 [03:26<00:42,  6.02s/it]2024-12-22 01:44:52,949 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:54,432 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:54,432 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2020])
2024-12-22 01:44:54,446 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a cough and a sneeze ?
Type: Reason
Question: What is the difference between a cough and a sneeze ?
Type: Reason
Question: What is the difference between a cough and a sneeze ?

 80%|████████  | 32/40 [03:28<00:51,  6.43s/it]2024-12-22 01:44:54,504 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:44:54,658 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:55,057 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a cougar and a puma ?
Type: Animal
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the difference between a candy cane and a candy cane ?
Type: Description of
 78%|███████▊  | 31/40 [03:28<00:59,  6.64s/it]2024-12-22 01:44:55,269 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the highest mountain peak in the solar system ?
Question: What is the most common blood type ?
Type: Definition of something
Question: What is the most popular sport in the world ?
Type: Other location
Question: What is the most common cause of death in the United States ?
 80%|████████  | 32/40 [03:29<00:53,  6.70s/it]2024-12-22 01:44:55,281 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:55,432 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:56,484 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:56,484 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 01:44:56,556 - [Process 0/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:0')
2024-12-22 01:44:57,250 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first man to walk on the moon
 78%|███████▊  | 31/40 [03:31<00:59,  6.56s/it]2024-12-22 01:44:57,449 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:44:58,245 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:58,245 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 01:44:58,317 - [Process 1/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:1')
2024-12-22 01:44:58,631 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:58,631 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1775])
2024-12-22 01:44:58,695 - [Process 3/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:3')
2024-12-22 01:44:58,847 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:44:58,847 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:44:58,919 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:44:59,126 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What is the
 85%|████████▌ | 34/40 [03:33<00:36,  6.11s/it]2024-12-22 01:44:59,204 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:01,042 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:01,042 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 01:45:01,047 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
 82%|████████▎ | 33/40 [03:34<00:45,  6.48s/it]2024-12-22 01:45:01,115 - [Process 4/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:4')
2024-12-22 01:45:01,287 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:01,569 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the smallest country in the world ?
Type: Country
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question
 82%|████████▎ | 33/40 [03:35<00:46,  6.58s/it]2024-12-22 01:45:01,671 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:01,671 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1416])
2024-12-22 01:45:01,723 - [Process 0/5] - DEBUG - predict_token:tensor([[4412]], device='cuda:0')
2024-12-22 01:45:01,765 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:01,826 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type
 80%|████████  | 32/40 [03:35<00:53,  6.68s/it]2024-12-22 01:45:02,001 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:03,860 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What
 80%|████████  | 32/40 [03:37<00:52,  6.57s/it]2024-12-22 01:45:04,083 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:04,096 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location
Question: What is the name of the first woman to fly solo across the Atlantic
 88%|████████▊ | 35/40 [03:37<00:28,  5.77s/it]2024-12-22 01:45:04,211 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:04,869 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:04,869 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:45:04,942 - [Process 1/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:1')
2024-12-22 01:45:05,329 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:05,329 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 01:45:05,399 - [Process 3/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:3')
2024-12-22 01:45:05,509 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:05,509 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1978])
2024-12-22 01:45:05,581 - [Process 2/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:2')
2024-12-22 01:45:07,671 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the difference between a cough and a sneeze ?
Type: Other entity
Question: What is the difference between a cough and a sneeze ?
Type: Other entity
Question: What is the difference between a cough and a
 85%|████████▌ | 34/40 [03:41<00:39,  6.53s/it]2024-12-22 01:45:07,675 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:07,675 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 01:45:07,736 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:07,737 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 01:45:07,747 - [Process 4/5] - DEBUG - predict_token:tensor([[6652]], device='cuda:4')
2024-12-22 01:45:07,808 - [Process 0/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:0')
2024-12-22 01:45:07,834 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:08,306 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a 
Question: What is the difference between a 
Question: What is the difference between a 
Question: What is the difference between a 
Question: What is the difference between a 
Question: What is the difference between a 
Question: What is
 85%|████████▌ | 34/40 [03:42<00:39,  6.63s/it]2024-12-22 01:45:08,494 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other entity

 82%|████████▎ | 33/40 [03:42<00:46,  6.67s/it]2024-12-22 01:45:08,520 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:08,708 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:10,376 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the summit of Mount Everest
 90%|█████████ | 36/40 [03:44<00:23,  5.92s/it]2024-12-22 01:45:10,457 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:10,490 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 82%|████████▎ | 33/40 [03:44<00:46,  6.59s/it]2024-12-22 01:45:10,722 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:11,017 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:11,018 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1775])
2024-12-22 01:45:11,081 - [Process 1/5] - DEBUG - predict_token:tensor([[15456]], device='cuda:1')
2024-12-22 01:45:12,117 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:12,117 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 01:45:12,190 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:45:12,278 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:12,279 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 01:45:12,350 - [Process 2/5] - DEBUG - predict_token:tensor([[9208]], device='cuda:2')
2024-12-22 01:45:13,178 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:13,179 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1481])
2024-12-22 01:45:13,233 - [Process 0/5] - DEBUG - predict_token:tensor([[2799]], device='cuda:0')
2024-12-22 01:45:13,724 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Other location

 88%|████████▊ | 35/40 [03:47<00:31,  6.38s/it]2024-12-22 01:45:13,906 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:14,318 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:14,318 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 01:45:14,391 - [Process 4/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:4')
2024-12-22 01:45:15,103 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the difference between a cougar and a puma ?
Type
 88%|████████▊ | 35/40 [03:48<00:33,  6.68s/it]2024-12-22 01:45:15,269 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first permanent English settlement in North
 85%|████████▌ | 34/40 [03:49<00:40,  6.70s/it]2024-12-22 01:45:15,311 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:15,470 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:15,619 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Musical instrument
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the sum
 92%|█████████▎| 37/40 [03:49<00:17,  5.72s/it]2024-12-22 01:45:15,740 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:17,131 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: How many
Question: How many hours of video are there in a day ?
Type: Number of something
Question: How many miles is it from New York to London ?
Type: Distance
Question: How many people are there in the world ?
Type: Number of something
Question:
 85%|████████▌ | 34/40 [03:51<00:39,  6.61s/it]2024-12-22 01:45:17,332 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:17,494 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:17,494 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 01:45:17,565 - [Process 1/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:1')
2024-12-22 01:45:18,909 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:18,909 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 01:45:18,981 - [Process 3/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:3')
2024-12-22 01:45:19,042 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:19,042 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:45:19,113 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:45:19,274 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:19,274 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 01:45:19,346 - [Process 0/5] - DEBUG - predict_token:tensor([[1894]], device='cuda:0')
2024-12-22 01:45:20,294 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the difference between a 
Question: What is the difference between a 
Question: What is the difference between a 
Question: What is the difference between a 
Question: What is the difference between a 
Question: What is the difference between a
 90%|█████████ | 36/40 [03:54<00:25,  6.44s/it]2024-12-22 01:45:20,529 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:20,931 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:20,932 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 01:45:21,004 - [Process 4/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:4')
2024-12-22 01:45:21,896 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the first woman to fly across the Atlantic Ocean
 90%|█████████ | 36/40 [03:55<00:26,  6.71s/it]2024-12-22 01:45:21,912 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Individual
Question: What is the name of the largest city in the world located entirely below sea level ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world located
 95%|█████████▌| 38/40 [03:55<00:11,  5.89s/it]2024-12-22 01:45:22,024 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:22,033 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the difference between a cougar and a mountain lion ?
Type: Description of something
Question: What is the difference between a cougar and a puma ?
Type: Description of something
Question: What is the difference between a cougar
 88%|████████▊ | 35/40 [03:55<00:33,  6.72s/it]2024-12-22 01:45:22,146 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:22,277 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:23,741 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other entity
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first permanent English settlement in North America ?
Type
 88%|████████▊ | 35/40 [03:57<00:33,  6.61s/it]2024-12-22 01:45:23,934 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:24,116 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:24,116 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2020])
2024-12-22 01:45:24,188 - [Process 1/5] - DEBUG - predict_token:tensor([[360]], device='cuda:1')
2024-12-22 01:45:25,560 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:25,560 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 01:45:25,632 - [Process 0/5] - DEBUG - predict_token:tensor([[5974]], device='cuda:0')
2024-12-22 01:45:25,746 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:25,747 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 01:45:25,820 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:45:25,843 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:25,844 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 01:45:25,915 - [Process 2/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:2')
2024-12-22 01:45:26,915 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the difference between a cough and a sneeze ?
Type: Other location
Question: What is the difference between a cough and a sneeze ?
Type: Other location
Question: What is the difference between a cough and a
 92%|█████████▎| 37/40 [04:00<00:19,  6.49s/it]2024-12-22 01:45:27,155 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:27,528 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:27,528 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:45:27,601 - [Process 4/5] - DEBUG - predict_token:tensor([[5974]], device='cuda:4')
2024-12-22 01:45:28,197 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
 98%|█████████▊| 39/40 [04:02<00:06,  6.01s/it]2024-12-22 01:45:28,323 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:28,731 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a sunspot and a solar flare ?
Type: Other location
Question: What is the difference between a sunspot and a coronal mass ejection ?
Type: Other location
Question: What is the difference between a sunspot and a solar flare ?
Type
 92%|█████████▎| 37/40 [04:02<00:20,  6.75s/it]2024-12-22 01:45:28,834 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 90%|█████████ | 36/40 [04:02<00:26,  6.75s/it]2024-12-22 01:45:28,978 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:29,089 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:30,338 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question
 90%|█████████ | 36/40 [04:04<00:26,  6.60s/it]2024-12-22 01:45:30,559 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:30,749 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:30,749 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2020])
2024-12-22 01:45:30,821 - [Process 1/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:1')
2024-12-22 01:45:31,804 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:31,804 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:45:31,873 - [Process 0/5] - DEBUG - predict_token:tensor([[360]], device='cuda:0')
2024-12-22 01:45:32,574 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:32,574 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1985])
2024-12-22 01:45:32,647 - [Process 3/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:3')
2024-12-22 01:45:32,661 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:32,662 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 01:45:32,733 - [Process 2/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:2')
2024-12-22 01:45:33,552 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the sum
 95%|█████████▌| 38/40 [04:07<00:13,  6.54s/it]2024-12-22 01:45:33,738 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:34,156 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:34,157 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2020])
2024-12-22 01:45:34,229 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:45:34,440 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:Question: Disease and medicine
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the highest mountain in the solar system ?
100%|██████████| 40/40 [04:08<00:00,  6.08s/it]100%|██████████| 40/40 [04:08<00:00,  6.21s/it]
2024-12-22 01:45:35,570 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the sum
 95%|█████████▌| 38/40 [04:09<00:13,  6.78s/it]2024-12-22 01:45:35,659 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the meaning of the word "sans" ?
Type: Definition of something
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the difference between a cougar and a puma ?
Type:
 92%|█████████▎| 37/40 [04:09<00:20,  6.77s/it]2024-12-22 01:45:35,696 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:35,866 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:36,968 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 92%|█████████▎| 37/40 [04:10<00:19,  6.61s/it]2024-12-22 01:45:37,120 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:37,274 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:37,274 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-22 01:45:37,347 - [Process 1/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:1')
2024-12-22 01:45:38,096 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:38,097 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1312])
2024-12-22 01:45:38,145 - [Process 3/5] - DEBUG - predict_token:tensor([[9681]], device='cuda:3')
2024-12-22 01:45:39,442 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:39,442 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2020])
2024-12-22 01:45:39,513 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:45:40,018 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:40,018 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1690])
2024-12-22 01:45:40,077 - [Process 4/5] - DEBUG - predict_token:tensor([[4306]], device='cuda:4')
2024-12-22 01:45:40,088 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the summit
 98%|█████████▊| 39/40 [04:13<00:06,  6.54s/it]2024-12-22 01:45:40,312 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:40,823 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Number of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 98%|█████████▊| 39/40 [04:14<00:06,  6.32s/it]2024-12-22 01:45:41,043 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:42,339 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Description of something
Question: What is the name of the first permanent English settlement in North America ?
Type: Other location
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first man to walk on the moon ?

 95%|█████████▌| 38/40 [04:16<00:13,  6.74s/it]2024-12-22 01:45:42,561 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:42,725 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Animal
Question: What is the name
 95%|█████████▌| 38/40 [04:16<00:12,  6.36s/it]2024-12-22 01:45:42,927 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:43,892 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:43,892 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 01:45:43,965 - [Process 1/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:1')
2024-12-22 01:45:44,640 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:44,641 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 01:45:44,714 - [Process 3/5] - DEBUG - predict_token:tensor([[4712]], device='cuda:3')
2024-12-22 01:45:46,133 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:46,134 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 01:45:46,205 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:45:46,516 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:46,517 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 01:45:46,588 - [Process 4/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:4')
2024-12-22 01:45:46,707 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:Question: Other location
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: Other location
Question: What is the name of the first woman to fly solo across the Atlantic Ocean
100%|██████████| 40/40 [04:20<00:00,  6.56s/it]100%|██████████| 40/40 [04:20<00:00,  6.51s/it]
2024-12-22 01:45:47,526 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Question: Date
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the first woman to fly solo across the Atlantic Ocean ?

100%|██████████| 40/40 [04:21<00:00,  6.43s/it]100%|██████████| 40/40 [04:21<00:00,  6.54s/it]
2024-12-22 01:45:49,027 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Question: Definition of something
Question: What is the name of the first man to walk on the moon ?
Type: Individual
Question: What is the name of the first woman to fly across the Atlantic Ocean ?
Type: Individual
Question: What is the name of the first person to reach the
 98%|█████████▊| 39/40 [04:22<00:06,  6.73s/it]2024-12-22 01:45:49,158 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:49,329 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the difference between a cashier and a teller ?
Type: Other entity
Question: What is the difference between a cashier and a teller ?
Type: Other entity
Question: What is the difference between a cashier and a teller ?
Question: What
 98%|█████████▊| 39/40 [04:23<00:06,  6.43s/it]2024-12-22 01:45:49,516 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:45:51,764 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:51,764 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1468])
2024-12-22 01:45:51,819 - [Process 2/5] - DEBUG - predict_token:tensor([[21940]], device='cuda:2')
2024-12-22 01:45:52,502 - [Process 2/5] - INFO - res.shape is :torch.Size([17])
results:Question

Please determine the type of the question based on the provided information.
100%|██████████| 40/40 [04:26<00:00,  5.75s/it]100%|██████████| 40/40 [04:26<00:00,  6.66s/it]
2024-12-22 01:45:53,106 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:45:53,107 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 01:45:53,179 - [Process 4/5] - DEBUG - predict_token:tensor([[5901]], device='cuda:4')
2024-12-22 01:45:55,899 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Question: What is the name of the largest city in the world ?
Type: City
Question: What is the name of the largest planet in our solar system ?
Type: Planet
Question: What is the name of the largest living thing on Earth ?
Type: Other location
Question: What is the
100%|██████████| 40/40 [04:29<00:00,  6.47s/it]100%|██████████| 40/40 [04:29<00:00,  6.74s/it]
2024-12-22 01:45:55,919 - [Process 2/5] - DEBUG - datasets_name:trec
2024-12-22 01:45:55,919 - [Process 3/5] - DEBUG - datasets_name:trec
2024-12-22 01:45:55,919 - [Process 1/5] - DEBUG - datasets_name:trec
2024-12-22 01:45:55,919 - [Process 4/5] - DEBUG - datasets_name:trec
2024-12-22 01:45:55,919 - [Process 0/5] - DEBUG - datasets_name:trec
Running evaluation for dataset: triviaqa
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.57s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.65s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:47:53,101 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:47:53,101 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:47:53,101 - [Process 3/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:47:53,112 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:47:53,112 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:47:53,112 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:47:53,120 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 01:47:53,120 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:47:53,121 - [Process 2/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:47:53,122 - [Process 1/5] - INFO - loading datasets finished
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:47:53,123 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:47:53,123 - [Process 1/5] - INFO - output_max_len: 32
2024-12-22 01:47:53,123 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:47:53,124 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:47:53,124 - [Process 0/5] - INFO - output_max_len: 32
2024-12-22 01:47:53,145 - [Process 3/5] - INFO - Max Length is 16633
2024-12-22 01:47:53,146 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:47:53,146 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:47:53,186 - [Process 4/5] - INFO - Max Length is 16633
2024-12-22 01:47:53,186 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:47:53,187 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:47:53,193 - [Process 1/5] - INFO - Max Length is 16633
2024-12-22 01:47:53,193 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:47:53,194 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 01:47:53,194 - [Process 2/5] - INFO - Max Length is 16633
2024-12-22 01:47:53,195 - [Process 2/5] - INFO - Finish loading dataset
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:47:53,195 - [Process 2/5] - INFO - get_predicted begin
2024-12-22 01:47:53,196 - [Process 0/5] - INFO - Max Length is 16633
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:47:53,196 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:47:53,197 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:47:57,891 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:47:57,972 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:47:57,973 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:47:57,974 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:47:57,976 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:02,492 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:02,492 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 01:48:02,571 - [Process 0/5] - DEBUG - predict_token:tensor([[2525]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:48:02,858 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:United States of America
  2%|▎         | 1/40 [00:09<06:16,  9.66s/it]2024-12-22 01:48:03,133 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:03,789 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:03,789 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2783])
2024-12-22 01:48:03,790 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:03,791 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2852])
2024-12-22 01:48:03,885 - [Process 2/5] - DEBUG - predict_token:tensor([[29909]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:48:03,892 - [Process 4/5] - DEBUG - predict_token:tensor([[1451]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:48:04,065 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Chile
  2%|▎         | 1/40 [00:10<07:04, 10.88s/it]2024-12-22 01:48:04,274 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:04,274 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3033])
2024-12-22 01:48:04,327 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:04,389 - [Process 3/5] - DEBUG - predict_token:tensor([[29999]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:48:04,648 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Khartoum
  2%|▎         | 1/40 [00:11<07:28, 11.50s/it]2024-12-22 01:48:04,826 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:04,877 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:04,878 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3206])
2024-12-22 01:48:05,000 - [Process 1/5] - DEBUG - predict_token:tensor([[29963]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:48:05,389 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:A guillemot is a bird of the auk family, specifically of the genus Uria or Cepphus.
Passage:
The first recorded
  2%|▎         | 1/40 [00:12<07:55, 12.20s/it]2024-12-22 01:48:05,658 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:05,927 - [Process 1/5] - INFO - res.shape is :torch.Size([18])
results:Vitamin K

Passage:
The answer is vitamin K.
  2%|▎         | 1/40 [00:12<08:16, 12.73s/it]2024-12-22 01:48:06,135 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:08,646 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:08,646 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2093])
2024-12-22 01:48:08,725 - [Process 3/5] - DEBUG - predict_token:tensor([[11639]], device='cuda:3')
2024-12-22 01:48:08,951 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:John Chilcot
  5%|▌         | 2/40 [00:15<04:36,  7.27s/it]2024-12-22 01:48:09,084 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:09,140 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:09,141 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2549])
2024-12-22 01:48:09,245 - [Process 4/5] - DEBUG - predict_token:tensor([[22628]], device='cuda:4')
2024-12-22 01:48:09,265 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:09,265 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3423])
2024-12-22 01:48:09,389 - [Process 0/5] - DEBUG - predict_token:tensor([[18700]], device='cuda:0')
2024-12-22 01:48:09,719 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Black Eyed Peas
  5%|▌         | 2/40 [00:16<05:04,  8.01s/it]2024-12-22 01:48:09,968 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:10,093 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:10,093 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2226])
2024-12-22 01:48:10,189 - [Process 2/5] - DEBUG - predict_token:tensor([[1576]], device='cuda:2')
2024-12-22 01:48:10,660 - [Process 2/5] - INFO - res.shape is :torch.Size([10])
results:The Owl





  5%|▌         | 2/40 [00:17<05:08,  8.12s/it]2024-12-22 01:48:10,751 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Passage:
The Great Fire of London - History.com
The Great Fire of London - History.com
The Great Fire of London
Share
  5%|▌         | 2/40 [00:17<05:19,  8.41s/it]2024-12-22 01:48:10,917 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:10,949 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:10,949 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2447])
2024-12-22 01:48:10,973 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:11,048 - [Process 1/5] - DEBUG - predict_token:tensor([[7976]], device='cuda:1')
2024-12-22 01:48:11,300 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Max Bygraves
  5%|▌         | 2/40 [00:18<05:19,  8.40s/it]2024-12-22 01:48:11,477 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:14,152 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:14,152 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2519])
2024-12-22 01:48:14,234 - [Process 0/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:0')
2024-12-22 01:48:14,393 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:55
  8%|▊         | 3/40 [00:21<04:00,  6.49s/it]2024-12-22 01:48:14,533 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:14,533 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2893])
2024-12-22 01:48:14,650 - [Process 3/5] - DEBUG - predict_token:tensor([[2744]], device='cuda:3')
2024-12-22 01:48:14,710 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:15,044 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Antoine Laurent Lavoisier
  8%|▊         | 3/40 [00:21<04:09,  6.73s/it]2024-12-22 01:48:15,164 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:15,294 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:15,294 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2213])
2024-12-22 01:48:15,366 - [Process 1/5] - DEBUG - predict_token:tensor([[8353]], device='cuda:1')
2024-12-22 01:48:15,414 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:15,415 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2736])
2024-12-22 01:48:15,495 - [Process 4/5] - DEBUG - predict_token:tensor([[9986]], device='cuda:4')
2024-12-22 01:48:15,656 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:DR. FINLAY
  8%|▊         | 3/40 [00:22<04:02,  6.56s/it]2024-12-22 01:48:15,851 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Arturo Toscanini
  8%|▊         | 3/40 [00:22<04:15,  6.90s/it]2024-12-22 01:48:15,869 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:16,087 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:16,615 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:16,616 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3169])
2024-12-22 01:48:16,731 - [Process 2/5] - DEBUG - predict_token:tensor([[9802]], device='cuda:2')
2024-12-22 01:48:16,954 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Mark Rothko
  8%|▊         | 3/40 [00:23<04:29,  7.29s/it]2024-12-22 01:48:17,098 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:19,356 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:19,357 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2700])
2024-12-22 01:48:19,446 - [Process 0/5] - DEBUG - predict_token:tensor([[13448]], device='cuda:0')
2024-12-22 01:48:19,742 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Antonio Vivaldi
 10%|█         | 4/40 [00:26<03:37,  6.04s/it]2024-12-22 01:48:19,975 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:20,206 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:20,206 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2646])
2024-12-22 01:48:20,310 - [Process 3/5] - DEBUG - predict_token:tensor([[29925]], device='cuda:3')
2024-12-22 01:48:20,473 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:20,473 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2554])
2024-12-22 01:48:20,562 - [Process 4/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:4')
2024-12-22 01:48:20,722 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:20,722 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2717])
2024-12-22 01:48:20,820 - [Process 1/5] - DEBUG - predict_token:tensor([[29906]], device='cuda:1')
2024-12-22 01:48:20,862 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Roddy Doyle
 10%|█         | 4/40 [00:27<03:41,  6.15s/it]2024-12-22 01:48:20,944 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:21,288 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:21,288 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2226])
2024-12-22 01:48:21,376 - [Process 2/5] - DEBUG - predict_token:tensor([[29924]], device='cuda:2')
2024-12-22 01:48:21,619 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Muriel Spark
 10%|█         | 4/40 [00:28<03:45,  6.25s/it]2024-12-22 01:48:21,746 - [Process 3/5] - INFO - res.shape is :torch.Size([32])
results:Passage:
The 1960s Saw the Rise of ...
The 1960s Saw the Rise of
 10%|█         | 4/40 [00:28<04:01,  6.72s/it]2024-12-22 01:48:21,799 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:21,901 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:22,334 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:2































 10%|█         | 4/40 [00:29<03:57,  6.60s/it]2024-12-22 01:48:22,584 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:23,689 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:23,690 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1514])
2024-12-22 01:48:23,744 - [Process 4/5] - DEBUG - predict_token:tensor([[29931]], device='cuda:4')
2024-12-22 01:48:23,923 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Lazarus
 12%|█▎        | 5/40 [00:30<02:56,  5.04s/it]2024-12-22 01:48:24,102 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:25,361 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:25,361 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2723])
2024-12-22 01:48:25,481 - [Process 0/5] - DEBUG - predict_token:tensor([[29928]], device='cuda:0')
2024-12-22 01:48:25,840 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Dartmoor National Park
 12%|█▎        | 5/40 [00:32<03:32,  6.06s/it]2024-12-22 01:48:26,062 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:26,228 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:26,228 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2610])
2024-12-22 01:48:26,308 - [Process 2/5] - DEBUG - predict_token:tensor([[29967]], device='cuda:2')
2024-12-22 01:48:26,596 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Jake LaMotta
 12%|█▎        | 5/40 [00:33<03:22,  5.79s/it]2024-12-22 01:48:26,887 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:27,386 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:27,386 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3014])
2024-12-22 01:48:27,490 - [Process 3/5] - DEBUG - predict_token:tensor([[5914]], device='cuda:3')
2024-12-22 01:48:27,698 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Charlie Chan
 12%|█▎        | 5/40 [00:34<03:45,  6.44s/it]2024-12-22 01:48:27,762 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:28,798 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:28,798 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3436])
2024-12-22 01:48:28,924 - [Process 1/5] - DEBUG - predict_token:tensor([[26197]], device='cuda:1')
2024-12-22 01:48:28,950 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:28,950 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2624])
2024-12-22 01:48:29,047 - [Process 4/5] - DEBUG - predict_token:tensor([[1451]], device='cuda:4')
2024-12-22 01:48:29,108 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Augustus
 12%|█▎        | 5/40 [00:35<03:53,  6.66s/it]2024-12-22 01:48:29,216 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:China
 15%|█▌        | 6/40 [00:36<02:54,  5.13s/it]2024-12-22 01:48:29,371 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:29,371 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:30,985 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:30,986 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1878])
2024-12-22 01:48:31,052 - [Process 3/5] - DEBUG - predict_token:tensor([[29950]], device='cuda:3')
2024-12-22 01:48:31,304 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:31,304 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2507])
2024-12-22 01:48:31,393 - [Process 2/5] - DEBUG - predict_token:tensor([[18650]], device='cuda:2')
2024-12-22 01:48:31,542 - [Process 3/5] - INFO - res.shape is :torch.Size([12])
results:Huey, Dewey, and Louie
 15%|█▌        | 6/40 [00:38<03:09,  5.56s/it]2024-12-22 01:48:31,710 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:32,649 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:32,649 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1765])
2024-12-22 01:48:32,717 - [Process 4/5] - DEBUG - predict_token:tensor([[1576]], device='cuda:4')
2024-12-22 01:48:32,852 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:HM Chief Inspector of Prisons
Passage:
The Great Fire of London
The Great Fire of London was a major conflagration that
 15%|█▌        | 6/40 [00:39<03:22,  5.95s/it]2024-12-22 01:48:32,905 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:



 18%|█▊        | 7/40 [00:39<02:33,  4.66s/it]2024-12-22 01:48:33,038 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:33,190 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:33,471 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:33,472 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2376])
2024-12-22 01:48:33,545 - [Process 1/5] - DEBUG - predict_token:tensor([[11639]], device='cuda:1')
2024-12-22 01:48:34,046 - [Process 1/5] - INFO - res.shape is :torch.Size([11])
results:John Glenn







 15%|█▌        | 6/40 [00:40<03:26,  6.08s/it]2024-12-22 01:48:34,240 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:34,241 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4180])
2024-12-22 01:48:34,263 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:34,403 - [Process 0/5] - DEBUG - predict_token:tensor([[29933]], device='cuda:0')
2024-12-22 01:48:35,981 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:35,981 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2542])
2024-12-22 01:48:36,065 - [Process 3/5] - DEBUG - predict_token:tensor([[2713]], device='cuda:3')
2024-12-22 01:48:36,311 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Sus






























 15%|█▌        | 6/40 [00:43<04:17,  7.56s/it]2024-12-22 01:48:36,519 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:36,555 - [Process 3/5] - INFO - res.shape is :torch.Size([11])
results:Shetland







 18%|█▊        | 7/40 [00:43<02:57,  5.38s/it]2024-12-22 01:48:36,751 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:36,787 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:36,787 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2278])
2024-12-22 01:48:36,858 - [Process 2/5] - DEBUG - predict_token:tensor([[1677]], device='cuda:2')
2024-12-22 01:48:37,010 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Origami
 18%|█▊        | 7/40 [00:43<02:57,  5.36s/it]2024-12-22 01:48:37,211 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:38,521 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:38,521 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2499])
2024-12-22 01:48:38,605 - [Process 1/5] - DEBUG - predict_token:tensor([[15930]], device='cuda:1')
2024-12-22 01:48:38,762 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Alloway
 18%|█▊        | 7/40 [00:45<03:05,  5.63s/it]2024-12-22 01:48:39,023 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:39,984 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:39,984 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3508])
2024-12-22 01:48:40,118 - [Process 4/5] - DEBUG - predict_token:tensor([[29903]], device='cuda:4')
2024-12-22 01:48:40,402 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Syriza
 20%|██        | 8/40 [00:47<02:57,  5.56s/it]2024-12-22 01:48:40,716 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:41,102 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:41,102 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2373])
2024-12-22 01:48:41,186 - [Process 3/5] - DEBUG - predict_token:tensor([[18275]], device='cuda:3')
2024-12-22 01:48:41,460 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Pauline Quirke
 20%|██        | 8/40 [00:48<02:47,  5.23s/it]2024-12-22 01:48:41,584 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:42,338 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:42,338 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2921])
2024-12-22 01:48:42,439 - [Process 2/5] - DEBUG - predict_token:tensor([[1433]], device='cuda:2')
2024-12-22 01:48:42,701 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Arthur Ashe
 20%|██        | 8/40 [00:49<02:54,  5.47s/it]2024-12-22 01:48:42,937 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:43,386 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:43,386 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3660])
2024-12-22 01:48:43,517 - [Process 0/5] - DEBUG - predict_token:tensor([[9824]], device='cuda:0')
2024-12-22 01:48:43,752 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Fontanelle
 18%|█▊        | 7/40 [00:50<04:08,  7.52s/it]2024-12-22 01:48:43,908 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:45,556 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:45,556 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3453])
2024-12-22 01:48:45,666 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:45,666 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2634])
2024-12-22 01:48:45,694 - [Process 1/5] - DEBUG - predict_token:tensor([[7083]], device='cuda:1')
2024-12-22 01:48:45,766 - [Process 4/5] - DEBUG - predict_token:tensor([[1666]], device='cuda:4')
2024-12-22 01:48:45,986 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Respect Party
 22%|██▎       | 9/40 [00:52<02:52,  5.57s/it]2024-12-22 01:48:46,109 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:47,159 - [Process 1/5] - INFO - res.shape is :torch.Size([28])
results:Marni Nixon


Note:
This answer is incorrect. The correct answer is "Audrey Hepburn".
 20%|██        | 8/40 [00:53<03:28,  6.51s/it]2024-12-22 01:48:47,400 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:47,400 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2240])
2024-12-22 01:48:47,417 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:47,497 - [Process 2/5] - DEBUG - predict_token:tensor([[29925]], device='cuda:2')
2024-12-22 01:48:48,094 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:48,094 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3418])
2024-12-22 01:48:48,231 - [Process 3/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:3')
2024-12-22 01:48:48,737 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:48,737 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2674])
2024-12-22 01:48:48,832 - [Process 0/5] - DEBUG - predict_token:tensor([[29933]], device='cuda:0')
2024-12-22 01:48:48,956 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:Supercontinents

Passage:
The term "supercontinent" refers to a landmass that contains most or all of the Earth's
 22%|██▎       | 9/40 [00:55<02:57,  5.71s/it]2024-12-22 01:48:49,048 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Berkshire
 20%|██        | 8/40 [00:55<03:38,  6.81s/it]2024-12-22 01:48:49,227 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:49,253 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:49,809 - [Process 3/5] - INFO - res.shape is :torch.Size([32])
results:Rennet

Passage:
The Great Train Robbery (1979) - IMDb
The Great Train Robbery (1
 22%|██▎       | 9/40 [00:56<03:12,  6.20s/it]2024-12-22 01:48:49,999 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:50,379 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:50,379 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2260])
2024-12-22 01:48:50,470 - [Process 4/5] - DEBUG - predict_token:tensor([[29903]], device='cuda:4')
2024-12-22 01:48:50,626 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Sail
 25%|██▌       | 10/40 [00:57<02:38,  5.28s/it]2024-12-22 01:48:50,873 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:52,654 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:52,654 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2810])
2024-12-22 01:48:52,761 - [Process 1/5] - DEBUG - predict_token:tensor([[29470]], device='cuda:1')
2024-12-22 01:48:52,975 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:James Hogg
 22%|██▎       | 9/40 [00:59<03:15,  6.29s/it]2024-12-22 01:48:53,214 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:53,685 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:53,685 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2345])
2024-12-22 01:48:53,783 - [Process 0/5] - DEBUG - predict_token:tensor([[29923]], device='cuda:0')
2024-12-22 01:48:53,912 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:53,912 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2605])
2024-12-22 01:48:54,001 - [Process 2/5] - DEBUG - predict_token:tensor([[1576]], device='cuda:2')
2024-12-22 01:48:54,131 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Elysian Fields
 22%|██▎       | 9/40 [01:00<03:14,  6.27s/it]2024-12-22 01:48:54,216 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:The White House
 25%|██▌       | 10/40 [01:01<02:47,  5.57s/it]2024-12-22 01:48:54,372 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:54,405 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:56,002 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:56,002 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3176])
2024-12-22 01:48:56,010 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:56,011 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2999])
2024-12-22 01:48:56,109 - [Process 4/5] - DEBUG - predict_token:tensor([[27034]], device='cuda:4')
2024-12-22 01:48:56,126 - [Process 3/5] - DEBUG - predict_token:tensor([[20841]], device='cuda:3')
2024-12-22 01:48:56,295 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Benfica
 25%|██▌       | 10/40 [01:03<03:08,  6.29s/it]2024-12-22 01:48:56,366 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:57,655 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:twenty

Passage:
The 1999-2000 season was the 100th anniversary of the birth
 28%|██▊       | 11/40 [01:04<02:48,  5.82s/it]2024-12-22 01:48:57,952 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:48:58,262 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:48:58,263 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2605])
2024-12-22 01:48:58,367 - [Process 1/5] - DEBUG - predict_token:tensor([[2499]], device='cuda:1')
2024-12-22 01:48:58,764 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:ALBERT EINSTEIN
 25%|██▌       | 10/40 [01:05<03:04,  6.14s/it]2024-12-22 01:48:58,990 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:00,378 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:00,379 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3198])
2024-12-22 01:49:00,505 - [Process 0/5] - DEBUG - predict_token:tensor([[20841]], device='cuda:0')
2024-12-22 01:49:00,850 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:00,851 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3769])
2024-12-22 01:49:00,966 - [Process 2/5] - DEBUG - predict_token:tensor([[17406]], device='cuda:2')
2024-12-22 01:49:01,278 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:01,279 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2692])
2024-12-22 01:49:01,311 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Alexander Dubcek
 28%|██▊       | 11/40 [01:08<02:55,  6.04s/it]2024-12-22 01:49:01,377 - [Process 3/5] - DEBUG - predict_token:tensor([[29950]], device='cuda:3')
2024-12-22 01:49:01,542 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:01,620 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Harrisburg
 28%|██▊       | 11/40 [01:08<02:53,  6.00s/it]2024-12-22 01:49:01,683 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:02,142 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Ben Hur

Passage:
The first known use of the term "Cyclades" was by the Greek historian Herodotus (c. 
 25%|██▌       | 10/40 [01:08<03:24,  6.81s/it]2024-12-22 01:49:02,448 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:02,696 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:02,696 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2579])
2024-12-22 01:49:02,789 - [Process 4/5] - DEBUG - predict_token:tensor([[29999]], device='cuda:4')
2024-12-22 01:49:04,265 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Zambezi River

Passage:
The Art Story
The Art Story
The Art Story is a free online guide to art, covering the
 30%|███       | 12/40 [01:11<02:49,  6.06s/it]2024-12-22 01:49:04,374 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:04,375 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2759])
2024-12-22 01:49:04,481 - [Process 1/5] - DEBUG - predict_token:tensor([[1451]], device='cuda:1')
2024-12-22 01:49:04,540 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:04,898 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:04,898 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1784])
2024-12-22 01:49:04,965 - [Process 3/5] - DEBUG - predict_token:tensor([[29924]], device='cuda:3')
2024-12-22 01:49:05,219 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Muhammad Ali
 30%|███       | 12/40 [01:12<02:27,  5.27s/it]2024-12-22 01:49:05,332 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:05,999 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:China
Passage:
The History of the Indian Railways
The History of the Indian Railways
The Indian Railways have a rich and fasc
 28%|██▊       | 11/40 [01:12<03:07,  6.47s/it]2024-12-22 01:49:06,239 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:08,352 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:08,352 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3139])
2024-12-22 01:49:08,476 - [Process 0/5] - DEBUG - predict_token:tensor([[29907]], device='cuda:0')
2024-12-22 01:49:08,507 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:08,508 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3715])
2024-12-22 01:49:08,643 - [Process 2/5] - DEBUG - predict_token:tensor([[29968]], device='cuda:2')
2024-12-22 01:49:08,808 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Crystal Gayle
 28%|██▊       | 11/40 [01:15<03:16,  6.76s/it]2024-12-22 01:49:09,050 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:09,151 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:Kathleen O'Hagan
 30%|███       | 12/40 [01:15<03:04,  6.59s/it]2024-12-22 01:49:09,419 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:09,837 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:09,837 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2348])
2024-12-22 01:49:09,937 - [Process 3/5] - DEBUG - predict_token:tensor([[11639]], device='cuda:3')
2024-12-22 01:49:10,176 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:John le Carré
 32%|███▎      | 13/40 [01:17<02:19,  5.17s/it]2024-12-22 01:49:10,291 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:10,298 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:10,298 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2120])
2024-12-22 01:49:10,379 - [Process 1/5] - DEBUG - predict_token:tensor([[29923]], device='cuda:1')
2024-12-22 01:49:10,576 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Emerald
 30%|███       | 12/40 [01:17<02:45,  5.90s/it]2024-12-22 01:49:10,776 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:13,035 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:13,036 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4395])
2024-12-22 01:49:13,202 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 01:49:13,245 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:13,246 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2232])
2024-12-22 01:49:13,334 - [Process 0/5] - DEBUG - predict_token:tensor([[3112]], device='cuda:0')
2024-12-22 01:49:14,779 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:FINE






























 30%|███       | 12/40 [01:21<03:02,  6.52s/it]2024-12-22 01:49:14,968 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:15,140 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:
































 32%|███▎      | 13/40 [01:21<03:22,  7.52s/it]2024-12-22 01:49:15,348 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:15,348 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2445])
2024-12-22 01:49:15,376 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:15,438 - [Process 1/5] - DEBUG - predict_token:tensor([[20130]], device='cuda:1')
2024-12-22 01:49:15,650 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:15,650 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3322])
2024-12-22 01:49:15,771 - [Process 2/5] - DEBUG - predict_token:tensor([[29954]], device='cuda:2')
2024-12-22 01:49:15,913 - [Process 1/5] - INFO - res.shape is :torch.Size([10])
results:Breakfast at Tiffany's
 32%|███▎      | 13/40 [01:22<02:34,  5.73s/it]2024-12-22 01:49:16,143 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:Gillian Gibbons
 32%|███▎      | 13/40 [01:22<03:01,  6.71s/it]2024-12-22 01:49:16,188 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:16,353 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:17,208 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:17,208 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3821])
2024-12-22 01:49:17,343 - [Process 3/5] - DEBUG - predict_token:tensor([[29907]], device='cuda:3')
2024-12-22 01:49:17,570 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Cambridge
 35%|███▌      | 14/40 [01:24<02:31,  5.84s/it]2024-12-22 01:49:17,701 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:19,136 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:19,136 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2281])
2024-12-22 01:49:19,225 - [Process 0/5] - DEBUG - predict_token:tensor([[29940]], device='cuda:0')
2024-12-22 01:49:19,516 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Nadine Coyle
 32%|███▎      | 13/40 [01:26<02:41,  5.98s/it]2024-12-22 01:49:19,765 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:20,386 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:20,387 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2728])
2024-12-22 01:49:20,490 - [Process 4/5] - DEBUG - predict_token:tensor([[6028]], device='cuda:4')
2024-12-22 01:49:20,628 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:20,629 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2530])
2024-12-22 01:49:20,657 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Canada
 35%|███▌      | 14/40 [01:27<02:59,  6.91s/it]2024-12-22 01:49:20,719 - [Process 1/5] - DEBUG - predict_token:tensor([[29967]], device='cuda:1')
2024-12-22 01:49:20,873 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:20,976 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Jimi Hendrix
 35%|███▌      | 14/40 [01:27<02:23,  5.53s/it]2024-12-22 01:49:21,062 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:22,191 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:22,191 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2489])
2024-12-22 01:49:22,283 - [Process 3/5] - DEBUG - predict_token:tensor([[5914]], device='cuda:3')
2024-12-22 01:49:22,519 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Charlie Chaplin
 38%|███▊      | 15/40 [01:29<02:19,  5.57s/it]2024-12-22 01:49:22,576 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:22,577 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3427])
2024-12-22 01:49:22,702 - [Process 2/5] - DEBUG - predict_token:tensor([[2517]], device='cuda:2')
2024-12-22 01:49:22,725 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:22,883 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Manchester
 35%|███▌      | 14/40 [01:29<02:54,  6.72s/it]2024-12-22 01:49:23,148 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:24,203 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:24,203 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2611])
2024-12-22 01:49:24,281 - [Process 0/5] - DEBUG - predict_token:tensor([[1762]], device='cuda:0')
2024-12-22 01:49:24,749 - [Process 0/5] - INFO - res.shape is :torch.Size([10])
results:Too Long; Didn't Read
 35%|███▌      | 14/40 [01:31<02:29,  5.76s/it]2024-12-22 01:49:24,933 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:24,933 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 01:49:24,933 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:25,015 - [Process 1/5] - DEBUG - predict_token:tensor([[29931]], device='cuda:1')
2024-12-22 01:49:25,942 - [Process 1/5] - INFO - res.shape is :torch.Size([21])
results:Archery

Note:
I will only give you the answer and not the full passage.
 38%|███▊      | 15/40 [01:32<02:13,  5.36s/it]2024-12-22 01:49:26,254 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:27,546 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:27,546 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3802])
2024-12-22 01:49:27,672 - [Process 4/5] - DEBUG - predict_token:tensor([[5323]], device='cuda:4')
2024-12-22 01:49:28,130 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:28,131 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2660])
2024-12-22 01:49:28,230 - [Process 2/5] - DEBUG - predict_token:tensor([[29911]], device='cuda:2')
2024-12-22 01:49:28,394 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Tittle
 38%|███▊      | 15/40 [01:35<02:38,  6.35s/it]2024-12-22 01:49:28,689 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:28,733 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:28,734 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3232])
2024-12-22 01:49:28,850 - [Process 3/5] - DEBUG - predict_token:tensor([[27501]], device='cuda:3')
2024-12-22 01:49:29,111 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Julian Fellowes
 40%|████      | 16/40 [01:35<02:21,  5.88s/it]2024-12-22 01:49:29,291 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:29,353 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Travel sickness

Passage:
The 1975 FIFA World Cup was the 10th FIFA World Cup, held in West
 38%|███▊      | 15/40 [01:36<03:06,  7.45s/it]2024-12-22 01:49:29,652 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:30,283 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:30,284 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2151])
2024-12-22 01:49:30,337 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:30,337 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2855])
2024-12-22 01:49:30,365 - [Process 1/5] - DEBUG - predict_token:tensor([[29890]], device='cuda:1')
2024-12-22 01:49:30,453 - [Process 0/5] - DEBUG - predict_token:tensor([[2772]], device='cuda:0')
2024-12-22 01:49:30,563 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Bassoon
 40%|████      | 16/40 [01:37<02:03,  5.14s/it]2024-12-22 01:49:30,803 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:32,020 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:
































 38%|███▊      | 15/40 [01:38<02:35,  6.21s/it]2024-12-22 01:49:32,210 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:33,648 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:33,648 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2191])
2024-12-22 01:49:33,741 - [Process 3/5] - DEBUG - predict_token:tensor([[29968]], device='cuda:3')
2024-12-22 01:49:33,888 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Kent
 42%|████▎     | 17/40 [01:40<02:07,  5.55s/it]2024-12-22 01:49:34,028 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:34,029 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2701])
2024-12-22 01:49:34,072 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:34,144 - [Process 2/5] - DEBUG - predict_token:tensor([[3226]], device='cuda:2')
2024-12-22 01:49:34,312 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Leeds
 40%|████      | 16/40 [01:41<02:29,  6.22s/it]2024-12-22 01:49:34,591 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:34,608 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:34,609 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2731])
2024-12-22 01:49:34,708 - [Process 4/5] - DEBUG - predict_token:tensor([[29909]], device='cuda:4')
2024-12-22 01:49:34,919 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:On the foot
 40%|████      | 16/40 [01:41<02:45,  6.88s/it]2024-12-22 01:49:35,155 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:35,629 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:35,630 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2444])
2024-12-22 01:49:35,727 - [Process 1/5] - DEBUG - predict_token:tensor([[2052]], device='cuda:1')
2024-12-22 01:49:35,888 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Apple
 42%|████▎     | 17/40 [01:42<01:59,  5.19s/it]2024-12-22 01:49:36,148 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:37,099 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:37,099 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2694])
2024-12-22 01:49:37,198 - [Process 0/5] - DEBUG - predict_token:tensor([[29933]], device='cuda:0')
2024-12-22 01:49:37,453 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Baby buggy
 40%|████      | 16/40 [01:44<02:23,  5.98s/it]2024-12-22 01:49:37,745 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:38,160 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:38,160 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2173])
2024-12-22 01:49:38,244 - [Process 3/5] - DEBUG - predict_token:tensor([[29177]], device='cuda:3')
2024-12-22 01:49:38,473 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Underground Railroad
 45%|████▌     | 18/40 [01:45<01:55,  5.26s/it]2024-12-22 01:49:38,555 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:39,578 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:39,578 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2520])
2024-12-22 01:49:39,669 - [Process 4/5] - DEBUG - predict_token:tensor([[20841]], device='cuda:4')
2024-12-22 01:49:39,966 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:BEN DREW
 42%|████▎     | 17/40 [01:46<02:25,  6.33s/it]2024-12-22 01:49:40,209 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:40,210 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:40,211 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2289])
2024-12-22 01:49:40,292 - [Process 1/5] - DEBUG - predict_token:tensor([[29933]], device='cuda:1')
2024-12-22 01:49:40,577 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Buddy Holly
 45%|████▌     | 18/40 [01:47<01:50,  5.04s/it]2024-12-22 01:49:40,867 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:40,880 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:40,880 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3271])
2024-12-22 01:49:41,006 - [Process 2/5] - DEBUG - predict_token:tensor([[2744]], device='cuda:2')
2024-12-22 01:49:41,284 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Ann Dunham
 42%|████▎     | 17/40 [01:48<02:28,  6.45s/it]2024-12-22 01:49:41,486 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:42,684 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:42,684 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2329])
2024-12-22 01:49:42,737 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:42,737 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2786])
2024-12-22 01:49:42,768 - [Process 3/5] - DEBUG - predict_token:tensor([[29909]], device='cuda:3')
2024-12-22 01:49:42,837 - [Process 0/5] - DEBUG - predict_token:tensor([[4297]], device='cuda:0')
2024-12-22 01:49:43,040 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Aldo Moro
 48%|████▊     | 19/40 [01:49<01:46,  5.05s/it]2024-12-22 01:49:43,140 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Baron of Brighton
 42%|████▎     | 17/40 [01:49<02:15,  5.89s/it]2024-12-22 01:49:43,191 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:43,267 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:44,975 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:44,976 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2334])
2024-12-22 01:49:45,058 - [Process 1/5] - DEBUG - predict_token:tensor([[10454]], device='cuda:1')
2024-12-22 01:49:45,257 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Nowhere Boy
 48%|████▊     | 19/40 [01:52<01:43,  4.93s/it]2024-12-22 01:49:45,411 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:45,411 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2775])
2024-12-22 01:49:45,506 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:45,510 - [Process 4/5] - DEBUG - predict_token:tensor([[29931]], device='cuda:4')
2024-12-22 01:49:46,001 - [Process 4/5] - INFO - res.shape is :torch.Size([10])
results:Baroness Royall of Blaisdon
 45%|████▌     | 18/40 [01:52<02:17,  6.24s/it]2024-12-22 01:49:46,044 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:46,045 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1575])
2024-12-22 01:49:46,102 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:46,103 - [Process 0/5] - DEBUG - predict_token:tensor([[29470]], device='cuda:0')
2024-12-22 01:49:46,284 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:James Blunt
 45%|████▌     | 18/40 [01:53<01:51,  5.07s/it]2024-12-22 01:49:46,562 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:47,869 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:47,869 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3316])
2024-12-22 01:49:47,996 - [Process 2/5] - DEBUG - predict_token:tensor([[11639]], device='cuda:2')
2024-12-22 01:49:49,615 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:John Gorman




























 45%|████▌     | 18/40 [01:56<02:34,  7.01s/it]2024-12-22 01:49:49,876 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:49,990 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:49,990 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3227])
2024-12-22 01:49:50,136 - [Process 3/5] - DEBUG - predict_token:tensor([[29903]], device='cuda:3')
2024-12-22 01:49:50,656 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:50,657 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2380])
2024-12-22 01:49:50,746 - [Process 4/5] - DEBUG - predict_token:tensor([[21133]], device='cuda:4')
2024-12-22 01:49:50,875 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:50,875 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2172])
2024-12-22 01:49:50,968 - [Process 0/5] - DEBUG - predict_token:tensor([[29933]], device='cuda:0')
2024-12-22 01:49:51,123 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Basketball
 48%|████▊     | 19/40 [01:57<01:44,  5.00s/it]2024-12-22 01:49:51,409 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:51,580 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:51,580 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3109])
2024-12-22 01:49:51,701 - [Process 3/5] - INFO - res.shape is :torch.Size([32])
results:South Africa





























 50%|█████     | 20/40 [01:58<02:02,  6.14s/it]2024-12-22 01:49:51,707 - [Process 1/5] - DEBUG - predict_token:tensor([[22628]], device='cuda:1')
2024-12-22 01:49:51,888 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:52,234 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Niagara Falls




Passage:
The Great Fire of London

The Great Fire of London occurred in 16
 48%|████▊     | 19/40 [01:59<02:11,  6.24s/it]2024-12-22 01:49:52,488 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:53,348 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:
































 50%|█████     | 20/40 [02:00<01:57,  5.88s/it]2024-12-22 01:49:53,565 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:56,231 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:56,231 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2574])
2024-12-22 01:49:56,297 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:56,298 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2272])
2024-12-22 01:49:56,328 - [Process 0/5] - DEBUG - predict_token:tensor([[29949]], device='cuda:0')
2024-12-22 01:49:56,371 - [Process 4/5] - DEBUG - predict_token:tensor([[29896]], device='cuda:4')
2024-12-22 01:49:56,490 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Orange
 50%|█████     | 20/40 [02:03<01:42,  5.11s/it]2024-12-22 01:49:56,675 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:56,676 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3581])
2024-12-22 01:49:56,734 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:56,815 - [Process 2/5] - DEBUG - predict_token:tensor([[29924]], device='cuda:2')
2024-12-22 01:49:57,051 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Mushrooms
 48%|████▊     | 19/40 [02:03<02:29,  7.14s/it]2024-12-22 01:49:57,294 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:57,786 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:12

Passage:
The History of the Eiffel Tower - Paris, France - TripSavvy
The History of the E
 50%|█████     | 20/40 [02:04<02:00,  6.03s/it]2024-12-22 01:49:58,049 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:49:58,846 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:49:58,847 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3645])
2024-12-22 01:49:58,990 - [Process 3/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:3')
2024-12-22 01:49:59,367 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
results:Ronald Wilson Reagan
 52%|█████▎    | 21/40 [02:06<02:05,  6.59s/it]2024-12-22 01:49:59,513 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:00,199 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:00,199 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3691])
2024-12-22 01:50:00,327 - [Process 1/5] - DEBUG - predict_token:tensor([[29954]], device='cuda:1')
2024-12-22 01:50:00,767 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:A Streetcar Named Desire
 52%|█████▎    | 21/40 [02:07<02:00,  6.34s/it]2024-12-22 01:50:00,972 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:01,488 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:01,489 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2272])
2024-12-22 01:50:01,578 - [Process 2/5] - DEBUG - predict_token:tensor([[29924]], device='cuda:2')
2024-12-22 01:50:01,780 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:M61
 50%|█████     | 20/40 [02:08<02:08,  6.42s/it]2024-12-22 01:50:02,067 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:02,095 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:02,096 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2114])
2024-12-22 01:50:02,175 - [Process 4/5] - DEBUG - predict_token:tensor([[29925]], device='cuda:4')
2024-12-22 01:50:02,371 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Pieman
 52%|█████▎    | 21/40 [02:09<01:46,  5.60s/it]2024-12-22 01:50:02,587 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:02,985 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:02,986 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3324])
2024-12-22 01:50:03,111 - [Process 0/5] - DEBUG - predict_token:tensor([[29925]], device='cuda:0')
2024-12-22 01:50:04,520 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:04,520 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2704])
2024-12-22 01:50:04,623 - [Process 3/5] - DEBUG - predict_token:tensor([[3629]], device='cuda:3')
2024-12-22 01:50:04,740 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:J James Parkinson

Passage:
The term "sugar plum" has been used since the 16th century to refer to
 52%|█████▎    | 21/40 [02:11<01:54,  6.05s/it]2024-12-22 01:50:04,825 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Beetroot
 55%|█████▌    | 22/40 [02:11<01:52,  6.25s/it]2024-12-22 01:50:04,923 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:04,971 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:05,330 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:05,330 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2373])
2024-12-22 01:50:05,411 - [Process 1/5] - DEBUG - predict_token:tensor([[24105]], device='cuda:1')
2024-12-22 01:50:05,698 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Larry Fortensky
 55%|█████▌    | 22/40 [02:12<01:46,  5.92s/it]2024-12-22 01:50:05,989 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:06,733 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:06,733 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2713])
2024-12-22 01:50:06,823 - [Process 2/5] - DEBUG - predict_token:tensor([[29636]], device='cuda:2')
2024-12-22 01:50:08,312 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:Scalene

Passage:
The History of the World in 100 Objects
The History of the World in 100 Object
 52%|█████▎    | 21/40 [02:15<02:02,  6.45s/it]2024-12-22 01:50:08,593 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:09,277 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:09,278 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3532])
2024-12-22 01:50:09,406 - [Process 4/5] - DEBUG - predict_token:tensor([[4819]], device='cuda:4')
2024-12-22 01:50:09,739 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Philadelphia Athletics
 55%|█████▌    | 22/40 [02:16<01:50,  6.13s/it]2024-12-22 01:50:09,943 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:10,400 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:10,401 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2966])
2024-12-22 01:50:10,511 - [Process 3/5] - DEBUG - predict_token:tensor([[29925]], device='cuda:3')
2024-12-22 01:50:10,960 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:10,960 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2628])
2024-12-22 01:50:11,060 - [Process 1/5] - DEBUG - predict_token:tensor([[29967]], device='cuda:1')
2024-12-22 01:50:11,105 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:11,105 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3446])
2024-12-22 01:50:11,231 - [Process 0/5] - DEBUG - predict_token:tensor([[9588]], device='cuda:0')
2024-12-22 01:50:11,316 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Johannesburg
 57%|█████▊    | 23/40 [02:18<01:39,  5.83s/it]2024-12-22 01:50:11,511 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Robert Stroud
 55%|█████▌    | 22/40 [02:18<01:52,  6.27s/it]2024-12-22 01:50:11,630 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:11,731 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:11,995 - [Process 3/5] - INFO - res.shape is :torch.Size([32])
results:Parsley

Passage:
Gland
A gland is an organ in an animal's body that synthesizes substances (such
 57%|█████▊    | 23/40 [02:18<01:50,  6.53s/it]2024-12-22 01:50:12,125 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:15,551 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:15,552 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3517])
2024-12-22 01:50:15,693 - [Process 2/5] - DEBUG - predict_token:tensor([[29903]], device='cuda:2')
2024-12-22 01:50:15,985 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:15,985 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2274])
2024-12-22 01:50:16,060 - [Process 3/5] - DEBUG - predict_token:tensor([[21972]], device='cuda:3')
2024-12-22 01:50:16,247 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Harold Wilson
 60%|██████    | 24/40 [02:23<01:33,  5.85s/it]2024-12-22 01:50:16,367 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:16,384 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:16,384 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3524])
2024-12-22 01:50:16,501 - [Process 4/5] - DEBUG - predict_token:tensor([[29968]], device='cuda:4')
2024-12-22 01:50:16,730 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Katy Perry
 57%|█████▊    | 23/40 [02:23<01:48,  6.39s/it]2024-12-22 01:50:16,872 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:16,873 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2848])
2024-12-22 01:50:16,926 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:16,977 - [Process 0/5] - DEBUG - predict_token:tensor([[29999]], device='cuda:0')
2024-12-22 01:50:17,354 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Zadok the Priest
 57%|█████▊    | 23/40 [02:24<01:44,  6.14s/it]2024-12-22 01:50:17,387 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:Bridle


Passage:
The Age of Reason
This article is about the science fiction series; The Age of Unreason (ISBN
 55%|█████▌    | 22/40 [02:24<02:10,  7.24s/it]2024-12-22 01:50:17,607 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:17,685 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:18,233 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:18,233 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3495])
2024-12-22 01:50:18,359 - [Process 1/5] - DEBUG - predict_token:tensor([[29906]], device='cuda:1')
2024-12-22 01:50:19,906 - [Process 1/5] - INFO - res.shape is :torch.Size([30])
results:1984


Note:
The answer is a bonus question, and there are no additional points awarded for answering it correctly.
 60%|██████    | 24/40 [02:26<01:46,  6.66s/it]2024-12-22 01:50:20,139 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:21,604 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:21,605 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2534])
2024-12-22 01:50:21,704 - [Process 4/5] - DEBUG - predict_token:tensor([[9588]], device='cuda:4')
2024-12-22 01:50:21,960 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Robert Maxwell
 60%|██████    | 24/40 [02:28<01:36,  6.04s/it]2024-12-22 01:50:22,175 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:22,858 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:22,858 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3560])
2024-12-22 01:50:22,987 - [Process 3/5] - DEBUG - predict_token:tensor([[29950]], device='cuda:3')
2024-12-22 01:50:23,209 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Hartford
 62%|██████▎   | 25/40 [02:30<01:32,  6.18s/it]2024-12-22 01:50:23,356 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:24,438 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:24,438 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3707])
2024-12-22 01:50:24,574 - [Process 0/5] - DEBUG - predict_token:tensor([[29924]], device='cuda:0')
2024-12-22 01:50:24,943 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:24,944 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3732])
2024-12-22 01:50:25,087 - [Process 2/5] - DEBUG - predict_token:tensor([[29594]], device='cuda:2')
2024-12-22 01:50:25,281 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:Wilkins
 57%|█████▊    | 23/40 [02:32<02:06,  7.44s/it]2024-12-22 01:50:25,574 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:26,274 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Moby Dick

Passage:
The Great Gatsby

The Great Gatsby is a novel by F. Scott Fitzgerald that
 60%|██████    | 24/40 [02:33<01:51,  6.97s/it]2024-12-22 01:50:26,562 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:26,564 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:26,565 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3313])
2024-12-22 01:50:26,693 - [Process 1/5] - DEBUG - predict_token:tensor([[29928]], device='cuda:1')
2024-12-22 01:50:26,808 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:26,808 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2442])
2024-12-22 01:50:26,898 - [Process 4/5] - DEBUG - predict_token:tensor([[29954]], device='cuda:4')
2024-12-22 01:50:27,079 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Daphne du Maurier
 62%|██████▎   | 25/40 [02:33<01:42,  6.81s/it]2024-12-22 01:50:27,160 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:28,371 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Gene Vincent

Passage:
The Great Gatsby (2013) - Cast and Crew - Moviefone
The Great
 62%|██████▎   | 25/40 [02:35<01:32,  6.15s/it]2024-12-22 01:50:28,614 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:29,672 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:29,672 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2311])
2024-12-22 01:50:29,754 - [Process 2/5] - DEBUG - predict_token:tensor([[25375]], device='cuda:2')
2024-12-22 01:50:29,904 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:29,905 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1542])
2024-12-22 01:50:29,922 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:29,922 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3344])
2024-12-22 01:50:29,959 - [Process 1/5] - DEBUG - predict_token:tensor([[4373]], device='cuda:1')
2024-12-22 01:50:30,059 - [Process 3/5] - DEBUG - predict_token:tensor([[29470]], device='cuda:3')
2024-12-22 01:50:30,085 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:OCTOPUSSY
 60%|██████    | 24/40 [02:36<01:46,  6.65s/it]2024-12-22 01:50:30,099 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Newbury
 65%|██████▌   | 26/40 [02:36<01:19,  5.67s/it]2024-12-22 01:50:30,279 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:James Woods
 65%|██████▌   | 26/40 [02:37<01:30,  6.45s/it]2024-12-22 01:50:30,303 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:30,402 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:30,410 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:32,435 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:32,436 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3125])
2024-12-22 01:50:32,556 - [Process 0/5] - DEBUG - predict_token:tensor([[1451]], device='cuda:0')
2024-12-22 01:50:32,886 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:32,887 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2415])
2024-12-22 01:50:32,968 - [Process 4/5] - DEBUG - predict_token:tensor([[15666]], device='cuda:4')
2024-12-22 01:50:33,022 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:Childe Harold's Pilgrimage
 62%|██████▎   | 25/40 [02:39<01:43,  6.91s/it]2024-12-22 01:50:33,213 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Peacockess
 65%|██████▌   | 26/40 [02:40<01:20,  5.76s/it]2024-12-22 01:50:33,278 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:33,368 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:34,690 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:34,691 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2405])
2024-12-22 01:50:34,772 - [Process 2/5] - DEBUG - predict_token:tensor([[29940]], device='cuda:2')
2024-12-22 01:50:34,838 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:34,838 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2468])
2024-12-22 01:50:34,928 - [Process 3/5] - DEBUG - predict_token:tensor([[21878]], device='cuda:3')
2024-12-22 01:50:35,525 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:35,526 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2892])
2024-12-22 01:50:35,630 - [Process 1/5] - DEBUG - predict_token:tensor([[1576]], device='cuda:1')
2024-12-22 01:50:36,200 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:Niagara Falls, Canada/USA

Passage:
The History of the World in 100 Objects
The History of
 62%|██████▎   | 25/40 [02:43<01:37,  6.49s/it]2024-12-22 01:50:36,310 - [Process 3/5] - INFO - res.shape is :torch.Size([32])
results:Madrid


Passage:
The 2010 FIFA World Cup was the 20th FIFA World Cup, the quadrenn
 68%|██████▊   | 27/40 [02:43<01:22,  6.32s/it]2024-12-22 01:50:36,479 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:36,496 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:36,842 - [Process 1/5] - INFO - res.shape is :torch.Size([25])
results:The original building, which now houses the Tate Modern Art Gallery, was the former site of Millbank Prison.
 68%|██████▊   | 27/40 [02:43<01:17,  6.00s/it]2024-12-22 01:50:37,055 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:38,108 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:38,108 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2686])
2024-12-22 01:50:38,199 - [Process 4/5] - DEBUG - predict_token:tensor([[7083]], device='cuda:4')
2024-12-22 01:50:38,361 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Marx
 68%|██████▊   | 27/40 [02:45<01:12,  5.58s/it]2024-12-22 01:50:38,666 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:39,298 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:39,298 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3193])
2024-12-22 01:50:39,424 - [Process 0/5] - DEBUG - predict_token:tensor([[29954]], device='cuda:0')
2024-12-22 01:50:39,650 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Gordon Jackson
 65%|██████▌   | 26/40 [02:46<01:35,  6.82s/it]2024-12-22 01:50:39,952 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:41,255 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:41,255 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2445])
2024-12-22 01:50:41,352 - [Process 2/5] - DEBUG - predict_token:tensor([[29933]], device='cuda:2')
2024-12-22 01:50:41,504 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:41,504 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2471])
2024-12-22 01:50:41,595 - [Process 1/5] - DEBUG - predict_token:tensor([[21599]], device='cuda:1')
2024-12-22 01:50:41,815 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:41,816 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2967])
2024-12-22 01:50:41,922 - [Process 3/5] - DEBUG - predict_token:tensor([[3421]], device='cuda:3')
2024-12-22 01:50:42,129 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:My Fair Lady
 70%|███████   | 28/40 [02:48<01:14,  6.17s/it]2024-12-22 01:50:42,289 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:42,822 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:George Blake

Passage:
BBC Storyville Preview: George Blake – Masterspy of Moscow + Q&A | Front
 65%|██████▌   | 26/40 [02:49<01:31,  6.53s/it]2024-12-22 01:50:43,069 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:Tomato

Passage:
The Great Barrier Reef
The Great Barrier Reef, the world's largest coral reef system
 70%|███████   | 28/40 [02:49<01:12,  6.06s/it]2024-12-22 01:50:43,142 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:43,288 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:44,723 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:44,723 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2461])
2024-12-22 01:50:44,821 - [Process 0/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:0')
2024-12-22 01:50:46,290 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Rum






























 68%|██████▊   | 27/40 [02:53<01:27,  6.77s/it]2024-12-22 01:50:46,340 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:46,341 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3965])
2024-12-22 01:50:46,479 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:46,494 - [Process 4/5] - DEBUG - predict_token:tensor([[1576]], device='cuda:4')
2024-12-22 01:50:47,643 - [Process 4/5] - INFO - res.shape is :torch.Size([21])
results:The flavouring of the liqueur Amaretto is almond-flavoured.
 70%|███████   | 28/40 [02:54<01:20,  6.69s/it]2024-12-22 01:50:47,836 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:47,953 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:47,954 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3016])
2024-12-22 01:50:48,064 - [Process 3/5] - DEBUG - predict_token:tensor([[24030]], device='cuda:3')
2024-12-22 01:50:48,364 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Henry Mancini
 72%|███████▎  | 29/40 [02:55<01:08,  6.19s/it]2024-12-22 01:50:48,517 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:49,383 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:49,384 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3077])
2024-12-22 01:50:49,511 - [Process 2/5] - DEBUG - predict_token:tensor([[2568]], device='cuda:2')
2024-12-22 01:50:49,832 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Independence Day
 68%|██████▊   | 27/40 [02:56<01:26,  6.67s/it]2024-12-22 01:50:50,065 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:50,313 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:50,314 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3707])
2024-12-22 01:50:50,454 - [Process 1/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:1')
2024-12-22 01:50:50,641 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Ron
 72%|███████▎  | 29/40 [02:57<01:11,  6.52s/it]2024-12-22 01:50:50,905 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:53,205 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:53,205 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3668])
2024-12-22 01:50:53,331 - [Process 0/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:0')
2024-12-22 01:50:53,421 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:53,421 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2417])
2024-12-22 01:50:53,528 - [Process 3/5] - DEBUG - predict_token:tensor([[29903]], device='cuda:3')
2024-12-22 01:50:53,616 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:River Trent
 70%|███████   | 28/40 [03:00<01:23,  6.94s/it]2024-12-22 01:50:53,783 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:54,075 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:54,076 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3336])
2024-12-22 01:50:54,202 - [Process 4/5] - DEBUG - predict_token:tensor([[26473]], device='cuda:4')
2024-12-22 01:50:54,583 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
results:Jesus Christ Superstar
 72%|███████▎  | 29/40 [03:01<01:14,  6.76s/it]2024-12-22 01:50:54,700 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:54,700 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2524])
2024-12-22 01:50:54,799 - [Process 2/5] - DEBUG - predict_token:tensor([[29950]], device='cuda:2')
2024-12-22 01:50:54,879 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:54,952 - [Process 3/5] - INFO - res.shape is :torch.Size([32])
results:Sarah Palin

Passage:
The 1969 moon landing was a historic event that marked a major milestone in the space
 75%|███████▌  | 30/40 [03:01<01:03,  6.31s/it]2024-12-22 01:50:55,067 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:55,994 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:55,994 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2721])
2024-12-22 01:50:56,102 - [Process 1/5] - DEBUG - predict_token:tensor([[29924]], device='cuda:1')
2024-12-22 01:50:56,284 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:Hovercraft





























 70%|███████   | 28/40 [03:03<01:19,  6.61s/it]2024-12-22 01:50:56,489 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:57,195 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:57,196 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1887])
2024-12-22 01:50:57,269 - [Process 0/5] - DEBUG - predict_token:tensor([[29928]], device='cuda:0')
2024-12-22 01:50:57,418 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Dog
 72%|███████▎  | 29/40 [03:04<01:05,  6.00s/it]2024-12-22 01:50:57,635 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:Muriel Spark

Passage:
The Prime of Miss Jean Brodie

The Prime of Miss Jean Brodie is a novel by Muriel
 75%|███████▌  | 30/40 [03:04<01:06,  6.66s/it]2024-12-22 01:50:57,666 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:57,866 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:50:59,138 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:50:59,139 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2200])
2024-12-22 01:50:59,229 - [Process 4/5] - DEBUG - predict_token:tensor([[8179]], device='cuda:4')
2024-12-22 01:50:59,387 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Carberry
 75%|███████▌  | 30/40 [03:06<01:01,  6.18s/it]2024-12-22 01:50:59,594 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:00,080 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:00,080 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2719])
2024-12-22 01:51:00,183 - [Process 3/5] - DEBUG - predict_token:tensor([[4035]], device='cuda:3')
2024-12-22 01:51:00,341 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Subway
 78%|███████▊  | 31/40 [03:07<00:54,  6.03s/it]2024-12-22 01:51:00,550 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:01,725 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:01,725 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2815])
2024-12-22 01:51:01,833 - [Process 2/5] - DEBUG - predict_token:tensor([[12984]], device='cuda:2')
2024-12-22 01:51:02,093 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:William Holden
 72%|███████▎  | 29/40 [03:08<01:10,  6.37s/it]2024-12-22 01:51:02,373 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:02,485 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:02,486 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2707])
2024-12-22 01:51:02,582 - [Process 0/5] - DEBUG - predict_token:tensor([[29928]], device='cuda:0')
2024-12-22 01:51:02,791 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Dakota
 75%|███████▌  | 30/40 [03:09<00:58,  5.81s/it]2024-12-22 01:51:03,026 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:04,130 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:04,130 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3399])
2024-12-22 01:51:04,255 - [Process 1/5] - DEBUG - predict_token:tensor([[20392]], device='cuda:1')
2024-12-22 01:51:04,823 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:04,824 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2258])
2024-12-22 01:51:04,914 - [Process 3/5] - DEBUG - predict_token:tensor([[29928]], device='cuda:3')
2024-12-22 01:51:05,176 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:05,176 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3017])
2024-12-22 01:51:05,271 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
results:Dame Anita Roddick
 80%|████████  | 32/40 [03:12<00:45,  5.70s/it]2024-12-22 01:51:05,283 - [Process 4/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:4')
2024-12-22 01:51:05,396 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:05,566 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Racecourses
 78%|███████▊  | 31/40 [03:12<00:55,  6.18s/it]2024-12-22 01:51:05,790 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:05,901 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:Salto Angel (Venezuela) - 979 m

Note: The list is not exhaustive and there may be other waterfalls with
 78%|███████▊  | 31/40 [03:12<01:04,  7.14s/it]2024-12-22 01:51:05,993 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:06,986 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:06,986 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2250])
2024-12-22 01:51:07,066 - [Process 0/5] - DEBUG - predict_token:tensor([[29909]], device='cuda:0')
2024-12-22 01:51:07,563 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:07,564 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2757])
2024-12-22 01:51:07,663 - [Process 2/5] - DEBUG - predict_token:tensor([[2499]], device='cuda:2')
2024-12-22 01:51:08,063 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:08,063 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1249])
2024-12-22 01:51:08,105 - [Process 1/5] - DEBUG - predict_token:tensor([[29979]], device='cuda:1')
2024-12-22 01:51:08,541 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Pepper



Passage:
The Great Fire of London
The Great Fire of London was a major conflagration that swept through the
 78%|███████▊  | 31/40 [03:15<00:52,  5.79s/it]2024-12-22 01:51:08,690 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:09,220 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:Give It to 'Em
Passage:
The Great Barrier Reef
The Great Barrier Reef is the world's largest coral
 75%|███████▌  | 30/40 [03:16<01:05,  6.60s/it]2024-12-22 01:51:09,365 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:YAOHO



Passage:
The History of the World Wide Web

The World Wide Web was invented by British
 80%|████████  | 32/40 [03:16<00:48,  6.04s/it]2024-12-22 01:51:09,457 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:09,494 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:09,494 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2145])
2024-12-22 01:51:09,514 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:09,579 - [Process 3/5] - DEBUG - predict_token:tensor([[412]], device='cuda:3')
2024-12-22 01:51:09,726 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:Peach
 82%|████████▎ | 33/40 [03:16<00:37,  5.33s/it]2024-12-22 01:51:09,914 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:12,143 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:12,143 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3297])
2024-12-22 01:51:12,271 - [Process 4/5] - DEBUG - predict_token:tensor([[29968]], device='cuda:4')
2024-12-22 01:51:12,867 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:12,867 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2516])
2024-12-22 01:51:12,949 - [Process 0/5] - DEBUG - predict_token:tensor([[29933]], device='cuda:0')
2024-12-22 01:51:13,106 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:Basketball
 80%|████████  | 32/40 [03:19<00:43,  5.42s/it]2024-12-22 01:51:13,382 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:13,673 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:13,673 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2281])
2024-12-22 01:51:13,743 - [Process 3/5] - DEBUG - predict_token:tensor([[4373]], device='cuda:3')
2024-12-22 01:51:13,887 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:New Zealand
 85%|████████▌ | 34/40 [03:20<00:29,  4.98s/it]2024-12-22 01:51:13,904 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Kim Smith

Passage:
Kim Wilde Net Worth - Get Kim Wilde Net Worth
Kim Wilde Net Worth
 80%|████████  | 32/40 [03:20<00:54,  6.83s/it]2024-12-22 01:51:14,017 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:14,154 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:14,667 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:14,667 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2868])
2024-12-22 01:51:14,758 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:14,758 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2776])
2024-12-22 01:51:14,770 - [Process 1/5] - DEBUG - predict_token:tensor([[29909]], device='cuda:1')
2024-12-22 01:51:14,864 - [Process 2/5] - DEBUG - predict_token:tensor([[29943]], device='cuda:2')
2024-12-22 01:51:15,078 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Florence
 78%|███████▊  | 31/40 [03:21<00:57,  6.37s/it]2024-12-22 01:51:15,126 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Fruit and vegetables.
 82%|████████▎ | 33/40 [03:21<00:41,  5.95s/it]2024-12-22 01:51:15,223 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:15,375 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:18,623 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:18,623 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3004])
2024-12-22 01:51:18,726 - [Process 0/5] - DEBUG - predict_token:tensor([[5914]], device='cuda:0')
2024-12-22 01:51:19,038 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Charlton Heston
 82%|████████▎ | 33/40 [03:25<00:39,  5.58s/it]2024-12-22 01:51:19,234 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:19,278 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:19,278 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2296])
2024-12-22 01:51:19,362 - [Process 2/5] - DEBUG - predict_token:tensor([[29925]], device='cuda:2')
2024-12-22 01:51:19,605 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Pentecost
 80%|████████  | 32/40 [03:26<00:46,  5.82s/it]2024-12-22 01:51:19,879 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:20,496 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:20,497 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3557])
2024-12-22 01:51:20,529 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:20,529 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3323])
2024-12-22 01:51:20,622 - [Process 3/5] - DEBUG - predict_token:tensor([[29968]], device='cuda:3')
2024-12-22 01:51:20,658 - [Process 4/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:4')
2024-12-22 01:51:20,890 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Kiss Flights
 88%|████████▊ | 35/40 [03:27<00:27,  5.59s/it]2024-12-22 01:51:21,049 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:21,049 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3035])
2024-12-22 01:51:21,076 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:21,160 - [Process 1/5] - DEBUG - predict_token:tensor([[1184]], device='cuda:1')
2024-12-22 01:51:21,392 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Prosimians
 85%|████████▌ | 34/40 [03:28<00:36,  6.05s/it]2024-12-22 01:51:21,717 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:22,320 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:River Severn

Passage:
The Great Fire of London
The Great Fire of London occurred in 1666 and lasted for
 82%|████████▎ | 33/40 [03:29<00:51,  7.30s/it]2024-12-22 01:51:22,449 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:25,670 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:25,670 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1752])
2024-12-22 01:51:25,737 - [Process 4/5] - DEBUG - predict_token:tensor([[28350]], device='cuda:4')
2024-12-22 01:51:25,816 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:25,816 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2355])
2024-12-22 01:51:25,879 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:25,879 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3600])
2024-12-22 01:51:25,898 - [Process 1/5] - DEBUG - predict_token:tensor([[29967]], device='cuda:1')
2024-12-22 01:51:26,009 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:26,010 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3301])
2024-12-22 01:51:26,012 - [Process 0/5] - DEBUG - predict_token:tensor([[1433]], device='cuda:0')
2024-12-22 01:51:26,126 - [Process 2/5] - DEBUG - predict_token:tensor([[1576]], device='cuda:2')
2024-12-22 01:51:26,251 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:Jasper Fforde
 88%|████████▊ | 35/40 [03:33<00:28,  5.69s/it]2024-12-22 01:51:26,357 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Arlene Phillips
 85%|████████▌ | 34/40 [03:33<00:36,  6.10s/it]2024-12-22 01:51:26,529 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:26,621 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:27,126 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Telephone calls



Passage:
The History of Valentine's Day - History.com
Valentine's Day,
 85%|████████▌ | 34/40 [03:33<00:39,  6.55s/it]2024-12-22 01:51:27,199 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:27,199 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3151])
2024-12-22 01:51:27,328 - [Process 3/5] - DEBUG - predict_token:tensor([[29896]], device='cuda:3')
2024-12-22 01:51:27,395 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:27,591 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:1948
 90%|█████████ | 36/40 [03:34<00:23,  5.92s/it]2024-12-22 01:51:27,659 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:27,744 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:The Perfect Storm

Passage:
The Great Fire of London
The Great Fire of London was a major conflagration that occurred in the English
 82%|████████▎ | 33/40 [03:34<00:45,  6.52s/it]2024-12-22 01:51:27,930 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:30,864 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:30,864 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2421])
2024-12-22 01:51:30,948 - [Process 1/5] - DEBUG - predict_token:tensor([[15784]], device='cuda:1')
2024-12-22 01:51:31,009 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:31,010 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1828])
2024-12-22 01:51:31,079 - [Process 3/5] - DEBUG - predict_token:tensor([[29907]], device='cuda:3')
2024-12-22 01:51:31,105 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Spain
 90%|█████████ | 36/40 [03:37<00:21,  5.44s/it]2024-12-22 01:51:31,257 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:Cary Grant
 92%|█████████▎| 37/40 [03:38<00:15,  5.24s/it]2024-12-22 01:51:31,328 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:31,403 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:32,290 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:32,291 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2721])
2024-12-22 01:51:32,388 - [Process 4/5] - DEBUG - predict_token:tensor([[29934]], device='cuda:4')
2024-12-22 01:51:32,644 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Raphael
 88%|████████▊ | 35/40 [03:39<00:31,  6.24s/it]2024-12-22 01:51:32,647 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:32,647 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2585])
2024-12-22 01:51:32,739 - [Process 2/5] - DEBUG - predict_token:tensor([[18687]], device='cuda:2')
2024-12-22 01:51:32,890 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:33,886 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:33,887 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3827])
2024-12-22 01:51:34,036 - [Process 0/5] - DEBUG - predict_token:tensor([[2182]], device='cuda:0')
2024-12-22 01:51:34,206 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:Christ Church

Passage:
The Great Fire of London
The Great Fire of London
The Great Fire of London
The Great Fire of London

 85%|████████▌ | 34/40 [03:41<00:38,  6.50s/it]2024-12-22 01:51:34,289 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:Quadrans
 88%|████████▊ | 35/40 [03:41<00:33,  6.65s/it]2024-12-22 01:51:34,445 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:34,619 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:35,719 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:35,719 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2370])
2024-12-22 01:51:35,803 - [Process 1/5] - DEBUG - predict_token:tensor([[29893]], device='cuda:1')
2024-12-22 01:51:36,713 - [Process 1/5] - INFO - res.shape is :torch.Size([20])
results:Curds and whey


Note:
The answer is given in the passage.
 92%|█████████▎| 37/40 [03:43<00:16,  5.49s/it]2024-12-22 01:51:36,988 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:36,989 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2113])
2024-12-22 01:51:37,024 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:37,071 - [Process 4/5] - DEBUG - predict_token:tensor([[29967]], device='cuda:4')
2024-12-22 01:51:37,312 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Jamie Oliver
 90%|█████████ | 36/40 [03:44<00:23,  5.77s/it]2024-12-22 01:51:37,567 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:38,644 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:38,645 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3739])
2024-12-22 01:51:38,790 - [Process 3/5] - DEBUG - predict_token:tensor([[13200]], device='cuda:3')
2024-12-22 01:51:40,415 - [Process 3/5] - INFO - res.shape is :torch.Size([32])
results:Cardigan

Note: The passage is about the 7th Earl of Cardigan, James Thomas Brudenell, who commanded the Light Brigade during the
 95%|█████████▌| 38/40 [03:47<00:12,  6.42s/it]2024-12-22 01:51:40,560 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:40,957 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:40,958 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3346])
2024-12-22 01:51:41,007 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:41,007 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3429])
2024-12-22 01:51:41,093 - [Process 2/5] - DEBUG - predict_token:tensor([[29909]], device='cuda:2')
2024-12-22 01:51:41,139 - [Process 0/5] - DEBUG - predict_token:tensor([[29956]], device='cuda:0')
2024-12-22 01:51:41,332 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Bonspiel
 88%|████████▊ | 35/40 [03:48<00:33,  6.69s/it]2024-12-22 01:51:41,586 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:42,728 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:42,728 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2911])
2024-12-22 01:51:42,806 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Wanderers





























 90%|█████████ | 36/40 [03:49<00:28,  7.21s/it]2024-12-22 01:51:42,851 - [Process 1/5] - DEBUG - predict_token:tensor([[7789]], device='cuda:1')
2024-12-22 01:51:42,876 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:43,023 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:Steel
 95%|█████████▌| 38/40 [03:49<00:11,  5.74s/it]2024-12-22 01:51:43,222 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:44,522 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:44,523 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 966])
2024-12-22 01:51:44,554 - [Process 0/5] - DEBUG - predict_token:tensor([[29968]], device='cuda:0')
2024-12-22 01:51:44,700 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:44,701 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3511])
2024-12-22 01:51:44,760 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:King George III
 92%|█████████▎| 37/40 [03:51<00:16,  5.63s/it]2024-12-22 01:51:44,846 - [Process 4/5] - DEBUG - predict_token:tensor([[7083]], device='cuda:4')
2024-12-22 01:51:45,078 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:46,525 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Marcel Marceau

Passage:
The Boston Globe

The Boston Globe is a daily newspaper founded in 1872
 92%|█████████▎| 37/40 [03:53<00:20,  6.80s/it]2024-12-22 01:51:46,746 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:47,491 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:47,491 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3503])
2024-12-22 01:51:47,629 - [Process 3/5] - DEBUG - predict_token:tensor([[29924]], device='cuda:3')
2024-12-22 01:51:47,633 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:47,634 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3134])
2024-12-22 01:51:47,760 - [Process 2/5] - DEBUG - predict_token:tensor([[1433]], device='cuda:2')
2024-12-22 01:51:47,900 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Melanin
 98%|█████████▊| 39/40 [03:54<00:06,  6.74s/it]2024-12-22 01:51:48,081 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Arthur Ransome
 90%|█████████ | 36/40 [03:54<00:26,  6.71s/it]2024-12-22 01:51:48,090 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:48,351 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:49,760 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:49,760 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3400])
2024-12-22 01:51:49,898 - [Process 1/5] - DEBUG - predict_token:tensor([[29911]], device='cuda:1')
2024-12-22 01:51:50,131 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:Taiwan
 98%|█████████▊| 39/40 [03:56<00:06,  6.15s/it]2024-12-22 01:51:50,387 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:51,275 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:51,276 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3453])
2024-12-22 01:51:51,402 - [Process 0/5] - DEBUG - predict_token:tensor([[27006]], device='cuda:0')
2024-12-22 01:51:51,682 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Jackson Pollock
 95%|█████████▌| 38/40 [03:58<00:12,  6.02s/it]2024-12-22 01:51:51,986 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:52,091 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:52,091 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3000])
2024-12-22 01:51:52,198 - [Process 4/5] - DEBUG - predict_token:tensor([[29903]], device='cuda:4')
2024-12-22 01:51:52,601 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:52,601 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2405])
2024-12-22 01:51:52,693 - [Process 3/5] - DEBUG - predict_token:tensor([[25120]], device='cuda:3')
2024-12-22 01:51:52,930 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Great Dane
100%|██████████| 40/40 [03:59<00:00,  6.23s/it]100%|██████████| 40/40 [03:59<00:00,  5.99s/it]
2024-12-22 01:51:53,759 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:Sweating

Passage:
The production and excretion of sweat.
[G. hidrōs, sweat, +
 95%|█████████▌| 38/40 [04:00<00:13,  6.93s/it]2024-12-22 01:51:54,041 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:54,587 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:54,587 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3021])
2024-12-22 01:51:54,721 - [Process 2/5] - DEBUG - predict_token:tensor([[7976]], device='cuda:2')
2024-12-22 01:51:55,458 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:55,458 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2677])
2024-12-22 01:51:55,559 - [Process 1/5] - DEBUG - predict_token:tensor([[29955]], device='cuda:1')
2024-12-22 01:51:56,339 - [Process 2/5] - INFO - res.shape is :torch.Size([32])
results:Maxwell (unit of magnetic flux)

Passage:
The Great Barrier Reef
The Great Barrier Reef is the world's
 92%|█████████▎| 37/40 [04:03<00:21,  7.17s/it]2024-12-22 01:51:56,623 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:57,064 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:7

Passage:
The History of the World in 1000 Objects
The History of the World in 100 Object
100%|██████████| 40/40 [04:03<00:00,  6.38s/it]100%|██████████| 40/40 [04:03<00:00,  6.10s/it]
2024-12-22 01:51:57,946 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:57,947 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3435])
2024-12-22 01:51:58,063 - [Process 0/5] - DEBUG - predict_token:tensor([[3624]], device='cuda:0')
2024-12-22 01:51:58,427 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:51:58,427 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2374])
2024-12-22 01:51:58,510 - [Process 4/5] - DEBUG - predict_token:tensor([[2499]], device='cuda:4')
2024-12-22 01:51:58,666 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:Almond
 98%|█████████▊| 39/40 [04:05<00:06,  6.32s/it]2024-12-22 01:51:58,806 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:51:58,886 - [Process 0/5] - INFO - res.shape is :torch.Size([16])
results:Island

Note: The answer is not provided in the passage.
 98%|█████████▊| 39/40 [04:05<00:06,  6.37s/it]2024-12-22 01:51:59,121 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:52:00,430 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:52:00,431 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2267])
2024-12-22 01:52:00,503 - [Process 2/5] - DEBUG - predict_token:tensor([[29903]], device='cuda:2')
2024-12-22 01:52:00,698 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:Syria
 95%|█████████▌| 38/40 [04:07<00:12,  6.33s/it]2024-12-22 01:52:00,869 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:52:02,353 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:52:02,354 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1954])
2024-12-22 01:52:02,427 - [Process 4/5] - DEBUG - predict_token:tensor([[29933]], device='cuda:4')
2024-12-22 01:52:02,618 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:Budapest
100%|██████████| 40/40 [04:09<00:00,  5.61s/it]100%|██████████| 40/40 [04:09<00:00,  6.24s/it]
2024-12-22 01:52:06,033 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:52:06,033 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3573])
2024-12-22 01:52:06,176 - [Process 0/5] - DEBUG - predict_token:tensor([[29963]], device='cuda:0')
2024-12-22 01:52:06,462 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Dubonnet
100%|██████████| 40/40 [04:13<00:00,  6.73s/it]100%|██████████| 40/40 [04:13<00:00,  6.33s/it]
2024-12-22 01:52:07,087 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:52:07,087 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3329])
2024-12-22 01:52:07,214 - [Process 2/5] - DEBUG - predict_token:tensor([[3253]], device='cuda:2')
2024-12-22 01:52:07,542 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Adrian Cronauer
 98%|█████████▊| 39/40 [04:14<00:06,  6.48s/it]2024-12-22 01:52:07,632 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:52:10,009 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:52:10,009 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1342])
2024-12-22 01:52:10,058 - [Process 2/5] - DEBUG - predict_token:tensor([[29968]], device='cuda:2')
2024-12-22 01:52:10,726 - [Process 2/5] - INFO - res.shape is :torch.Size([17])
results:King Arthur's Round Table


Please provide the answer only.
100%|██████████| 40/40 [04:17<00:00,  5.49s/it]100%|██████████| 40/40 [04:17<00:00,  6.44s/it]
2024-12-22 01:52:10,760 - [Process 4/5] - DEBUG - datasets_name:triviaqa
2024-12-22 01:52:10,760 - [Process 0/5] - DEBUG - datasets_name:triviaqa
2024-12-22 01:52:10,760 - [Process 2/5] - DEBUG - datasets_name:triviaqa
2024-12-22 01:52:10,760 - [Process 1/5] - DEBUG - datasets_name:triviaqa
2024-12-22 01:52:10,760 - [Process 3/5] - DEBUG - datasets_name:triviaqa
Running evaluation for dataset: passage_count
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]n_windows:[2]

model_name:/mnt/Data/xiongjing/llama2chatmodel_name:/mnt/Data/xiongjing/llama2chat

multi_gpus:True
torch.cuda.device_count():5
multi_gpus:Truemulti_gpus:True

torch.cuda.device_count():5torch.cuda.device_count():5

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:54:16,966 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:54:16,966 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:54:16,966 - [Process 3/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:54:16,971 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:54:16,972 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:54:16,972 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:54:16,981 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 01:54:16,982 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:54:16,982 - [Process 1/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:54:16,983 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 01:54:16,983 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:54:16,984 - [Process 2/5] - INFO - output_max_len: 32
2024-12-22 01:54:16,984 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:54:16,984 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:54:16,984 - [Process 0/5] - INFO - output_max_len: 32
2024-12-22 01:54:17,018 - [Process 3/5] - INFO - Max Length is 22099
2024-12-22 01:54:17,018 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:54:17,018 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:54:17,059 - [Process 4/5] - INFO - Max Length is 22099
2024-12-22 01:54:17,059 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:54:17,059 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:54:17,069 - [Process 2/5] - INFO - Max Length is 22099
2024-12-22 01:54:17,069 - [Process 1/5] - INFO - Max Length is 22099
2024-12-22 01:54:17,069 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:54:17,069 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:54:17,070 - [Process 2/5] - INFO - get_predicted begin
2024-12-22 01:54:17,070 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:54:17,071 - [Process 0/5] - INFO - Max Length is 22099
2024-12-22 01:54:17,072 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:54:17,072 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:54:21,751 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:21,832 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:21,832 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:21,837 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:21,838 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:25,920 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:25,920 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2111])
2024-12-22 01:54:25,993 - [Process 3/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:54:26,166 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:26,167 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2060])
2024-12-22 01:54:26,174 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:26,175 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 01:54:26,180 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:

5
  2%|▎         | 1/40 [00:09<05:57,  9.16s/it]2024-12-22 01:54:26,239 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:54:26,247 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:54:26,302 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:26,302 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1973])
2024-12-22 01:54:26,351 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
  2%|▎         | 1/40 [00:09<06:01,  9.28s/it]2024-12-22 01:54:26,358 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
  2%|▎         | 1/40 [00:09<06:02,  9.30s/it]2024-12-22 01:54:26,364 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:26,368 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:26,369 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:54:26,382 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:54:26,440 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:54:26,498 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
  2%|▎         | 1/40 [00:09<06:07,  9.43s/it]2024-12-22 01:54:26,554 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
  2%|▎         | 1/40 [00:09<06:09,  9.48s/it]2024-12-22 01:54:26,612 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:26,659 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:26,795 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:26,850 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:30,129 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:30,130 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 01:54:30,209 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:54:30,209 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:30,210 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1902])
2024-12-22 01:54:30,240 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:30,240 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 01:54:30,290 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:54:30,311 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
  5%|▌         | 2/40 [00:13<03:55,  6.20s/it]2024-12-22 01:54:30,313 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:54:30,398 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
  5%|▌         | 2/40 [00:13<03:55,  6.20s/it]2024-12-22 01:54:30,454 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:30,454 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2104])
2024-12-22 01:54:30,463 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
3
  5%|▌         | 2/40 [00:13<03:57,  6.24s/it]2024-12-22 01:54:30,501 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:30,526 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:54:30,535 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:30,535 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2213])
2024-12-22 01:54:30,603 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:54:30,651 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:30,685 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:30,714 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
  5%|▌         | 2/40 [00:13<04:01,  6.36s/it]2024-12-22 01:54:30,723 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:

2
  5%|▌         | 2/40 [00:13<04:01,  6.36s/it]2024-12-22 01:54:30,943 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:30,975 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:34,258 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:34,258 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:54:34,290 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:34,290 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2121])
2024-12-22 01:54:34,322 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:34,322 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 01:54:34,331 - [Process 4/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:4')
2024-12-22 01:54:34,362 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:54:34,394 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:54:34,505 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
4
  8%|▊         | 3/40 [00:17<03:15,  5.29s/it]2024-12-22 01:54:34,523 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:

3
  8%|▊         | 3/40 [00:17<03:14,  5.26s/it]2024-12-22 01:54:34,544 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
3
  8%|▊         | 3/40 [00:17<03:14,  5.25s/it]2024-12-22 01:54:34,678 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:34,686 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:34,686 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 01:54:34,747 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:34,747 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2163])
2024-12-22 01:54:34,754 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:34,764 - [Process 2/5] - DEBUG - predict_token:tensor([[29955]], device='cuda:2')
2024-12-22 01:54:34,819 - [Process 1/5] - DEBUG - predict_token:tensor([[29896]], device='cuda:1')
2024-12-22 01:54:34,872 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:3
  8%|▊         | 3/40 [00:17<03:17,  5.35s/it]2024-12-22 01:54:34,874 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:34,929 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:3
  8%|▊         | 3/40 [00:17<03:19,  5.38s/it]2024-12-22 01:54:35,106 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:35,190 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:38,304 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:38,304 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2094])
2024-12-22 01:54:38,378 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:54:38,453 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:38,453 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 01:54:38,481 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 10%|█         | 4/40 [00:21<02:51,  4.77s/it]2024-12-22 01:54:38,490 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:38,490 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 01:54:38,526 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:54:38,568 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:54:38,640 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:38,676 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
2024-12-22 01:54:38,676 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:5
results:
4
 10%|█         | 4/40 [00:21<02:53,  4.82s/it] 10%|█         | 4/40 [00:21<02:53,  4.81s/it]2024-12-22 01:54:38,824 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:38,824 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 01:54:38,883 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:38,883 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2115])
2024-12-22 01:54:38,898 - [Process 1/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:1')
2024-12-22 01:54:38,955 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:54:38,970 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:39,051 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:39,053 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 10%|█         | 4/40 [00:21<02:55,  4.89s/it]2024-12-22 01:54:39,106 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
4
 10%|█         | 4/40 [00:22<02:56,  4.91s/it]2024-12-22 01:54:39,350 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:39,357 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:42,430 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:42,430 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2151])
2024-12-22 01:54:42,502 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:54:42,576 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:42,576 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2104])
2024-12-22 01:54:42,605 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 12%|█▎        | 5/40 [00:25<02:38,  4.54s/it]2024-12-22 01:54:42,630 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:42,630 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 01:54:42,649 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:54:42,702 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:54:42,733 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:42,757 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:3
 12%|█▎        | 5/40 [00:25<02:39,  4.55s/it]2024-12-22 01:54:42,810 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:3
 12%|█▎        | 5/40 [00:25<02:39,  4.57s/it]2024-12-22 01:54:42,968 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:42,968 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:54:42,999 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:43,000 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2093])
2024-12-22 01:54:43,037 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:43,042 - [Process 2/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:2')
2024-12-22 01:54:43,045 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:43,073 - [Process 1/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:1')
2024-12-22 01:54:43,181 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:5
 12%|█▎        | 5/40 [00:26<02:41,  4.61s/it]2024-12-22 01:54:43,192 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
3
 12%|█▎        | 5/40 [00:26<02:41,  4.61s/it]2024-12-22 01:54:43,456 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:43,462 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:46,367 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:46,367 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2064])
2024-12-22 01:54:46,442 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:54:46,545 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 15%|█▌        | 6/40 [00:29<02:27,  4.33s/it]2024-12-22 01:54:46,767 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:46,778 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:46,779 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1988])
2024-12-22 01:54:46,809 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:46,810 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2117])
2024-12-22 01:54:46,857 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:54:46,881 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:54:46,966 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:3
 15%|█▌        | 6/40 [00:29<02:30,  4.43s/it]2024-12-22 01:54:47,031 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
4
 15%|█▌        | 6/40 [00:29<02:31,  4.46s/it]2024-12-22 01:54:47,064 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:47,065 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2320])
2024-12-22 01:54:47,129 - [Process 2/5] - DEBUG - predict_token:tensor([[29896]], device='cuda:2')
2024-12-22 01:54:47,167 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:47,212 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:47,212 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2207])
2024-12-22 01:54:47,281 - [Process 1/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:1')
2024-12-22 01:54:47,283 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
3
 15%|█▌        | 6/40 [00:30<02:30,  4.43s/it]2024-12-22 01:54:47,325 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:47,431 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
4
 15%|█▌        | 6/40 [00:30<02:32,  4.49s/it]2024-12-22 01:54:47,529 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:47,658 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:50,403 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:50,403 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 01:54:50,478 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:54:50,661 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:

4
 18%|█▊        | 7/40 [00:33<02:20,  4.26s/it]2024-12-22 01:54:50,788 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:50,914 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:50,914 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2124])
2024-12-22 01:54:50,985 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:54:51,090 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:51,091 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2170])
2024-12-22 01:54:51,093 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:5
 18%|█▊        | 7/40 [00:34<02:22,  4.33s/it]2024-12-22 01:54:51,148 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:51,149 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2061])
2024-12-22 01:54:51,162 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:54:51,222 - [Process 2/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:2')
2024-12-22 01:54:51,318 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 18%|█▊        | 7/40 [00:34<02:25,  4.40s/it]2024-12-22 01:54:51,378 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
3
 18%|█▊        | 7/40 [00:34<02:22,  4.32s/it]2024-12-22 01:54:51,386 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:51,409 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:51,409 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2188])
2024-12-22 01:54:51,479 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:54:51,586 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:3
 18%|█▊        | 7/40 [00:34<02:24,  4.38s/it]2024-12-22 01:54:51,622 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:51,650 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:51,856 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:54,438 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:54,438 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 01:54:54,513 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:54:54,656 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
3
 20%|██        | 8/40 [00:37<02:13,  4.18s/it]2024-12-22 01:54:54,815 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:54,967 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:54,967 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2053])
2024-12-22 01:54:55,040 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:54:55,233 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:

3
 20%|██        | 8/40 [00:38<02:16,  4.27s/it]2024-12-22 01:54:55,403 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:55,403 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 01:54:55,432 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:55,432 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2117])
2024-12-22 01:54:55,483 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:54:55,494 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:55,494 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 01:54:55,504 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:54:55,535 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:55,568 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:54:55,618 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:3
 20%|██        | 8/40 [00:38<02:19,  4.37s/it]2024-12-22 01:54:55,640 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
3
 20%|██        | 8/40 [00:38<02:17,  4.30s/it]2024-12-22 01:54:55,718 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 20%|██        | 8/40 [00:38<02:17,  4.30s/it]2024-12-22 01:54:55,870 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:55,935 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:55,936 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:58,518 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:58,518 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 01:54:58,592 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:54:58,736 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
4
 22%|██▎       | 9/40 [00:41<02:08,  4.15s/it]2024-12-22 01:54:58,904 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:59,118 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:59,118 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2054])
2024-12-22 01:54:59,191 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:54:59,384 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:

3
 22%|██▎       | 9/40 [00:42<02:11,  4.23s/it]2024-12-22 01:54:59,483 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:59,483 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 01:54:59,554 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:59,554 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2261])
2024-12-22 01:54:59,557 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:54:59,560 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:54:59,560 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 01:54:59,620 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:54:59,634 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:54:59,664 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:59,665 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 22%|██▎       | 9/40 [00:42<02:10,  4.22s/it]2024-12-22 01:54:59,729 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:3
 22%|██▎       | 9/40 [00:42<02:10,  4.21s/it]2024-12-22 01:54:59,789 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 22%|██▎       | 9/40 [00:42<02:13,  4.31s/it]2024-12-22 01:54:59,944 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:54:59,976 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:00,121 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:02,608 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:02,609 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 01:55:02,682 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:55:02,866 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:

3
 25%|██▌       | 10/40 [00:45<02:04,  4.14s/it]2024-12-22 01:55:03,034 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:03,264 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:03,264 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 01:55:03,337 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:55:03,488 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
5
 25%|██▌       | 10/40 [00:46<02:05,  4.19s/it]2024-12-22 01:55:03,567 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:03,568 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2093])
2024-12-22 01:55:03,641 - [Process 2/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:2')
2024-12-22 01:55:03,776 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:03,777 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 01:55:03,790 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
5
 25%|██▌       | 10/40 [00:46<02:05,  4.19s/it]2024-12-22 01:55:03,800 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:03,804 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:03,804 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 01:55:03,858 - [Process 1/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:1')
2024-12-22 01:55:03,878 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:55:03,966 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
 25%|██▌       | 10/40 [00:46<02:06,  4.22s/it]2024-12-22 01:55:04,033 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 25%|██▌       | 10/40 [00:46<02:08,  4.29s/it]2024-12-22 01:55:04,038 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:04,203 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:04,308 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:06,804 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:06,804 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1969])
2024-12-22 01:55:06,886 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:55:07,030 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:42
 28%|██▊       | 11/40 [00:50<02:00,  4.15s/it]2024-12-22 01:55:07,198 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:07,383 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:07,383 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:55:07,456 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:55:07,606 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
5
 28%|██▊       | 11/40 [00:50<02:00,  4.17s/it]2024-12-22 01:55:07,824 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:07,824 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:55:07,851 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:07,851 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:55:07,869 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:07,904 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:07,925 - [Process 1/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:1')
2024-12-22 01:55:07,955 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:07,955 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2081])
2024-12-22 01:55:08,029 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:55:08,056 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
4
 28%|██▊       | 11/40 [00:50<02:02,  4.21s/it]2024-12-22 01:55:08,076 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
5
 28%|██▊       | 11/40 [00:51<02:01,  4.19s/it]2024-12-22 01:55:08,140 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:3
 28%|██▊       | 11/40 [00:51<02:02,  4.23s/it]2024-12-22 01:55:08,277 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:08,337 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:08,476 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:10,973 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:10,974 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2188])
2024-12-22 01:55:11,043 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:55:11,186 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
4
 30%|███       | 12/40 [00:54<01:56,  4.15s/it]2024-12-22 01:55:11,318 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:11,624 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:11,624 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2131])
2024-12-22 01:55:11,695 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:55:11,803 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 30%|███       | 12/40 [00:54<01:56,  4.18s/it]2024-12-22 01:55:11,989 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:12,073 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:12,073 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2159])
2024-12-22 01:55:12,117 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:12,117 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2054])
2024-12-22 01:55:12,134 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:12,135 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2140])
2024-12-22 01:55:12,144 - [Process 1/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:1')
2024-12-22 01:55:12,191 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:55:12,206 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:55:12,295 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
5
 30%|███       | 12/40 [00:55<01:57,  4.20s/it]2024-12-22 01:55:12,320 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:3
 30%|███       | 12/40 [00:55<01:58,  4.23s/it]2024-12-22 01:55:12,349 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 30%|███       | 12/40 [00:55<01:58,  4.23s/it]2024-12-22 01:55:12,562 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:12,604 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:12,620 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:15,033 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:15,034 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 01:55:15,108 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:55:15,251 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
3
 32%|███▎      | 13/40 [00:58<01:51,  4.12s/it]2024-12-22 01:55:15,419 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:15,587 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:15,587 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 01:55:15,660 - [Process 0/5] - DEBUG - predict_token:tensor([[29955]], device='cuda:0')
2024-12-22 01:55:16,213 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:16,213 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:55:16,252 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:16,252 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2067])
2024-12-22 01:55:16,286 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:55:16,326 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:16,412 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:16,413 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 01:55:16,414 - [Process 0/5] - INFO - res.shape is :torch.Size([17])
results:
7

There are 7 unique paragraphs in the provided text.
 32%|███▎      | 13/40 [00:59<01:56,  4.31s/it]2024-12-22 01:55:16,434 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 32%|███▎      | 13/40 [00:59<01:53,  4.19s/it]2024-12-22 01:55:16,445 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 32%|███▎      | 13/40 [00:59<01:52,  4.18s/it]2024-12-22 01:55:16,494 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:55:16,649 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
4
 32%|███▎      | 13/40 [00:59<01:54,  4.25s/it]2024-12-22 01:55:16,727 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:16,751 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:16,783 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:16,990 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:19,202 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:19,202 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2194])
2024-12-22 01:55:19,272 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:55:19,375 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 35%|███▌      | 14/40 [01:02<01:47,  4.12s/it]2024-12-22 01:55:19,546 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:20,449 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:20,449 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2086])
2024-12-22 01:55:20,523 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:20,523 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2169])
2024-12-22 01:55:20,524 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:55:20,563 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:20,564 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 01:55:20,594 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:20,632 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:3
 35%|███▌      | 14/40 [01:03<01:51,  4.28s/it]2024-12-22 01:55:20,641 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:20,642 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2093])
2024-12-22 01:55:20,645 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:55:20,702 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 35%|███▌      | 14/40 [01:03<01:49,  4.22s/it]2024-12-22 01:55:20,715 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:55:20,757 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:3
 35%|███▌      | 14/40 [01:03<01:49,  4.22s/it]2024-12-22 01:55:20,870 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 35%|███▌      | 14/40 [01:03<01:50,  4.24s/it]2024-12-22 01:55:20,917 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:21,025 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:21,030 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:21,123 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:23,363 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:23,364 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:55:23,445 - [Process 3/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:3')
2024-12-22 01:55:23,589 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
3
 38%|███▊      | 15/40 [01:06<01:43,  4.15s/it]2024-12-22 01:55:23,789 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:24,538 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:24,538 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:55:24,612 - [Process 0/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:0')
2024-12-22 01:55:24,683 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:24,683 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:55:24,758 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:24,773 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:24,773 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2056])
2024-12-22 01:55:24,805 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:

5
 38%|███▊      | 15/40 [01:07<01:46,  4.25s/it]2024-12-22 01:55:24,833 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:24,833 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:55:24,847 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:55:24,866 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 38%|███▊      | 15/40 [01:07<01:45,  4.20s/it]2024-12-22 01:55:24,913 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:55:25,002 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 38%|███▊      | 15/40 [01:07<01:45,  4.21s/it]2024-12-22 01:55:25,069 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 38%|███▊      | 15/40 [01:07<01:46,  4.25s/it]2024-12-22 01:55:25,072 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:25,178 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:25,225 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:25,298 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:27,603 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:27,603 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2168])
2024-12-22 01:55:27,676 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:55:27,778 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 40%|████      | 16/40 [01:10<01:39,  4.16s/it]2024-12-22 01:55:27,947 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:28,830 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:28,830 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 01:55:28,836 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:28,836 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2514])
2024-12-22 01:55:28,893 - [Process 4/5] - DEBUG - predict_token:tensor([[29896]], device='cuda:4')
2024-12-22 01:55:28,908 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:55:28,945 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:28,945 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1847])
2024-12-22 01:55:28,976 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:28,977 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:55:29,029 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:55:29,057 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:29,060 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
4
 40%|████      | 16/40 [01:11<01:42,  4.25s/it]2024-12-22 01:55:29,088 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
results:
10
 40%|████      | 16/40 [01:12<01:40,  4.17s/it]2024-12-22 01:55:29,141 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:3
 40%|████      | 16/40 [01:12<01:40,  4.20s/it]2024-12-22 01:55:29,168 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:3
 40%|████      | 16/40 [01:12<01:41,  4.23s/it]2024-12-22 01:55:29,314 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:29,375 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:29,397 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:29,451 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:31,782 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:31,783 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2127])
2024-12-22 01:55:31,855 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:55:31,958 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:3
 42%|████▎     | 17/40 [01:14<01:35,  4.17s/it]2024-12-22 01:55:32,079 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:33,074 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:33,075 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 01:55:33,098 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:33,098 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2104])
2024-12-22 01:55:33,153 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:55:33,172 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:33,191 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:33,191 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 01:55:33,214 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:33,215 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 01:55:33,262 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:3
 42%|████▎     | 17/40 [01:16<01:37,  4.24s/it]2024-12-22 01:55:33,272 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:55:33,280 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 42%|████▎     | 17/40 [01:16<01:36,  4.20s/it]2024-12-22 01:55:33,286 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:55:33,397 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
 42%|████▎     | 17/40 [01:16<01:36,  4.21s/it]2024-12-22 01:55:33,428 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
4
 42%|████▎     | 17/40 [01:16<01:37,  4.22s/it]2024-12-22 01:55:33,519 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:33,547 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:33,619 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:33,732 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:35,908 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:35,908 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 01:55:35,990 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:55:36,094 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:3
 45%|████▌     | 18/40 [01:19<01:31,  4.16s/it]2024-12-22 01:55:36,219 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:37,102 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:37,103 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2246])
2024-12-22 01:55:37,168 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:55:37,197 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:37,197 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 01:55:37,271 - [Process 2/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:2')
2024-12-22 01:55:37,318 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
3
 45%|████▌     | 18/40 [01:20<01:32,  4.18s/it]2024-12-22 01:55:37,420 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
5
 45%|████▌     | 18/40 [01:20<01:31,  4.18s/it]2024-12-22 01:55:37,433 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:37,434 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 01:55:37,506 - [Process 4/5] - DEBUG - predict_token:tensor([[29955]], device='cuda:4')
2024-12-22 01:55:37,537 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:37,537 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-22 01:55:37,616 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:55:37,619 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:37,647 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:37,661 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
7
 45%|████▌     | 18/40 [01:20<01:33,  4.23s/it]2024-12-22 01:55:37,767 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 45%|████▌     | 18/40 [01:20<01:33,  4.26s/it]2024-12-22 01:55:37,970 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:38,099 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:39,892 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:39,892 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2102])
2024-12-22 01:55:39,967 - [Process 3/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:3')
2024-12-22 01:55:40,150 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:

5
 48%|████▊     | 19/40 [01:23<01:26,  4.13s/it]2024-12-22 01:55:40,333 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:41,290 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:41,290 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 01:55:41,364 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:55:41,450 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:41,451 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:55:41,514 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
3
 48%|████▊     | 19/40 [01:24<01:27,  4.19s/it]2024-12-22 01:55:41,523 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:41,630 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 48%|████▊     | 19/40 [01:24<01:27,  4.19s/it]2024-12-22 01:55:41,718 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:41,718 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1898])
2024-12-22 01:55:41,723 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:41,788 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:41,788 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2123])
2024-12-22 01:55:41,799 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:55:41,817 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:41,860 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:55:41,908 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
 48%|████▊     | 19/40 [01:24<01:28,  4.22s/it]2024-12-22 01:55:41,968 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:3
 48%|████▊     | 19/40 [01:24<01:29,  4.25s/it]2024-12-22 01:55:42,201 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:42,221 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:44,058 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:44,058 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 01:55:44,133 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:55:44,236 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 50%|█████     | 20/40 [01:27<01:22,  4.12s/it]2024-12-22 01:55:44,410 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:45,399 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:45,399 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 01:55:45,471 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:45,471 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2090])
2024-12-22 01:55:45,472 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:55:45,545 - [Process 2/5] - DEBUG - predict_token:tensor([[29947]], device='cuda:2')
2024-12-22 01:55:45,623 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
5
 50%|█████     | 20/40 [01:28<01:23,  4.16s/it]2024-12-22 01:55:45,695 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
7
 50%|█████     | 20/40 [01:28<01:23,  4.15s/it]2024-12-22 01:55:45,857 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:45,857 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2056])
2024-12-22 01:55:45,916 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:45,931 - [Process 4/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:4')
2024-12-22 01:55:45,990 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:46,034 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:46,034 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 01:55:46,084 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
4
 50%|█████     | 20/40 [01:29<01:24,  4.21s/it]2024-12-22 01:55:46,114 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:55:46,223 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
 50%|█████     | 20/40 [01:29<01:25,  4.25s/it]2024-12-22 01:55:46,364 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:46,504 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:48,133 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:48,133 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 01:55:48,211 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:55:48,314 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 52%|█████▎    | 21/40 [01:31<01:17,  4.10s/it]2024-12-22 01:55:48,498 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:49,695 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:49,695 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2118])
2024-12-22 01:55:49,765 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:49,766 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2181])
2024-12-22 01:55:49,767 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:55:49,835 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:49,874 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 52%|█████▎    | 21/40 [01:32<01:19,  4.19s/it]2024-12-22 01:55:49,942 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 52%|█████▎    | 21/40 [01:32<01:19,  4.18s/it]2024-12-22 01:55:50,167 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:50,173 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:50,177 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:50,177 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2147])
2024-12-22 01:55:50,220 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:50,220 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 01:55:50,250 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:55:50,297 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:55:50,358 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
 52%|█████▎    | 21/40 [01:33<01:20,  4.23s/it]2024-12-22 01:55:50,405 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
 52%|█████▎    | 21/40 [01:33<01:20,  4.23s/it]2024-12-22 01:55:50,639 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:50,677 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:52,331 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:52,331 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:55:52,404 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:55:52,547 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
4
 55%|█████▌    | 22/40 [01:35<01:14,  4.14s/it]2024-12-22 01:55:52,696 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:53,762 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:53,762 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1918])
2024-12-22 01:55:53,843 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:55:53,950 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:5
 55%|█████▌    | 22/40 [01:36<01:14,  4.13s/it]2024-12-22 01:55:53,953 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:53,954 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2117])
2024-12-22 01:55:54,025 - [Process 0/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:0')
2024-12-22 01:55:54,175 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
5
 55%|█████▌    | 22/40 [01:37<01:16,  4.22s/it]2024-12-22 01:55:54,249 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:54,269 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:54,270 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2302])
2024-12-22 01:55:54,335 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:55:54,443 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
 55%|█████▌    | 22/40 [01:37<01:15,  4.19s/it]2024-12-22 01:55:54,458 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:54,499 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:54,500 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:55:54,572 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:55:54,719 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:55,277 - [Process 1/5] - INFO - res.shape is :torch.Size([16])
results:4

There are 4 unique paragraphs in the provided text.
 55%|█████▌    | 22/40 [01:38<01:19,  4.42s/it]2024-12-22 01:55:55,565 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:56,533 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:56,533 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2116])
2024-12-22 01:55:56,606 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:55:56,750 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
5
 57%|█████▊    | 23/40 [01:39<01:10,  4.16s/it]2024-12-22 01:55:56,941 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:58,053 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:58,054 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2116])
2024-12-22 01:55:58,073 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:58,074 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 01:55:58,126 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:55:58,147 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:55:58,234 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:3
 57%|█████▊    | 23/40 [01:41<01:10,  4.18s/it]2024-12-22 01:55:58,297 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
4
 57%|█████▊    | 23/40 [01:41<01:11,  4.19s/it]2024-12-22 01:55:58,448 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:58,509 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:58,538 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:58,538 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 01:55:58,619 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:55:58,728 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
 57%|█████▊    | 23/40 [01:41<01:11,  4.22s/it]2024-12-22 01:55:58,999 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:55:59,286 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:55:59,286 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 01:55:59,362 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:55:59,471 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
 57%|█████▊    | 23/40 [01:42<01:14,  4.35s/it]2024-12-22 01:55:59,754 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:00,778 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:00,779 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 01:56:00,860 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:01,003 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
4
 60%|██████    | 24/40 [01:43<01:07,  4.19s/it]2024-12-22 01:56:01,179 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:02,100 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:02,100 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 01:56:02,124 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:02,125 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-22 01:56:02,174 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:56:02,198 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:56:02,306 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 60%|██████    | 24/40 [01:45<01:06,  4.14s/it]2024-12-22 01:56:02,324 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
4
 60%|██████    | 24/40 [01:45<01:06,  4.15s/it]2024-12-22 01:56:02,517 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:02,578 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:02,831 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:02,831 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2139])
2024-12-22 01:56:02,904 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:56:03,054 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 60%|██████    | 24/40 [01:45<01:07,  4.25s/it]2024-12-22 01:56:03,289 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:03,424 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:03,424 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:56:03,499 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:56:03,692 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
results:

3
 60%|██████    | 24/40 [01:46<01:09,  4.31s/it]2024-12-22 01:56:03,909 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:05,012 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:05,012 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2170])
2024-12-22 01:56:05,085 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:05,188 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 62%|██████▎   | 25/40 [01:48<01:02,  4.19s/it]2024-12-22 01:56:05,343 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:06,236 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:06,236 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 01:56:06,298 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:06,298 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 01:56:06,310 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:56:06,378 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:56:06,460 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
4
 62%|██████▎   | 25/40 [01:49<01:02,  4.15s/it]2024-12-22 01:56:06,529 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
3
 62%|██████▎   | 25/40 [01:49<01:02,  4.16s/it]2024-12-22 01:56:06,799 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:06,811 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:06,948 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:06,948 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 01:56:07,022 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:56:07,513 - [Process 4/5] - INFO - res.shape is :torch.Size([11])
results:

There are 3 unique paragraphs.
 62%|██████▎   | 25/40 [01:50<01:04,  4.31s/it]2024-12-22 01:56:07,723 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:07,724 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 01:56:07,749 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:07,805 - [Process 1/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:1')
2024-12-22 01:56:07,956 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
4
 62%|██████▎   | 25/40 [01:50<01:04,  4.30s/it]2024-12-22 01:56:08,267 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:09,170 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:09,171 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 01:56:09,251 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:09,355 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 65%|██████▌   | 26/40 [01:52<00:58,  4.18s/it]2024-12-22 01:56:09,552 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:10,550 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:10,550 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2191])
2024-12-22 01:56:10,592 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:10,593 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2176])
2024-12-22 01:56:10,619 - [Process 0/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:0')
2024-12-22 01:56:10,664 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:56:10,726 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:5
 65%|██████▌   | 26/40 [01:53<00:58,  4.17s/it]2024-12-22 01:56:10,772 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:3
 65%|██████▌   | 26/40 [01:53<00:58,  4.20s/it]2024-12-22 01:56:10,930 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:11,053 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:11,580 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:11,580 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 01:56:11,661 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:56:11,812 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 65%|██████▌   | 26/40 [01:54<01:00,  4.31s/it]2024-12-22 01:56:11,938 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:11,939 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 01:56:12,013 - [Process 1/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:1')
2024-12-22 01:56:12,060 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:12,164 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 65%|██████▌   | 26/40 [01:55<00:59,  4.27s/it]2024-12-22 01:56:12,416 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:13,235 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:13,235 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2081])
2024-12-22 01:56:13,310 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:56:13,453 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
3
 68%|██████▊   | 27/40 [01:56<00:54,  4.16s/it]2024-12-22 01:56:13,642 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:14,708 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:14,708 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2169])
2024-12-22 01:56:14,780 - [Process 0/5] - DEBUG - predict_token:tensor([[29955]], device='cuda:0')
2024-12-22 01:56:14,850 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:14,850 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2176])
2024-12-22 01:56:14,887 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:7
 68%|██████▊   | 27/40 [01:57<00:54,  4.17s/it]2024-12-22 01:56:14,922 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:56:15,030 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 68%|██████▊   | 27/40 [01:57<00:54,  4.21s/it]2024-12-22 01:56:15,225 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:15,354 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:15,894 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:15,895 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2116])
2024-12-22 01:56:15,967 - [Process 4/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:4')
2024-12-22 01:56:16,075 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:5
 68%|██████▊   | 27/40 [01:59<00:55,  4.29s/it]2024-12-22 01:56:16,232 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:16,233 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 01:56:16,314 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:56:16,365 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:16,423 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
 68%|██████▊   | 27/40 [01:59<00:55,  4.27s/it]2024-12-22 01:56:16,641 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:17,374 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:17,374 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2080])
2024-12-22 01:56:17,449 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:17,552 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 70%|███████   | 28/40 [02:00<00:49,  4.14s/it]2024-12-22 01:56:17,681 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:19,001 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:19,002 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2051])
2024-12-22 01:56:19,007 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:19,007 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 01:56:19,076 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:56:19,088 - [Process 0/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:0')
2024-12-22 01:56:19,238 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
5
 70%|███████   | 28/40 [02:02<00:50,  4.22s/it]2024-12-22 01:56:19,269 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:

3
 70%|███████   | 28/40 [02:02<00:50,  4.22s/it]2024-12-22 01:56:19,502 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:19,577 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:20,198 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:20,199 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2124])
2024-12-22 01:56:20,271 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:56:20,379 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
 70%|███████   | 28/40 [02:03<00:51,  4.30s/it]2024-12-22 01:56:20,479 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:20,479 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 01:56:20,552 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:56:20,622 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:20,660 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:4
 70%|███████   | 28/40 [02:03<00:51,  4.26s/it]2024-12-22 01:56:20,893 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:21,534 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:21,534 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2143])
2024-12-22 01:56:21,608 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:21,711 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 72%|███████▎  | 29/40 [02:04<00:45,  4.14s/it]2024-12-22 01:56:21,864 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:23,116 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:23,116 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 01:56:23,190 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:56:23,340 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
3
 72%|███████▎  | 29/40 [02:06<00:46,  4.19s/it]2024-12-22 01:56:23,381 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:23,381 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1988])
2024-12-22 01:56:23,461 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:56:23,612 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
4
 72%|███████▎  | 29/40 [02:06<00:46,  4.26s/it]2024-12-22 01:56:23,631 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:23,859 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:24,342 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:24,342 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 01:56:24,416 - [Process 4/5] - DEBUG - predict_token:tensor([[29955]], device='cuda:4')
2024-12-22 01:56:24,525 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
 72%|███████▎  | 29/40 [02:07<00:46,  4.25s/it]2024-12-22 01:56:24,701 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:24,701 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 01:56:24,781 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:56:24,848 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:24,933 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 72%|███████▎  | 29/40 [02:07<00:46,  4.26s/it]2024-12-22 01:56:25,185 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:25,707 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:25,708 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 01:56:25,790 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:56:25,894 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:3
 75%|███████▌  | 30/40 [02:08<00:41,  4.16s/it]2024-12-22 01:56:26,021 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:27,420 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:27,420 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2114])
2024-12-22 01:56:27,492 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:56:27,517 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:27,517 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 01:56:27,591 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:56:27,600 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 75%|███████▌  | 30/40 [02:10<00:42,  4.21s/it]2024-12-22 01:56:27,783 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:

5
 75%|███████▌  | 30/40 [02:10<00:42,  4.23s/it]2024-12-22 01:56:27,894 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:28,034 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:28,526 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:28,526 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 01:56:28,601 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:56:28,752 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
4
 75%|███████▌  | 30/40 [02:11<00:42,  4.24s/it]2024-12-22 01:56:29,001 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:29,001 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2163])
2024-12-22 01:56:29,027 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:29,074 - [Process 1/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:1')
2024-12-22 01:56:29,182 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:5
 75%|███████▌  | 30/40 [02:12<00:42,  4.26s/it]2024-12-22 01:56:29,434 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:29,852 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:29,852 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 01:56:29,933 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:30,076 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
3
 78%|███████▊  | 31/40 [02:13<00:37,  4.16s/it]2024-12-22 01:56:30,268 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:31,665 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:31,666 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:56:31,737 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:56:31,844 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:31,845 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 01:56:31,844 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 78%|███████▊  | 31/40 [02:14<00:37,  4.22s/it]2024-12-22 01:56:31,917 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:56:32,067 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
3
 78%|███████▊  | 31/40 [02:14<00:38,  4.25s/it]2024-12-22 01:56:32,091 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:32,367 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:32,700 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:32,701 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2059])
2024-12-22 01:56:32,775 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:56:32,926 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 78%|███████▊  | 31/40 [02:15<00:38,  4.22s/it]2024-12-22 01:56:33,105 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:33,106 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 01:56:33,149 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:33,180 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:56:33,331 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 78%|███████▊  | 31/40 [02:16<00:38,  4.23s/it]2024-12-22 01:56:33,588 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:34,111 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:34,112 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 01:56:34,194 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:34,297 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:3
 80%|████████  | 32/40 [02:17<00:33,  4.18s/it]2024-12-22 01:56:34,419 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:35,764 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:35,765 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-22 01:56:35,840 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:56:35,990 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
4
 80%|████████  | 32/40 [02:18<00:33,  4.20s/it]2024-12-22 01:56:36,015 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:36,015 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2100])
2024-12-22 01:56:36,089 - [Process 2/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:2')
2024-12-22 01:56:36,196 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:5
 80%|████████  | 32/40 [02:19<00:33,  4.21s/it]2024-12-22 01:56:36,239 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:36,487 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:36,825 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:36,825 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2100])
2024-12-22 01:56:36,899 - [Process 4/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:4')
2024-12-22 01:56:37,007 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:5
 80%|████████  | 32/40 [02:19<00:33,  4.18s/it]2024-12-22 01:56:37,251 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:37,253 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:37,253 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2093])
2024-12-22 01:56:37,327 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:56:37,478 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
4
 80%|████████  | 32/40 [02:20<00:33,  4.20s/it]2024-12-22 01:56:37,734 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:38,263 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:38,264 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2171])
2024-12-22 01:56:38,336 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:38,439 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 82%|████████▎ | 33/40 [02:21<00:29,  4.17s/it]2024-12-22 01:56:38,591 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:40,025 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:40,026 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2136])
2024-12-22 01:56:40,097 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:56:40,204 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 82%|████████▎ | 33/40 [02:23<00:29,  4.20s/it]2024-12-22 01:56:40,287 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:40,287 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2160])
2024-12-22 01:56:40,360 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:56:40,467 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:4
 82%|████████▎ | 33/40 [02:23<00:29,  4.23s/it]2024-12-22 01:56:40,478 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:40,722 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:40,922 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:40,923 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2072])
2024-12-22 01:56:40,997 - [Process 4/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:4')
2024-12-22 01:56:41,148 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
4
 82%|████████▎ | 33/40 [02:24<00:29,  4.17s/it]2024-12-22 01:56:41,452 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:41,453 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2086])
2024-12-22 01:56:41,459 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:41,529 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:56:41,637 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:3
 82%|████████▎ | 33/40 [02:24<00:29,  4.19s/it]2024-12-22 01:56:41,869 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:42,270 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:42,271 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 01:56:42,345 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:42,489 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
4
 85%|████████▌ | 34/40 [02:25<00:24,  4.13s/it]2024-12-22 01:56:42,682 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:44,092 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:44,093 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:56:44,166 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:56:44,274 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 85%|████████▌ | 34/40 [02:27<00:24,  4.16s/it]2024-12-22 01:56:44,521 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:44,521 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 01:56:44,593 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:56:44,604 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:44,700 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:3
 85%|████████▌ | 34/40 [02:27<00:25,  4.23s/it]2024-12-22 01:56:44,907 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:45,194 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:45,194 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 01:56:45,269 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:56:45,377 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:5
 85%|████████▌ | 34/40 [02:28<00:25,  4.19s/it]2024-12-22 01:56:45,590 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:45,591 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 01:56:45,636 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:45,665 - [Process 1/5] - DEBUG - predict_token:tensor([[29955]], device='cuda:1')
2024-12-22 01:56:45,815 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
7
 85%|████████▌ | 34/40 [02:28<00:25,  4.19s/it]2024-12-22 01:56:46,039 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:46,513 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:46,514 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2168])
2024-12-22 01:56:46,587 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:56:46,689 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 88%|████████▊ | 35/40 [02:29<00:20,  4.15s/it]2024-12-22 01:56:46,857 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:48,232 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:48,232 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 01:56:48,306 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:56:48,457 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
4
 88%|████████▊ | 35/40 [02:31<00:20,  4.17s/it]2024-12-22 01:56:48,546 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:48,547 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 01:56:48,621 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:56:48,710 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:48,813 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
results:

4
 88%|████████▊ | 35/40 [02:31<00:20,  4.20s/it]2024-12-22 01:56:49,083 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:49,543 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:49,543 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1777])
2024-12-22 01:56:49,635 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:56:49,786 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
5
 88%|████████▊ | 35/40 [02:32<00:21,  4.25s/it]2024-12-22 01:56:49,866 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:49,866 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 01:56:49,947 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:56:50,095 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:50,098 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
4
 88%|████████▊ | 35/40 [02:33<00:21,  4.21s/it]2024-12-22 01:56:50,396 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:50,706 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:50,707 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2117])
2024-12-22 01:56:50,780 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:56:50,882 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
 90%|█████████ | 36/40 [02:33<00:16,  4.17s/it]2024-12-22 01:56:51,030 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:52,451 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:52,451 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2234])
2024-12-22 01:56:52,520 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:56:52,628 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 90%|█████████ | 36/40 [02:35<00:16,  4.17s/it]2024-12-22 01:56:52,792 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:52,793 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:56:52,869 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:56:52,915 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:53,019 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
4
 90%|█████████ | 36/40 [02:35<00:16,  4.20s/it]2024-12-22 01:56:53,210 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:53,880 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:53,881 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1926])
2024-12-22 01:56:53,963 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:56:54,065 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:54,065 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2056])
2024-12-22 01:56:54,114 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:
3
 90%|█████████ | 36/40 [02:37<00:17,  4.28s/it]2024-12-22 01:56:54,141 - [Process 1/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:1')
2024-12-22 01:56:54,291 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
4
 90%|█████████ | 36/40 [02:37<00:16,  4.21s/it]2024-12-22 01:56:54,416 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:54,633 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:54,829 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:54,830 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1980])
2024-12-22 01:56:54,913 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:56:55,057 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
3
 92%|█████████▎| 37/40 [02:38<00:12,  4.17s/it]2024-12-22 01:56:55,187 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:56,703 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:56,703 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2132])
2024-12-22 01:56:56,775 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:56:56,922 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:56,922 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 01:56:56,924 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
3
 92%|█████████▎| 37/40 [02:39<00:12,  4.21s/it]2024-12-22 01:56:56,997 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 01:56:57,266 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:57,740 - [Process 2/5] - INFO - res.shape is :torch.Size([17])
results:
4

There are 4 unique paragraphs in the provided text.
 92%|█████████▎| 37/40 [02:40<00:13,  4.36s/it]2024-12-22 01:56:57,970 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:58,199 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:58,200 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1980])
2024-12-22 01:56:58,282 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:56:58,314 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:58,315 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 01:56:58,390 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:56:58,432 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:41
 92%|█████████▎| 37/40 [02:41<00:12,  4.29s/it]2024-12-22 01:56:58,498 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:5
 92%|█████████▎| 37/40 [02:41<00:12,  4.21s/it]2024-12-22 01:56:58,649 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:58,703 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:56:59,042 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:56:59,043 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2118])
2024-12-22 01:56:59,116 - [Process 3/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:3')
2024-12-22 01:56:59,259 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
7
 95%|█████████▌| 38/40 [02:42<00:08,  4.18s/it]2024-12-22 01:56:59,425 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:00,945 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:00,945 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 01:57:01,021 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:57:01,171 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
4
 95%|█████████▌| 38/40 [02:44<00:08,  4.22s/it]2024-12-22 01:57:01,445 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:01,788 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:01,789 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2124])
2024-12-22 01:57:01,861 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:57:02,011 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
4
 95%|█████████▌| 38/40 [02:44<00:08,  4.33s/it]2024-12-22 01:57:02,295 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:02,402 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:02,403 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1300])
2024-12-22 01:57:02,507 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:57:02,556 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:02,556 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 01:57:02,637 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 01:57:02,659 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
results:23
 95%|█████████▌| 38/40 [02:45<00:08,  4.27s/it]2024-12-22 01:57:02,789 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
3
 95%|█████████▌| 38/40 [02:45<00:08,  4.23s/it]2024-12-22 01:57:02,987 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:02,994 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:03,280 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:03,280 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 01:57:03,354 - [Process 3/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:3')
2024-12-22 01:57:03,497 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
results:
5
 98%|█████████▊| 39/40 [02:46<00:04,  4.20s/it]2024-12-22 01:57:03,665 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:05,216 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:05,217 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 01:57:05,295 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:57:05,403 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:4
 98%|█████████▊| 39/40 [02:48<00:04,  4.22s/it]2024-12-22 01:57:05,667 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:06,066 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:06,066 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1941])
2024-12-22 01:57:06,148 - [Process 2/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:2')
2024-12-22 01:57:06,256 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:3
 98%|█████████▊| 39/40 [02:49<00:04,  4.30s/it]2024-12-22 01:57:06,537 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:06,670 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:06,670 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2061])
2024-12-22 01:57:06,745 - [Process 1/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:1')
2024-12-22 01:57:06,820 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:06,820 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1988])
2024-12-22 01:57:06,853 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:5
 98%|█████████▊| 39/40 [02:49<00:04,  4.18s/it]2024-12-22 01:57:06,901 - [Process 4/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:4')
2024-12-22 01:57:07,009 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:4
 98%|█████████▊| 39/40 [02:49<00:04,  4.29s/it]2024-12-22 01:57:07,127 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:07,229 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:57:07,522 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:07,522 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 01:57:07,595 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 01:57:07,698 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:4
100%|██████████| 40/40 [02:50<00:00,  4.20s/it]100%|██████████| 40/40 [02:50<00:00,  4.27s/it]
2024-12-22 01:57:09,297 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:09,297 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 01:57:09,370 - [Process 0/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:0')
2024-12-22 01:57:09,520 - [Process 0/5] - INFO - res.shape is :torch.Size([3])
results:
4
100%|██████████| 40/40 [02:52<00:00,  4.19s/it]100%|██████████| 40/40 [02:52<00:00,  4.31s/it]
2024-12-22 01:57:10,351 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:10,351 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 01:57:10,432 - [Process 2/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:2')
2024-12-22 01:57:10,582 - [Process 2/5] - INFO - res.shape is :torch.Size([3])
results:
4
100%|██████████| 40/40 [02:53<00:00,  4.31s/it]100%|██████████| 40/40 [02:53<00:00,  4.34s/it]
2024-12-22 01:57:10,911 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:10,911 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 01:57:10,981 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:57:10,981 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2143])
2024-12-22 01:57:10,986 - [Process 4/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:4')
2024-12-22 01:57:11,054 - [Process 1/5] - DEBUG - predict_token:tensor([[29953]], device='cuda:1')
2024-12-22 01:57:11,094 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results:3
100%|██████████| 40/40 [02:54<00:00,  4.23s/it]100%|██████████| 40/40 [02:54<00:00,  4.35s/it]
2024-12-22 01:57:11,162 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:5
100%|██████████| 40/40 [02:54<00:00,  4.22s/it]100%|██████████| 40/40 [02:54<00:00,  4.35s/it]
2024-12-22 01:57:11,205 - [Process 3/5] - DEBUG - datasets_name:passage_count
2024-12-22 01:57:11,205 - [Process 4/5] - DEBUG - datasets_name:passage_count
2024-12-22 01:57:11,205 - [Process 2/5] - DEBUG - datasets_name:passage_count
2024-12-22 01:57:11,205 - [Process 0/5] - DEBUG - datasets_name:passage_count
2024-12-22 01:57:11,205 - [Process 1/5] - DEBUG - datasets_name:passage_count
Running evaluation for dataset: passage_retrieval_en
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.65s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.63s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.73s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:59:08,242 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 01:59:08,242 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 01:59:08,242 - [Process 1/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:59:08,250 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 01:59:08,251 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 01:59:08,251 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:59:08,260 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 01:59:08,260 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 01:59:08,260 - [Process 3/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:59:08,261 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 01:59:08,262 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 01:59:08,262 - [Process 2/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 01:59:08,265 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 01:59:08,265 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 01:59:08,265 - [Process 0/5] - INFO - output_max_len: 32
2024-12-22 01:59:08,286 - [Process 1/5] - INFO - Max Length is 11516
2024-12-22 01:59:08,287 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 01:59:08,287 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:59:08,326 - [Process 4/5] - INFO - Max Length is 11516
2024-12-22 01:59:08,326 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 01:59:08,327 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:59:08,334 - [Process 3/5] - INFO - Max Length is 11516
2024-12-22 01:59:08,334 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 01:59:08,334 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:59:08,336 - [Process 2/5] - INFO - Max Length is 11516
2024-12-22 01:59:08,336 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 01:59:08,337 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:59:08,340 - [Process 0/5] - INFO - Max Length is 11516
2024-12-22 01:59:08,340 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 01:59:08,341 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 01:59:13,029 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:13,112 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:13,115 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:13,116 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:13,116 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:17,500 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:17,500 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2084])
2024-12-22 01:59:17,580 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:59:17,757 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:17,758 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2319])
2024-12-22 01:59:17,815 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:17,816 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2266])
2024-12-22 01:59:17,839 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:59:17,848 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
  2%|▎         | 1/40 [00:09<06:10,  9.51s/it]2024-12-22 01:59:17,893 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:59:17,937 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:17,938 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2191])
2024-12-22 01:59:18,001 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:18,019 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:59:18,114 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:18,115 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2207])
2024-12-22 01:59:18,143 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
  2%|▎         | 1/40 [00:09<06:24,  9.86s/it]2024-12-22 01:59:18,180 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
  2%|▎         | 1/40 [00:09<06:24,  9.85s/it]2024-12-22 01:59:18,202 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 01:59:18,324 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
  2%|▎         | 1/40 [00:09<06:29,  9.99s/it]2024-12-22 01:59:18,421 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:18,455 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:18,456 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 3
  2%|▎         | 1/40 [00:10<06:34, 10.12s/it]2024-12-22 01:59:18,602 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:18,726 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:21,745 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:21,745 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2300])
2024-12-22 01:59:21,818 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 01:59:22,086 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
  5%|▌         | 2/40 [00:13<04:03,  6.41s/it]2024-12-22 01:59:22,232 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:22,233 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 01:59:22,242 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:22,315 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:59:22,597 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
  5%|▌         | 2/40 [00:14<04:12,  6.66s/it]2024-12-22 01:59:22,683 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:22,683 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2429])
2024-12-22 01:59:22,764 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 01:59:22,790 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:22,791 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2157])
2024-12-22 01:59:22,837 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:22,879 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 01:59:23,003 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:23,003 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2384])
2024-12-22 01:59:23,064 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
  5%|▌         | 2/40 [00:14<04:24,  6.95s/it]2024-12-22 01:59:23,085 - [Process 2/5] - DEBUG - predict_token:tensor([[29896]], device='cuda:2')
2024-12-22 01:59:23,136 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 3
  5%|▌         | 2/40 [00:14<04:23,  6.94s/it]2024-12-22 01:59:23,336 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:23,372 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
  5%|▌         | 2/40 [00:15<04:28,  7.06s/it]2024-12-22 01:59:23,407 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:23,618 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:26,177 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:26,177 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2250])
2024-12-22 01:59:26,257 - [Process 0/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:0')
2024-12-22 01:59:26,527 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
  8%|▊         | 3/40 [00:18<03:23,  5.51s/it]2024-12-22 01:59:26,679 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:26,849 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:26,849 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2193])
2024-12-22 01:59:26,930 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:59:27,213 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
  8%|▊         | 3/40 [00:18<03:31,  5.72s/it]2024-12-22 01:59:27,392 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:27,392 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2214])
2024-12-22 01:59:27,397 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:27,397 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2282])
2024-12-22 01:59:27,460 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:27,474 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 01:59:27,478 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 01:59:27,604 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:27,604 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2262])
2024-12-22 01:59:27,685 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 01:59:27,778 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
  8%|▊         | 3/40 [00:19<03:39,  5.93s/it]2024-12-22 01:59:27,783 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
  8%|▊         | 3/40 [00:19<03:38,  5.90s/it]2024-12-22 01:59:27,969 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
  8%|▊         | 3/40 [00:19<03:39,  5.93s/it]2024-12-22 01:59:28,064 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:28,069 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:28,211 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:30,655 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:30,655 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2198])
2024-12-22 01:59:30,735 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 01:59:31,044 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:
Paragraph 28
 10%|█         | 4/40 [00:22<03:04,  5.12s/it]2024-12-22 01:59:31,193 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:31,474 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:31,475 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2194])
2024-12-22 01:59:31,555 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 01:59:31,838 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 10%|█         | 4/40 [00:23<03:10,  5.29s/it]2024-12-22 01:59:32,079 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:32,091 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:32,091 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2335])
2024-12-22 01:59:32,172 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 01:59:32,246 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:32,247 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2216])
2024-12-22 01:59:32,329 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 01:59:32,460 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 10%|█         | 4/40 [00:24<03:14,  5.41s/it]2024-12-22 01:59:32,463 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:32,463 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2461])
2024-12-22 01:59:32,546 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 01:59:32,616 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 10%|█         | 4/40 [00:24<03:15,  5.43s/it]2024-12-22 01:59:32,709 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:32,843 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 10%|█         | 4/40 [00:24<03:21,  5.59s/it]2024-12-22 01:59:32,874 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:33,084 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:35,211 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:35,211 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2394])
2024-12-22 01:59:35,282 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:59:35,550 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 12%|█▎        | 5/40 [00:27<02:51,  4.90s/it]2024-12-22 01:59:35,699 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:36,077 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:36,077 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2221])
2024-12-22 01:59:36,157 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 01:59:36,443 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 12%|█▎        | 5/40 [00:28<02:56,  5.04s/it]2024-12-22 01:59:36,693 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:36,772 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:36,773 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2346])
2024-12-22 01:59:36,853 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 01:59:36,885 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:36,885 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2164])
2024-12-22 01:59:36,968 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 01:59:37,066 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:37,066 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2241])
2024-12-22 01:59:37,139 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 12%|█▎        | 5/40 [00:28<03:00,  5.15s/it]2024-12-22 01:59:37,147 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 01:59:37,253 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 12%|█▎        | 5/40 [00:28<02:59,  5.14s/it]2024-12-22 01:59:37,388 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:37,441 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 12%|█▎        | 5/40 [00:29<03:03,  5.23s/it]2024-12-22 01:59:37,495 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:37,707 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:39,658 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:39,658 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2277])
2024-12-22 01:59:39,739 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:59:40,009 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 15%|█▌        | 6/40 [00:31<02:41,  4.75s/it]2024-12-22 01:59:40,158 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:40,749 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:40,749 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2195])
2024-12-22 01:59:40,832 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 01:59:41,075 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 6
 15%|█▌        | 6/40 [00:32<02:46,  4.90s/it]2024-12-22 01:59:41,323 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:41,477 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:41,478 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2216])
2024-12-22 01:59:41,482 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:41,482 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2140])
2024-12-22 01:59:41,560 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 01:59:41,561 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 01:59:41,723 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:41,723 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2189])
2024-12-22 01:59:41,803 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 01:59:41,843 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 24
 15%|█▌        | 6/40 [00:33<02:48,  4.95s/it]2024-12-22 01:59:41,846 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 15%|█▌        | 6/40 [00:33<02:49,  5.00s/it]2024-12-22 01:59:42,079 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:42,098 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 15%|█▌        | 6/40 [00:33<02:51,  5.04s/it]2024-12-22 01:59:42,117 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:42,363 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:44,101 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:44,101 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2244])
2024-12-22 01:59:44,181 - [Process 0/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:0')
2024-12-22 01:59:44,451 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 18%|█▊        | 7/40 [00:36<02:33,  4.65s/it]2024-12-22 01:59:44,603 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:45,319 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:45,320 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2189])
2024-12-22 01:59:45,400 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 01:59:45,684 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 18%|█▊        | 7/40 [00:37<02:38,  4.81s/it]2024-12-22 01:59:45,924 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:46,124 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:46,124 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2227])
2024-12-22 01:59:46,145 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:46,146 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 01:59:46,205 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 01:59:46,227 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 01:59:46,489 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 18%|█▊        | 7/40 [00:38<02:40,  4.85s/it]2024-12-22 01:59:46,512 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 18%|█▊        | 7/40 [00:38<02:41,  4.89s/it]2024-12-22 01:59:46,734 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:46,744 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:46,850 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:46,850 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2384])
2024-12-22 01:59:46,939 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 01:59:47,231 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 18%|█▊        | 7/40 [00:38<02:47,  5.07s/it]2024-12-22 01:59:47,474 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:48,566 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:48,566 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2290])
2024-12-22 01:59:48,646 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 01:59:48,916 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 20%|██        | 8/40 [00:40<02:26,  4.59s/it]2024-12-22 01:59:49,058 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:49,923 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:49,924 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2136])
2024-12-22 01:59:50,003 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 01:59:50,285 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 20%|██        | 8/40 [00:41<02:31,  4.74s/it]2024-12-22 01:59:50,530 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:50,694 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:50,695 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2277])
2024-12-22 01:59:50,740 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:50,740 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2242])
2024-12-22 01:59:50,775 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 01:59:50,822 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 01:59:51,061 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 20%|██        | 8/40 [00:42<02:32,  4.76s/it]2024-12-22 01:59:51,108 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 20%|██        | 8/40 [00:42<02:33,  4.80s/it]2024-12-22 01:59:51,315 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:51,369 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:51,819 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:51,820 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2377])
2024-12-22 01:59:51,902 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 01:59:52,189 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 20%|██        | 8/40 [00:43<02:41,  5.03s/it]2024-12-22 01:59:52,447 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:53,027 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:53,028 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2168])
2024-12-22 01:59:53,108 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:59:53,375 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 22%|██▎       | 9/40 [00:45<02:21,  4.55s/it]2024-12-22 01:59:53,533 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:54,851 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:54,852 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2389])
2024-12-22 01:59:54,935 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 01:59:55,224 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 22%|██▎       | 9/40 [00:46<02:28,  4.80s/it]2024-12-22 01:59:55,426 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:55,426 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2178])
2024-12-22 01:59:55,461 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:55,505 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 01:59:55,617 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:55,617 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2272])
2024-12-22 01:59:55,710 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 01:59:55,787 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 22%|██▎       | 9/40 [00:47<02:27,  4.76s/it]2024-12-22 01:59:56,000 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 22%|██▎       | 9/40 [00:47<02:29,  4.82s/it]2024-12-22 01:59:56,029 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:56,234 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:56,303 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:56,304 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2344])
2024-12-22 01:59:56,377 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 01:59:56,663 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 22%|██▎       | 9/40 [00:48<02:30,  4.86s/it]2024-12-22 01:59:56,916 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:57,319 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:57,319 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2059])
2024-12-22 01:59:57,400 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 01:59:57,667 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 25%|██▌       | 10/40 [00:49<02:14,  4.47s/it]2024-12-22 01:59:57,819 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 01:59:59,455 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 01:59:59,456 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2173])
2024-12-22 01:59:59,537 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 01:59:59,821 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 25%|██▌       | 10/40 [00:51<02:22,  4.74s/it]2024-12-22 02:00:00,061 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:00,092 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:00,092 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2284])
2024-12-22 02:00:00,175 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:00:00,279 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:00,279 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2239])
2024-12-22 02:00:00,359 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:00:00,461 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 25%|██▌       | 10/40 [00:52<02:21,  4.73s/it]2024-12-22 02:00:00,643 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 25%|██▌       | 10/40 [00:52<02:22,  4.76s/it]2024-12-22 02:00:00,700 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:00,894 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:00,982 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:00,982 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 02:00:01,066 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:01,351 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 25%|██▌       | 10/40 [00:53<02:24,  4.81s/it]2024-12-22 02:00:01,596 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:01,742 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:01,742 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2252])
2024-12-22 02:00:01,822 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:00:02,093 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 28%|██▊       | 11/40 [00:53<02:09,  4.46s/it]2024-12-22 02:00:02,249 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:04,646 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:04,647 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2397])
2024-12-22 02:00:04,737 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:04,764 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:04,764 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2207])
2024-12-22 02:00:04,845 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:00:04,922 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:04,922 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2298])
2024-12-22 02:00:05,005 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:00:05,030 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 28%|██▊       | 11/40 [00:56<02:21,  4.88s/it]2024-12-22 02:00:05,130 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 28%|██▊       | 11/40 [00:56<02:16,  4.71s/it]2024-12-22 02:00:05,292 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:05,345 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:
Paragraph 28
 28%|██▊       | 11/40 [00:57<02:17,  4.75s/it]2024-12-22 02:00:05,378 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:05,576 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:05,673 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:05,673 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2327])
2024-12-22 02:00:05,754 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:06,039 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 28%|██▊       | 11/40 [00:57<02:18,  4.77s/it]2024-12-22 02:00:06,280 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:06,508 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:06,508 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2379])
2024-12-22 02:00:06,587 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:00:06,859 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 30%|███       | 12/40 [00:58<02:07,  4.55s/it]2024-12-22 02:00:07,011 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:09,114 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:09,114 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2279])
2024-12-22 02:00:09,188 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:09,471 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 30%|███       | 12/40 [01:01<02:12,  4.75s/it]2024-12-22 02:00:09,485 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:09,486 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2226])
2024-12-22 02:00:09,567 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:00:09,601 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:09,601 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2116])
2024-12-22 02:00:09,682 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:00:09,728 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:09,851 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 30%|███       | 12/40 [01:01<02:12,  4.72s/it]2024-12-22 02:00:09,965 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 30%|███       | 12/40 [01:01<02:11,  4.71s/it]2024-12-22 02:00:10,098 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:10,193 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:10,288 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:10,288 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2156])
2024-12-22 02:00:10,368 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 02:00:10,651 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 30%|███       | 12/40 [01:02<02:12,  4.72s/it]2024-12-22 02:00:10,906 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:11,038 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:11,039 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2190])
2024-12-22 02:00:11,121 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:00:11,393 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 32%|███▎      | 13/40 [01:03<02:02,  4.55s/it]2024-12-22 02:00:11,550 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:13,804 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:13,804 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2233])
2024-12-22 02:00:13,886 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:14,170 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 32%|███▎      | 13/40 [01:05<02:07,  4.73s/it]2024-12-22 02:00:14,211 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:14,211 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2285])
2024-12-22 02:00:14,219 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:14,219 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2335])
2024-12-22 02:00:14,292 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:00:14,303 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 02:00:14,421 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:14,581 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 32%|███▎      | 13/40 [01:06<02:06,  4.68s/it]2024-12-22 02:00:14,591 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 32%|███▎      | 13/40 [01:06<02:07,  4.72s/it]2024-12-22 02:00:14,826 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:14,836 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:15,216 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:15,216 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2300])
2024-12-22 02:00:15,308 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:15,597 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 32%|███▎      | 13/40 [01:07<02:09,  4.79s/it]2024-12-22 02:00:15,766 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:15,766 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2197])
2024-12-22 02:00:15,837 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:15,855 - [Process 0/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:0')
2024-12-22 02:00:16,086 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 5
 35%|███▌      | 14/40 [01:07<01:59,  4.59s/it]2024-12-22 02:00:16,253 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:18,479 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:18,479 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2184])
2024-12-22 02:00:18,560 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:18,683 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:18,683 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 02:00:18,765 - [Process 3/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:3')
2024-12-22 02:00:18,817 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:18,817 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2162])
2024-12-22 02:00:18,845 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 26
 35%|███▌      | 14/40 [01:10<02:02,  4.72s/it]2024-12-22 02:00:18,898 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:00:19,047 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 35%|███▌      | 14/40 [01:10<02:00,  4.64s/it]2024-12-22 02:00:19,094 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:19,189 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 35%|███▌      | 14/40 [01:10<02:01,  4.66s/it]2024-12-22 02:00:19,293 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:19,435 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:20,148 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:20,149 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2313])
2024-12-22 02:00:20,239 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:20,280 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:20,280 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2193])
2024-12-22 02:00:20,363 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:00:20,529 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 35%|███▌      | 14/40 [01:12<02:05,  4.83s/it]2024-12-22 02:00:20,635 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 38%|███▊      | 15/40 [01:12<01:54,  4.58s/it]2024-12-22 02:00:20,780 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:20,788 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:23,127 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:23,127 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2270])
2024-12-22 02:00:23,208 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 02:00:23,466 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:23,466 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2206])
2024-12-22 02:00:23,494 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 38%|███▊      | 15/40 [01:15<01:57,  4.70s/it]2024-12-22 02:00:23,547 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:00:23,645 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:23,646 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2173])
2024-12-22 02:00:23,738 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:00:23,771 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:23,840 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 38%|███▊      | 15/40 [01:15<01:56,  4.66s/it]2024-12-22 02:00:24,024 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 38%|███▊      | 15/40 [01:15<01:58,  4.74s/it]2024-12-22 02:00:24,098 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:24,275 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:24,782 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:24,782 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2320])
2024-12-22 02:00:24,853 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:24,853 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2312])
2024-12-22 02:00:24,863 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:00:24,933 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:25,136 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 40%|████      | 16/40 [01:16<01:49,  4.55s/it]2024-12-22 02:00:25,218 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 38%|███▊      | 15/40 [01:16<01:59,  4.79s/it]2024-12-22 02:00:25,285 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:25,475 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:27,874 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:27,875 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2357])
2024-12-22 02:00:27,956 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 02:00:28,130 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:28,130 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2280])
2024-12-22 02:00:28,212 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:00:28,245 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 40%|████      | 16/40 [01:19<01:53,  4.71s/it]2024-12-22 02:00:28,342 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:28,342 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2703])
2024-12-22 02:00:28,406 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:00:28,506 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 40%|████      | 16/40 [01:20<01:51,  4.66s/it]2024-12-22 02:00:28,530 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:28,690 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 40%|████      | 16/40 [01:20<01:53,  4.72s/it]2024-12-22 02:00:28,755 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:28,947 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:29,331 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:29,331 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2342])
2024-12-22 02:00:29,414 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:00:29,688 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 42%|████▎     | 17/40 [01:21<01:44,  4.55s/it]2024-12-22 02:00:29,835 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:29,900 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:29,901 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2443])
2024-12-22 02:00:29,982 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:30,272 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 40%|████      | 16/40 [01:21<01:56,  4.87s/it]2024-12-22 02:00:30,517 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:32,592 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:32,592 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2310])
2024-12-22 02:00:32,645 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:32,645 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2704])
2024-12-22 02:00:32,673 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:32,704 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:00:32,797 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:32,797 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2081])
2024-12-22 02:00:32,879 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:00:32,974 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 42%|████▎     | 17/40 [01:24<01:48,  4.72s/it]2024-12-22 02:00:33,005 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 42%|████▎     | 17/40 [01:24<01:46,  4.61s/it]2024-12-22 02:00:33,161 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 42%|████▎     | 17/40 [01:24<01:46,  4.65s/it]2024-12-22 02:00:33,229 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:33,294 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:33,406 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:33,795 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:33,796 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2287])
2024-12-22 02:00:33,877 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:00:34,149 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 45%|████▌     | 18/40 [01:25<01:39,  4.53s/it]2024-12-22 02:00:34,299 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:35,145 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:35,145 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2430])
2024-12-22 02:00:35,240 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:35,534 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 42%|████▎     | 17/40 [01:27<01:54,  4.99s/it]2024-12-22 02:00:35,772 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:37,294 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:37,294 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2140])
2024-12-22 02:00:37,331 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:37,331 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2268])
2024-12-22 02:00:37,376 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:37,415 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:00:37,549 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:37,549 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2327])
2024-12-22 02:00:37,633 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:00:37,677 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 45%|████▌     | 18/40 [01:29<01:43,  4.71s/it]2024-12-22 02:00:37,718 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 26
 45%|████▌     | 18/40 [01:29<01:42,  4.64s/it]2024-12-22 02:00:37,922 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 45%|████▌     | 18/40 [01:29<01:42,  4.68s/it]2024-12-22 02:00:37,953 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:37,988 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:38,168 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:38,263 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:38,264 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 02:00:38,344 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:00:38,615 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 48%|████▊     | 19/40 [01:30<01:34,  4.51s/it]2024-12-22 02:00:38,780 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:39,835 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:39,836 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2280])
2024-12-22 02:00:39,918 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:40,204 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 45%|████▌     | 18/40 [01:31<01:47,  4.89s/it]2024-12-22 02:00:40,463 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:41,854 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:41,854 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2199])
2024-12-22 02:00:41,928 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:00:42,041 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:42,042 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 02:00:42,125 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:00:42,209 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 48%|████▊     | 19/40 [01:33<01:36,  4.60s/it]2024-12-22 02:00:42,405 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 48%|████▊     | 19/40 [01:34<01:37,  4.62s/it]2024-12-22 02:00:42,468 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:42,616 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:42,617 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2416])
2024-12-22 02:00:42,662 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:42,713 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 02:00:42,793 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:42,794 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2310])
2024-12-22 02:00:42,874 - [Process 0/5] - DEBUG - predict_token:tensor([[29906]], device='cuda:0')
2024-12-22 02:00:43,009 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 48%|████▊     | 19/40 [01:34<01:42,  4.90s/it]2024-12-22 02:00:43,143 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 50%|█████     | 20/40 [01:34<01:30,  4.51s/it]2024-12-22 02:00:43,258 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:43,299 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:44,512 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:44,512 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2243])
2024-12-22 02:00:44,593 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:44,834 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 2
 48%|████▊     | 19/40 [01:36<01:41,  4.81s/it]2024-12-22 02:00:45,092 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:46,761 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:46,762 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2165])
2024-12-22 02:00:46,844 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:00:46,861 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:46,861 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2348])
2024-12-22 02:00:46,956 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:00:47,110 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:47,110 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 02:00:47,127 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 50%|█████     | 20/40 [01:38<01:33,  4.65s/it]2024-12-22 02:00:47,193 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:47,249 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 50%|█████     | 20/40 [01:38<01:34,  4.73s/it]2024-12-22 02:00:47,290 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:47,290 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 02:00:47,380 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:00:47,389 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:47,487 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 50%|█████     | 20/40 [01:39<01:35,  4.77s/it]2024-12-22 02:00:47,515 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:47,653 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 52%|█████▎    | 21/40 [01:39<01:25,  4.51s/it]2024-12-22 02:00:47,757 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:47,809 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:49,112 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:49,113 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2426])
2024-12-22 02:00:49,182 - [Process 1/5] - DEBUG - predict_token:tensor([[29906]], device='cuda:1')
2024-12-22 02:00:49,465 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 50%|█████     | 20/40 [01:41<01:35,  4.76s/it]2024-12-22 02:00:49,725 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:51,532 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:51,532 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2221])
2024-12-22 02:00:51,617 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:00:51,800 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:51,800 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2261])
2024-12-22 02:00:51,839 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:51,839 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2189])
2024-12-22 02:00:51,859 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 3
 52%|█████▎    | 21/40 [01:43<01:28,  4.68s/it]2024-12-22 02:00:51,870 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:51,870 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2354])
2024-12-22 02:00:51,882 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:51,920 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:00:51,961 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:00:52,099 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:52,181 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 52%|█████▎    | 21/40 [01:43<01:30,  4.75s/it]2024-12-22 02:00:52,190 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 55%|█████▌    | 22/40 [01:43<01:21,  4.52s/it]2024-12-22 02:00:52,265 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 52%|█████▎    | 21/40 [01:43<01:31,  4.82s/it]2024-12-22 02:00:52,338 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:52,466 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:52,541 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:53,803 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:53,803 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2172])
2024-12-22 02:00:53,887 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:54,128 - [Process 1/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 1
 52%|█████▎    | 21/40 [01:45<01:29,  4.73s/it]2024-12-22 02:00:54,386 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:56,184 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:56,184 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2214])
2024-12-22 02:00:56,266 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:00:56,511 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:56,512 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2243])
2024-12-22 02:00:56,551 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 55%|█████▌    | 22/40 [01:48<01:24,  4.68s/it]2024-12-22 02:00:56,571 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:56,571 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2244])
2024-12-22 02:00:56,593 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:00:56,661 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:00:56,807 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:56,879 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 55%|█████▌    | 22/40 [01:48<01:25,  4.73s/it]2024-12-22 02:00:56,915 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:56,915 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2439])
2024-12-22 02:00:56,935 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 57%|█████▊    | 23/40 [01:48<01:17,  4.59s/it]2024-12-22 02:00:56,996 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:00:57,086 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:57,156 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:57,288 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 55%|█████▌    | 22/40 [01:48<01:27,  4.88s/it]2024-12-22 02:00:57,535 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:00:58,446 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:00:58,446 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2256])
2024-12-22 02:00:58,529 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:00:58,814 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 26
 55%|█████▌    | 22/40 [01:50<01:24,  4.72s/it]2024-12-22 02:00:59,065 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:01,043 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:01,044 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2285])
2024-12-22 02:01:01,124 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:01:01,351 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:01,351 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2064])
2024-12-22 02:01:01,398 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 60%|██████    | 24/40 [01:53<01:12,  4.55s/it]2024-12-22 02:01:01,432 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:01,515 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:01,516 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2199])
2024-12-22 02:01:01,545 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:01,606 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:01,674 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 4
 57%|█████▊    | 23/40 [01:53<01:20,  4.73s/it]2024-12-22 02:01:01,707 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:01,708 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2451])
2024-12-22 02:01:01,809 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:01:01,904 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 57%|█████▊    | 23/40 [01:53<01:21,  4.82s/it]2024-12-22 02:01:01,936 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:02,106 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 57%|█████▊    | 23/40 [01:53<01:24,  4.94s/it]2024-12-22 02:01:02,158 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:02,350 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:03,176 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:03,177 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2195])
2024-12-22 02:01:03,259 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:03,544 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 57%|█████▊    | 23/40 [01:55<01:20,  4.72s/it]2024-12-22 02:01:03,802 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:05,550 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:05,550 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2217])
2024-12-22 02:01:05,632 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:01:05,861 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 6
 62%|██████▎   | 25/40 [01:57<01:07,  4.52s/it]2024-12-22 02:01:06,004 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:06,028 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:06,028 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2205])
2024-12-22 02:01:06,109 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:06,174 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:06,175 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2243])
2024-12-22 02:01:06,256 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 02:01:06,401 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 60%|██████    | 24/40 [01:58<01:15,  4.73s/it]2024-12-22 02:01:06,425 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:06,425 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2179])
2024-12-22 02:01:06,507 - [Process 3/5] - DEBUG - predict_token:tensor([[29945]], device='cuda:3')
2024-12-22 02:01:06,555 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 60%|██████    | 24/40 [01:58<01:16,  4.77s/it]2024-12-22 02:01:06,674 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:06,791 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 60%|██████    | 24/40 [01:58<01:17,  4.87s/it]2024-12-22 02:01:06,805 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:07,049 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:07,907 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:07,907 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2204])
2024-12-22 02:01:07,989 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 02:01:08,273 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 60%|██████    | 24/40 [01:59<01:15,  4.72s/it]2024-12-22 02:01:08,529 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:10,040 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:10,040 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2227])
2024-12-22 02:01:10,123 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:01:10,394 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 65%|██████▌   | 26/40 [02:02<01:03,  4.53s/it]2024-12-22 02:01:10,552 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:10,751 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:10,751 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2359])
2024-12-22 02:01:10,832 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:10,874 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:10,874 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2430])
2024-12-22 02:01:10,947 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:11,125 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:11,125 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2309])
2024-12-22 02:01:11,128 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 62%|██████▎   | 25/40 [02:02<01:10,  4.73s/it]2024-12-22 02:01:11,207 - [Process 3/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:3')
2024-12-22 02:01:11,245 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 62%|██████▎   | 25/40 [02:02<01:11,  4.75s/it]2024-12-22 02:01:11,398 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:11,493 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 62%|██████▎   | 25/40 [02:03<01:12,  4.82s/it]2024-12-22 02:01:11,506 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:11,740 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:12,531 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:12,531 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1983])
2024-12-22 02:01:12,621 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:12,905 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 62%|██████▎   | 25/40 [02:04<01:10,  4.70s/it]2024-12-22 02:01:13,142 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:14,857 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:14,858 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2438])
2024-12-22 02:01:14,939 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:01:15,213 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 68%|██████▊   | 27/40 [02:06<00:59,  4.61s/it]2024-12-22 02:01:15,364 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:15,469 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:15,469 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2240])
2024-12-22 02:01:15,542 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:15,542 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2122])
2024-12-22 02:01:15,548 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:15,624 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:15,836 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:15,836 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2145])
2024-12-22 02:01:15,846 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 65%|██████▌   | 26/40 [02:07<01:06,  4.73s/it]2024-12-22 02:01:15,880 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 3
 65%|██████▌   | 26/40 [02:07<01:05,  4.71s/it]2024-12-22 02:01:15,920 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:01:16,134 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:16,140 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:16,205 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 65%|██████▌   | 26/40 [02:07<01:06,  4.78s/it]2024-12-22 02:01:16,448 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:17,212 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:17,212 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2129])
2024-12-22 02:01:17,294 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:17,576 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 65%|██████▌   | 26/40 [02:09<01:05,  4.69s/it]2024-12-22 02:01:17,817 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:19,361 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:19,361 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2293])
2024-12-22 02:01:19,441 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:01:19,713 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 70%|███████   | 28/40 [02:11<00:54,  4.58s/it]2024-12-22 02:01:19,869 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:20,223 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:20,224 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2212])
2024-12-22 02:01:20,252 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:20,253 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2197])
2024-12-22 02:01:20,305 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:01:20,337 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:20,526 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:20,526 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2228])
2024-12-22 02:01:20,607 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 68%|██████▊   | 27/40 [02:12<01:01,  4.74s/it]2024-12-22 02:01:20,608 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:01:20,640 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 68%|██████▊   | 27/40 [02:12<01:01,  4.73s/it]2024-12-22 02:01:20,865 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:20,892 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 68%|██████▊   | 27/40 [02:12<01:01,  4.76s/it]2024-12-22 02:01:20,914 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:21,137 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:21,901 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:21,901 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2173])
2024-12-22 02:01:21,983 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 02:01:22,267 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 68%|██████▊   | 27/40 [02:13<01:00,  4.69s/it]2024-12-22 02:01:22,509 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:23,908 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:23,908 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2207])
2024-12-22 02:01:23,989 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:01:24,259 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 72%|███████▎  | 29/40 [02:15<00:50,  4.57s/it]2024-12-22 02:01:24,397 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:24,753 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:24,753 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2227])
2024-12-22 02:01:24,828 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:24,995 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:24,996 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2337])
2024-12-22 02:01:25,047 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:25,047 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 02:01:25,077 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:25,111 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 70%|███████   | 28/40 [02:16<00:55,  4.67s/it]2024-12-22 02:01:25,131 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:01:25,375 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 70%|███████   | 28/40 [02:17<00:56,  4.73s/it]2024-12-22 02:01:25,380 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:25,414 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 70%|███████   | 28/40 [02:17<00:56,  4.69s/it]2024-12-22 02:01:25,629 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:25,668 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:26,847 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:26,847 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2401])
2024-12-22 02:01:26,930 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:27,262 - [Process 1/5] - INFO - res.shape is :torch.Size([7])
results:
Paragraph 27
 70%|███████   | 28/40 [02:18<00:57,  4.78s/it]2024-12-22 02:01:27,504 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:28,387 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:28,387 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2336])
2024-12-22 02:01:28,468 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:01:28,741 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 75%|███████▌  | 30/40 [02:20<00:45,  4.54s/it]2024-12-22 02:01:28,899 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:29,432 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:29,432 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2265])
2024-12-22 02:01:29,516 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:29,693 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:29,694 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2270])
2024-12-22 02:01:29,756 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:29,757 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2235])
2024-12-22 02:01:29,776 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:29,804 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 72%|███████▎  | 29/40 [02:21<00:51,  4.67s/it]2024-12-22 02:01:29,839 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:01:30,064 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:30,071 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 72%|███████▎  | 29/40 [02:21<00:51,  4.72s/it]2024-12-22 02:01:30,124 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 72%|███████▎  | 29/40 [02:21<00:51,  4.69s/it]2024-12-22 02:01:30,328 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:30,366 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:31,894 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:31,894 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2343])
2024-12-22 02:01:31,986 - [Process 1/5] - DEBUG - predict_token:tensor([[29946]], device='cuda:1')
2024-12-22 02:01:32,277 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 72%|███████▎  | 29/40 [02:23<00:53,  4.85s/it]2024-12-22 02:01:32,546 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:32,919 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:32,919 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2162])
2024-12-22 02:01:33,000 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:01:33,270 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 78%|███████▊  | 31/40 [02:24<00:40,  4.54s/it]2024-12-22 02:01:33,417 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:34,073 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:34,073 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2293])
2024-12-22 02:01:34,154 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:01:34,408 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:34,409 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2160])
2024-12-22 02:01:34,441 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 75%|███████▌  | 30/40 [02:26<00:46,  4.66s/it]2024-12-22 02:01:34,492 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:34,700 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:34,785 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 75%|███████▌  | 30/40 [02:26<00:47,  4.72s/it]2024-12-22 02:01:35,028 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:35,029 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2396])
2024-12-22 02:01:35,033 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:35,121 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:01:35,411 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 75%|███████▌  | 30/40 [02:27<00:48,  4.87s/it]2024-12-22 02:01:35,650 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:36,825 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:36,826 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2356])
2024-12-22 02:01:36,915 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:37,207 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 75%|███████▌  | 30/40 [02:28<00:48,  4.87s/it]2024-12-22 02:01:37,448 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:37,448 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2235])
2024-12-22 02:01:37,461 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:37,527 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:01:37,797 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 80%|████████  | 32/40 [02:29<00:36,  4.54s/it]2024-12-22 02:01:37,955 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:38,790 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:38,790 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2229])
2024-12-22 02:01:38,871 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:39,136 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:39,136 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2205])
2024-12-22 02:01:39,155 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 78%|███████▊  | 31/40 [02:30<00:42,  4.68s/it]2024-12-22 02:01:39,219 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:39,423 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:39,512 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 78%|███████▊  | 31/40 [02:31<00:42,  4.72s/it]2024-12-22 02:01:39,696 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:39,696 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2158])
2024-12-22 02:01:39,747 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:39,778 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:01:40,061 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 78%|███████▊  | 31/40 [02:31<00:43,  4.80s/it]2024-12-22 02:01:40,316 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:41,535 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:41,535 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 02:01:41,616 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:41,899 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 78%|███████▊  | 31/40 [02:33<00:43,  4.82s/it]2024-12-22 02:01:41,987 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:41,987 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2327])
2024-12-22 02:01:42,068 - [Process 0/5] - DEBUG - predict_token:tensor([[29906]], device='cuda:0')
2024-12-22 02:01:42,142 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:42,339 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 82%|████████▎ | 33/40 [02:33<00:31,  4.54s/it]2024-12-22 02:01:42,483 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:43,733 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:43,733 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2403])
2024-12-22 02:01:43,813 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:43,813 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2254])
2024-12-22 02:01:43,817 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:43,897 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:44,120 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 80%|████████  | 32/40 [02:35<00:38,  4.76s/it]2024-12-22 02:01:44,200 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 80%|████████  | 32/40 [02:35<00:37,  4.71s/it]2024-12-22 02:01:44,386 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:44,464 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:44,620 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:44,620 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2475])
2024-12-22 02:01:44,704 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:01:44,996 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 80%|████████  | 32/40 [02:36<00:38,  4.84s/it]2024-12-22 02:01:45,227 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:46,110 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:46,110 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2356])
2024-12-22 02:01:46,187 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:46,439 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:46,439 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2296])
2024-12-22 02:01:46,471 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 80%|████████  | 32/40 [02:38<00:37,  4.75s/it]2024-12-22 02:01:46,520 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:01:46,710 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:46,792 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 85%|████████▌ | 34/40 [02:38<00:27,  4.51s/it]2024-12-22 02:01:46,947 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:48,455 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:48,455 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 02:01:48,539 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:01:48,588 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:48,588 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2217])
2024-12-22 02:01:48,673 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:48,786 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
results:Paragraph 4
 82%|████████▎ | 33/40 [02:40<00:33,  4.73s/it]2024-12-22 02:01:48,971 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 82%|████████▎ | 33/40 [02:40<00:33,  4.73s/it]2024-12-22 02:01:49,047 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:49,119 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:49,119 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2112])
2024-12-22 02:01:49,201 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:01:49,231 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:49,482 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 24
 82%|████████▎ | 33/40 [02:41<00:33,  4.74s/it]2024-12-22 02:01:49,731 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:50,791 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:50,791 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2174])
2024-12-22 02:01:50,873 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:50,906 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:50,906 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2294])
2024-12-22 02:01:50,987 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:01:51,157 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 82%|████████▎ | 33/40 [02:42<00:33,  4.73s/it]2024-12-22 02:01:51,259 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 88%|████████▊ | 35/40 [02:42<00:22,  4.50s/it]2024-12-22 02:01:51,403 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:51,413 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:53,109 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:53,109 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2367])
2024-12-22 02:01:53,191 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:01:53,259 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:53,259 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2258])
2024-12-22 02:01:53,341 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:01:53,486 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 85%|████████▌ | 34/40 [02:45<00:28,  4.72s/it]2024-12-22 02:01:53,640 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 85%|████████▌ | 34/40 [02:45<00:28,  4.71s/it]2024-12-22 02:01:53,755 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:53,806 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:53,806 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2202])
2024-12-22 02:01:53,887 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:01:53,904 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:54,171 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 25
 85%|████████▌ | 34/40 [02:45<00:28,  4.72s/it]2024-12-22 02:01:54,427 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:55,650 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:55,651 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2145])
2024-12-22 02:01:55,696 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:55,696 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2212])
2024-12-22 02:01:55,741 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:01:55,787 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:01:56,031 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 85%|████████▌ | 34/40 [02:47<00:28,  4.77s/it]2024-12-22 02:01:56,060 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 90%|█████████ | 36/40 [02:47<00:18,  4.59s/it]2024-12-22 02:01:56,218 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:56,284 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:57,843 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:57,843 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2189])
2024-12-22 02:01:57,927 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:01:58,002 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:58,003 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 02:01:58,086 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 02:01:58,220 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 88%|████████▊ | 35/40 [02:49<00:23,  4.73s/it]2024-12-22 02:01:58,384 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 88%|████████▊ | 35/40 [02:50<00:23,  4.72s/it]2024-12-22 02:01:58,500 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:58,644 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:01:58,834 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:01:58,834 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2382])
2024-12-22 02:01:58,918 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:01:59,207 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 88%|████████▊ | 35/40 [02:50<00:24,  4.82s/it]2024-12-22 02:01:59,439 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:00,230 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:00,230 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2271])
2024-12-22 02:02:00,313 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:02:00,517 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:00,517 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2470])
2024-12-22 02:02:00,586 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 92%|█████████▎| 37/40 [02:52<00:13,  4.57s/it]2024-12-22 02:02:00,600 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 02:02:00,738 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:00,890 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 88%|████████▊ | 35/40 [02:52<00:23,  4.80s/it]2024-12-22 02:02:01,142 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:02,728 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:02,728 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2316])
2024-12-22 02:02:02,809 - [Process 4/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:4')
2024-12-22 02:02:02,811 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:02,811 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2322])
2024-12-22 02:02:02,901 - [Process 2/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:2')
2024-12-22 02:02:03,107 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 90%|█████████ | 36/40 [02:54<00:18,  4.72s/it]2024-12-22 02:02:03,204 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 30
 90%|█████████ | 36/40 [02:54<00:19,  4.80s/it]2024-12-22 02:02:03,371 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:03,457 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:03,841 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:03,841 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2392])
2024-12-22 02:02:03,925 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:02:04,214 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 90%|█████████ | 36/40 [02:55<00:19,  4.87s/it]2024-12-22 02:02:04,468 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:04,633 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:04,634 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2325])
2024-12-22 02:02:04,710 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:02:04,978 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 95%|█████████▌| 38/40 [02:56<00:09,  4.52s/it]2024-12-22 02:02:05,134 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:05,250 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:05,251 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2211])
2024-12-22 02:02:05,332 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 02:02:05,615 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 90%|█████████ | 36/40 [02:57<00:19,  4.78s/it]2024-12-22 02:02:05,851 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:07,498 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:07,498 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2131])
2024-12-22 02:02:07,578 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:02:07,859 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 92%|█████████▎| 37/40 [02:59<00:14,  4.76s/it]2024-12-22 02:02:08,012 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:08,012 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2451])
2024-12-22 02:02:08,102 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:02:08,103 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:08,395 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
 92%|█████████▎| 37/40 [03:00<00:14,  4.89s/it]2024-12-22 02:02:08,656 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:08,816 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:08,816 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2417])
2024-12-22 02:02:08,899 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:02:09,144 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:09,144 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2276])
2024-12-22 02:02:09,186 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 92%|█████████▎| 37/40 [03:00<00:14,  4.90s/it]2024-12-22 02:02:09,228 - [Process 0/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:0')
2024-12-22 02:02:09,429 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:09,500 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 98%|█████████▊| 39/40 [03:01<00:04,  4.52s/it]2024-12-22 02:02:09,642 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:09,714 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:09,714 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2286])
2024-12-22 02:02:09,789 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:02:10,072 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 92%|█████████▎| 37/40 [03:01<00:14,  4.68s/it]2024-12-22 02:02:10,316 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:12,139 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:12,139 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2186])
2024-12-22 02:02:12,220 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:02:12,505 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 95%|█████████▌| 38/40 [03:04<00:09,  4.73s/it]2024-12-22 02:02:12,747 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:12,748 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:12,748 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2174])
2024-12-22 02:02:12,830 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:02:13,113 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 95%|█████████▌| 38/40 [03:04<00:09,  4.84s/it]2024-12-22 02:02:13,352 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:13,504 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:13,505 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2179])
2024-12-22 02:02:13,586 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:02:13,654 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:13,655 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2206])
2024-12-22 02:02:13,735 - [Process 0/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:0')
2024-12-22 02:02:13,870 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 95%|█████████▌| 38/40 [03:05<00:09,  4.84s/it]2024-12-22 02:02:14,006 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 26
100%|██████████| 40/40 [03:05<00:00,  4.51s/it]100%|██████████| 40/40 [03:05<00:00,  4.64s/it]
2024-12-22 02:02:14,134 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:14,434 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:14,434 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2194])
2024-12-22 02:02:14,517 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 02:02:14,801 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 95%|█████████▌| 38/40 [03:06<00:09,  4.69s/it]2024-12-22 02:02:15,042 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:17,102 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:17,103 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2352])
2024-12-22 02:02:17,195 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:02:17,257 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:17,258 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 02:02:17,340 - [Process 4/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:4')
2024-12-22 02:02:17,492 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 98%|█████████▊| 39/40 [03:09<00:04,  4.80s/it]2024-12-22 02:02:17,637 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
 98%|█████████▊| 39/40 [03:09<00:04,  4.74s/it]2024-12-22 02:02:17,762 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:17,909 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:18,495 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:18,495 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2233])
2024-12-22 02:02:18,586 - [Process 3/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:3')
2024-12-22 02:02:18,873 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 98%|█████████▊| 39/40 [03:10<00:04,  4.89s/it]2024-12-22 02:02:19,129 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:19,240 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:19,240 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2080])
2024-12-22 02:02:19,331 - [Process 1/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:1')
2024-12-22 02:02:19,616 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
 98%|█████████▊| 39/40 [03:11<00:04,  4.73s/it]2024-12-22 02:02:19,856 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:02:21,818 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:21,818 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2262])
2024-12-22 02:02:21,902 - [Process 2/5] - DEBUG - predict_token:tensor([[1459]], device='cuda:2')
2024-12-22 02:02:21,968 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:21,968 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 02:02:22,059 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:02:22,196 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
100%|██████████| 40/40 [03:13<00:00,  4.77s/it]100%|██████████| 40/40 [03:13<00:00,  4.85s/it]
2024-12-22 02:02:22,354 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 29
100%|██████████| 40/40 [03:14<00:00,  4.74s/it]100%|██████████| 40/40 [03:14<00:00,  4.85s/it]
2024-12-22 02:02:23,082 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:23,082 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2080])
2024-12-22 02:02:23,167 - [Process 3/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:3')
2024-12-22 02:02:23,449 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 27
100%|██████████| 40/40 [03:15<00:00,  4.79s/it]100%|██████████| 40/40 [03:15<00:00,  4.88s/it]
2024-12-22 02:02:23,729 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:02:23,729 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2109])
2024-12-22 02:02:23,810 - [Process 1/5] - DEBUG - predict_token:tensor([[29941]], device='cuda:1')
2024-12-22 02:02:24,089 - [Process 1/5] - INFO - res.shape is :torch.Size([6])
results:Paragraph 28
100%|██████████| 40/40 [03:15<00:00,  4.65s/it]100%|██████████| 40/40 [03:15<00:00,  4.90s/it]
2024-12-22 02:02:24,125 - [Process 2/5] - DEBUG - datasets_name:passage_retrieval_en
2024-12-22 02:02:24,125 - [Process 3/5] - DEBUG - datasets_name:passage_retrieval_en
2024-12-22 02:02:24,125 - [Process 1/5] - DEBUG - datasets_name:passage_retrieval_en
2024-12-22 02:02:24,125 - [Process 4/5] - DEBUG - datasets_name:passage_retrieval_en
2024-12-22 02:02:24,125 - [Process 0/5] - DEBUG - datasets_name:passage_retrieval_en
Running evaluation for dataset: qmsum
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.71s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:04:32,105 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 02:04:32,105 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 02:04:32,105 - [Process 0/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:04:32,111 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 02:04:32,112 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 02:04:32,112 - [Process 4/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:04:32,125 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 02:04:32,125 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 02:04:32,125 - [Process 2/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里!!!!!!!!!!!!!!!!!!!!!!!! 这里

2024-12-22 02:04:32,128 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 02:04:32,128 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 02:04:32,128 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 02:04:32,128 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 02:04:32,128 - [Process 1/5] - INFO - output_max_len: 512
2024-12-22 02:04:32,128 - [Process 3/5] - INFO - output_max_len: 512
2024-12-22 02:04:32,137 - [Process 0/5] - INFO - Max Length is 24585
2024-12-22 02:04:32,137 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 02:04:32,137 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:04:32,156 - [Process 4/5] - INFO - Max Length is 24585
2024-12-22 02:04:32,157 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 02:04:32,157 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:04:32,176 - [Process 2/5] - INFO - Max Length is 24585
2024-12-22 02:04:32,177 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 02:04:32,177 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:04:32,179 - [Process 3/5] - INFO - Max Length is 24585
2024-12-22 02:04:32,179 - [Process 1/5] - INFO - Max Length is 24585
2024-12-22 02:04:32,180 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 02:04:32,180 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 02:04:32,180 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 02:04:32,180 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:04:36,888 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:36,972 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:36,974 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:36,975 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:36,976 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:41,108 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:41,108 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 02:04:41,174 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:41,175 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2089])
2024-12-22 02:04:41,181 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:04:41,246 - [Process 1/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:04:41,307 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:41,307 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 02:04:41,308 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:41,309 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 02:04:41,336 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:41,337 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 02:04:41,379 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:04:41,380 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:04:41,410 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:04:43,828 - [Process 4/5] - INFO - res.shape is :torch.Size([58])
results:During the meeting, there was no discussion about the seniors care crisis. The transcript only mentions the COVID-19 pandemic and the government's response to it, including the provision of emergency programs and the introduction of the Canada Emergency Response Benefit.
  2%|▎         | 1/40 [00:11<07:35, 11.67s/it]2024-12-22 02:04:43,986 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:45,914 - [Process 1/5] - INFO - res.shape is :torch.Size([104])
results:During the meeting, the group discussed the design goal of the remote control, which is to create a new remote control for a television that is original, user-friendly, and has a trendy design. They also discussed the importance of keeping the design simple and intuitive, with a focus on the user experience. Additionally, they talked about the need to identify the components of the product and how they will be used, as well as the importance of considering the user requirements specification in the design process.
  2%|▎         | 1/40 [00:13<08:55, 13.74s/it]2024-12-22 02:04:46,194 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:46,714 - [Process 0/5] - INFO - res.shape is :torch.Size([121])
results:During the meeting, the team discussed the working design of the remote control, focusing on the user interface and user experience. They shared their ideas and opinions on how to make the remote control more user-friendly, such as adding a speaker to enable voice control, implementing a clap feature, and considering the needs of people with disabilities. They also discussed the survey results on people's preferences for remote controls, including the importance of aesthetics and ease of use. The team agreed to think about how to incorporate these features into the design of the remote control.
  2%|▎         | 1/40 [00:14<09:28, 14.58s/it]2024-12-22 02:04:46,964 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:47,099 - [Process 3/5] - INFO - res.shape is :torch.Size([125])
results:During the meeting, the team discussed the design of the interface concept for the remote control, focusing on the shape and size of the buttons, the use of colors and icons, and the placement of the menu button. They also discussed the importance of making the design visually appealing and easy to use, with a focus on younger people's preferences. The team agreed to use a yellow case with round edges and the logo at the bottom, and to use different colors for each set of buttons. Additionally, they discussed the use of rubber around the buttons and the importance of having big buttons.
  2%|▎         | 1/40 [00:14<09:42, 14.92s/it]2024-12-22 02:04:47,343 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:47,527 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:47,527 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 02:04:47,600 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:04:48,537 - [Process 2/5] - INFO - res.shape is :torch.Size([158])
results:The meeting discussed the project "Meeting Recorder" which aims to record and analyze meetings using various audio and video devices. The team will work on converting the transcripts to English and will have the opportunity to edit them later. They will also identify speakers and merge the data into a harmonious system. The team will also work on a new feature to show the state of the system and will have the opportunity to bleep out any unwanted information. The meeting also discussed the issue of identifying speakers and the problem of taking people to the wrong location using car navigation systems. The team will use high-quality pressure zone mikes and close-talking mikes to collect data and will have a chance to edit the transcripts later.
  2%|▎         | 1/40 [00:16<10:38, 16.37s/it]2024-12-22 02:04:48,914 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:49,772 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:49,772 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 02:04:49,846 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:04:50,247 - [Process 4/5] - INFO - res.shape is :torch.Size([65])
results:Based on the meeting transcript, it appears that the group did not reach a conclusion on LCD screens and the speech recognition suggested by Marketing. The group discussed the idea but did not come to a consensus, as they agreed to "go to our rooms" and "decide things on their own."
  5%|▌         | 2/40 [00:18<05:26,  8.58s/it]2024-12-22 02:04:50,382 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:50,505 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:50,505 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 02:04:50,578 - [Process 0/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:0')
2024-12-22 02:04:50,923 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:50,924 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 02:04:50,996 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:04:52,477 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:52,478 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 02:04:52,550 - [Process 2/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:2')
2024-12-22 02:04:53,369 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:During the meeting, the group discussed the possibility of incorporating a touch screen into the remote control design but noted that it would exceed their budget constraints. They considered using a different type of screen, such as an iPod screen, but were concerned about the cost and decided to focus on other design elements instead.
  5%|▌         | 2/40 [00:21<06:16,  9.92s/it]2024-12-22 02:04:53,652 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:53,772 - [Process 3/5] - INFO - res.shape is :torch.Size([65])
results:During the discussion, the team discussed the presentation on the prototype model, focusing on the design and functionality of the device. They also evaluated the prototype's performance and provided feedback on its usability. Additionally, they discussed the budget and timeline for the project, and whether the costs were within the budget.
  5%|▌         | 2/40 [00:21<06:22, 10.07s/it]2024-12-22 02:04:53,927 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:54,051 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:54,052 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1966])
2024-12-22 02:04:54,130 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:04:54,351 - [Process 1/5] - INFO - res.shape is :torch.Size([103])
results:The allocation of funds to universities will help them transition away from EU funding by providing additional funding for research, as recommended by the Reid review. The money will go towards improving universities' ability to access funding from UKRI, specifically Innovate UK, and will help institutions compete for funding in the UK-wide research portfolio. The allocation will also help universities generate surplus cash to meet their costs, including the costs of servicing their borrowings.
  5%|▌         | 2/40 [00:22<06:43, 10.62s/it]2024-12-22 02:04:54,502 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:57,324 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:57,324 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1690])
2024-12-22 02:04:57,325 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:57,325 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2139])
2024-12-22 02:04:57,379 - [Process 1/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:1')
2024-12-22 02:04:57,392 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:04:57,521 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:04:57,522 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 02:04:57,596 - [Process 3/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:3')
2024-12-22 02:04:58,694 - [Process 2/5] - INFO - res.shape is :torch.Size([140])
results:During the meeting, the discussion centered around the government's policy to improve long-term care facilities and protect seniors. The government is adopting an approach with consistent criteria for all employees in all sectors, and introducing specific measures to help certain sectors, such as the fishing industry. The government is also providing emergency wage subsidies and the Canada emergency response benefit to help employees who have lost their income due to COVID-19. However, Ms. May expressed concern that the government's policy may not be protecting all jobs and sectors of the economy, and requested the government to consider returning the money to those who need it.
  5%|▌         | 2/40 [00:26<08:03, 12.71s/it]2024-12-22 02:04:58,974 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:04:59,752 - [Process 4/5] - INFO - res.shape is :torch.Size([127])
results:During the meeting, the team discussed the appearance design of the remote control, focusing on the shape and material. They agreed to use titanium for the body and rubber for the buttons, as it provides a better grip and is more durable. However, they also considered using a spongy material for the buttons, which would provide a different feel. The team also discussed the possibility of using a double-curved shape, but it was deemed problematic in manufacturing. They ultimately decided to stick with a simple push-button design and explore the use of speech recognition technology to reduce costs.
  8%|▊         | 3/40 [00:27<05:33,  9.00s/it]2024-12-22 02:04:59,967 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:01,015 - [Process 0/5] - INFO - res.shape is :torch.Size([83])
results:The general discussion in the meeting centered around the recording of the meeting, with a focus on the gain levels and the need for better audio quality. The group also discussed the upcoming visit of John Fiscus and the agenda items for the meeting. Additionally, they talked about the digit and recess forms that need to be filled out by the speakers and the consent form that needs to be signed.
  8%|▊         | 3/40 [00:28<05:28,  8.88s/it]2024-12-22 02:05:01,270 - [Process 3/5] - INFO - res.shape is :torch.Size([91])
results:During the meeting, Dr. David Blaney mentioned that Trinity Saint David had secured £1,000 bursaries for care leavers, which is an example of investment in student support-related activities from the fee and access plans. Bethan Owen also mentioned that universities have reserves, but a large amount of that is tied up in their estates, so they're not immediately realisable.
  8%|▊         | 3/40 [00:29<05:29,  8.90s/it]2024-12-22 02:05:01,281 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:01,504 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:01,605 - [Process 1/5] - INFO - res.shape is :torch.Size([98])
results:Based on the meeting transcript, it appears that the decision regarding the remote feature design has not been finalized. The team is still discussing and brainstorming ideas for the remote control, including the size and number of buttons, the use of LED indicators, and the compatibility with other products. The project manager mentions that they will think about what they will make next and how to make it, but no definitive decision has been made regarding the remote feature design.
  8%|▊         | 3/40 [00:29<05:36,  9.08s/it]2024-12-22 02:05:01,872 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:02,579 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:02,579 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2083])
2024-12-22 02:05:02,652 - [Process 2/5] - DEBUG - predict_token:tensor([[10537]], device='cuda:2')
2024-12-22 02:05:03,529 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:03,529 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2064])
2024-12-22 02:05:03,600 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:05:04,946 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:04,946 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-22 02:05:05,025 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:05:05,094 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:05,094 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2023])
2024-12-22 02:05:05,168 - [Process 3/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:3')
2024-12-22 02:05:05,462 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:05,462 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 02:05:05,536 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:05:06,887 - [Process 2/5] - INFO - res.shape is :torch.Size([96])
results:Albert Heaney did not explicitly state his thoughts on the coronavirus Act during the meeting. However, he did mention that the Welsh Government has a plan for surge capacity in paediatric care, which includes flexing up capacity when needed, but the challenge is in moving out of lockdown, and the Government is cautious about doing so as it may lead to an increase in the number of cases, including paediatric ones.
  8%|▊         | 3/40 [00:34<06:33, 10.65s/it]2024-12-22 02:05:07,008 - [Process 4/5] - INFO - res.shape is :torch.Size([77])
results:During the meeting, the team discussed the project and overall process, with the project manager expressing satisfaction with the teamwork and creativity, while the marketing and industrial designer had some concerns about the product's technological innovation and lack of new features. The team also agreed to evaluate the product based on a set of criteria and give a rating for each one.
 10%|█         | 4/40 [00:34<04:59,  8.31s/it]2024-12-22 02:05:07,120 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:07,211 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:08,610 - [Process 0/5] - INFO - res.shape is :torch.Size([81])
results:The Project Manager seems to be generally satisfied with the prototype presented by the Industrial Designer and User Interface. They mention that it is "nice" and "handy," and that they went through the agenda and reviewed the presentations from the team. However, they also mention that they still have to evaluate the costs and make sure everything is in order before they can close the project.
 10%|█         | 4/40 [00:36<05:01,  8.37s/it]2024-12-22 02:05:08,716 - [Process 3/5] - INFO - res.shape is :torch.Size([88])
results:During the meeting, there was a debate over the situation of long-term care facilities across Ontario, with Mrs. Marilne Gill expressing concern that the government is not doing enough to help seasonal workers who have no assurances for their future, while Hon. Bill Morneau emphasized the government's approach of providing emergency programs to protect employees and ensure that the economy works well after the pandemic.
 10%|█         | 4/40 [00:36<04:59,  8.32s/it]2024-12-22 02:05:08,853 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:08,869 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:10,529 - [Process 1/5] - INFO - res.shape is :torch.Size([114])
results:The meeting discussed the design of a new remote control for a TV, with a focus on making it easy to use and visually appealing. The team also discussed the importance of incorporating the latest trends and technological innovations, such as touch-pads and LCD screens. They also considered the material used for the remote and how it should be designed to be both functional and aesthetically pleasing. The team agreed to follow the trends and make the remote control more user-friendly, with a focus on the younger public.
 10%|█         | 4/40 [00:38<05:24,  9.02s/it]2024-12-22 02:05:10,695 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:10,695 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2104])
2024-12-22 02:05:10,719 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:10,766 - [Process 2/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:2')
2024-12-22 02:05:10,912 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:10,912 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2114])
2024-12-22 02:05:10,980 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:05:12,512 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:12,512 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:05:12,537 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:12,537 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1967])
2024-12-22 02:05:12,584 - [Process 3/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:3')
2024-12-22 02:05:12,615 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:05:14,396 - [Process 2/5] - INFO - res.shape is :torch.Size([82])
results:The product manager wants to achieve the expected profit by making the product design more trendy and modern, using fronts that can be changed, and making it universal so that it can work with all brands and types of televisions. They also want to make it easy to use and have a nice user interface, which can be achieved by using a simple and easy-to-use remote control.
 10%|█         | 4/40 [00:42<05:38,  9.41s/it]2024-12-22 02:05:14,455 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:14,455 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2124])
2024-12-22 02:05:14,524 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:05:14,674 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:15,585 - [Process 4/5] - INFO - res.shape is :torch.Size([106])
results:The meeting discussed the design of a new remote control for a project, with a focus on making it user-friendly, adaptable, and with a unique design. The team brainstormed ideas and decided to use a whiteboard to work on the project. They also discussed the price of the remote control and aimed to make a profit of 25 Euros. The team members shared their experiences and ideas, and the project manager asked them to work on the project for the next 30 minutes.
 12%|█▎        | 5/40 [00:43<04:54,  8.41s/it]2024-12-22 02:05:15,798 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:16,251 - [Process 3/5] - INFO - res.shape is :torch.Size([91])
results:Based on the transcript, the professor seemed to be open to the idea of improving the back recognizer for the Aurora task, as PhD B mentioned that they were discussing ways to improve it, and the professor did not express any strong objections to the idea. However, the professor also mentioned that they were not sure if there was much that could be done to improve the recognizer, as it was already quite good.
 12%|█▎        | 5/40 [00:44<04:41,  8.04s/it]2024-12-22 02:05:16,419 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:18,291 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:18,292 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-22 02:05:18,364 - [Process 2/5] - DEBUG - predict_token:tensor([[478]], device='cuda:2')
2024-12-22 02:05:18,857 - [Process 1/5] - INFO - res.shape is :torch.Size([99])
results:The decision of the discussion is to design a remote control that is original, trendy, and user-friendly, with a focus on adaptability and simplicity. The team aims to make the remote control have three main actions, with the possibility of adding more actions based on user feedback. The team also discussed the importance of considering the user's preferences and needs, such as left-handed or right-handed individuals, and the profit margin for the product.
 12%|█▎        | 5/40 [00:46<05:06,  8.77s/it]2024-12-22 02:05:19,153 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:19,372 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:19,372 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 02:05:19,444 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:05:20,020 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:20,020 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 02:05:20,093 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:05:20,176 - [Process 0/5] - INFO - res.shape is :torch.Size([171])
results:Based on the meeting transcript, the system will be involved in demonstrating the following demos:

1. A demonstration of the system's ability to generate sentences in English using the German sythesis module.
2. A demonstration of the system's ability to speak into the SmartKom system and have it recognize and respond to voice commands.
3. A demonstration of the system's ability to generate different trees for different languages, including English.
4. A demonstration of the system's ability to work with the Texas speech version of the system, which is simpler than the current version.
5. A demonstration of the system's ability to work with the gestural recognition running with the Siemens virtual touch screen, which is not available in the current setup.
 12%|█▎        | 5/40 [00:48<05:33,  9.53s/it]2024-12-22 02:05:20,397 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:21,122 - [Process 4/5] - INFO - res.shape is :torch.Size([39])
results:During the meeting, the team did not discuss the product cost as it was not mentioned in the transcript. The discussion focused on the product design, user interface, and evaluation criteria.
 15%|█▌        | 6/40 [00:48<04:12,  7.43s/it]2024-12-22 02:05:21,481 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:22,410 - [Process 2/5] - INFO - res.shape is :torch.Size([91])
results:Vaughan Gething did not elaborate on the crisis care or the increasing suicide crimes during the COVID-19 and the long-term special care for pregnant women in the given transcript. The transcript only mentions the provision of free childcare for critical workers and vulnerable children, the capacity of NHS services, and the monitoring of parents not taking their children into healthcare systems for other conditions during the pandemic.
 12%|█▎        | 5/40 [00:50<05:11,  8.91s/it]2024-12-22 02:05:22,633 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:22,939 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:22,939 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 02:05:23,020 - [Process 1/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:1')
2024-12-22 02:05:23,935 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:23,935 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2064])
2024-12-22 02:05:24,007 - [Process 0/5] - DEBUG - predict_token:tensor([[4911]], device='cuda:0')
2024-12-22 02:05:24,936 - [Process 3/5] - INFO - res.shape is :torch.Size([120])
results:The discussion centered around setting up the recording equipment for the meeting, with Grad A explaining that the table-top microphones and close-talking mikes are already in place, while the far-field mikes need to be set up. Grad B mentioned that they have to take out a bit of the road work, and Professor C asked if Nancy knows that they are meeting in that room. Grad B also mentioned that they will anonymize the transcript but not the audio, and that they will go through and tag the speakers in the transcript with their names.
 15%|█▌        | 6/40 [00:52<04:40,  8.26s/it]2024-12-22 02:05:25,000 - [Process 0/5] - INFO - res.shape is :torch.Size([23])
results:User Interface introduced the prototype of the remote control by saying "This is it" and pointing to the device.
 15%|█▌        | 6/40 [00:52<04:29,  7.92s/it]2024-12-22 02:05:25,054 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:25,184 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:25,184 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1943])
2024-12-22 02:05:25,264 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:05:25,276 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:26,139 - [Process 1/5] - INFO - res.shape is :torch.Size([71])
results:Eluned Morgan acknowledged that the Welsh Government is aware of the issue regarding pay inequality between schoolteachers and FE and mentioned that they are keeping a close eye on the situation. However, she also stated that they are not in a position to provide financial incentives to address the issue at this time due to budget constraints.
 15%|█▌        | 6/40 [00:53<04:41,  8.26s/it]2024-12-22 02:05:26,332 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:26,333 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1967])
2024-12-22 02:05:26,403 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:26,412 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:05:28,665 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:28,665 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2059])
2024-12-22 02:05:28,737 - [Process 3/5] - DEBUG - predict_token:tensor([[8010]], device='cuda:3')
2024-12-22 02:05:28,824 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:28,824 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 02:05:28,894 - [Process 0/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:0')
2024-12-22 02:05:29,767 - [Process 2/5] - INFO - res.shape is :torch.Size([77])
results:During the meeting, the team discussed the user interface concept for the remote control, focusing on the design and features. They agreed to use push buttons instead of a liquid crystal display (LCD) and to keep the overall design simple and user-friendly. The team also discussed the use of speech recognition technology to make the remote control more convenient and reduce costs.
 15%|█▌        | 6/40 [00:57<04:44,  8.38s/it]2024-12-22 02:05:29,821 - [Process 0/5] - INFO - res.shape is :torch.Size([20])
results:C did not express any specific thoughts or opinions about setting up the recording equipment during the meeting.
 18%|█▊        | 7/40 [00:57<03:48,  6.91s/it]2024-12-22 02:05:29,978 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:30,005 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:30,006 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 02:05:30,079 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:05:30,105 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:30,438 - [Process 3/5] - INFO - res.shape is :torch.Size([42])
results:Project Manager questioned the Industrial Designer about the technical requirements of the remote control, specifically asking for clarification on the working of the remote control, the number of buttons, and the menu system.
 18%|█▊        | 7/40 [00:58<04:02,  7.36s/it]2024-12-22 02:05:30,601 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:31,659 - [Process 4/5] - INFO - res.shape is :torch.Size([145])
results:The semantic specification is a formal notation for representing the meaning of natural language sentences. It is based on a set of rules and constraints that define how words and phrases can be combined to form meaningful expressions. The specification includes a set of constructs, such as "throw," "happy," and "directional specifier," which are used to represent different types of constructions, and a set of formal constraints that define how these constructs can be combined to form meaningful sentences. The specification also includes a way to represent the type of a construction, such as "I" for a personal pronoun, and a way to represent the meaning of a construction, such as "throw" for an action.
 18%|█▊        | 7/40 [00:59<04:38,  8.45s/it]2024-12-22 02:05:31,871 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:33,166 - [Process 1/5] - INFO - res.shape is :torch.Size([69])
results:During the meeting, the team discussed some issues from previous meetings, including the lack of clarity in the project scope, the need for more detailed design, and the challenge of balancing technical feasibility with user needs. They also touched on the issue of the project's budget and the need to prioritize certain features.
 18%|█▊        | 7/40 [01:00<04:19,  7.86s/it]2024-12-22 02:05:33,401 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:33,544 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:33,544 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 02:05:33,616 - [Process 2/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:2')
2024-12-22 02:05:33,636 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:33,636 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 02:05:33,707 - [Process 0/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:0')
2024-12-22 02:05:34,194 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:34,194 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 02:05:34,267 - [Process 3/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:3')
2024-12-22 02:05:35,451 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:35,451 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 02:05:35,523 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
2024-12-22 02:05:36,365 - [Process 2/5] - INFO - res.shape is :torch.Size([60])
results:The market found that the remote control is a difficult thing to explain to people who haven't seen one before, and there are a lot of plastic and wires inside. The market also found that the remote control is not too expensive to build and has a lot of small electronics.
 18%|█▊        | 7/40 [01:04<04:17,  7.80s/it]2024-12-22 02:05:36,767 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:36,809 - [Process 0/5] - INFO - res.shape is :torch.Size([68])
results:Based on the transcript, it is not clear what the professor thought about MSG as there is no direct mention of MSG in the conversation. However, the professor did mention that they are limited in time and have to prioritize their tasks, which may suggest that they have other things to focus on besides MSG.
 20%|██        | 8/40 [01:04<03:41,  6.94s/it]2024-12-22 02:05:37,004 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:37,004 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:37,004 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 02:05:37,079 - [Process 1/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:1')
2024-12-22 02:05:37,331 - [Process 3/5] - INFO - res.shape is :torch.Size([76])
results:PhD A thought that the results were interesting and mentioned that the gap between the different training sets was much smaller when using a multi-band belief-net structure with delta. They also mentioned that the numbers for English training on TIMIT were still better than the other languages, and that the net performed quite well when trained on a large dataset containing sentences from different languages.
 20%|██        | 8/40 [01:05<03:50,  7.21s/it]2024-12-22 02:05:37,456 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:39,944 - [Process 1/5] - INFO - res.shape is :torch.Size([62])
results:During the meeting, the group did not specifically discuss disposable income as a topic. However, they did mention that they want to focus on a specific target group for the remote control, such as people with disposable income, and that they want the remote control to be simple and easy to use.
 20%|██        | 8/40 [01:07<04:00,  7.52s/it]2024-12-22 02:05:40,179 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:40,405 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:40,406 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 02:05:40,481 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:05:40,487 - [Process 4/5] - INFO - res.shape is :torch.Size([111])
results:The Industrial Designer, during the meeting, expressed some confusion about the difference between the functional design and conceptual design, and mentioned that they were not sure what the specific features of the remote control should be, but they were open to the idea of making it "trendy, user-friendly, and original." They also mentioned that they liked the idea of using a menu-based system instead of buttons, which would make the remote control less cluttered. However, they did not express any strong opinions about fashion design specifically.
 20%|██        | 8/40 [01:08<04:34,  8.57s/it]2024-12-22 02:05:40,561 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:40,562 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2094])
2024-12-22 02:05:40,633 - [Process 0/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:0')
2024-12-22 02:05:40,732 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:41,069 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:41,069 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 02:05:41,144 - [Process 3/5] - DEBUG - predict_token:tensor([[4911]], device='cuda:3')
2024-12-22 02:05:43,287 - [Process 3/5] - INFO - res.shape is :torch.Size([53])
results:User Interface said that the interface concept design should be easy to use and recognizable, with big buttons and uh, round edges, and the logo at the bottom. They also suggested using different colors for each set of buttons to make it more recognizable.
 22%|██▎       | 9/40 [01:11<03:31,  6.82s/it]2024-12-22 02:05:43,445 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:43,578 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:During the meeting, the Marketing thought that buttons on the remote control were unnecessary and confusing, as most people only use a small percentage of the buttons on their remote controls. They suggested that instead of having multiple buttons, a menu-based system or a single button for accessing the menu would be more practical.
 22%|██▎       | 9/40 [01:11<03:33,  6.88s/it]2024-12-22 02:05:43,795 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:43,795 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 02:05:43,820 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:43,867 - [Process 1/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:1')
2024-12-22 02:05:44,319 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:44,320 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 02:05:44,393 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:05:44,803 - [Process 2/5] - INFO - res.shape is :torch.Size([96])
results:The query was about the reports on long-term care facilities, specifically the questioner, Mrs. Marilne Gill, asked about the government's approach to protecting employees and the economy, particularly in regions that rely heavily on a single industry, such as fishing. She also questioned the government's decision to provide emergency programs to certain sectors, while others, such as seasonal workers, are left with no assurances for their future.
 20%|██        | 8/40 [01:12<04:16,  8.00s/it]2024-12-22 02:05:45,092 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:46,153 - [Process 1/5] - INFO - res.shape is :torch.Size([50])
results:Based on the meeting transcript, there is no clear conclusion or agreement on the marketing strategy discussed. The group seems to be still in the process of brainstorming and discussing ideas, with no clear direction or plan established.
 22%|██▎       | 9/40 [01:13<03:40,  7.11s/it]2024-12-22 02:05:46,394 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:47,058 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:47,059 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 02:05:47,112 - [Process 4/5] - INFO - res.shape is :torch.Size([60])
results:During the meeting, the Industrial Designer and the User Interface had a disagreement when discussing the financial issue. The Industrial Designer believed that the problem was not just a financial issue but also a design issue, while the User Interface thought it was only a financial issue.
 22%|██▎       | 9/40 [01:14<04:06,  7.96s/it]2024-12-22 02:05:47,133 - [Process 3/5] - DEBUG - predict_token:tensor([[4485]], device='cuda:3')
2024-12-22 02:05:47,374 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:47,374 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 02:05:47,376 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:47,446 - [Process 0/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:0')
2024-12-22 02:05:48,102 - [Process 0/5] - INFO - res.shape is :torch.Size([15])
results:The professor did not mention anything about clustering in the transcript.
 25%|██▌       | 10/40 [01:15<03:04,  6.15s/it]2024-12-22 02:05:48,352 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:48,799 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:48,799 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1975])
2024-12-22 02:05:48,879 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:05:49,002 - [Process 3/5] - INFO - res.shape is :torch.Size([46])
results:Marketing's presentation is mainly about the latest trends and fashion updates in the industry, and how the new remote control design should be visually appealing and easy to use, with a focus on the younger public.
 25%|██▌       | 10/40 [01:16<03:14,  6.48s/it]2024-12-22 02:05:49,149 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:50,003 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:50,003 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 02:05:50,077 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:05:50,972 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:50,973 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 02:05:51,046 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:05:51,887 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:51,888 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:05:51,958 - [Process 0/5] - DEBUG - predict_token:tensor([[19295]], device='cuda:0')
2024-12-22 02:05:52,773 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:52,773 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 02:05:52,848 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:05:53,336 - [Process 1/5] - INFO - res.shape is :torch.Size([72])
results:During the meeting, the marketing expert suggested that they should aim at young people because they are the ones who are most likely to use remote controls and are open to new and innovative ideas. They also mentioned that young people are willing to pay more for a nice-looking remote control, which could be a potential revenue stream for the company.
 25%|██▌       | 10/40 [01:21<03:33,  7.13s/it]2024-12-22 02:05:53,640 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:53,918 - [Process 2/5] - INFO - res.shape is :torch.Size([115])
results:During the meeting, the committee discussed the issue of university mergers, with Kirsty Williams AM stating that the Welsh Government is aware of the situation regarding teacher training and is keeping an eye on it, but that it's not in a position to provide financial support for mergers at this time. She also mentioned that the UK Government has been approached to commission a QAA to publish a draft Bill on the matter, and that the Welsh Government is awaiting reports from an advisory board on the recruitment and retention of teaching staff.
 22%|██▎       | 9/40 [01:21<04:18,  8.35s/it]2024-12-22 02:05:54,320 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:54,961 - [Process 0/5] - INFO - res.shape is :torch.Size([67])
results:Grad F said that he is still working on his proposal and has been getting down and dirty into writing his proposal, and he hopes to give a full rough draft by Monday so he can give it to Morgan. He also mentioned that he is working in the cepstral domain and is not sure if it will be useful.
 28%|██▊       | 11/40 [01:22<03:04,  6.37s/it]2024-12-22 02:05:55,186 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:55,274 - [Process 3/5] - INFO - res.shape is :torch.Size([60])
results:The group disagreed with the Project Manager when reflecting on the previous meetings and their project progress because they felt that the Project Manager did not accurately capture the key points discussed during the meetings, leading to a disconnect in their understanding of the project's progress and goals.
 28%|██▊       | 11/40 [01:23<03:06,  6.41s/it]2024-12-22 02:05:55,385 - [Process 4/5] - INFO - res.shape is :torch.Size([97])
2024-12-22 02:05:55,387 - [Process 3/5] - INFO - len(per_windows_prompt):2
results:The group discusses the problem of dust in the control and how it affects the design of the remote. They agree that it's a problem that needs to be addressed, but they are not sure how to solve it. They consider different options, such as using a material that is easy to clean or designing the remote in a way that minimizes the amount of dust that accumulates. However, they do not come to a definitive solution in the meeting.
 25%|██▌       | 10/40 [01:23<04:01,  8.06s/it]2024-12-22 02:05:55,626 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:05:57,252 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:57,252 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 02:05:57,324 - [Process 1/5] - DEBUG - predict_token:tensor([[512]], device='cuda:1')
2024-12-22 02:05:57,911 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:57,912 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2022])
2024-12-22 02:05:57,985 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:05:58,791 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:58,791 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 02:05:58,866 - [Process 0/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:0')
2024-12-22 02:05:59,146 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:59,147 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2128])
2024-12-22 02:05:59,217 - [Process 3/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:3')
2024-12-22 02:05:59,236 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:05:59,236 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 02:05:59,310 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
2024-12-22 02:06:02,098 - [Process 4/5] - INFO - res.shape is :torch.Size([60])
results:During the meeting, the group discussed the wood case as one of the options for the key remote control, with the Industrial Designer mentioning that it was one of the options they were considering. However, no clear consensus was reached on whether to use the wood case or not.
 28%|██▊       | 11/40 [01:29<03:41,  7.65s/it]2024-12-22 02:06:02,396 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:02,966 - [Process 3/5] - INFO - res.shape is :torch.Size([93])
results:During the meeting, the team discussed the decision of buttons size and scale design for the remote control. They agreed that the buttons should be small and compact to fit in the user's hand comfortably, and the scale design should be simple and easy to use. The team also considered the user requirements specifications and aimed to design the remote control with a price of twenty-five euros and a production cost of twelve hundred fifty euros maximum.
 30%|███       | 12/40 [01:30<03:10,  6.80s/it]2024-12-22 02:06:03,089 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:03,198 - [Process 1/5] - INFO - res.shape is :torch.Size([132])
results:The early implementer areas for the Welsh Government's childcare offer have been introduced in some local authorities, with parents able to access 38 weeks of free childcare per year. However, there are challenges with the current approach, including bureaucracy and communication issues, and the early implementer areas are being used to learn lessons for a wider roll-out by 2020. The Welsh Government is considering integrating its offer with the UK Government's tax-free childcare scheme, but there are concerns about the complexity of the UK offer and the need for better communication with parents and providers.
 28%|██▊       | 11/40 [01:31<03:51,  7.97s/it]2024-12-22 02:06:03,460 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:03,556 - [Process 2/5] - INFO - res.shape is :torch.Size([123])
results:The debate in the transcript centers around a fundraising campaign for the "Canada Emergency Response Benefit" (CERB) and how the government is helping employees of the Liberal Party in preparation for the next election campaign, while communities in certain regions are struggling due to the pandemic. Ms. Marilne Gill expresses her dissatisfaction with the government's approach, stating that they are not protecting all jobs or sectors of the economy, and that the fundraising campaign is for the benefit of the Liberal Party, not the communities in need.
 25%|██▌       | 10/40 [01:31<04:22,  8.75s/it]2024-12-22 02:06:03,837 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:03,890 - [Process 0/5] - INFO - res.shape is :torch.Size([110])
results:During the meeting, the group discussed the possibility of incorporating speech recognition technology into the new remote control design. They mentioned that some people find it difficult to use the current remote controls due to their lack of ease of use and the fact that they are often lost in the home. The group also discussed the idea of implementing speech recognition functionality, where users could simply speak commands to the remote control instead of pressing buttons. However, they noted that this feature may not be suitable for people without hands or those who are not familiar with the technology.
 30%|███       | 12/40 [01:31<03:20,  7.15s/it]2024-12-22 02:06:04,168 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:06,004 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:06,005 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 02:06:06,077 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:06:06,708 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:06,708 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 02:06:06,783 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:06:07,139 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:07,139 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2080])
2024-12-22 02:06:07,212 - [Process 1/5] - DEBUG - predict_token:tensor([[2739]], device='cuda:1')
2024-12-22 02:06:07,496 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:07,497 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 02:06:07,569 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:06:07,727 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:07,727 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 02:06:07,797 - [Process 0/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:0')
2024-12-22 02:06:09,095 - [Process 4/5] - INFO - res.shape is :torch.Size([67])
results:The intensity of demands for the services in the early implementer areas is not spread across Wales because there are some economic issues and cultural issues that are affecting the demand for the services in some areas, such as a family tradition of providing free, unregistered childcare within families in some south Wales Valleys constituencies.
 30%|███       | 12/40 [01:36<03:28,  7.45s/it]2024-12-22 02:06:09,493 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:10,451 - [Process 3/5] - INFO - res.shape is :torch.Size([91])
results:The Project Manager elaborated on the members that the remote control should be made of plastic primarily, with a focus on presentation and aesthetics, and that too much metal in the design could cause interference with the remote's ability to send commands, and also cause it to behave unexpectedly by receiving false signals, and also mentioned that they should consider using modern types of polymers or plastics for aesthetic value.
 32%|███▎      | 13/40 [01:38<03:09,  7.01s/it]2024-12-22 02:06:10,712 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:10,718 - [Process 1/5] - INFO - res.shape is :torch.Size([78])
results:Julie Morgan elaborated on the online survey by stating that the Welsh Government had conducted an online survey to gather information on the evaluations of the legitimacy of children's rights, protection, and demands. The survey aimed to understand the public's perception of the Government's actions in relation to children's rights and protection during the pandemic.
 30%|███       | 12/40 [01:38<03:39,  7.83s/it]2024-12-22 02:06:10,926 - [Process 0/5] - INFO - res.shape is :torch.Size([68])
results:The two lessons that can be learned from early implementer areas are:


1. The administrative burden on local authorities in terms of bureaucracy and paperwork is high, and
2. Communication with parents and providers is crucial to ensure that they understand the offer and how to access it.
 32%|███▎      | 13/40 [01:38<03:12,  7.12s/it]2024-12-22 02:06:10,937 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:11,208 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:11,647 - [Process 2/5] - INFO - res.shape is :torch.Size([90])
results:The problem is that while the Welsh Government has a plan for surge capacity in paediatric care, there is a lack of assurance that all vulnerable children will have access to necessary technology to keep in contact with social workers and other key workers, and there is a concern that parents may not be engaging with the service due to fear and anxiety, which could result in the family not receiving adequate social support.
 28%|██▊       | 11/40 [01:39<04:07,  8.55s/it]2024-12-22 02:06:11,838 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:13,108 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:13,108 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 02:06:13,182 - [Process 4/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:4')
2024-12-22 02:06:14,336 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:14,336 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 02:06:14,411 - [Process 3/5] - DEBUG - predict_token:tensor([[350]], device='cuda:3')
2024-12-22 02:06:14,554 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:14,554 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 02:06:14,626 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:06:14,942 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:14,942 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1986])
2024-12-22 02:06:15,021 - [Process 0/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:0')
2024-12-22 02:06:15,393 - [Process 3/5] - INFO - res.shape is :torch.Size([24])
results:Bains did not say anything about the arts, culture, and tourism industry in the provided meeting transcript.
 35%|███▌      | 14/40 [01:43<02:46,  6.38s/it]2024-12-22 02:06:15,431 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:15,431 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2089])
2024-12-22 02:06:15,502 - [Process 2/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:2')
2024-12-22 02:06:15,529 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:17,889 - [Process 0/5] - INFO - res.shape is :torch.Size([63])
results:Based on the meeting transcript, the User Interface seemed to have mixed feelings about using titanium when discussing product cost and quotation. While they acknowledged that titanium is a good material for the product, they also expressed concerns about the cost and the potential impact on the quotation.
 35%|███▌      | 14/40 [01:45<03:03,  7.07s/it]2024-12-22 02:06:18,132 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:19,150 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:19,150 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:06:19,225 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:06:19,274 - [Process 2/5] - INFO - res.shape is :torch.Size([84])
results:During the meeting, the team discussed the functional design of the remote control, with a focus on identifying the specific functions the remote control needs to perform, such as turning on the TV, changing channels, and adjusting the volume. They also discussed the importance of keeping the design simple and intuitive, with a minimal number of buttons, and how the remote control should be easy to use for most people.
 30%|███       | 12/40 [01:47<03:51,  8.27s/it]2024-12-22 02:06:19,515 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:19,754 - [Process 1/5] - INFO - res.shape is :torch.Size([111])
results:During the meeting, the discussion centered around the desire for a remote control with a simple and modern design, with a focus on making it trendy and suitable for all ages. The group also discussed the importance of having the right buttons in the right places, as well as the need for a universal remote that can work with various brands of televisions. Additionally, they mentioned the possibility of using different colors for the remote, with a preference for black and silver, and the importance of saving any drawings or designs made on the remote.
 32%|███▎      | 13/40 [01:47<03:41,  8.20s/it]2024-12-22 02:06:19,988 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:21,715 - [Process 4/5] - INFO - res.shape is :torch.Size([190])
results:During the meeting, the opposition party member, Mrs. Marilne Gill, expressed her disappointment and frustration towards the government for not doing enough to help seasonal workers who have been laid off due to the pandemic. She stated that the government is only helping employees of the Liberal Party in preparation for the next election campaign, while communities in her region are dying because their economies revolve around a single industry. The Prime Minister, Hon. Bill Morneau, replied that the government's approach is based on consistent criteria and that they are using emergency programs to ensure employees are protected and maintain their relationship with their employer. He also mentioned that the emergency wage subsidy is meant for any sector of the economy where revenues have dropped by 30% or more, and the Canada emergency response benefit is meant to help employees who have lost their income due to COVID-19.
 32%|███▎      | 13/40 [01:49<04:03,  9.02s/it]2024-12-22 02:06:21,811 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:21,811 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1970])
2024-12-22 02:06:21,891 - [Process 0/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:0')
2024-12-22 02:06:21,945 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:23,102 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:23,102 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 02:06:23,175 - [Process 2/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:2')
2024-12-22 02:06:23,602 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:23,602 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 02:06:23,674 - [Process 1/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:1')
2024-12-22 02:06:24,067 - [Process 3/5] - INFO - res.shape is :torch.Size([120])
results:The meeting discussed the design and features of a new remote control for a television set-top box. The team debated on the design of the remote, including the use of a touch screen, buttons, and a case. They also discussed the possibility of using a solar battery or a lithium battery, and the team agreed to look into other manufacturers for the battery. The team also discussed the design of the circuit board and the use of visible light signaling. The meeting concluded with the team agreeing to work on multiple case colors and to look into marketing piloting.
 38%|███▊      | 15/40 [01:51<02:56,  7.08s/it]2024-12-22 02:06:24,325 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:25,611 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:25,611 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 02:06:25,683 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:06:25,993 - [Process 2/5] - INFO - res.shape is :torch.Size([61])
results:During the meeting, the group discussed the product evaluation and decided to give two points on usefulness because they wanted to recognize the product's ability to meet the needs of the users. They believed that the product was useful and practical, and therefore, deserved two points for its usefulness.
 32%|███▎      | 13/40 [01:53<03:30,  7.80s/it]2024-12-22 02:06:26,019 - [Process 1/5] - INFO - res.shape is :torch.Size([54])
results:During the meeting, team members discussed the color of the remote control, with one member suggesting pink as a color that would be more appropriate for young people. However, it is not specified which color was ultimately considered more appropriate or agreed upon by the team.
 35%|███▌      | 14/40 [01:53<03:17,  7.61s/it]2024-12-22 02:06:26,271 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:26,299 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:27,615 - [Process 0/5] - INFO - res.shape is :torch.Size([129])
results:During the meeting, the team discussed trend watching and appearance design for the new remote control product. They agreed to explore two options for the design of the remote control: a titanium-smelling fruit-shaped design and a spongy, curved design. The team also discussed the use of push buttons instead of liquid crystal display (LCD) for the user interface and the importance of keeping the design simple and user-friendly. Additionally, they talked about the possibility of using modelling clay to create a prototype of the design and the potential challenges of manufacturing double-curved shapes.
 38%|███▊      | 15/40 [01:55<03:16,  7.87s/it]2024-12-22 02:06:27,872 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:27,946 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:27,947 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 02:06:28,022 - [Process 3/5] - DEBUG - predict_token:tensor([[3879]], device='cuda:3')
2024-12-22 02:06:29,850 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:29,850 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2054])
2024-12-22 02:06:29,922 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:06:30,097 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:30,097 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 02:06:30,179 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:06:30,784 - [Process 4/5] - INFO - res.shape is :torch.Size([117])
results:The discussion focused on improving the back recognizer for the Aurora task by reducing the number of states in the LDA filters, trying simpler models, and looking at the frequency characteristic of the downsampling problem. They also discussed the idea of trying filters with shorter delays and taking into account the delay of the recursion for the mean estimation. Additionally, they mentioned that Hynek proposed again to his TRAPS to tune the LDA filters, and Grad E mentioned that Octave is a free software that can be used for batch mode running of Matlab commands.
 35%|███▌      | 14/40 [01:58<03:54,  9.03s/it]2024-12-22 02:06:30,994 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:31,423 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:31,423 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 02:06:31,453 - [Process 3/5] - INFO - res.shape is :torch.Size([85])
results:According to the transcript, Morneau stated that the government has put measures in place to detect fraud and that anyone who received money fraudulently will have to repay it. He also stated that the government is in a fiscal position that enables them to take on debt at this time, which they think is appropriate to get the economy to a better position at the end of this crisis.
 40%|████      | 16/40 [01:59<02:52,  7.17s/it]2024-12-22 02:06:31,495 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:06:31,632 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:32,472 - [Process 0/5] - INFO - res.shape is :torch.Size([21])
results:The professor did not mention anything about echoes and reverberation in the meeting transcript provided.
 40%|████      | 16/40 [02:00<02:47,  6.96s/it]2024-12-22 02:06:32,691 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:32,937 - [Process 1/5] - INFO - res.shape is :torch.Size([63])
results:The Industrial Designer disagreed to replace the titanium because they believed it was not necessary to do so, as the current design was already functional and met the requirements. They also mentioned that the cost of replacing the titanium would be too high, which was a concern for the project.
 38%|███▊      | 15/40 [02:00<03:05,  7.40s/it]2024-12-22 02:06:33,197 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:34,614 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:34,615 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2094])
2024-12-22 02:06:34,686 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:06:35,385 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:35,385 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1975])
2024-12-22 02:06:35,466 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:06:35,858 - [Process 4/5] - INFO - res.shape is :torch.Size([27])
results:During the meeting, no specific discussion or mention was made about battery design when discussing the functional design of the remote control.
 38%|███▊      | 15/40 [02:03<03:15,  7.84s/it]2024-12-22 02:06:36,113 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:36,253 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:36,253 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 02:06:36,326 - [Process 0/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:0')
2024-12-22 02:06:36,815 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:36,815 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 02:06:36,887 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:06:39,244 - [Process 2/5] - INFO - res.shape is :torch.Size([212])
results:The meeting discussed the impact of the COVID-19 pandemic on children and young people in Wales, with a focus on the challenges faced by the childcare sector. The Welsh Government has implemented various measures to support childcare providers, including the coronavirus childcare assistance scheme, which provides free access to childcare for critical workers and vulnerable children. The committee discussed the need for a bespoke grant package for providers that are falling between the cracks in the sector. The Minister for Health and Social Services, Vaughan Gething, explained that while children are less likely to be affected by COVID-19 than older people, they can still become unwell and that the Government is working to ensure that there is sufficient capacity in place to manage any increase in paediatric cases. The committee also discussed the impact of the pandemic on the NHS, including the use of virtual hearings in family courts and the need to reconsider the plan for surge capacity in paediatric care.
 35%|███▌      | 14/40 [02:07<04:05,  9.45s/it]2024-12-22 02:06:39,533 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:39,723 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:39,723 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 02:06:39,797 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:06:40,019 - [Process 3/5] - INFO - res.shape is :torch.Size([113])
results:During the meeting, the committee discussed the pay dispute situation in Wales, particularly regarding teacher training and the recruitment of teachers. The Cabinet Secretary for Education, Kirsty Williams, mentioned that the Welsh Government is aware of the issue and is taking steps to address it, including increasing financial incentives for graduates to pursue teaching and establishing a national ITE recruitment marketing exercise. The committee also discussed the issue of legalized cheating in universities and the need for a UK-wide approach to address it.
 42%|████▎     | 17/40 [02:07<02:54,  7.59s/it]2024-12-22 02:06:40,167 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:40,324 - [Process 0/5] - INFO - res.shape is :torch.Size([89])
results:During the meeting, the group discussed the appearance of the television remote, specifically the color scheme and design. They agreed that the remote should have a modern and sleek look, with a focus on plastic construction to reduce the risk of interference and improve aesthetics. They also discussed the possibility of including company colors and logos on the remote, and the importance of considering the user's preferences and expectations.
 42%|████▎     | 17/40 [02:08<02:46,  7.23s/it]2024-12-22 02:06:40,588 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:42,987 - [Process 1/5] - INFO - res.shape is :torch.Size([137])
results:During the meeting, Vaughan Gething AM mentioned that the Welsh Government has provided guidance on staying at home and away from others, and the government has also provided guidance on compliance with family court child arrangement orders. However, the government is still learning about the virus and its impact on children and young people, and they are still developing evidence about a Kawasaki-like syndrome. Despite these challenges, the government has planned for surge capacity in paediatric care and is looking at ways to flex up capacity. The government is also monitoring the situation and has had to think about how to communicate with people in a very different environment.
 40%|████      | 16/40 [02:10<03:16,  8.20s/it]2024-12-22 02:06:43,193 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:43,256 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:43,257 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 02:06:43,325 - [Process 2/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:2')
2024-12-22 02:06:43,797 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:43,797 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 02:06:43,872 - [Process 3/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:3')
2024-12-22 02:06:44,141 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:44,141 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2023])
2024-12-22 02:06:44,214 - [Process 0/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:0')
2024-12-22 02:06:45,145 - [Process 0/5] - INFO - res.shape is :torch.Size([20])
results:The group did not discuss Dave leaving, as he had already left before they started their meeting.
 45%|████▌     | 18/40 [02:13<02:23,  6.51s/it]2024-12-22 02:06:45,460 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:46,826 - [Process 4/5] - INFO - res.shape is :torch.Size([158])
results:The loss of funds to leave the European Union has had an impact on the Welsh higher education sector, with a deficit of £40 million compared to the English system. The Welsh Government has committed to delivering the Reid review recommendations, which include investing in research and innovation, and improving universities' ability to access funding from UKRI. The sector is facing financial challenges, including increased pension costs, and the impact of the demographic dip of 18-year-olds, but there are solutions such as the £15 million innovation funding to be provided in 2020-21, and the intention to find money to improve universities' ability to access funding from UKRI.
 40%|████      | 16/40 [02:14<03:30,  8.78s/it]2024-12-22 02:06:46,946 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:46,946 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2121])
2024-12-22 02:06:46,985 - [Process 2/5] - INFO - res.shape is :torch.Size([84])
results:During the meeting, it was decided that the equipment would be changed to include new microphones and wireless stuff. Specifically, Postdoc G suggested that they should use eight transcribers and there would be ten people in total, and they would have their own meeting to discuss the new equipment. Grad E also mentioned that they would be doing a new speaker form and would need to fill out a consent form.
 38%|███▊      | 15/40 [02:14<03:43,  8.93s/it]2024-12-22 02:06:47,014 - [Process 1/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:1')
2024-12-22 02:06:47,074 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:47,277 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:48,399 - [Process 3/5] - INFO - res.shape is :torch.Size([112])
results:Based on the transcript, the Project Manager seemed to be satisfied with their presentation contents, as they mentioned that they had established that the costs weren't within budget but could "do it {disfmarker}" and had a "remote with uh voice recognition" that could be used for the project. They also mentioned that they had evaluated the project based on the previous meeting's minutes and had developed a prototype for the team to look at. These statements suggest that the Project Manager was content with their presentation and progress on the project.
 45%|████▌     | 18/40 [02:16<02:52,  7.83s/it]2024-12-22 02:06:48,555 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:49,208 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:49,209 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 02:06:49,288 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:06:50,697 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:50,697 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2053])
2024-12-22 02:06:50,770 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:06:51,044 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:51,044 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 02:06:51,124 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:06:51,674 - [Process 1/5] - INFO - res.shape is :torch.Size([107])
results:During the meeting, the team discussed the design of a remote control for children, focusing on making it user-friendly, adaptable, and with a simple interface. They also discussed the importance of considering the age and abilities of the child, as well as the need for a button layout that is easy to use for both right- and left-handed individuals. The team also talked about the possibility of including a "smiling fish" button, which could be used to play a fun animation or sound.
 42%|████▎     | 17/40 [02:19<03:11,  8.35s/it]2024-12-22 02:06:51,954 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:52,305 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:52,305 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1984])
2024-12-22 02:06:52,386 - [Process 3/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:3')
2024-12-22 02:06:52,911 - [Process 4/5] - INFO - res.shape is :torch.Size([48])
results:Based on the meeting transcript, Marketing agreed with the group mates that the spin wheel with LCD display was a good idea and looked quite nice, and they also thought it was a good way to display the information.
 42%|████▎     | 17/40 [02:20<03:03,  7.97s/it]2024-12-22 02:06:53,107 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:54,541 - [Process 0/5] - INFO - res.shape is :torch.Size([117])
results:During the meeting, the group discussed the trend of teacher recruiting and the reasons for it, including the issue of filling the 300 priority places for secondary school teachers. They also discussed the steps they had taken, such as making sure that ITE provision is world-class, providing financial incentives, and embarking on a national ITE recruitment marketing exercise. Additionally, they mentioned that they are aware of the position of ColegauCymru regarding the pay dispute and are waiting for the outcome of the negotiations.
 48%|████▊     | 19/40 [02:22<02:34,  7.37s/it]2024-12-22 02:06:54,892 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:55,158 - [Process 2/5] - INFO - res.shape is :torch.Size([88])
results:During the meeting, the committee discussed the decision not to accredit the University of South Wales with the ability to deliver teacher training. The Cabinet Secretary for Education, Kirsty Williams, explained that the decision was made due to concerns about the university's ability to meet the standards required for teacher training. The committee also discussed the potential impact of this decision on the university and the students who were planning to study there.
 40%|████      | 16/40 [02:22<03:28,  8.70s/it]2024-12-22 02:06:55,255 - [Process 3/5] - INFO - res.shape is :torch.Size([71])
results:Based on the meeting transcript, the Industrial Designer and the Project Manager discussed the design of the LCD screen, including the shape, size, and color. They also talked about the logo and buttons on the screen, and the Industrial Designer showed the Project Manager and the Marketing team a design of the LCD screen.
 48%|████▊     | 19/40 [02:23<02:38,  7.54s/it]2024-12-22 02:06:55,367 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:55,404 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:06:55,556 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:55,557 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:06:55,631 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:06:56,725 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:56,725 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2061])
2024-12-22 02:06:56,797 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:06:58,459 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:58,459 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:06:58,532 - [Process 0/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:0')
2024-12-22 02:06:59,005 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:59,005 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 02:06:59,029 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:06:59,029 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2037])
2024-12-22 02:06:59,080 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:06:59,104 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:06:59,675 - [Process 4/5] - INFO - res.shape is :torch.Size([62])
results:Based on the meeting transcript, Marketing designed the product evaluation by creating a seven-point scale with criteria based on marketing strategy, latest trends, and user preferences. They also created a Word document with the evaluation criteria and asked the team to give a rating for each criterion.
 45%|████▌     | 18/40 [02:27<02:47,  7.61s/it]2024-12-22 02:06:59,974 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:00,356 - [Process 1/5] - INFO - res.shape is :torch.Size([105])
results:Dr David Blaney mentioned that the solution to risk appetite is not to take out cost, but to invest in the Welsh research base in order to compete and have a research base that's able to compete at a UK level. He also stated that the Welsh Government has established a presence in London in respect of research, and they have a colleague in HEFCW who is fractionally embedded in the United Kingdom Research and Innovation specifically to respond to the recommendation from Reid.
 45%|████▌     | 18/40 [02:28<03:05,  8.45s/it]2024-12-22 02:07:00,581 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:02,693 - [Process 3/5] - INFO - res.shape is :torch.Size([89])
results:The group reflected on their progress and discussed their thoughts on the meetings, including their evaluation of the project's costs, the development of a remote with voice recognition, and the design of a prototype. They also discussed the importance of considering the limitations and risks of their approach and the need for further evaluation and development. Additionally, they touched on the topic of their progress in terms of the project's overall goals and objectives.
 50%|█████     | 20/40 [02:30<02:30,  7.51s/it]2024-12-22 02:07:02,856 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:03,492 - [Process 2/5] - INFO - res.shape is :torch.Size([96])
results:The meeting discussed the design of a new remote control for a device, with a focus on the shape, size, and materials used. The industrial designer presented several options, including a hand-shaped design with buttons on the back, and the marketing team provided feedback on the design. The project manager also provided input and set deadlines for the next meeting. The meeting ended with a summary of the main points discussed and a reminder to check emails for further instructions.
 42%|████▎     | 17/40 [02:31<03:17,  8.59s/it]2024-12-22 02:07:03,669 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:03,669 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:07:03,743 - [Process 4/5] - DEBUG - predict_token:tensor([[379]], device='cuda:4')
2024-12-22 02:07:03,765 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:04,321 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:04,321 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 02:07:04,401 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:07:06,201 - [Process 0/5] - INFO - res.shape is :torch.Size([172])
results:During the meeting, there was a discussion on speech overlap, specifically how to handle it when multiple speakers overlap each other. Grad G mentioned that they had found a way to map the location to the disfmarker without having to give their names each time, and Professor E suggested that they could start by giving the transcript number and then sort of go around the room for each person to share their thoughts. Postdoc B also brought up the idea of paying subjects to come down and do some work on that, and PhD A suggested that they could exploit the subj human subject pool in the positive sense of the word. However, Grad G mentioned that they had already talked to some people at the Haas Business School who were interested in speech recognition but later decided they weren't interested in holding meetings downstairs.
 50%|█████     | 20/40 [02:34<02:53,  8.66s/it]2024-12-22 02:07:06,455 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:06,455 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2037])
2024-12-22 02:07:06,468 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:06,528 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:07:07,338 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:07,338 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:07:07,410 - [Process 2/5] - DEBUG - predict_token:tensor([[19295]], device='cuda:2')
2024-12-22 02:07:07,556 - [Process 1/5] - INFO - res.shape is :torch.Size([68])
results:The industrial designer recommended using push buttons instead of liquid crystal display (LCD) for the remote control's user interface, as it would be simpler and less expensive. They also suggested using titanium instead of rubber for the remote control's body, as it would be more durable and have a better look.
 48%|████▊     | 19/40 [02:35<02:49,  8.07s/it]2024-12-22 02:07:07,775 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:08,970 - [Process 4/5] - INFO - res.shape is :torch.Size([116])
results:Huw Irranca-Davies ensured the high qualification and quality of the workforce by learning lessons from the early implementer local authorities, expanding the offer to more areas, and phasing in the roll-out carefully. He also mentioned that they are only rolling out to areas where they need to learn a lesson about whether it's rurality or density of population, and they are getting HMRC to say whether parents are eligible for the offer, which requires them to set up an account with HMRC.
 48%|████▊     | 19/40 [02:36<02:50,  8.11s/it]2024-12-22 02:07:09,230 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:10,172 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:10,173 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 02:07:10,240 - [Process 0/5] - DEBUG - predict_token:tensor([[739]], device='cuda:0')
2024-12-22 02:07:11,006 - [Process 2/5] - INFO - res.shape is :torch.Size([83])
results:Grad F did not have a clear opinion on intermediate categories during the meeting. However, they mentioned that they were thinking about working with VTS in the cepstral domain and wanted to know if there was a feeling that the result would be useful. They also mentioned that they were not sure if applying VTS in the cepstral domain would work better than applying it in the filter bank domain.
 45%|████▌     | 18/40 [02:38<03:01,  8.27s/it]2024-12-22 02:07:11,283 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:11,383 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:11,383 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 02:07:11,457 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:07:12,413 - [Process 3/5] - INFO - res.shape is :torch.Size([146])
results:The meeting discussed the progress of a research project on speech recognition, with a focus on noise robustness. The team leader, Professor D, asked questions and provided feedback on the work done so far, including the use of different temporal properties and the training set. The team members, PhDs A and B, provided updates on their work, including feature combination experiments and the use of a large training set. The team also discussed the results of the experiments and the need for more machines to run the experiments. Grad C mentioned that their work on helping with data preparation is ongoing. The team leader, Professor D, encouraged the team to contribute to the meeting and provided instructions on how to proceed with the work.
 52%|█████▎    | 21/40 [02:40<02:35,  8.17s/it]2024-12-22 02:07:12,569 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:12,783 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:12,784 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1976])
2024-12-22 02:07:12,858 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:07:13,416 - [Process 0/5] - INFO - res.shape is :torch.Size([73])
results:It was decided that the speakers would wear their microphones higher up on their mouths, around the upper lip area, and that the gain wouldn't be too good. Also, the form for the speaker information would be created, and everyone would have to fill it out once, and the digit form would be filled out every time they speak.
 52%|█████▎    | 21/40 [02:41<02:36,  8.23s/it]2024-12-22 02:07:13,659 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:14,081 - [Process 1/5] - INFO - res.shape is :torch.Size([59])
results:The Project Manager agreed with Marketing when discussing his personal ideas towards the design because he believed that using plastic as the primary material for the remote control would be more cost-effective and practical, given the company's budget and the potential risks of using too much metal.
 50%|█████     | 20/40 [02:41<02:32,  7.61s/it]2024-12-22 02:07:14,322 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:14,925 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:14,925 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2083])
2024-12-22 02:07:14,999 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:07:16,195 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:16,195 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 02:07:16,270 - [Process 3/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:3')
2024-12-22 02:07:16,818 - [Process 4/5] - INFO - res.shape is :torch.Size([90])
results:During the meeting, the team discussed the product quotation for their latest design project. They reviewed the design and made suggestions for improvement. The team also discussed the costs and the need to put the fashion in electronics, but they couldn't due to budget constraints. They also talked about the logo, buttons, and the LCD screen, and one of the team members mentioned that they would like to make suggestions for the design.
 50%|█████     | 20/40 [02:44<02:40,  8.03s/it]2024-12-22 02:07:17,089 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:17,239 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:17,239 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:07:17,312 - [Process 0/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:0')
2024-12-22 02:07:17,941 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:17,942 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 02:07:18,016 - [Process 1/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:1')
2024-12-22 02:07:18,965 - [Process 2/5] - INFO - res.shape is :torch.Size([90])
results:During the meeting, the group did not discuss any evaluations on the increasing suicide crimes during the COVID-19 or the long-term special care for pregnant women. The discussion focused on the impact of the pandemic on health and social services, particularly in relation to childcare, and the Welsh Government's response to the crisis, including the provision of free childcare for critical workers and vulnerable children.
 48%|████▊     | 19/40 [02:46<02:51,  8.17s/it]2024-12-22 02:07:19,242 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:19,697 - [Process 3/5] - INFO - res.shape is :torch.Size([85])
results:
The three main causes for the financial challenges facing the universities are:

1. Reduction in EU funding, which has resulted in a financial impact on universities in Wales.
2. Increased pension costs and general increased costs.
3. Uncertainty surrounding the Augar review in England and Brexit, which may impact funding for Welsh universities.
 55%|█████▌    | 22/40 [02:47<02:22,  7.90s/it]2024-12-22 02:07:19,856 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:20,038 - [Process 0/5] - INFO - res.shape is :torch.Size([61])
results:According to the transcript, the industrial designer recommended using new materials, new colors, and a fancy strawberry design for the product appearance. They also suggested that the team should not go far from their titanium idea as it is a modern material that is practical and meets their needs.
 55%|█████▌    | 22/40 [02:47<02:19,  7.74s/it]2024-12-22 02:07:20,284 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:20,547 - [Process 1/5] - INFO - res.shape is :torch.Size([59])
results:Based on the meeting transcript, it appears that the final decision of the discussion was that automatic power control is not possible for remote controls, as it requires electromagnetic waves, which are already used for other devices, and people do not want to interfere with other devices.
 52%|█████▎    | 21/40 [02:48<02:18,  7.26s/it]2024-12-22 02:07:20,686 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:20,686 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:07:20,759 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:07:20,780 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:22,792 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:22,793 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1973])
2024-12-22 02:07:22,867 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:07:23,482 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:23,482 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 02:07:23,557 - [Process 3/5] - DEBUG - predict_token:tensor([[12157]], device='cuda:3')
2024-12-22 02:07:23,835 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:23,835 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 02:07:23,906 - [Process 0/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:0')
2024-12-22 02:07:24,316 - [Process 4/5] - INFO - res.shape is :torch.Size([83])
results:The discussion centered around future directions for the project, with the main points being to continue exploring different temporal properties, focusing on bringing in new features, and starting to work on a large dataset containing sentences from different languages. Additionally, there was a suggestion to try coupling the HMMs instead of having an arrow that flows from one sub-band to another, and to combine features and net outputs together.
 52%|█████▎    | 21/40 [02:52<02:29,  7.87s/it]2024-12-22 02:07:24,393 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:24,393 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 02:07:24,466 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:07:24,534 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:25,256 - [Process 3/5] - INFO - res.shape is :torch.Size([42])
results:Industrial Designer's presentation is mainly about the design of the remote control and its features, including the material it should be made of, the color and design, and the ease of use.
 57%|█████▊    | 23/40 [02:53<02:02,  7.20s/it]2024-12-22 02:07:25,408 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:28,019 - [Process 2/5] - INFO - res.shape is :torch.Size([114])
results:The meeting discussed the design of a new product, specifically the industrial designer's and user interface's (UI) presentation of their prototype, which was well-received by the project manager (PM). The PM also mentioned that the team will evaluate the design and make necessary changes before moving forward. The team also discussed the logo, buttons, and the layout of the product, with the PM emphasizing the importance of saving the design in a shared folder. The meeting ended with the PM expressing their satisfaction with the design and the team's work.
 50%|█████     | 20/40 [02:55<02:48,  8.44s/it]2024-12-22 02:07:28,158 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:28,158 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2102])
2024-12-22 02:07:28,231 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
2024-12-22 02:07:28,259 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:28,516 - [Process 1/5] - INFO - res.shape is :torch.Size([94])
results:During the meeting, the group discussed general requirements for the remote control, including the need for simplicity, ease of use, and a clear design. They also identified the components of the remote control and how they work together, and discussed the importance of incorporating color and the company's standard slogan into the design. The group also discussed the need to keep the design simple and easy to use, and to incorporate new functions as technology advances.
 55%|█████▌    | 22/40 [02:56<02:14,  7.48s/it]2024-12-22 02:07:28,840 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:28,984 - [Process 0/5] - INFO - res.shape is :torch.Size([112])
results:During the meeting, the team discussed the project's finances and evaluated whether the costs were within budget. They found that the costs were within budget, including a small amount left over for unexpected expenses. The team also evaluated the project's progress and found that it was well within budget, with no major problems with the project's process wheel. The discussion also touched on the issue of the remote control's design, with some team members expressing concerns about its size and shape, but ultimately deciding to proceed with the design.
 57%|█████▊    | 23/40 [02:56<02:17,  8.11s/it]2024-12-22 02:07:29,014 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:29,015 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 02:07:29,088 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:07:29,214 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:31,975 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:31,976 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1966])
2024-12-22 02:07:32,056 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:07:32,174 - [Process 4/5] - INFO - res.shape is :torch.Size([91])
results:During the meeting, the group members discussed the desired features of the remote control, including the buttons. They agreed that the buttons should be easy to use and have a simple design. The Industrial Designer mentioned that they should be placed in a logical order, and the Marketing member suggested that they should be made more trendy. However, there is no direct mention of the group members' thoughts on the buttons' appearance or functionality.
 55%|█████▌    | 22/40 [03:00<02:21,  7.87s/it]2024-12-22 02:07:32,536 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:32,587 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:32,587 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 02:07:32,667 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:07:32,789 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:32,789 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 02:07:32,862 - [Process 0/5] - DEBUG - predict_token:tensor([[12157]], device='cuda:0')
2024-12-22 02:07:33,847 - [Process 3/5] - INFO - res.shape is :torch.Size([118])
results:The meeting discussed the progress of a research project, specifically the development of a Wiener filter for speech recognition. The team leader, Professor B, provided an update on the project's status, mentioning that they have been working on the project for two years and have made some progress. The team also discussed the use of TIMIT and Schw - Switchboard datasets for training and testing the filter, and how they plan to use VAD to remove noise from the speech signals before applying the filter. Additionally, they talked about the possibility of combining different methods to improve the recognition accuracy.
 60%|██████    | 24/40 [03:01<02:01,  7.62s/it]2024-12-22 02:07:33,991 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:35,488 - [Process 0/5] - INFO - res.shape is :torch.Size([57])
results:Industrial Designer agreed with Marketing when discussing their presentations on the appearance design because they both acknowledged that they would stick with a traditional design for the buttons and use rubber for the outside case, which would be easy to use and familiar to most people.
 60%|██████    | 24/40 [03:03<02:01,  7.62s/it]2024-12-22 02:07:35,751 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:36,271 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:36,272 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1945])
2024-12-22 02:07:36,353 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
2024-12-22 02:07:36,651 - [Process 2/5] - INFO - res.shape is :torch.Size([103])
results:During the meeting, the team discussed the industrial components of the remote control, including the design of the device, the materials to be used, and the features to be included. They also discussed the possibility of using titanium instead of rubber for the body and the importance of keeping the design simple and user-friendly. Additionally, they talked about the use of push buttons instead of liquid crystal display (LCD) and the inclusion of speech recognition technology to make the remote control more convenient.
 52%|█████▎    | 21/40 [03:04<02:41,  8.50s/it]2024-12-22 02:07:36,863 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:37,622 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:37,622 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 02:07:37,697 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:07:39,343 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:39,344 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 02:07:39,417 - [Process 0/5] - DEBUG - predict_token:tensor([[8010]], device='cuda:0')
2024-12-22 02:07:39,855 - [Process 1/5] - INFO - res.shape is :torch.Size([162])
results:The meeting discussed the recruitment and retention of teachers in Wales, particularly in secondary schools. The Cabinet Secretary for Education, Kirsty Williams, provided an update on the situation, including the fact that the Welsh Government is aware of the issue and is taking steps to address it, such as increasing financial incentives for teachers and establishing a UK centre for academic integrity. The committee also discussed the problem of essay mills and the need for a UK-wide approach to addressing the issue. Additionally, the committee considered papers to note, including a letter from the Cabinet Secretary for Education regarding eligibility criteria for free school meals and a letter from the Minister for Children, Older People and Social Care on the Childcare Funding (Wales) Bill.
 57%|█████▊    | 23/40 [03:07<02:26,  8.64s/it]2024-12-22 02:07:40,142 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:40,474 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:40,474 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 02:07:40,546 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:07:41,418 - [Process 4/5] - INFO - res.shape is :torch.Size([112])
results:According to the transcript, the mental spaces would operate by throwing everything into a pot and trying to come up with the most likely construction, which would be computed through a combination of rules of inference and the coupling of multiple constructions. The mental spaces would also allow for the inclusion of formal considerations and specific meanings, as well as the ability to refer to any of the available elements and scope. Additionally, the mental spaces would operate based on the notion of generalized rules about the way things relate, which would be instantiated in each case.
 57%|█████▊    | 23/40 [03:09<02:20,  8.28s/it]2024-12-22 02:07:41,642 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:42,135 - [Process 3/5] - INFO - res.shape is :torch.Size([110])
results:The meeting attendees discussed and decided on several key remote control concepts, including using a simple battery, advanced chip, curved case with a folded design, rubber buttons, and voice recognition. They also considered various options for the remote's power source, such as solar energy, kinetic energy from a hand dynamo, or a simple battery. The attendees agreed to work on the look and feel design, user interface design, and product evaluation, and to use modelling clay to create a prototype.
 62%|██████▎   | 25/40 [03:09<01:57,  7.82s/it]2024-12-22 02:07:42,336 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:43,757 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:43,757 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 02:07:43,831 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:07:43,927 - [Process 0/5] - INFO - res.shape is :torch.Size([100])
results:Project Manager agreed with Marketing when assessing the criteria of matching the operating behaviour of the user because during the meeting, Marketing mentioned that the remote control's shape was a bit phallic, and Project Manager acknowledged this by stating that it was a "dual use" and "perfect" design, indicating that they shared the same opinion. This suggests that they were both concerned about the potential implications of the design and its resemblance to a human body part.
 62%|██████▎   | 25/40 [03:11<01:58,  7.87s/it]2024-12-22 02:07:44,190 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:44,354 - [Process 2/5] - INFO - res.shape is :torch.Size([83])
results:The Industrial Designer recommended that the team should come up with a fuzzy name for the remote control, which would be something like "leopard print" or "something cool" for the cold winter days. This recommendation was made to avoid asking or telling the team members directly what to name the remote control, instead, they should come up with a collective name that everyone can agree on.
 55%|█████▌    | 22/40 [03:12<02:28,  8.26s/it]2024-12-22 02:07:44,548 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:45,256 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:45,257 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 02:07:45,330 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
2024-12-22 02:07:45,969 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:45,969 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 02:07:46,044 - [Process 3/5] - DEBUG - predict_token:tensor([[1551]], device='cuda:3')
2024-12-22 02:07:47,752 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:47,753 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 02:07:47,826 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:07:48,156 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:48,156 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 02:07:48,228 - [Process 2/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:2')
2024-12-22 02:07:48,677 - [Process 4/5] - INFO - res.shape is :torch.Size([72])
results:From the meeting transcript, it is evident that the marketing learned that users often use the "zap" function frequently, and they would like to have an easy way to control it. Additionally, users are frustrated with the current remote controls as they are often lost, and users find it difficult to use the buttons on the remote controls.
 60%|██████    | 24/40 [03:16<02:07,  7.97s/it]2024-12-22 02:07:48,939 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:49,472 - [Process 3/5] - INFO - res.shape is :torch.Size([85])
results:On the sample transcripts, it was agreed that they would record an extra channel of a distant mike for new meetings. They would also ask people to give their names before starting the meeting, and they would try to exploit the subject human subject pool in a positive sense. Additionally, they would pay subjects for their participation and discuss potential collaboration with Jane and Lokendra about some work they had been doing.
 65%|██████▌   | 26/40 [03:17<01:47,  7.67s/it]2024-12-22 02:07:49,705 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:49,882 - [Process 1/5] - INFO - res.shape is :torch.Size([135])
results:During the presentation and discussion, the technical design team discussed the design of the remote control for a new television set-top box. They agreed on the placement of buttons, including a menu button, channel up and down buttons, and a button for going directly to the video channel. They also discussed the importance of incorporating buttons for channel setting, especially for new TVs, and the need for a recognizable button for the most important functions. Additionally, they talked about the design of the remote control, including the use of hard plastic with rubber, and the labelling of buttons to make them indestructible and recognizable at all times.
 60%|██████    | 24/40 [03:17<02:24,  9.05s/it]2024-12-22 02:07:50,305 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:51,191 - [Process 0/5] - INFO - res.shape is :torch.Size([74])
results:The meeting discussed the progress of the project, including the prototype, technical specifications, and budget. The team also evaluated the project and decided to celebrate the success. The program manager proposed the celebration and suggested going to an Italian restaurant. The team agreed to go out after the meeting and decided to give the remote control a nickname, "Bobby."
 65%|██████▌   | 26/40 [03:19<01:47,  7.69s/it]2024-12-22 02:07:51,418 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:51,479 - [Process 2/5] - INFO - res.shape is :torch.Size([71])
results:The team seemed to be unsure about the new idea of Industrial Designer concerning the cover of the device, as they discussed the pros and cons of using a single battery versus two batteries, and the impact of the size of the remote control on the cost and ease of use. However, no clear decision was made regarding the cover design.
 57%|█████▊    | 23/40 [03:19<02:14,  7.92s/it]2024-12-22 02:07:51,737 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:52,548 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:52,548 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 02:07:52,622 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:07:53,391 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:53,391 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 02:07:53,468 - [Process 3/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:3')
2024-12-22 02:07:53,920 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:53,920 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 02:07:53,993 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:07:54,988 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:54,988 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 02:07:55,061 - [Process 0/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:0')
2024-12-22 02:07:55,334 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:07:55,335 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 02:07:55,408 - [Process 2/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:2')
2024-12-22 02:07:56,535 - [Process 4/5] - INFO - res.shape is :torch.Size([87])
results:The meeting discussed the development of a new remote control for a television, with a focus on its design and functionality. The team reviewed the prototype and provided feedback, with a particular emphasis on the use of filler words and disfluencies in the conversation. The meeting also touched on the history of the internet and its impact on society, as well as the potential for the remote control to be used in various settings.
 62%|██████▎   | 25/40 [03:24<01:59,  7.94s/it]2024-12-22 02:07:56,837 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:57,063 - [Process 1/5] - INFO - res.shape is :torch.Size([67])
results:The Minister of Seniors, Mr. Yves-François Blanchet, mentioned that the government is providing support to seniors through various programs, including the Canada Emergency Response Benefit, and that they will continue to work towards providing more balanced support for the entire Canadian economy, not just big corporations.
 62%|██████▎   | 25/40 [03:24<02:07,  8.49s/it]2024-12-22 02:07:57,276 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:57,884 - [Process 0/5] - INFO - res.shape is :torch.Size([62])
results:During the meeting, Marketing discussed market trend findings that younger people prefer soft mat materials and primary colors, such as yellow and red, for remote controls. They also mentioned that the shapes of the design should be curved and round, like a hand, to make it easier to use.
 68%|██████▊   | 27/40 [03:25<01:36,  7.39s/it]2024-12-22 02:07:58,069 - [Process 2/5] - INFO - res.shape is :torch.Size([58])
results:Based on the conversation, Project Manager seemed to think that the manual design was not very user-friendly as they mentioned that it would be "bit uh waste" and "too complicated" and that they would have to "design it later" (emphasis added).
 60%|██████    | 24/40 [03:25<02:00,  7.52s/it]2024-12-22 02:07:58,278 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:58,350 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:07:58,478 - [Process 3/5] - INFO - res.shape is :torch.Size([124])
results:During the debate, Ms. Jansen raised concerns about the government's decision to give money to companies that did not use it properly, citing the example of the $252 billion deficit predicted by the PBO and the fact that the government's debt is set to hit $1 trillion after five years. She also mentioned that many industries in Canada are either closed or struggling, and that Canadian workers need a prime minister who will support the energy sector, get natural resources and agriculture products to market, and make taxes encourage job creation and growth.
 68%|██████▊   | 27/40 [03:26<01:44,  8.07s/it]2024-12-22 02:07:58,620 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:00,462 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:00,462 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 02:08:00,534 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:08:00,892 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:00,892 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 02:08:00,965 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:08:01,441 - [Process 4/5] - INFO - res.shape is :torch.Size([20])
results:B did not express any opinion about the experimental setup of the computer navigation system during the meeting.
 65%|██████▌   | 26/40 [03:29<01:38,  7.03s/it]2024-12-22 02:08:01,826 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:01,970 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:01,971 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1945])
2024-12-22 02:08:02,011 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:02,011 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:08:02,050 - [Process 0/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:0')
2024-12-22 02:08:02,087 - [Process 2/5] - DEBUG - predict_token:tensor([[4911]], device='cuda:2')
2024-12-22 02:08:02,245 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:02,245 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 02:08:02,320 - [Process 3/5] - DEBUG - predict_token:tensor([[8010]], device='cuda:3')
2024-12-22 02:08:04,661 - [Process 3/5] - INFO - res.shape is :torch.Size([58])
results:Project Manager thought of the advanced technology design when presenting the technical functions because he wanted to keep the design simple and clear, as per the preferences of Industrial Designer, and also to incorporate the company's standard color and slogan in the remote control design.
 70%|███████   | 28/40 [03:32<01:30,  7.51s/it]2024-12-22 02:08:04,818 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:05,117 - [Process 2/5] - INFO - res.shape is :torch.Size([66])
results:User Interface thought they should use more buttons because they believed it would be easier for users to go directly to the video channel from any channel by pressing a single button, rather than having to navigate through menus. They also thought it would be more recognizable and familiar for users to have a dedicated button for the video channel.
 62%|██████▎   | 25/40 [03:32<01:50,  7.38s/it]2024-12-22 02:08:05,209 - [Process 1/5] - INFO - res.shape is :torch.Size([98])
results:The Project Manager seemed to be concerned about the profit when discussing the financial issue, as he mentioned that selling a $40 remote control would be expensive and that they might need to trade off some features for a lower price without getting into the whole "go down to bargain store remote" situation. This suggests that he was thinking about the financial aspect of the project and trying to find a balance between making a profitable product and keeping the cost reasonable for the customers.
 65%|██████▌   | 26/40 [03:33<01:57,  8.39s/it]2024-12-22 02:08:05,365 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:05,435 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:05,577 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:05,577 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1950])
2024-12-22 02:08:05,658 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
2024-12-22 02:08:07,481 - [Process 0/5] - INFO - res.shape is :torch.Size([122])
results:It was concluded that semantic specification is a crucial aspect of the project, and the group discussed the idea of using PRMs (Probabilistic Relational Models) to specify the semantics of the ontology. They also talked about the importance of having a clear and consistent notation system for representing the semantics of the ontology, and how the PRMs could be unfolded into a straightforward Bayes net. Additionally, they discussed the possibility of using generalized rules to specify the relations between the constructs in the PRMs, and how these rules could be instantiated in each case.
 70%|███████   | 28/40 [03:35<01:36,  8.05s/it]2024-12-22 02:08:07,746 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:08,443 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:08,443 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 02:08:08,518 - [Process 3/5] - DEBUG - predict_token:tensor([[19295]], device='cuda:3')
2024-12-22 02:08:08,973 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:08,973 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 02:08:09,047 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:08:09,050 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:09,050 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 02:08:09,124 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:08:09,780 - [Process 3/5] - INFO - res.shape is :torch.Size([31])
results:Grad E did not contribute to the discussion on storage disks as the transcript does not mention anything about Grad E's input on the topic.
 72%|███████▎  | 29/40 [03:37<01:14,  6.79s/it]2024-12-22 02:08:09,906 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:11,166 - [Process 4/5] - INFO - res.shape is :torch.Size([125])
results:The description of the domains in the mental spaces is not explicitly mentioned in the transcript. However, based on the discussion, it can be inferred that the domains in the mental spaces are related to the formal pole of the construal, which refers to the specific meaning or interpretation of a particular construction or phrase. The mental spaces may also involve the ontology of the domain, which is the collection of concepts and categories that are relevant to the particular domain of discourse. Additionally, the mental spaces may include the context-dependent modules, which are the specific rules and constraints that apply to a particular situation or context.
 68%|██████▊   | 27/40 [03:39<01:41,  7.84s/it]2024-12-22 02:08:11,311 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:11,312 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 02:08:11,380 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:11,384 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:08:11,857 - [Process 2/5] - INFO - res.shape is :torch.Size([65])
results:During the meeting, the industrial designer presented on the working design of the remote control, focusing on identifying the basic components and how they work together. The project manager also discussed technical functions and functional requirements, highlighting the need to keep the design simple and clear while incorporating new features such as voice recognition.
 65%|██████▌   | 26/40 [03:39<01:40,  7.19s/it]2024-12-22 02:08:12,137 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:12,331 - [Process 1/5] - INFO - res.shape is :torch.Size([71])
results:During the meeting, the user interface designer recommended incorporating a feature that would allow users to easily locate their remote control by using a whistle or clap command. They also suggested that the remote control should be designed with a small display screen to show the channel number or other information, making it easier for users to navigate through the menu.
 68%|██████▊   | 27/40 [03:40<01:44,  8.01s/it]2024-12-22 02:08:12,544 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:13,540 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:13,540 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2061])
2024-12-22 02:08:13,613 - [Process 3/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:3')
2024-12-22 02:08:15,003 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:15,003 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2059])
2024-12-22 02:08:15,076 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:08:15,751 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:15,752 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 02:08:15,824 - [Process 2/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:2')
2024-12-22 02:08:16,148 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:16,148 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 02:08:16,178 - [Process 0/5] - INFO - res.shape is :torch.Size([111])
results:The meeting discussed the transcription status of various meetings, with a focus on the Cambridge system and the SRI system, and how they both train postiors. They also talked about the size of the net and how many hours of training data are needed for Jane's transcribers. Additionally, they discussed the logistics of sending data to Munich for transcription and the potential for gender-dependent training. Finally, they touched on the idea of participant approval and the use of web pages to keep track of transcript status.
 72%|███████▎  | 29/40 [03:44<01:30,  8.25s/it]2024-12-22 02:08:16,220 - [Process 1/5] - DEBUG - predict_token:tensor([[450]], device='cuda:1')
2024-12-22 02:08:16,424 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:16,763 - [Process 3/5] - INFO - res.shape is :torch.Size([78])
results:During the meeting, the group discussed the functions of the remote control, including the idea of adding a touch screen feature, multi-format capabilities, and the possibility of using a pen to navigate through the menu. They also talked about the importance of keeping the remote control design sleek and slick, and the need to consider the user experience and balance the features with the price.
 75%|███████▌  | 30/40 [03:44<01:08,  6.85s/it]2024-12-22 02:08:16,910 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:19,509 - [Process 4/5] - INFO - res.shape is :torch.Size([98])
results:The meeting discussed the design and evaluation of a new product, specifically a remote control, with a focus on its technical innovation and user experience. The team evaluated the product based on seven criteria and gave ratings ranging from 1 to 7. The project manager and marketing team agreed on a final rating of 3, indicating that the product was technically innovative but lacked new features. The team also discussed the importance of teamwork and communication in the design process.
 70%|███████   | 28/40 [03:47<01:35,  7.99s/it]2024-12-22 02:08:19,748 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:19,924 - [Process 2/5] - INFO - res.shape is :torch.Size([95])
results:The transcript does not contain any direct evaluations or statements on the legitimacy of children's rights, protection, or demands. However, it is evident that the Welsh Government is working to ensure that childcare settings remain open during the pandemic, and they are providing support to vulnerable children and families. The government is also monitoring the situation regarding parents not taking their children to healthcare services for other conditions while the pandemic is ongoing.
 68%|██████▊   | 27/40 [03:47<01:36,  7.45s/it]2024-12-22 02:08:20,048 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:20,048 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 02:08:20,124 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:08:20,187 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:20,527 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:20,527 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 02:08:20,602 - [Process 3/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:3')
2024-12-22 02:08:21,742 - [Process 1/5] - INFO - res.shape is :torch.Size([123])
results:The meeting discussed the design and development of a new remote control for a television, with a focus on making it sleek, user-friendly, and fashionable. The team brainstormed ideas for the remote's design, including using a touch screen and adding a pen for note-taking. They also discussed the importance of considering the target audience and their needs, as well as the potential challenges of marketing and selling the product. The team agreed to create a detailed design document and to keep the project's goals and objectives in mind as they worked on the design.
 70%|███████   | 28/40 [03:49<01:41,  8.43s/it]2024-12-22 02:08:21,989 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:22,948 - [Process 3/5] - INFO - res.shape is :torch.Size([58])
results:During the discussion, the team evaluated the product by presenting the prototype, analyzing the technical specifications, and discussing the budget and cost. They also evaluated the product's performance, user experience, and design, and decided to celebrate the project's success.
 78%|███████▊  | 31/40 [03:50<00:59,  6.65s/it]2024-12-22 02:08:23,107 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:23,497 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:23,497 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 02:08:23,578 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:08:23,800 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:23,801 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 02:08:23,875 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:08:24,702 - [Process 0/5] - INFO - res.shape is :torch.Size([106])
results:During the meeting, the team discussed the technical function design for a new remote control, focusing on user preferences and needs. They identified several issues with current remote controls, such as their ugliness, lack of ease of use, and limited functionality. The team also discussed the possibility of incorporating speech recognition technology and the importance of making the remote control easy to use and find. They agreed to conduct further research and consider incorporating features such as clapping or whistling to locate the remote control.
 75%|███████▌  | 30/40 [03:52<01:23,  8.33s/it]2024-12-22 02:08:25,055 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:25,609 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:25,609 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2051])
2024-12-22 02:08:25,683 - [Process 1/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:1')
2024-12-22 02:08:26,334 - [Process 4/5] - INFO - res.shape is :torch.Size([63])
results:The marketing expert seems to be open to the idea of user identification, but also highlights the importance of keeping the design simple and user-friendly. They also mention that people tend to change their TVs more often than remote controls, so the design should prioritize ease of use and functionality.
 72%|███████▎  | 29/40 [03:54<01:24,  7.64s/it]2024-12-22 02:08:26,731 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:26,736 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:26,736 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 02:08:26,811 - [Process 3/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:3')
2024-12-22 02:08:28,387 - [Process 2/5] - INFO - res.shape is :torch.Size([104])
results:The discussion centered around the issue of storage disks and computer connections, with the group discussing the possibility of using a different feature set in a system and the potential benefits of using a different front-end. They also talked about the Cambridge system and their system, which both use training posters, and how they are coming from alignments from the SRI system. Additionally, they discussed the idea of embedded training and how it could potentially improve the system, as well as the issue of gender-dependent training.
 70%|███████   | 28/40 [03:56<01:33,  7.75s/it]2024-12-22 02:08:28,581 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:28,634 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:28,634 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 02:08:28,708 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:08:29,760 - [Process 3/5] - INFO - res.shape is :torch.Size([73])
results:Based on the meeting transcript, the group has not made any specific decisions on LSD screens, colors, or materials. They discuss the importance of having a touch-pad on the remote control and the need for it to be fancy looking and good feeling, but they do not provide any specific details on the screen, color, or materials used.
 80%|████████  | 32/40 [03:57<00:53,  6.70s/it]2024-12-22 02:08:29,876 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:30,292 - [Process 1/5] - INFO - res.shape is :torch.Size([106])
results:Based on the meeting transcript, it appears that there is no clear conclusion or agreement on the topic of intuitive when presenting on evaluation criteria. The discussion seems to have focused on the design of the remote control and the user interface, with some team members expressing their opinions on the shape and design of the remote control, and the need for a patent. There is no clear mention of intuitive when presenting on evaluation criteria, and it seems that the topic was not a major focus of the discussion.
 72%|███████▎  | 29/40 [03:58<01:33,  8.47s/it]2024-12-22 02:08:30,413 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:30,413 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 02:08:30,489 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:08:30,598 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:32,188 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:32,188 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 02:08:32,260 - [Process 2/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:2')
2024-12-22 02:08:33,300 - [Process 0/5] - INFO - res.shape is :torch.Size([106])
results:The main discussion points of the meeting were related to recording meetings, specifically, finding a way to record meetings without having to restart the recording, and the possibility of using the State of California as a source of subjects for speech recognition research. The group also discussed the idea of exploiting the human subject pool on campus for research purposes and the potential for collaboration with the Haas Business School. Additionally, they touched on the topic of inference structures and the need to build in mechanisms for understanding language in speech recognition research.
 78%|███████▊  | 31/40 [04:01<01:15,  8.41s/it]2024-12-22 02:08:33,516 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:33,516 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 02:08:33,589 - [Process 3/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:3')
2024-12-22 02:08:33,600 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:34,345 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:34,345 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1976])
2024-12-22 02:08:34,426 - [Process 1/5] - DEBUG - predict_token:tensor([[476]], device='cuda:1')
2024-12-22 02:08:34,579 - [Process 4/5] - INFO - res.shape is :torch.Size([95])
results:The government was going to roll out a policy to use consistent criteria to help all employees and all Canadians in any sector of the economy struggling during the pandemic. The emergency wage subsidy program was designed to ensure employees are protected and maintain their relationship with their employer, while the Canada emergency response benefit would provide financial support to people who don't have a job or who don't know if they'll even be working this summer.
 75%|███████▌  | 30/40 [04:02<01:18,  7.82s/it]2024-12-22 02:08:34,779 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:35,013 - [Process 3/5] - INFO - res.shape is :torch.Size([35])
results:During the meeting, no specific target price, target cost, or target profit of the remote control was mentioned or discussed when discussing the design of the remote control.
 82%|████████▎ | 33/40 [04:02<00:43,  6.26s/it]2024-12-22 02:08:35,135 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:37,186 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:37,187 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2059])
2024-12-22 02:08:37,258 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:08:37,982 - [Process 2/5] - INFO - res.shape is :torch.Size([131])
results:Industrial Designer mentioned that the device has a lot of small electronics, plastic, and wires inside, making it difficult to explain to people who haven't seen the inside of a remote control before. He also mentioned that the device has a chip on top with numbers, teletext button, and a battery, and when you push the button, it will send a signal to the chip, which will then send it to the sub-components. Additionally, he suggested that the device should have a menu button that will take you to the menu, and you can use the same buttons to scroll through the menu and change options.
 72%|███████▎  | 29/40 [04:05<01:31,  8.31s/it]2024-12-22 02:08:38,138 - [Process 1/5] - INFO - res.shape is :torch.Size([86])
results:During the meeting, Kirsty Williams, the Cabinet Secretary for Education, mentioned that she is aware of the issue of university mergers and that the Welsh Government is keeping a close eye on it. She also mentioned that the UK Government has not provided any financial support for the mergers, and that the Welsh Government is waiting to see what the outcome of the negotiations will be before responding formally.
 75%|███████▌  | 30/40 [04:05<01:22,  8.28s/it]2024-12-22 02:08:38,276 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:38,541 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:38,541 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2119])
2024-12-22 02:08:38,562 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:38,610 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:08:38,944 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:38,944 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 02:08:39,026 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:08:41,616 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:The Project Manager put forward to cut down the functions of the remote control because the board had specified that the remote control should only have television functions and no other features, such as DVD or teletext, and also because the board had specified that the remote control should have a simple and easy-to-use design.
 85%|████████▌ | 34/40 [04:09<00:38,  6.37s/it]2024-12-22 02:08:41,753 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:41,858 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:41,858 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 02:08:41,930 - [Process 2/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:2')
2024-12-22 02:08:42,036 - [Process 4/5] - INFO - res.shape is :torch.Size([80])
results:During the discussion, the team decided to prioritize function design for the remote control, focusing on making it original, trendy, and user-friendly. They also agreed to keep the design simple and adaptable, with two switches for easy use. Additionally, they decided to include a whiteboard for brainstorming and to make it controlled by a brain.
 78%|███████▊  | 31/40 [04:09<01:09,  7.71s/it]2024-12-22 02:08:42,180 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:42,180 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 02:08:42,253 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:42,254 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:08:43,252 - [Process 1/5] - INFO - res.shape is :torch.Size([23])
results:The Prime Minister did not mention anything about fraud in the distribution of funds during the meeting transcript provided.
 78%|███████▊  | 31/40 [04:11<01:05,  7.33s/it]2024-12-22 02:08:43,511 - [Process 0/5] - INFO - res.shape is :torch.Size([143])
results:During the discussion, the Minister, Huw Irranca-Davies, mentioned that the Welsh Government's childcare offer will have no substantial impact on net income, poverty, or work behavior for families with children. He also stated that the early implementer local authorities have seen examples of parents making savings and increasing disposable income, and some parents are able to spend more time with their children due to the available childcare provision. However, there are challenges with the current approach, including bureaucracy and communication issues, and the administrative burden on local authorities is falling to each pilot area. The Minister did not mention anything about hourly rates of the service.
 80%|████████  | 32/40 [04:11<01:11,  8.95s/it]2024-12-22 02:08:43,528 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:43,831 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:45,450 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:45,450 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 02:08:45,471 - [Process 2/5] - INFO - res.shape is :torch.Size([80])
results:Based on the conversation in the transcript, it seems that the Professor was generally pleased with the results of the experiments conducted by the PhD student, particularly the improvement in recognition accuracy when using a combination of features and MLP outputs. However, the Professor also mentioned that the gap between the different training sets is still quite large, and that they will need to continue working on this issue.
 75%|███████▌  | 30/40 [04:13<01:20,  8.06s/it]2024-12-22 02:08:45,523 - [Process 3/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:3')
2024-12-22 02:08:45,696 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:45,873 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:45,874 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2093])
2024-12-22 02:08:45,946 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:08:47,149 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:47,149 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 02:08:47,221 - [Process 1/5] - DEBUG - predict_token:tensor([[23032]], device='cuda:1')
2024-12-22 02:08:47,568 - [Process 3/5] - INFO - res.shape is :torch.Size([47])
results:PhD B approached the back-end of the Aurora recognizer by sharing the state of the center state of the silence model, which doesn't require its own training data, and instead, it just shares that state.
 88%|████████▊ | 35/40 [04:15<00:31,  6.24s/it]2024-12-22 02:08:47,584 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:47,584 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 02:08:47,665 - [Process 0/5] - DEBUG - predict_token:tensor([[476]], device='cuda:0')
2024-12-22 02:08:47,969 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:48,810 - [Process 1/5] - INFO - res.shape is :torch.Size([36])
results:Barry Hugh Irranca-Davies AM did not mention anything about the registration of grandparents when talking about childcare offer during the meeting transcript provided.
 80%|████████  | 32/40 [04:16<00:54,  6.80s/it]2024-12-22 02:08:48,829 - [Process 0/5] - INFO - res.shape is :torch.Size([27])
results:Kirsty Williams AM did not mention anything about the University of South Wales or its ability to deliver teacher training during the meeting.
 82%|████████▎ | 33/40 [04:16<00:55,  7.86s/it]2024-12-22 02:08:49,051 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:49,085 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:49,298 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:49,298 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 02:08:49,370 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:08:50,449 - [Process 4/5] - INFO - res.shape is :torch.Size([111])
results:The group plans to design a new television remote control with a universal design that can work with various brands and models of TVs. They will make the remote control modern, trendy, and easy to use, with buttons placed in a convenient location. The group will also investigate the possibility of adding a front panel to the remote control to make it more aesthetically pleasing. Additionally, they will look into the technical design and form of the remote control, and save any drawings or designs made during the meeting for future reference.
 80%|████████  | 32/40 [04:18<01:03,  7.92s/it]2024-12-22 02:08:50,576 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:51,613 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:51,613 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 02:08:51,688 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:08:52,665 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:52,665 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 02:08:52,737 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:52,737 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 02:08:52,739 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:08:52,810 - [Process 0/5] - DEBUG - predict_token:tensor([[478]], device='cuda:0')
2024-12-22 02:08:53,791 - [Process 2/5] - INFO - res.shape is :torch.Size([102])
results:The meeting discussed the tuning of LDA models for speech recognition, with a focus on the length of the filters and the downsampling problem. The group discussed the idea of trying shorter filters to simplify the system and improve performance. They also talked about the on-line normalization and the delay of the recursion for the mean estimation, which Hynek suggested trying but did not provide a specific plan. The group also discussed the coordination between the two sides and how to better communicate and work together.
 78%|███████▊  | 31/40 [04:21<01:13,  8.14s/it]2024-12-22 02:08:54,053 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:54,189 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:54,189 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 02:08:54,264 - [Process 4/5] - DEBUG - predict_token:tensor([[4911]], device='cuda:4')
2024-12-22 02:08:54,324 - [Process 1/5] - INFO - res.shape is :torch.Size([34])
results:According to the transcript, the user interface designer recommended using a {disfmarker} (a fancy strawberry design) for the product quotation.
 82%|████████▎ | 33/40 [04:22<00:44,  6.41s/it]2024-12-22 02:08:54,518 - [Process 0/5] - INFO - res.shape is :torch.Size([38])
results:Vaughan Gething did not mention anything about a mental health recovery plan when discussing the ways of continuing mental health services during the lockdown in the provided meeting transcript.
 85%|████████▌ | 34/40 [04:22<00:43,  7.21s/it]2024-12-22 02:08:54,600 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:54,796 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:55,855 - [Process 4/5] - INFO - res.shape is :torch.Size([39])
results:User Interface replied that the icon design on the buttons would be uh, yellow, round edges, and the logo at the bottom, and each set of buttons would have its own color.
 82%|████████▎ | 33/40 [04:23<00:50,  7.17s/it]2024-12-22 02:08:56,013 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:57,253 - [Process 3/5] - INFO - res.shape is :torch.Size([127])
results:During the meeting, the Hon. Bill Morneau, the Minister of Finance, mentioned that the government has adopted an approach to ensure that all employees in all sectors of the economy have consistent criteria for emergency programs during the pandemic. He also mentioned that the emergency wage subsidy is meant for any sector of the economy where revenues have dropped by 30% or more, and the Canada emergency response benefit is meant for people who have lost their income due to COVID-19. However, there was no specific discussion about imports and supply chains in the provided transcript.
 90%|█████████ | 36/40 [04:25<00:29,  7.28s/it]2024-12-22 02:08:57,450 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:08:57,641 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:57,641 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:08:57,713 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:08:58,304 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:58,304 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:08:58,378 - [Process 1/5] - DEBUG - predict_token:tensor([[478]], device='cuda:1')
2024-12-22 02:08:58,449 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:58,450 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2077])
2024-12-22 02:08:58,522 - [Process 0/5] - DEBUG - predict_token:tensor([[379]], device='cuda:0')
2024-12-22 02:08:59,619 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:08:59,620 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 02:08:59,694 - [Process 4/5] - DEBUG - predict_token:tensor([[4911]], device='cuda:4')
2024-12-22 02:09:00,061 - [Process 1/5] - INFO - res.shape is :torch.Size([39])
results:Vaughan Gething did not mention anything about the perinatal mental health service when discussing the long-term special care for pregnant women during the meeting transcript provided.
 85%|████████▌ | 34/40 [04:27<00:37,  6.21s/it]2024-12-22 02:09:00,272 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:01,080 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:01,080 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2051])
2024-12-22 02:09:01,153 - [Process 3/5] - DEBUG - predict_token:tensor([[450]], device='cuda:3')
2024-12-22 02:09:01,523 - [Process 4/5] - INFO - res.shape is :torch.Size([45])
results:User Interface's presentation is mainly about the latest trends in design, including the importance of making the remote control fancy looking and feeling good, as well as the need to consider the younger public's preferences.
 85%|████████▌ | 34/40 [04:29<00:40,  6.72s/it]2024-12-22 02:09:01,677 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:01,979 - [Process 0/5] - INFO - res.shape is :torch.Size([75])
results:Huw Irranca-Davies thought that the Welsh Government has the confidence that they will have the full roll-out by 2020, but he also mentioned that they are learning about the bureaucracy around the current approach and the administrative burden that is falling to each pilot area, which might cause some problems in the future.
 88%|████████▊ | 35/40 [04:29<00:36,  7.28s/it]2024-12-22 02:09:02,226 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:02,839 - [Process 2/5] - INFO - res.shape is :torch.Size([114])
results:The meeting discussed the design and evaluation of a new remote control for a television set-top box, with the team presenting their ideas and designs. The project manager took notes and evaluated the project's progress, including the costs, which were within budget. The team also discussed the user interface and the design of the remote control, including the shape, buttons, and color. They also evaluated the project's progress and identified areas for improvement. The meeting ended with the team agreeing to complete a final questionnaire and meeting summary when the request is received.
 80%|████████  | 32/40 [04:30<01:07,  8.41s/it]2024-12-22 02:09:03,131 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:03,908 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:03,908 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2072])
2024-12-22 02:09:03,980 - [Process 1/5] - DEBUG - predict_token:tensor([[12157]], device='cuda:1')
2024-12-22 02:09:05,297 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:05,298 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 02:09:05,373 - [Process 4/5] - DEBUG - predict_token:tensor([[23408]], device='cuda:4')
2024-12-22 02:09:05,804 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:05,804 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 02:09:05,877 - [Process 0/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:0')
2024-12-22 02:09:06,913 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:06,913 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 02:09:06,993 - [Process 2/5] - DEBUG - predict_token:tensor([[476]], device='cuda:2')
2024-12-22 02:09:07,680 - [Process 0/5] - INFO - res.shape is :torch.Size([41])
results:The User Interface thought that their suggestions were not fully considered during the previous discussion. They mentioned that their ideas were not fully addressed and that they were not sure if their suggestions were going to be implemented.
 90%|█████████ | 36/40 [04:35<00:27,  6.81s/it]2024-12-22 02:09:07,933 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:08,076 - [Process 3/5] - INFO - res.shape is :torch.Size([155])
results:The meeting discussed the design requirements for a new remote control for a television, including the technical specifications, functional requirements, and user interface design. The team agreed on the need for a simple and easy-to-use remote control with a menu button, volume up and down buttons, and a power button. They also discussed the importance of making the remote control affordable and using only plastic materials to keep costs low. Additionally, they talked about the size of the remote control, the number of buttons, and the use of a bleep signal to indicate when the battery is low. The team decided to proceed to the next phase of the project, which is the conceptual design, and they will finalize the design requirements and send them to the industrial designer.
 92%|█████████▎| 37/40 [04:35<00:25,  8.34s/it]2024-12-22 02:09:08,370 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:08,924 - [Process 1/5] - INFO - res.shape is :torch.Size([108])
results:During the meeting, Industrial Designer suggested that the appearance design should be given attention when discussing the user interface because they believe it is important to make the remote control look nice and slick, as many universal remotes look cheap and low quality. They also mentioned that adding a fuzzy feature, like a leopard print or something, could be a nice touch. This implies that the appearance design is an important aspect of the product, as it can greatly impact the user's overall experience with the remote.
 88%|████████▊ | 35/40 [04:36<00:35,  7.01s/it]2024-12-22 02:09:09,134 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:09,256 - [Process 4/5] - INFO - res.shape is :torch.Size([96])
results:Bethan Owen mentioned that one aspect that they intend to fund is placing funding back into the system for innovation, and they are currently consulting with the intention of providing £15 million back into the sector in innovation funding in 2020-21. This funding will help improve universities' ability to access funding from UKRI, and it is part of promoting the innovation and research portfolio in Wales.
 88%|████████▊ | 35/40 [04:37<00:35,  7.02s/it]2024-12-22 02:09:09,390 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:09,793 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:Kirsty Williams did not mention any disadvantages to Cardiff Met because of the merger not going ahead during the meeting. In fact, she highlighted the benefits of the merger, such as the creation of a larger institution with a stronger research profile and the potential to attract more students and staff.
 82%|████████▎ | 33/40 [04:37<00:55,  7.97s/it]2024-12-22 02:09:10,053 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:11,513 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:11,513 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 02:09:11,586 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:09:11,986 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:11,986 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 02:09:12,059 - [Process 3/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:3')
2024-12-22 02:09:12,762 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:12,763 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 02:09:12,837 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:09:13,047 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:13,047 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 02:09:13,124 - [Process 4/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:4')
2024-12-22 02:09:13,659 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:13,659 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 02:09:13,732 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:09:14,590 - [Process 0/5] - INFO - res.shape is :torch.Size([69])
results:The group thought of the locator when presenting the functional requirements because they wanted to ensure that they could easily locate the remote control, especially since they were discussing the target group and functions of the remote control. They did not want to have difficulty finding the remote control during the meeting, so they considered the locator as a solution.
 92%|█████████▎| 37/40 [04:42<00:20,  6.84s/it]2024-12-22 02:09:14,873 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:15,202 - [Process 1/5] - INFO - res.shape is :torch.Size([51])
results:According to the transcript, Industrial Designer mentioned that younger people want soft mat materials and primary colors, like yellow and red, and shapes that are curved and round, like the shape of their hand, making it easier to use.
 90%|█████████ | 36/40 [04:43<00:27,  6.79s/it]2024-12-22 02:09:15,458 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:15,751 - [Process 3/5] - INFO - res.shape is :torch.Size([82])
results:Based on the meeting transcript, it seems that the team was generally satisfied with their progress, as they mentioned that they have made some improvements in noise robustness and have started to see some interesting results from their experiments. However, they also acknowledged that there is still more work to be done, particularly in terms of feature combination experiments and improving the performance of the nets on different languages.
 95%|█████████▌| 38/40 [04:43<00:16,  8.14s/it]2024-12-22 02:09:15,949 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:17,408 - [Process 4/5] - INFO - res.shape is :torch.Size([106])
results:During the meeting, the team discussed how to select channels for the new remote control. They considered factors such as user preferences, ease of use, and the ability to customize channels. The marketing expert suggested conducting a survey to gather information on users' preferences, while the industrial designer and project manager discussed the importance of making the remote control easy to use and visually appealing. The team also discussed the possibility of incorporating speech recognition technology to allow users to control the remote control with voice commands.
 90%|█████████ | 36/40 [04:45<00:29,  7.36s/it]2024-12-22 02:09:17,576 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:18,582 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:18,583 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2144])
2024-12-22 02:09:18,651 - [Process 0/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:0')
2024-12-22 02:09:19,082 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:19,082 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 02:09:19,157 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:09:19,169 - [Process 2/5] - INFO - res.shape is :torch.Size([125])
results:The meeting discussed the design of a new remote control for a television set-top box, with a focus on incorporating buttons for channel setting, menu, and screen width settings. The team also discussed the importance of keeping the design familiar and recognizable, and using a slogan or colour to represent the company's corporate image. The meeting also touched on the new project requirements from the bosses, including the removal of teletext and the inclusion of a button for immediate access to the video channel. The team agreed to go to their respective rooms to decide on the design further and meet again later.
 85%|████████▌ | 34/40 [04:46<00:50,  8.40s/it]2024-12-22 02:09:19,413 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:19,583 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:19,584 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 02:09:19,656 - [Process 3/5] - DEBUG - predict_token:tensor([[7133]], device='cuda:3')
2024-12-22 02:09:21,193 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:21,193 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 02:09:21,266 - [Process 4/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:4')
2024-12-22 02:09:21,749 - [Process 1/5] - INFO - res.shape is :torch.Size([57])
results:The Marketing Expert thought they should have LCD when presenting the market trends because they believed it would be a good option for the remote control, as it would be a cost-effective and environmentally friendly option, as the company already makes LCDs.
 92%|█████████▎| 37/40 [04:49<00:20,  6.72s/it]2024-12-22 02:09:22,104 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:22,993 - [Process 0/5] - INFO - res.shape is :torch.Size([101])
results:During the meeting, the group discussed the microphone data collected during a previous meeting. They observed that there was a lot of overlap in the data and discussed ways to handle it, including adjusting the microphone position and using echo cancellation. They also talked about the need to fill out a speaker form and a consent form for each participant. Additionally, they mentioned that they would be recording the meeting and that they would need to decide how to handle the gain and the channel assignments.
 95%|█████████▌| 38/40 [04:50<00:14,  7.31s/it]2024-12-22 02:09:23,023 - [Process 3/5] - INFO - res.shape is :torch.Size([75])
results:During the meeting, the team discussed the product evaluation criteria and rating system, with the project manager leading the discussion. They also evaluated the product's technological innovation, user interface, and overall design, with some members giving ratings and feedback. Additionally, they discussed the product's target market and the need for a smaller monitor and mouse for better convenience.
 98%|█████████▊| 39/40 [04:50<00:07,  7.88s/it]2024-12-22 02:09:23,141 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:23,141 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1964])
2024-12-22 02:09:23,220 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:23,221 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:23,222 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
2024-12-22 02:09:23,252 - [Process 4/5] - INFO - res.shape is :torch.Size([49])
results:A thought it was a good idea to set up the recording equipment in the meeting room, as it would allow them to collect data on the meetings and have a portable device to do information retrieval on meetings in the future.
 92%|█████████▎| 37/40 [04:51<00:20,  6.91s/it]2024-12-22 02:09:23,381 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:25,734 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:25,734 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2002])
2024-12-22 02:09:25,808 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:09:26,850 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:26,850 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 02:09:26,922 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:26,922 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 02:09:26,923 - [Process 3/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:3')
2024-12-22 02:09:26,996 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:26,996 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2086])
2024-12-22 02:09:27,001 - [Process 0/5] - DEBUG - predict_token:tensor([[450]], device='cuda:0')
2024-12-22 02:09:27,068 - [Process 4/5] - DEBUG - predict_token:tensor([[450]], device='cuda:4')
2024-12-22 02:09:28,665 - [Process 2/5] - INFO - res.shape is :torch.Size([125])
results:The meeting discussed the design of a new remote control for a TV, focusing on the shape, materials, and user interface. The team agreed to use titanium for the body and push buttons instead of LCD display, and to reduce costs by using simple technology and materials. They also discussed the possibility of including speech recognition technology and explored different design options for the remote control. The team decided to keep the LCD display but reduce the number of buttons to make it more user-friendly. The meeting ended with the team agreeing to meet again in 30 minutes to finalize the design.
 88%|████████▊ | 35/40 [04:56<00:43,  8.73s/it]2024-12-22 02:09:28,945 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:29,643 - [Process 1/5] - INFO - res.shape is :torch.Size([85])
results:Based on the meeting transcript, it is unclear what the future of data collection will be as the discussion revolves around the challenges of recording meetings, the need for more space, and the possibility of asking people to record their own meetings. However, Professor E mentions that they might be able to get some scattered things from this and that, and Joachim might be able to help with the radio.
 95%|█████████▌| 38/40 [04:57<00:14,  7.07s/it]2024-12-22 02:09:29,927 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:30,109 - [Process 0/5] - INFO - res.shape is :torch.Size([70])
results:The user interface designer proposed using push buttons instead of liquid crystal display (LCD) for the remote control's interface, as it would be simpler and less expensive. Additionally, the designer suggested using titanium for the remote control's body instead of rubber, as it would be more durable and have a better look.
 98%|█████████▊| 39/40 [04:57<00:07,  7.25s/it]2024-12-22 02:09:30,388 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:30,513 - [Process 3/5] - INFO - res.shape is :torch.Size([79])
results:During the meeting, the team discussed the new product requirements for the remote control, specifically the working of the remote control, and the team agreed that they need to make decisions on what controls they need and how to make it easy to use. They also discussed the technical function or the functional requirements of the remote control and how it should be designed to be easy to use and understand.
100%|██████████| 40/40 [04:58<00:00,  7.76s/it]100%|██████████| 40/40 [04:58<00:00,  7.46s/it]
2024-12-22 02:09:30,985 - [Process 4/5] - INFO - res.shape is :torch.Size([97])
results:The meeting discussed the design of a new television remote control, with a focus on making it universal, trendy, and easy to use. The project manager outlined the project's goals and timeline, and the team brainstormed ideas for the remote's design. The meeting also touched on the importance of market research and user interface design. The team agreed to start the design process and meet again in 30 minutes to continue working on the project.
 95%|█████████▌| 38/40 [04:58<00:14,  7.15s/it]2024-12-22 02:09:31,163 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:32,673 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:32,673 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1969])
2024-12-22 02:09:32,753 - [Process 2/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:2')
2024-12-22 02:09:33,629 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:33,630 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:09:33,703 - [Process 1/5] - DEBUG - predict_token:tensor([[2739]], device='cuda:1')
2024-12-22 02:09:34,048 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:34,048 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:09:34,121 - [Process 0/5] - DEBUG - predict_token:tensor([[379]], device='cuda:0')
2024-12-22 02:09:34,967 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:34,967 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 02:09:35,048 - [Process 4/5] - DEBUG - predict_token:tensor([[476]], device='cuda:4')
2024-12-22 02:09:36,008 - [Process 2/5] - INFO - res.shape is :torch.Size([73])
results:During the meeting, Professor C mentioned that the SmartKom system encountered problems with battery life, as they go through a lot of batteries. Additionally, there were issues with the syntactic structure of the system, which Professor C believes can be addressed by exchanging a few files and modifying the system's approach to speech recognition.
 90%|█████████ | 36/40 [05:03<00:33,  8.31s/it]2024-12-22 02:09:36,019 - [Process 0/5] - INFO - res.shape is :torch.Size([41])
results:Huw Irranca-Davies did not mention anything about the affordability of lowest-income working families when talking about hourly rates of the service during the meeting transcript provided.
100%|██████████| 40/40 [05:03<00:00,  6.85s/it]100%|██████████| 40/40 [05:03<00:00,  7.60s/it]
2024-12-22 02:09:36,248 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:37,475 - [Process 1/5] - INFO - res.shape is :torch.Size([88])
results:Julie Morgan AM elaborated on their work with foster organizations by stating that they have been working closely with them to ensure that they have the necessary resources and support to provide adequate care for children in foster, including access to mental health services and other forms of support. She also mentioned that they have been providing funding to support foster organizations to help them continue to provide high-quality care during the pandemic.
 98%|█████████▊| 39/40 [05:05<00:07,  7.30s/it]2024-12-22 02:09:37,755 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:39,801 - [Process 4/5] - INFO - res.shape is :torch.Size([117])
results:Kirsty Williams, the Cabinet Secretary for Education, mentioned that the Welsh Government is aware of the issue of teacher recruitment and retention, particularly in secondary schools, and has taken steps such as making sure that ITE provision is world-class, providing financial incentives, and embarking on a national ITE recruitment marketing exercise to address the problem. However, she acknowledged that it is not a uniquely Welsh issue and that the problem is common across the globe, with even the UK Government struggling to address it.
 98%|█████████▊| 39/40 [05:07<00:07,  7.65s/it]2024-12-22 02:09:39,843 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:39,843 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 02:09:39,916 - [Process 2/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:2')
2024-12-22 02:09:39,927 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:41,384 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:41,385 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 02:09:41,457 - [Process 1/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:1')
2024-12-22 02:09:42,414 - [Process 1/5] - INFO - res.shape is :torch.Size([22])
results:D did not express any opinion about the experimental setup of the computer navigation system in the transcript provided.
100%|██████████| 40/40 [05:10<00:00,  6.59s/it]100%|██████████| 40/40 [05:10<00:00,  7.76s/it]
2024-12-22 02:09:43,245 - [Process 2/5] - INFO - res.shape is :torch.Size([78])
results:During the meeting, the design team discussed the advantages of the design for the prototype, including the use of titanium as a modern material, the ultimate form and colors chosen, and the ability to touch the device with one's hands. The team also mentioned that the design is practical and meets the needs of the users, making it a good choice for the prototype.
 92%|█████████▎| 37/40 [05:11<00:23,  7.99s/it]2024-12-22 02:09:43,482 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:43,545 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:43,546 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 02:09:43,620 - [Process 4/5] - DEBUG - predict_token:tensor([[16564]], device='cuda:4')
2024-12-22 02:09:46,572 - [Process 4/5] - INFO - res.shape is :torch.Size([73])
results:During the meeting, the team discussed the use of advanced chips and LCD display for the remote control design. They talked about the cost implications of using advanced chips and how it would affect the overall design of the remote control. They also discussed the importance of keeping the design small and compact while still providing easy access to the buttons and features.
100%|██████████| 40/40 [05:14<00:00,  7.39s/it]100%|██████████| 40/40 [05:14<00:00,  7.86s/it]
2024-12-22 02:09:47,088 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:47,088 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 02:09:47,163 - [Process 2/5] - DEBUG - predict_token:tensor([[12157]], device='cuda:2')
2024-12-22 02:09:48,871 - [Process 2/5] - INFO - res.shape is :torch.Size([40])
results:According to the transcript, Industrial Designer thought the scroll wheel was a good option and mentioned that it could be incorporated into the design (lines 50-52).
 95%|█████████▌| 38/40 [05:16<00:14,  7.28s/it]2024-12-22 02:09:49,125 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:52,850 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:52,850 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1964])
2024-12-22 02:09:52,931 - [Process 2/5] - DEBUG - predict_token:tensor([[2860]], device='cuda:2')
2024-12-22 02:09:55,946 - [Process 2/5] - INFO - res.shape is :torch.Size([71])
results:After the demo, Professor C suggested that Keith and Ellen should work on formalizing the image schemas, and he would make a note to follow up with Ellen regarding the same. Additionally, the group would try to finish the remaining work by the next Monday, and Keith and Ellen would come up with a proposal and questions to present to the group.
 98%|█████████▊| 39/40 [05:23<00:07,  7.22s/it]2024-12-22 02:09:56,363 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:09:59,966 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:09:59,967 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:10:00,040 - [Process 2/5] - DEBUG - predict_token:tensor([[7579]], device='cuda:2')
2024-12-22 02:10:00,779 - [Process 2/5] - INFO - res.shape is :torch.Size([17])
results:The Prime Minister did not mention anything about students in the provided meeting transcript.
100%|██████████| 40/40 [05:28<00:00,  6.50s/it]100%|██████████| 40/40 [05:28<00:00,  8.22s/it]
2024-12-22 02:10:00,807 - [Process 2/5] - DEBUG - datasets_name:qmsum
2024-12-22 02:10:00,807 - [Process 4/5] - DEBUG - datasets_name:qmsum
2024-12-22 02:10:00,807 - [Process 1/5] - DEBUG - datasets_name:qmsum
2024-12-22 02:10:00,807 - [Process 3/5] - DEBUG - datasets_name:qmsum
2024-12-22 02:10:00,807 - [Process 0/5] - DEBUG - datasets_name:qmsum
Running evaluation for dataset: samsum
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:Truemulti_gpus:True

torch.cuda.device_count():5torch.cuda.device_count():5

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.57s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:11:58,117 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 02:11:58,117 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 02:11:58,117 - [Process 2/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:11:58,128 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 02:11:58,128 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 02:11:58,128 - [Process 4/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:11:58,138 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 02:11:58,138 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 02:11:58,138 - [Process 3/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:11:58,140 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 02:11:58,141 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 02:11:58,141 - [Process 1/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:11:58,143 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 02:11:58,143 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 02:11:58,143 - [Process 0/5] - INFO - output_max_len: 128
2024-12-22 02:11:58,160 - [Process 2/5] - INFO - Max Length is 12252
2024-12-22 02:11:58,161 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 02:11:58,161 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:11:58,203 - [Process 4/5] - INFO - Max Length is 12252
2024-12-22 02:11:58,204 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 02:11:58,204 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:11:58,212 - [Process 3/5] - INFO - Max Length is 12252
2024-12-22 02:11:58,212 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 02:11:58,213 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:11:58,216 - [Process 1/5] - INFO - Max Length is 12252
2024-12-22 02:11:58,216 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 02:11:58,216 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:11:58,220 - [Process 0/5] - INFO - Max Length is 12252
2024-12-22 02:11:58,221 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 02:11:58,221 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:12:02,909 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:02,992 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:02,994 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:02,994 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:02,995 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:07,281 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:07,282 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 02:12:07,333 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:07,333 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2194])
2024-12-22 02:12:07,353 - [Process 0/5] - DEBUG - predict_token:tensor([[2812]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:12:07,384 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:07,384 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 02:12:07,408 - [Process 2/5] - DEBUG - predict_token:tensor([[435]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:12:07,457 - [Process 3/5] - DEBUG - predict_token:tensor([[382]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:12:07,542 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:07,543 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:12:07,623 - [Process 1/5] - DEBUG - predict_token:tensor([[21730]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:12:07,659 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:07,659 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2269])
2024-12-22 02:12:07,690 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:Emilia is still angry.
  2%|▎         | 1/40 [00:09<06:09,  9.47s/it]2024-12-22 02:12:07,738 - [Process 4/5] - DEBUG - predict_token:tensor([[25914]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:12:07,832 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:08,184 - [Process 3/5] - INFO - res.shape is :torch.Size([16])
results:Ewan graduated and Uncle Jayson congratulates him.
  2%|▎         | 1/40 [00:09<06:28,  9.97s/it]2024-12-22 02:12:08,369 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:08,690 - [Process 2/5] - INFO - res.shape is :torch.Size([31])
results:Jasmine likes a new song by Charlie Puth. Paola also likes the song and a different song by Ed Sheeran.
  2%|▎         | 1/40 [00:10<06:50, 10.53s/it]2024-12-22 02:12:08,854 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:09,787 - [Process 4/5] - INFO - res.shape is :torch.Size([43])
results:Susan went on a date with a guy who picked her up from home, looked nice, and took her to a roller skating disco. Jill is skeptical about the date.
  2%|▎         | 1/40 [00:11<07:31, 11.58s/it]2024-12-22 02:12:09,812 - [Process 1/5] - INFO - res.shape is :torch.Size([47])
results:Andrea can't come to work today because her son is sick and needs to take him to the doctor. Patrick is sorry to hear that and agrees that Andrea can take the day off to take care of her son.
  2%|▎         | 1/40 [00:11<07:32, 11.60s/it]2024-12-22 02:12:09,905 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:09,987 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:10,674 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:10,674 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1705])
2024-12-22 02:12:10,732 - [Process 0/5] - DEBUG - predict_token:tensor([[12208]], device='cuda:0')
2024-12-22 02:12:11,942 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:11,943 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1233])
2024-12-22 02:12:11,984 - [Process 1/5] - DEBUG - predict_token:tensor([[2088]], device='cuda:1')
2024-12-22 02:12:12,907 - [Process 0/5] - INFO - res.shape is :torch.Size([53])
results:Jeff, Lia, and Mico are going to a village party. Mico wants to go because there might be hot boys, but Lia is too tired after hiking. Jeff will go as well, but he doesn't drink.
  5%|▌         | 2/40 [00:14<04:24,  6.97s/it]2024-12-22 02:12:12,966 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:12,966 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2507])
2024-12-22 02:12:13,063 - [Process 3/5] - DEBUG - predict_token:tensor([[27665]], device='cuda:3')
2024-12-22 02:12:13,158 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:13,182 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:13,183 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2490])
2024-12-22 02:12:13,271 - [Process 2/5] - DEBUG - predict_token:tensor([[21631]], device='cuda:2')
2024-12-22 02:12:13,633 - [Process 2/5] - INFO - res.shape is :torch.Size([8])
results:







  5%|▌         | 2/40 [00:15<04:35,  7.24s/it]2024-12-22 02:12:13,753 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:14,369 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:14,369 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2520])
2024-12-22 02:12:14,457 - [Process 4/5] - DEBUG - predict_token:tensor([[15316]], device='cuda:4')
2024-12-22 02:12:16,073 - [Process 1/5] - INFO - res.shape is :torch.Size([99])
results:Guido is having trouble understanding Agata's posts on Facebook and asks her about it. Agata explains that she posts her Twitter messages to her Facebook account. Guido is not familiar with JavaScript and finds it hard to understand. Agata enjoys coding and found something interesting for herself. Guido asks if her husband complains about her new job, and Agata replies that he doesn't. Guido jokes about Agata speaking to her husband in JavaScript language.
  5%|▌         | 2/40 [00:17<05:21,  8.46s/it]2024-12-22 02:12:16,089 - [Process 3/5] - INFO - res.shape is :torch.Size([66])
results:Bobby is annoyed by pharmaceutical companies inventing illnesses to sell their products. He found a YouTube video of a doctor explaining that the particular illness mentioned in a commercial does not exist. He believes that the pharmaceutical industry is worth at least $1.2 billion.
  5%|▌         | 2/40 [00:17<05:32,  8.76s/it]2024-12-22 02:12:16,291 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:16,319 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:16,740 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:16,740 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2070])
2024-12-22 02:12:16,812 - [Process 0/5] - DEBUG - predict_token:tensor([[5765]], device='cuda:0')
2024-12-22 02:12:17,176 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
results:







  8%|▊         | 3/40 [00:18<03:32,  5.74s/it]2024-12-22 02:12:17,381 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:17,496 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:17,497 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2171])
2024-12-22 02:12:17,568 - [Process 2/5] - DEBUG - predict_token:tensor([[10785]], device='cuda:2')
2024-12-22 02:12:18,470 - [Process 4/5] - INFO - res.shape is :torch.Size([86])
results:Samuel is talking to Tim in Telugu, but Tim corrects him that his name is Tim, not Anna. Samuel apologizes and explains that in his language, Anna means elder brother. Tim asks about the word for younger brother and Samuel tells him it's Thammu or Thammadu. Samuel is 35, but looks older due to living a difficult life. Tim is 55.
  5%|▌         | 2/40 [00:20<06:15,  9.88s/it]2024-12-22 02:12:18,705 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:19,329 - [Process 2/5] - INFO - res.shape is :torch.Size([43])
results:Ali lost his wallet at Mohammad's place and asks him to check if he left it there. Mohammad finds it and agrees to bring it to uni the next day.
  8%|▊         | 3/40 [00:21<04:01,  6.54s/it]2024-12-22 02:12:19,447 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:20,175 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:20,175 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2204])
2024-12-22 02:12:20,249 - [Process 3/5] - DEBUG - predict_token:tensor([[319]], device='cuda:3')
2024-12-22 02:12:21,118 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:21,118 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2608])
2024-12-22 02:12:21,215 - [Process 1/5] - DEBUG - predict_token:tensor([[22045]], device='cuda:1')
2024-12-22 02:12:21,334 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:21,334 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2229])
2024-12-22 02:12:21,413 - [Process 0/5] - DEBUG - predict_token:tensor([[5202]], device='cuda:0')
2024-12-22 02:12:23,049 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:23,050 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2435])
2024-12-22 02:12:23,052 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:23,052 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 02:12:23,124 - [Process 2/5] - DEBUG - predict_token:tensor([[5678]], device='cuda:2')
2024-12-22 02:12:23,132 - [Process 4/5] - DEBUG - predict_token:tensor([[29947]], device='cuda:4')
2024-12-22 02:12:23,424 - [Process 2/5] - INFO - res.shape is :torch.Size([7])
results:






 10%|█         | 4/40 [00:25<03:20,  5.57s/it]2024-12-22 02:12:23,569 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:23,822 - [Process 3/5] - INFO - res.shape is :torch.Size([78])
results:Adele is listening to a song recommended by Kode. Kode has the song in his music library for 3 years. Adele wants to know if Kode can suggest some other songs from the same singer. Kode offers to give Adele a USB with the singer's album and is available to come to Adele's home right away.
  8%|▊         | 3/40 [00:25<05:06,  8.29s/it]2024-12-22 02:12:24,115 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:24,309 - [Process 0/5] - INFO - res.shape is :torch.Size([62])
results:Autumn wants to see Israel's sewing work, but Israel hasn't had the time to do any yet. Autumn thinks that Israel should prioritize sewing and shows him some photos of her dress that will be on the site soon. Israel is excited to see the whole dress.
 10%|█         | 4/40 [00:26<03:46,  6.29s/it]2024-12-22 02:12:24,527 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:24,766 - [Process 1/5] - INFO - res.shape is :torch.Size([75])
results:Gail and Julia are talking about social media. Julia likes Instagram, Snapchat, Twitter, and Facebook. Gail likes to spy on people on social media. Julia asks if Gail spied on her, and Gail says no. Julia thinks Hillary thinks she overdoes it on social media and has no life in real life.
  8%|▊         | 3/40 [00:26<05:16,  8.57s/it]2024-12-22 02:12:25,043 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:26,888 - [Process 4/5] - INFO - res.shape is :torch.Size([81])
results:Cora and Ellie are talking about how the British media reacted to a meet and greet with James Charles in Birmingham. 80000 fans showed up, causing a gridlock and negative comments from the media. Cora finds it funny, while Ellie thinks it's ridiculous that they can't understand that a guy can wear makeup.
  8%|▊         | 3/40 [00:28<05:40,  9.21s/it]2024-12-22 02:12:27,100 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:27,524 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:27,524 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2169])
2024-12-22 02:12:27,605 - [Process 2/5] - DEBUG - predict_token:tensor([[12815]], device='cuda:2')
2024-12-22 02:12:28,193 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:28,194 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2333])
2024-12-22 02:12:28,275 - [Process 3/5] - DEBUG - predict_token:tensor([[4699]], device='cuda:3')
2024-12-22 02:12:29,053 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:29,053 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2540])
2024-12-22 02:12:29,070 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:29,070 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2144])
2024-12-22 02:12:29,148 - [Process 0/5] - DEBUG - predict_token:tensor([[1771]], device='cuda:0')
2024-12-22 02:12:29,151 - [Process 1/5] - DEBUG - predict_token:tensor([[4755]], device='cuda:1')
2024-12-22 02:12:29,641 - [Process 2/5] - INFO - res.shape is :torch.Size([49])
results:Igor is feeling unmotivated about the work he has to do at his notice period. John thinks it's irresponsible to give so much work to someone leaving the job. Igor is unsure what to do.
 12%|█▎        | 5/40 [00:31<03:23,  5.81s/it]2024-12-22 02:12:29,748 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:30,640 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:30,641 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2037])
2024-12-22 02:12:30,646 - [Process 1/5] - INFO - res.shape is :torch.Size([34])
results:Robert wants to meet up with Gabriel for coffee, but Gabriel has quit his office a month ago and is now at home, looking into what to do next.
 10%|█         | 4/40 [00:32<04:30,  7.50s/it]2024-12-22 02:12:30,712 - [Process 4/5] - DEBUG - predict_token:tensor([[5011]], device='cuda:4')
2024-12-22 02:12:30,897 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:32,312 - [Process 0/5] - INFO - res.shape is :torch.Size([65])
results:Brenda and Sandra used to work together at a clothes factory and they had a reunion. They talked about their lives and families, and Sandra suggested organizing a reunion for all the girls from the factory. They agreed to meet up next Saturday for lunch and to reminisce about old times.
 12%|█▎        | 5/40 [00:34<04:01,  6.91s/it]2024-12-22 02:12:32,544 - [Process 3/5] - INFO - res.shape is :torch.Size([92])
results:
David asks Victor how he is and Victor replies he is fine. David mentions that he heard Victor took over Chris's company and Victor confirms it. David expresses gratitude towards Victor for helping Chris out of a difficult situation. Victor explains that he sold off Chris's office space but Chris is still working as a director. They discuss the current state of their respective businesses and express hope for improvement by the end of the year.
 10%|█         | 4/40 [00:34<05:04,  8.46s/it]2024-12-22 02:12:32,582 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:32,737 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:33,870 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:33,870 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2400])
2024-12-22 02:12:33,946 - [Process 2/5] - DEBUG - predict_token:tensor([[12986]], device='cuda:2')
2024-12-22 02:12:34,687 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:34,688 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2163])
2024-12-22 02:12:34,759 - [Process 1/5] - DEBUG - predict_token:tensor([[12540]], device='cuda:1')
2024-12-22 02:12:35,122 - [Process 2/5] - INFO - res.shape is :torch.Size([28])
results:Kamden wants to see Mckinley's Facebook photos but Mckinley is not a phone selfie person.
 15%|█▌        | 6/40 [00:36<03:13,  5.70s/it]2024-12-22 02:12:35,225 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:36,313 - [Process 4/5] - INFO - res.shape is :torch.Size([128])
results:



Dialogue: 


Dialogue: 


Summary: 


Dialogue: 


Summary: 


Dialogue: 


Summary: 


Dialogue: 


Summary: 


Dialogue: 


Summary: 


Dialogue: 


Summary: 


Dialogue: 


Summary: 


Dialogue: 


Summary: 


Dialogue: 


Summary: 



 10%|█         | 4/40 [00:38<05:34,  9.30s/it]2024-12-22 02:12:36,485 - [Process 1/5] - INFO - res.shape is :torch.Size([37])
results:Vinny likes Willy's car and wants a red Mustang. Willy jokes about lending him his car. Vinny suggests they carpool together.
 12%|█▎        | 5/40 [00:38<04:01,  6.90s/it]2024-12-22 02:12:36,526 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:36,527 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2224])
2024-12-22 02:12:36,589 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:36,606 - [Process 0/5] - DEBUG - predict_token:tensor([[951]], device='cuda:0')
2024-12-22 02:12:36,725 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:36,759 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:36,759 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2277])
2024-12-22 02:12:36,841 - [Process 3/5] - DEBUG - predict_token:tensor([[27281]], device='cuda:3')
2024-12-22 02:12:37,836 - [Process 0/5] - INFO - res.shape is :torch.Size([26])
results:Leah wants to get 20% discount coupon from Kristi. Kristi agrees to do it.
 15%|█▌        | 6/40 [00:39<03:38,  6.44s/it]2024-12-22 02:12:38,127 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:38,867 - [Process 3/5] - INFO - res.shape is :torch.Size([44])
results:Miriam invites Pegah to a gathering and Pegah can't make it because she has work commitments. Pegah will have a cup of tea with Miriam when she gets back.
 12%|█▎        | 5/40 [00:40<04:29,  7.69s/it]2024-12-22 02:12:38,915 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:38,915 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1968])
2024-12-22 02:12:38,995 - [Process 2/5] - DEBUG - predict_token:tensor([[12821]], device='cuda:2')
2024-12-22 02:12:39,060 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:40,256 - [Process 2/5] - INFO - res.shape is :torch.Size([31])
results:Chris and June were at the pool and pushed some girls into the pool. Chris thinks it's funny but June doesn't agree.
 18%|█▊        | 7/40 [00:42<03:01,  5.51s/it]2024-12-22 02:12:40,355 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:40,395 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:40,395 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 02:12:40,475 - [Process 4/5] - DEBUG - predict_token:tensor([[23774]], device='cuda:4')
2024-12-22 02:12:41,111 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:41,111 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2506])
2024-12-22 02:12:41,199 - [Process 1/5] - DEBUG - predict_token:tensor([[751]], device='cuda:1')
2024-12-22 02:12:42,060 - [Process 4/5] - INFO - res.shape is :torch.Size([35])
results:Jennifer wants to watch Mad Max on Netflix with Jack and Reece. They all like the movie and agree to watch it together the next evening.
 12%|█▎        | 5/40 [00:43<04:40,  8.02s/it]2024-12-22 02:12:42,132 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:42,132 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2324])
2024-12-22 02:12:42,212 - [Process 0/5] - DEBUG - predict_token:tensor([[29420]], device='cuda:0')
2024-12-22 02:12:42,227 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:43,063 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:43,063 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2157])
2024-12-22 02:12:43,144 - [Process 3/5] - DEBUG - predict_token:tensor([[2443]], device='cuda:3')
2024-12-22 02:12:43,910 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:43,910 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 02:12:43,983 - [Process 2/5] - DEBUG - predict_token:tensor([[19748]], device='cuda:2')
2024-12-22 02:12:45,397 - [Process 2/5] - INFO - res.shape is :torch.Size([35])
results:Vincent broke the lamp with his bare hand while playing with his cat. He ordered a new one and it will be ready to pick up on Tuesday.
 20%|██        | 8/40 [00:47<02:52,  5.39s/it]2024-12-22 02:12:45,410 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:45,411 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1848])
2024-12-22 02:12:45,456 - [Process 0/5] - INFO - res.shape is :torch.Size([70])
results:Lisa and Peter are having a conversation about their health. Peter has been working out to improve his health, and Lisa is thinking of starting to work out as well. Peter goes to a gym near their office and does weights and runs on the treadmill. They also discuss the importance of sleep and eating well for good health.
 18%|█▊        | 7/40 [00:47<03:45,  6.82s/it]2024-12-22 02:12:45,476 - [Process 4/5] - DEBUG - predict_token:tensor([[2178]], device='cuda:4')
2024-12-22 02:12:45,532 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:45,745 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:45,971 - [Process 1/5] - INFO - res.shape is :torch.Size([103])
results:Quinn and Kyle are going out tonight. Quinn is unhappy with school and wants to leave as soon as possible. Kyle suggests talking to his uncle about getting a job on a building site. Quinn is also unhappy about his mock exams and has to retake them. Kyle offers to ask his brother for vodka and cigarettes. Quinn knows Kirsty will be there tonight but doesn't know if she will be with Nathan Baker.
 15%|█▌        | 6/40 [00:47<04:24,  7.78s/it]2024-12-22 02:12:46,111 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:46,746 - [Process 4/5] - INFO - res.shape is :torch.Size([29])
results:Allison has received a scholarship and is very happy. Maya and Sarah are congratulating her and want to celebrate.
 15%|█▌        | 6/40 [00:48<03:54,  6.88s/it]2024-12-22 02:12:47,007 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:48,225 - [Process 3/5] - INFO - res.shape is :torch.Size([112])
results:Steffen is unable to go to the infinity pool due to a twisted ankle and is looking for a ride. Irene offers to give him a lift but the car can't make it all the way up, so they will park at the bottom and hike up. Dan suggests Mr. Budd's car, which is a 4-wheel drive, but Steffen decides to skip it due to his injured leg. Luke asks about the road conditions and Ben confirms it's Vistas de Olas.
 15%|█▌        | 6/40 [00:50<04:40,  8.26s/it]2024-12-22 02:12:48,492 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:48,840 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:48,840 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1592])
2024-12-22 02:12:48,894 - [Process 1/5] - DEBUG - predict_token:tensor([[4485]], device='cuda:1')
2024-12-22 02:12:49,239 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:49,239 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2132])
2024-12-22 02:12:49,308 - [Process 2/5] - DEBUG - predict_token:tensor([[26259]], device='cuda:2')
2024-12-22 02:12:49,550 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:49,550 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2110])
2024-12-22 02:12:49,631 - [Process 0/5] - DEBUG - predict_token:tensor([[5879]], device='cuda:0')
2024-12-22 02:12:50,818 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:50,819 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 02:12:50,898 - [Process 4/5] - DEBUG - predict_token:tensor([[13772]], device='cuda:4')
2024-12-22 02:12:51,076 - [Process 0/5] - INFO - res.shape is :torch.Size([33])
results:Petra and Andy are feeling sleepy and bored at work. Ezgi is working. Petra jokes about asking the HR woman for help.
 20%|██        | 8/40 [00:52<03:26,  6.44s/it]2024-12-22 02:12:51,098 - [Process 1/5] - INFO - res.shape is :torch.Size([54])
results:Mark and Jeff are talking about a new car that one of their friends bought. They are amazed by its luxury and want to try it out. Mark wants to be the first one to drive it, but Jeff is skeptical and offers a bet.
 18%|█▊        | 7/40 [00:52<03:48,  6.91s/it]2024-12-22 02:12:51,305 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:51,320 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:52,349 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:52,350 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2161])
2024-12-22 02:12:52,360 - [Process 4/5] - INFO - res.shape is :torch.Size([33])
results:Ethan sent a photo of Scott to Toby and Marshall, who found it funny. Scott said they are the same and they made fun of him.
 18%|█▊        | 7/40 [00:54<03:33,  6.47s/it]2024-12-22 02:12:52,424 - [Process 3/5] - DEBUG - predict_token:tensor([[5677]], device='cuda:3')
2024-12-22 02:12:52,587 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:53,955 - [Process 3/5] - INFO - res.shape is :torch.Size([35])
results:Jeremih wants Hansel to tell his sister to text him back, but Hansel refuses to get involved and laughs at Jeremih's situation.
 18%|█▊        | 7/40 [00:55<04:05,  7.43s/it]2024-12-22 02:12:54,206 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:54,483 - [Process 2/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 22%|██▎       | 9/40 [00:56<03:23,  6.55s/it]2024-12-22 02:12:54,629 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:55,054 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:55,055 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1962])
2024-12-22 02:12:55,135 - [Process 1/5] - DEBUG - predict_token:tensor([[23350]], device='cuda:1')
2024-12-22 02:12:55,281 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:55,281 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2137])
2024-12-22 02:12:55,359 - [Process 0/5] - DEBUG - predict_token:tensor([[16216]], device='cuda:0')
2024-12-22 02:12:56,155 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:56,155 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 02:12:56,228 - [Process 4/5] - DEBUG - predict_token:tensor([[29420]], device='cuda:4')
2024-12-22 02:12:56,813 - [Process 0/5] - INFO - res.shape is :torch.Size([33])
results:Nova shows Dominic some artworks of Timothée Chalamet photoshopped into paintings. They find it hilarious and accurate.
 22%|██▎       | 9/40 [00:58<03:12,  6.22s/it]2024-12-22 02:12:56,967 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:57,124 - [Process 4/5] - INFO - res.shape is :torch.Size([19])
results:Daisy wants Lisa to be back before 11 pm. Lisa agrees.
 20%|██        | 8/40 [00:58<03:09,  5.93s/it]2024-12-22 02:12:57,327 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:12:58,319 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:58,320 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2364])
2024-12-22 02:12:58,403 - [Process 3/5] - DEBUG - predict_token:tensor([[8965]], device='cuda:3')
2024-12-22 02:12:58,892 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:12:58,892 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2371])
2024-12-22 02:12:58,973 - [Process 2/5] - DEBUG - predict_token:tensor([[18364]], device='cuda:2')
2024-12-22 02:13:00,109 - [Process 3/5] - INFO - res.shape is :torch.Size([38])
results:Carlton is planning to join Ana and Katy to see a film by Lola Arias, but he can't make it on Saturday as he is fully booked.
 20%|██        | 8/40 [01:01<03:44,  7.02s/it]2024-12-22 02:13:00,294 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:00,484 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:00,484 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1995])
2024-12-22 02:13:00,556 - [Process 0/5] - DEBUG - predict_token:tensor([[319]], device='cuda:0')
2024-12-22 02:13:00,706 - [Process 1/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 20%|██        | 8/40 [01:02<04:08,  7.77s/it]2024-12-22 02:13:00,800 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:00,948 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:00,948 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 02:13:01,021 - [Process 4/5] - DEBUG - predict_token:tensor([[11783]], device='cuda:4')
2024-12-22 02:13:01,527 - [Process 2/5] - INFO - res.shape is :torch.Size([61])
results:Margaret wants to meet on December 4th and 11th at 10:00 or 11:00. Evans is not sure about December 18th but suggests meeting on the 14th instead. They agree to set the dates later.
 25%|██▌       | 10/40 [01:03<03:21,  6.70s/it]2024-12-22 02:13:01,606 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:01,814 - [Process 4/5] - INFO - res.shape is :torch.Size([18])
results:Adam is nervous about an exam and Dave is trying to calm him down.
 22%|██▎       | 9/40 [01:03<02:51,  5.54s/it]2024-12-22 02:13:02,032 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:02,790 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:02,790 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1151])
2024-12-22 02:13:02,829 - [Process 1/5] - DEBUG - predict_token:tensor([[435]], device='cuda:1')
2024-12-22 02:13:03,242 - [Process 0/5] - INFO - res.shape is :torch.Size([63])
results:Adele got a new dog named Bones, a 4-month-old puppy biscuit lab. The other animals in the house, Poppy and Lulu seem to mother him, while Speedy wants to play. Lola and Adele are excited to see Bones.
 25%|██▌       | 10/40 [01:05<03:08,  6.28s/it]2024-12-22 02:13:03,462 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:03,966 - [Process 1/5] - INFO - res.shape is :torch.Size([29])
results:Jill is bored and needs to find a job. Nate is still at work and will call Jill when he gets off.
 22%|██▎       | 9/40 [01:05<03:17,  6.36s/it]2024-12-22 02:13:04,200 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:04,319 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:04,319 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2153])
2024-12-22 02:13:04,390 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:04,391 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1678])
2024-12-22 02:13:04,401 - [Process 3/5] - DEBUG - predict_token:tensor([[8965]], device='cuda:3')
2024-12-22 02:13:04,446 - [Process 2/5] - DEBUG - predict_token:tensor([[12208]], device='cuda:2')
2024-12-22 02:13:05,208 - [Process 3/5] - INFO - res.shape is :torch.Size([18])
results:

Please let me know if you have any questions or need further clarification.
 22%|██▎       | 9/40 [01:06<03:19,  6.42s/it]2024-12-22 02:13:05,307 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:06,251 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:06,252 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2323])
2024-12-22 02:13:06,341 - [Process 4/5] - DEBUG - predict_token:tensor([[26977]], device='cuda:4')
2024-12-22 02:13:06,773 - [Process 2/5] - INFO - res.shape is :torch.Size([61])
results:Jeff, Vladimir, Tanya, and Donald are discussing an agreement about the Caspian Sea. They know that it will have a special legal status and will be divided into two parts. They also know that it's rich in resources, including gas, oil, and caviar.
 28%|██▊       | 11/40 [01:08<03:01,  6.26s/it]2024-12-22 02:13:06,905 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:07,205 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:07,205 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2037])
2024-12-22 02:13:07,284 - [Process 0/5] - DEBUG - predict_token:tensor([[1704]], device='cuda:0')
2024-12-22 02:13:07,526 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:07,526 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1252])
2024-12-22 02:13:07,574 - [Process 3/5] - DEBUG - predict_token:tensor([[2811]], device='cuda:3')
2024-12-22 02:13:08,022 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:08,022 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2226])
2024-12-22 02:13:08,093 - [Process 1/5] - DEBUG - predict_token:tensor([[4827]], device='cuda:1')
2024-12-22 02:13:08,629 - [Process 3/5] - INFO - res.shape is :torch.Size([23])
results:Emma is not hungry and doesn't want Will to cook dinner. She will be home soon.
 25%|██▌       | 10/40 [01:10<02:44,  5.50s/it]2024-12-22 02:13:08,816 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:09,544 - [Process 1/5] - INFO - res.shape is :torch.Size([31])
results:Alexa confesses to Hunter that she asked Ethan to insult him. She was jealous of Hunter and Ethan spending time together.
 25%|██▌       | 10/40 [01:11<03:03,  6.12s/it]2024-12-22 02:13:09,737 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:10,183 - [Process 4/5] - INFO - res.shape is :torch.Size([83])
results:Larry and Kirsten are discussing an email about insurance. They will post it on a Sunday. Kirsten wants to chat with 'old heads' about the matter and Larry agrees. Kirsten wants to stay busy and learn, and Larry thinks she is doing them a favor. They also discuss legal action and want to see a menorah in the lobby next year.
 25%|██▌       | 10/40 [01:11<03:12,  6.41s/it]2024-12-22 02:13:10,437 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:10,690 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:10,690 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2125])
2024-12-22 02:13:10,762 - [Process 2/5] - DEBUG - predict_token:tensor([[319]], device='cuda:2')
2024-12-22 02:13:11,265 - [Process 2/5] - INFO - res.shape is :torch.Size([12])
results:
Dialogue: 






 30%|███       | 12/40 [01:13<02:40,  5.72s/it]2024-12-22 02:13:11,420 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:12,881 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:12,881 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2124])
2024-12-22 02:13:12,886 - [Process 0/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 28%|██▊       | 11/40 [01:14<03:32,  7.31s/it]2024-12-22 02:13:12,963 - [Process 3/5] - DEBUG - predict_token:tensor([[11230]], device='cuda:3')
2024-12-22 02:13:13,116 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:13,380 - [Process 3/5] - INFO - res.shape is :torch.Size([9])
results:








 28%|██▊       | 11/40 [01:15<02:32,  5.27s/it]2024-12-22 02:13:13,593 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:13,768 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:13,768 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 02:13:13,847 - [Process 1/5] - DEBUG - predict_token:tensor([[10558]], device='cuda:1')
2024-12-22 02:13:15,018 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:15,018 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2395])
2024-12-22 02:13:15,037 - [Process 1/5] - INFO - res.shape is :torch.Size([27])
results:Anne hates Miranda because she was sweet with Tom, who Anne is dating, and Catherine and Nora agree.
 28%|██▊       | 11/40 [01:16<02:51,  5.93s/it]2024-12-22 02:13:15,094 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:15,094 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 02:13:15,109 - [Process 4/5] - DEBUG - predict_token:tensor([[5011]], device='cuda:4')
2024-12-22 02:13:15,170 - [Process 2/5] - DEBUG - predict_token:tensor([[20283]], device='cuda:2')
2024-12-22 02:13:15,306 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:15,956 - [Process 2/5] - INFO - res.shape is :torch.Size([19])
results:



Please let me know if you have any questions or need further assistance!
 32%|███▎      | 13/40 [01:17<02:26,  5.41s/it]2024-12-22 02:13:16,087 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:17,138 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:17,138 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2203])
2024-12-22 02:13:17,218 - [Process 0/5] - DEBUG - predict_token:tensor([[23647]], device='cuda:0')
2024-12-22 02:13:17,667 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:17,668 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2301])
2024-12-22 02:13:17,751 - [Process 3/5] - DEBUG - predict_token:tensor([[306]], device='cuda:3')
2024-12-22 02:13:18,443 - [Process 4/5] - INFO - res.shape is :torch.Size([74])
results:James and Mia are going to an art exhibition of their lecturer in philosophy, Evans, who also teaches ethics. They will go with Amelia. Mia is not sure about spending time with her, but James reassures her that she is laid-back and likable. They will hang out after the exhibition at James' place.
 28%|██▊       | 11/40 [01:20<03:22,  6.98s/it]2024-12-22 02:13:18,593 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:19,115 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:19,115 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 02:13:19,196 - [Process 1/5] - DEBUG - predict_token:tensor([[15350]], device='cuda:1')
2024-12-22 02:13:19,592 - [Process 0/5] - INFO - res.shape is :torch.Size([51])
results:Janice and Rachel are talking about the best films of 2018. Janice has watched almost all of them, including Deadpool 2, which her boyfriend forced her to watch. Rachel also likes Avengers.
 30%|███       | 12/40 [01:21<03:19,  7.13s/it]2024-12-22 02:13:19,681 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:19,682 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 02:13:19,754 - [Process 2/5] - DEBUG - predict_token:tensor([[4121]], device='cuda:2')
2024-12-22 02:13:19,792 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:19,962 - [Process 3/5] - INFO - res.shape is :torch.Size([47])
results:Inez wants to plan another Food Evening. She sends a proposal for the next one. Everyone liked the previous evening and wants to do it again. They discuss when they can do it again before the holidays.
 30%|███       | 12/40 [01:21<02:38,  5.67s/it]2024-12-22 02:13:20,210 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:20,632 - [Process 1/5] - INFO - res.shape is :torch.Size([33])
results:Gene sent a package to Jack on Friday, but Jack hasn't received it yet. Jack asks Gene for the tracking number to check on the package.
 30%|███       | 12/40 [01:22<02:43,  5.83s/it]2024-12-22 02:13:20,883 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:21,763 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:21,763 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1833])
2024-12-22 02:13:21,827 - [Process 4/5] - DEBUG - predict_token:tensor([[25281]], device='cuda:4')
2024-12-22 02:13:23,807 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:23,808 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2178])
2024-12-22 02:13:23,854 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:23,854 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2021])
2024-12-22 02:13:23,888 - [Process 0/5] - DEBUG - predict_token:tensor([[25556]], device='cuda:0')
2024-12-22 02:13:23,929 - [Process 3/5] - DEBUG - predict_token:tensor([[365]], device='cuda:3')
2024-12-22 02:13:24,161 - [Process 4/5] - INFO - res.shape is :torch.Size([56])
results:Lauren and Pam are discussing if Lauren is still needed for tomorrow. Pam confirms that she does need Lauren and will ring in the morning to let her know if there are any more shifts available. They also discuss Lauren's holiday.
 30%|███       | 12/40 [01:25<03:04,  6.60s/it]2024-12-22 02:13:24,323 - [Process 0/5] - INFO - res.shape is :torch.Size([9])
results:








 32%|███▎      | 13/40 [01:26<02:52,  6.40s/it]2024-12-22 02:13:24,387 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:24,583 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:24,924 - [Process 2/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 35%|███▌      | 14/40 [01:26<02:48,  6.48s/it]2024-12-22 02:13:24,950 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:24,950 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2178])
2024-12-22 02:13:25,032 - [Process 1/5] - DEBUG - predict_token:tensor([[12051]], device='cuda:1')
2024-12-22 02:13:25,067 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:25,481 - [Process 3/5] - INFO - res.shape is :torch.Size([35])
results:Linda missed the train and the next one is in one hour. Darcy is not too worried. Linda paid 80 euros for the ticket.
 32%|███▎      | 13/40 [01:27<02:31,  5.62s/it]2024-12-22 02:13:25,722 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:26,663 - [Process 1/5] - INFO - res.shape is :torch.Size([37])
results:Greg needs to stay after hours at work but Betsy can't pick up Johnny because she has to work long hours on Tuesdays at the kindergarten.
 32%|███▎      | 13/40 [01:28<02:38,  5.89s/it]2024-12-22 02:13:26,900 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:27,982 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:27,983 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2046])
2024-12-22 02:13:28,056 - [Process 4/5] - DEBUG - predict_token:tensor([[10537]], device='cuda:4')
2024-12-22 02:13:28,423 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:28,424 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 02:13:28,502 - [Process 0/5] - DEBUG - predict_token:tensor([[317]], device='cuda:0')
2024-12-22 02:13:28,856 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:28,856 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2135])
2024-12-22 02:13:28,929 - [Process 2/5] - DEBUG - predict_token:tensor([[11131]], device='cuda:2')
2024-12-22 02:13:29,313 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:

Dialogue: 


 38%|███▊      | 15/40 [01:31<02:26,  5.85s/it]2024-12-22 02:13:29,367 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:29,368 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 02:13:29,442 - [Process 3/5] - DEBUG - predict_token:tensor([[8432]], device='cuda:3')
2024-12-22 02:13:29,483 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:30,114 - [Process 0/5] - INFO - res.shape is :torch.Size([36])
results:

Dialogue: 


Please provide the dialogue in a short paragraph or a few sentences at a time, and I will help you summarize it.
 35%|███▌      | 14/40 [01:31<02:41,  6.22s/it]2024-12-22 02:13:30,360 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:30,370 - [Process 3/5] - INFO - res.shape is :torch.Size([20])
results:Missy will be out of work at 6 and they will have drinks after dinner.
 35%|███▌      | 14/40 [01:32<02:20,  5.40s/it]2024-12-22 02:13:30,586 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:31,414 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:31,414 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2467])
2024-12-22 02:13:31,506 - [Process 1/5] - DEBUG - predict_token:tensor([[390]], device='cuda:1')
2024-12-22 02:13:33,201 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:33,201 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1961])
2024-12-22 02:13:33,246 - [Process 1/5] - INFO - res.shape is :torch.Size([36])
results:Rael hates his job and wants to quit. Zach suggests he consider IT as a new career path. Rael is hesitant but agrees to consider it.
 35%|███▌      | 14/40 [01:35<02:38,  6.10s/it]2024-12-22 02:13:33,281 - [Process 2/5] - DEBUG - predict_token:tensor([[16543]], device='cuda:2')
2024-12-22 02:13:33,439 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:33,543 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
results:





 40%|████      | 16/40 [01:35<02:08,  5.36s/it]2024-12-22 02:13:33,658 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:33,678 - [Process 4/5] - INFO - res.shape is :torch.Size([128])
results:

Dialogue: 


Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 
 32%|███▎      | 13/40 [01:35<03:21,  7.48s/it]2024-12-22 02:13:33,939 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:34,335 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:34,336 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2148])
2024-12-22 02:13:34,416 - [Process 0/5] - DEBUG - predict_token:tensor([[6502]], device='cuda:0')
2024-12-22 02:13:34,612 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:34,612 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2249])
2024-12-22 02:13:34,694 - [Process 3/5] - DEBUG - predict_token:tensor([[28444]], device='cuda:3')
2024-12-22 02:13:35,851 - [Process 0/5] - INFO - res.shape is :torch.Size([31])
results:Martin won two cinema tickets online through a Facebook movie magazine. The new film with Redford. Will see it till the end of the week.
 38%|███▊      | 15/40 [01:37<02:31,  6.07s/it]2024-12-22 02:13:36,087 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:36,545 - [Process 3/5] - INFO - res.shape is :torch.Size([40])
results:Kaylin and Amir are sharing funny gifs of Cynthia and laughing about her crazy hair. They are not looking forward to going to school the next day.
 38%|███▊      | 15/40 [01:38<02:20,  5.63s/it]2024-12-22 02:13:36,796 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:37,190 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:37,190 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1944])
2024-12-22 02:13:37,271 - [Process 1/5] - DEBUG - predict_token:tensor([[26259]], device='cuda:1')
2024-12-22 02:13:37,308 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:37,308 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 02:13:37,381 - [Process 2/5] - DEBUG - predict_token:tensor([[4335]], device='cuda:2')
2024-12-22 02:13:37,720 - [Process 1/5] - INFO - res.shape is :torch.Size([10])
results:Dialogue: 





 38%|███▊      | 15/40 [01:39<02:20,  5.61s/it]2024-12-22 02:13:37,922 - [Process 2/5] - INFO - res.shape is :torch.Size([13])
results:Jeffrey can't see Tom and Elena.
 42%|████▎     | 17/40 [01:39<01:56,  5.07s/it]2024-12-22 02:13:37,944 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:37,946 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:37,946 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2282])
2024-12-22 02:13:38,028 - [Process 4/5] - DEBUG - predict_token:tensor([[20349]], device='cuda:4')
2024-12-22 02:13:38,078 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:38,834 - [Process 4/5] - INFO - res.shape is :torch.Size([18])
results:

Please let me know if you have any questions or need further clarification.
 35%|███▌      | 14/40 [01:40<02:56,  6.78s/it]2024-12-22 02:13:39,025 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:39,857 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:39,857 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2151])
2024-12-22 02:13:39,929 - [Process 0/5] - DEBUG - predict_token:tensor([[5918]], device='cuda:0')
2024-12-22 02:13:40,537 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:40,537 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 02:13:40,612 - [Process 3/5] - DEBUG - predict_token:tensor([[6502]], device='cuda:3')
2024-12-22 02:13:41,330 - [Process 0/5] - INFO - res.shape is :torch.Size([31])
results:Max shared his latest music project with Jim, who promised to listen to it later. Max is not sure if he will be a famous music producer.
 40%|████      | 16/40 [01:43<02:21,  5.89s/it]2024-12-22 02:13:41,528 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:41,587 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:41,588 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 02:13:41,662 - [Process 1/5] - DEBUG - predict_token:tensor([[26259]], device='cuda:1')
2024-12-22 02:13:41,849 - [Process 3/5] - INFO - res.shape is :torch.Size([27])
results:Martin just bought two jars of milk yesterday, but he will buy another one for Alex who is craving milk lately.
 40%|████      | 16/40 [01:43<02:12,  5.53s/it]2024-12-22 02:13:41,869 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:41,870 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2145])
2024-12-22 02:13:41,942 - [Process 2/5] - DEBUG - predict_token:tensor([[23738]], device='cuda:2')
2024-12-22 02:13:41,993 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:42,453 - [Process 1/5] - INFO - res.shape is :torch.Size([18])
results:Monica will send Natalie the recipe for her famous cheesecake.
 40%|████      | 16/40 [01:44<02:08,  5.34s/it]2024-12-22 02:13:42,715 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:43,104 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:43,104 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2217])
2024-12-22 02:13:43,185 - [Process 4/5] - DEBUG - predict_token:tensor([[27650]], device='cuda:4')
2024-12-22 02:13:44,060 - [Process 2/5] - INFO - res.shape is :torch.Size([52])
results:Milena asks Kate and Regina how the presentation went. They respond that it was good and they received positive feedback. They also share that only three people came to listen to their presentation, but they expected it to be a small and intimate event.
 45%|████▌     | 18/40 [01:45<01:58,  5.39s/it]2024-12-22 02:13:44,133 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:45,238 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:45,239 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1911])
2024-12-22 02:13:45,305 - [Process 3/5] - DEBUG - predict_token:tensor([[365]], device='cuda:3')
2024-12-22 02:13:45,318 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:45,318 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2243])
2024-12-22 02:13:45,392 - [Process 0/5] - DEBUG - predict_token:tensor([[399]], device='cuda:0')
2024-12-22 02:13:45,438 - [Process 4/5] - INFO - res.shape is :torch.Size([51])
results:Nathan wants to buy a bike in spring but doesn't have enough space in his apartment. He plans to hang it on the wall or keep it in the hallway. He also has a stationary bike for winter exercise.
 38%|███▊      | 15/40 [01:47<02:48,  6.73s/it]2024-12-22 02:13:45,616 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:46,431 - [Process 0/5] - INFO - res.shape is :torch.Size([22])
results:Wanda wants to have a party and Gina agrees to help her with the groceries.
 42%|████▎     | 17/40 [01:48<02:10,  5.66s/it]2024-12-22 02:13:46,607 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:46,782 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:46,782 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1665])
2024-12-22 02:13:46,832 - [Process 2/5] - DEBUG - predict_token:tensor([[14713]], device='cuda:2')
2024-12-22 02:13:47,567 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:47,567 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2587])
2024-12-22 02:13:47,664 - [Process 1/5] - DEBUG - predict_token:tensor([[24190]], device='cuda:1')
2024-12-22 02:13:48,422 - [Process 2/5] - INFO - res.shape is :torch.Size([42])
results:Bella and Eric were in the boss's room today. Bella tells Eric that the boss appreciated their decision to dismiss a client's request, despite the client being a potential one.
 48%|████▊     | 19/40 [01:50<01:46,  5.08s/it]2024-12-22 02:13:48,559 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:49,073 - [Process 3/5] - INFO - res.shape is :torch.Size([88])
results:Linda asks Helen if Jamie was at school today, and Helen replies that he was. She also mentions that Jack is sick again, and Linda expresses her concern. Helen recommends a pediatrician, Dr. Tornez, who is great with kids and treats each case individually. Helen also informs Linda that the school trip has been cancelled due to half of the class being sick.
 42%|████▎     | 17/40 [01:50<02:19,  6.04s/it]2024-12-22 02:13:49,202 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:50,024 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:50,025 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1947])
2024-12-22 02:13:50,093 - [Process 0/5] - DEBUG - predict_token:tensor([[4104]], device='cuda:0')
2024-12-22 02:13:50,158 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:50,158 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2388])
2024-12-22 02:13:50,247 - [Process 4/5] - DEBUG - predict_token:tensor([[23647]], device='cuda:4')
2024-12-22 02:13:50,730 - [Process 1/5] - INFO - res.shape is :torch.Size([66])
results:Nancy, Phil, Vic and Phil are discussing social media platforms. They talk about their preferences and what they use. Nancy asks about Instagram, Twitter, Facebook, Tumblr and other platforms. Vic and Phil have different preferences, but they all have their reasons for using the platforms they do.
 42%|████▎     | 17/40 [01:52<02:23,  6.23s/it]2024-12-22 02:13:50,851 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:51,667 - [Process 0/5] - INFO - res.shape is :torch.Size([37])
results:Madison saw an offer for a trip to Thailand today. Adam and Taylor are interested. Taylor mentions that children under 12 years old get 50% off.
 45%|████▌     | 18/40 [01:53<02:01,  5.53s/it]2024-12-22 02:13:51,865 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:51,865 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1530])
2024-12-22 02:13:51,916 - [Process 3/5] - DEBUG - predict_token:tensor([[476]], device='cuda:3')
2024-12-22 02:13:51,919 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:52,352 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:52,352 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2168])
2024-12-22 02:13:52,425 - [Process 2/5] - DEBUG - predict_token:tensor([[20916]], device='cuda:2')
2024-12-22 02:13:52,803 - [Process 3/5] - INFO - res.shape is :torch.Size([22])
results:

Dialogue: 


Please provide a few short sentences summarizing the dialogue.
 45%|████▌     | 18/40 [01:54<01:57,  5.35s/it]2024-12-22 02:13:52,852 - [Process 4/5] - INFO - res.shape is :torch.Size([57])
results:Adam and Rachel are talking about Rachel's new cat, Portia. Rachel is allergic to cats but doesn't want to get rid of her. Adam is supportive and suggests that she take medication to help with the allergy.
 40%|████      | 16/40 [01:54<02:46,  6.93s/it]2024-12-22 02:13:52,991 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:53,052 - [Process 2/5] - INFO - res.shape is :torch.Size([15])
results:Jack is going to the casting and Ryan wants to come with him.
 50%|█████     | 20/40 [01:54<01:38,  4.95s/it]2024-12-22 02:13:53,094 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:53,199 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:53,214 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:53,214 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1293])
2024-12-22 02:13:53,260 - [Process 1/5] - DEBUG - predict_token:tensor([[1913]], device='cuda:1')
2024-12-22 02:13:54,518 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:Dialogue: 



Please let me know if you have any questions or if you would like me to summarize any other dialogues.
 45%|████▌     | 18/40 [01:56<02:00,  5.49s/it]2024-12-22 02:13:54,704 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:55,726 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:55,726 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 02:13:55,800 - [Process 0/5] - DEBUG - predict_token:tensor([[2896]], device='cuda:0')
2024-12-22 02:13:56,609 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:56,609 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 02:13:56,681 - [Process 3/5] - DEBUG - predict_token:tensor([[12828]], device='cuda:3')
2024-12-22 02:13:57,135 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:57,135 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2258])
2024-12-22 02:13:57,217 - [Process 4/5] - DEBUG - predict_token:tensor([[402]], device='cuda:4')
2024-12-22 02:13:57,221 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:57,221 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2255])
2024-12-22 02:13:57,233 - [Process 0/5] - INFO - res.shape is :torch.Size([32])
results:Charlee is in a Portuguese class where they are preparing a performance of a Polish play translated into Portuguese. The play is by Mrożek.
 48%|████▊     | 19/40 [01:59<01:56,  5.54s/it]2024-12-22 02:13:57,304 - [Process 2/5] - DEBUG - predict_token:tensor([[5791]], device='cuda:2')
2024-12-22 02:13:57,383 - [Process 3/5] - INFO - res.shape is :torch.Size([15])
results:

Dialogue: 








 48%|████▊     | 19/40 [01:59<01:47,  5.12s/it]2024-12-22 02:13:57,435 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:57,574 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:13:58,765 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:13:58,766 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2136])
2024-12-22 02:13:58,847 - [Process 1/5] - DEBUG - predict_token:tensor([[10447]], device='cuda:1')
2024-12-22 02:14:00,495 - [Process 1/5] - INFO - res.shape is :torch.Size([35])
results:Jane wants to know what David's dad would like for Christmas and David suggests an iPad. David will order it online and have it shipped home.
 48%|████▊     | 19/40 [02:02<01:58,  5.64s/it]2024-12-22 02:14:00,583 - [Process 4/5] - INFO - res.shape is :torch.Size([74])
results:Gino is unsure of what to wear to an event and seeks advice from Renee. Renee suggests he wear a black shirt with black trousers and shoes, but Gino is unsure. Renee advises him to wear a white shirt with brown shoes instead, to avoid looking like a waiter.
 42%|████▎     | 17/40 [02:02<02:44,  7.17s/it]2024-12-22 02:14:00,730 - [Process 2/5] - INFO - res.shape is :torch.Size([82])
results:Sonia and Toni are planning to go to San Sebastian. Toni recommends an Airbnb place that they stayed in last year, which is next to Playa de la Concha. The landlady is an old Basque lady who is lonely but nice. She even made a tortilla for them. Sonia is unsure if they will use the Airbnb place.
 52%|█████▎    | 21/40 [02:02<01:49,  5.77s/it]2024-12-22 02:14:00,787 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:00,795 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:00,871 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:01,131 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:01,132 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1953])
2024-12-22 02:14:01,212 - [Process 0/5] - DEBUG - predict_token:tensor([[435]], device='cuda:0')
2024-12-22 02:14:01,531 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
results:






 50%|█████     | 20/40 [02:03<01:43,  5.17s/it]2024-12-22 02:14:01,650 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:01,650 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2180])
2024-12-22 02:14:01,730 - [Process 3/5] - DEBUG - predict_token:tensor([[24239]], device='cuda:3')
2024-12-22 02:14:01,798 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:02,189 - [Process 3/5] - INFO - res.shape is :torch.Size([10])
results:









 50%|█████     | 20/40 [02:03<01:40,  5.02s/it]2024-12-22 02:14:02,457 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:04,434 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:04,434 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:14:04,508 - [Process 4/5] - DEBUG - predict_token:tensor([[1913]], device='cuda:4')
2024-12-22 02:14:05,110 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:05,110 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2347])
2024-12-22 02:14:05,200 - [Process 1/5] - DEBUG - predict_token:tensor([[26631]], device='cuda:1')
2024-12-22 02:14:05,373 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:05,373 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 02:14:05,401 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:05,401 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2440])
2024-12-22 02:14:05,446 - [Process 0/5] - DEBUG - predict_token:tensor([[13727]], device='cuda:0')
2024-12-22 02:14:05,490 - [Process 2/5] - DEBUG - predict_token:tensor([[27650]], device='cuda:2')
2024-12-22 02:14:06,130 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:06,130 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2084])
2024-12-22 02:14:06,131 - [Process 4/5] - INFO - res.shape is :torch.Size([36])
results:Emily asked Amelia about her favorite color, and Amelia replied blue. Amelia can't tell Emily anything more about it, and Emily is curious.
 45%|████▌     | 18/40 [02:07<02:27,  6.68s/it]2024-12-22 02:14:06,204 - [Process 3/5] - DEBUG - predict_token:tensor([[21758]], device='cuda:3')
2024-12-22 02:14:06,414 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:07,837 - [Process 2/5] - INFO - res.shape is :torch.Size([55])
results:Nathan and Deborah are planning their trip for the next day. They will pack their things, take the girls to the dentist, and go on a trip to the forest. Nathan will buy some syrup for travel sickness on his way from work.
 55%|█████▌    | 22/40 [02:09<01:51,  6.17s/it]2024-12-22 02:14:07,973 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:09,116 - [Process 1/5] - INFO - res.shape is :torch.Size([86])
results:Sebastian has been in the new place for a year and it's the best time of his life. He learned to be resourceful, responsible and can make his dreams come true. He has someone he loves by his side. Kevin is happy for him but wishes he had someone by his side. Sebastian thinks Kevin could win the lottery if he devoted his life to analyzing the winning numbers.
 50%|█████     | 20/40 [02:10<02:10,  6.53s/it]2024-12-22 02:14:09,295 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:10,815 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:10,816 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2466])
2024-12-22 02:14:10,905 - [Process 4/5] - DEBUG - predict_token:tensor([[20212]], device='cuda:4')
2024-12-22 02:14:11,280 - [Process 0/5] - INFO - res.shape is :torch.Size([128])
results:

Dialogue: 


Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 
 52%|█████▎    | 21/40 [02:13<02:04,  6.54s/it]2024-12-22 02:14:11,566 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:11,992 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:11,992 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2260])
2024-12-22 02:14:12,068 - [Process 3/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































2024-12-22 02:14:12,074 - [Process 2/5] - DEBUG - predict_token:tensor([[13693]], device='cuda:2')
 52%|█████▎    | 21/40 [02:13<02:03,  6.48s/it]2024-12-22 02:14:12,327 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:12,867 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:12,867 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1977])
2024-12-22 02:14:12,941 - [Process 1/5] - DEBUG - predict_token:tensor([[28533]], device='cuda:1')
2024-12-22 02:14:13,168 - [Process 2/5] - INFO - res.shape is :torch.Size([26])
results:


Dialogue: 


Please let me know if you have any questions or need further clarification.
 57%|█████▊    | 23/40 [02:15<01:40,  5.92s/it]2024-12-22 02:14:13,267 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:13,926 - [Process 4/5] - INFO - res.shape is :torch.Size([65])
results:Benjamin wants to take a nap after a long day yesterday, but Hilary wants him to join them for lunch with some French people who work on the history of food in colonial Mexico. They will meet at the conference hall entrance at 2 pm and then go to La Cantina for Italian cuisine.
 48%|████▊     | 19/40 [02:15<02:27,  7.02s/it]2024-12-22 02:14:14,005 - [Process 1/5] - INFO - res.shape is :torch.Size([23])
results:Dialogue: 


Please let me know if you have any questions or need further clarification.
 52%|█████▎    | 21/40 [02:15<01:54,  6.04s/it]2024-12-22 02:14:14,125 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:14,267 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:15,366 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:15,367 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2124])
2024-12-22 02:14:15,440 - [Process 0/5] - DEBUG - predict_token:tensor([[10920]], device='cuda:0')
2024-12-22 02:14:16,214 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:16,214 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 02:14:16,296 - [Process 3/5] - DEBUG - predict_token:tensor([[9811]], device='cuda:3')
2024-12-22 02:14:16,689 - [Process 0/5] - INFO - res.shape is :torch.Size([28])
results:Jones and Angelina are planning to meet later in the afternoon in town. They will confirm the meeting place by calling each other.
 55%|█████▌    | 22/40 [02:18<01:51,  6.20s/it]2024-12-22 02:14:16,780 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:16,780 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-22 02:14:16,852 - [Process 2/5] - DEBUG - predict_token:tensor([[8660]], device='cuda:2')
2024-12-22 02:14:16,869 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:18,119 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:18,120 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2268])
2024-12-22 02:14:18,201 - [Process 4/5] - DEBUG - predict_token:tensor([[10447]], device='cuda:4')
2024-12-22 02:14:18,359 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:18,360 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2229])
2024-12-22 02:14:18,441 - [Process 1/5] - DEBUG - predict_token:tensor([[1530]], device='cuda:1')
2024-12-22 02:14:18,676 - [Process 3/5] - INFO - res.shape is :torch.Size([54])
results:Matt and Nick are having a conversation about the internet connection they have. Matt needs it for an application, but he can't remember the details. Nick doesn't watch anything when he's at home and doesn't know the internet connection either.
 55%|█████▌    | 22/40 [02:20<01:57,  6.52s/it]2024-12-22 02:14:18,705 - [Process 2/5] - INFO - res.shape is :torch.Size([46])
results:Judy is attracted to jerks, but it didn't work out with Andrew because he only wanted to have sex with her. She also had a problem with Bruce because he is too sweet for her.
 60%|██████    | 24/40 [02:20<01:32,  5.80s/it]2024-12-22 02:14:18,796 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:18,864 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:19,394 - [Process 1/5] - INFO - res.shape is :torch.Size([20])
results:Colin tells Ava that she is shorter than penguins, which makes her confused.
 55%|█████▌    | 22/40 [02:21<01:45,  5.84s/it]2024-12-22 02:14:19,602 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:20,261 - [Process 4/5] - INFO - res.shape is :torch.Size([45])
results:Jane had an allergic reaction after eating a cake at La Perle because it contained crushed peanuts despite her warning them she's allergic. She wants to sue the restaurant.
 50%|█████     | 20/40 [02:22<02:16,  6.81s/it]2024-12-22 02:14:20,514 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:20,899 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:20,900 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2182])
2024-12-22 02:14:20,981 - [Process 0/5] - DEBUG - predict_token:tensor([[350]], device='cuda:0')
2024-12-22 02:14:22,003 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:22,003 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1866])
2024-12-22 02:14:22,069 - [Process 2/5] - DEBUG - predict_token:tensor([[17773]], device='cuda:2')
2024-12-22 02:14:22,931 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:22,931 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2154])
2024-12-22 02:14:23,012 - [Process 3/5] - DEBUG - predict_token:tensor([[10686]], device='cuda:3')
2024-12-22 02:14:23,466 - [Process 0/5] - INFO - res.shape is :torch.Size([56])
results:Bella wants to know if Clara will be home tonight and if she can pop in. Clara will be back at 7 but wants to go out for a drink first. Bella wants to talk to Clara but doesn't want to drink beer or wine.
 57%|█████▊    | 23/40 [02:25<01:48,  6.37s/it]2024-12-22 02:14:23,677 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:23,677 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2228])
2024-12-22 02:14:23,680 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:23,758 - [Process 1/5] - DEBUG - predict_token:tensor([[26259]], device='cuda:1')
2024-12-22 02:14:24,237 - [Process 2/5] - INFO - res.shape is :torch.Size([55])
results:Kristina is watching America's top model on TV. She is excited about the new season. Jannette is not home yet and Estefania is also watching the show. They both think Tyra Banks is great and want to look like her.
 62%|██████▎   | 25/40 [02:26<01:25,  5.72s/it]2024-12-22 02:14:24,368 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:24,580 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:24,581 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2207])
2024-12-22 02:14:24,662 - [Process 4/5] - DEBUG - predict_token:tensor([[10447]], device='cuda:4')
2024-12-22 02:14:25,529 - [Process 1/5] - INFO - res.shape is :torch.Size([39])
results:Natalie wants to go to a new club at Regents Street with Judy. Judy is going on Saturday with Miranda and Helen. Natalie wants to go on Friday.
 57%|█████▊    | 23/40 [02:27<01:40,  5.93s/it]2024-12-22 02:14:25,756 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:27,006 - [Process 4/5] - INFO - res.shape is :torch.Size([52])
results:Jane is running late and will be by the left entrance. Alex is waiting for her. Jane remembers to bring the file with xerox copies. They hope that their materials will be useful and convince the people they are trying to reach.
 52%|█████▎    | 21/40 [02:28<02:09,  6.79s/it]2024-12-22 02:14:27,188 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:27,989 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:27,990 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2357])
2024-12-22 02:14:28,080 - [Process 0/5] - DEBUG - predict_token:tensor([[23529]], device='cuda:0')
2024-12-22 02:14:28,382 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:28,382 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2132])
2024-12-22 02:14:28,462 - [Process 2/5] - DEBUG - predict_token:tensor([[7740]], device='cuda:2')
2024-12-22 02:14:28,645 - [Process 3/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 57%|█████▊    | 23/40 [02:30<02:08,  7.56s/it]2024-12-22 02:14:28,856 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:29,343 - [Process 0/5] - INFO - res.shape is :torch.Size([27])
results:Gary is driving for Uber and enjoying it. Ellie is surprised as Gary is not good at meeting new people.
 60%|██████    | 24/40 [02:31<01:39,  6.23s/it]2024-12-22 02:14:29,386 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:29,387 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 02:14:29,459 - [Process 1/5] - DEBUG - predict_token:tensor([[323]], device='cuda:1')
2024-12-22 02:14:29,590 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:29,822 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
results:







 60%|██████    | 24/40 [02:31<01:27,  5.44s/it]2024-12-22 02:14:30,066 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:30,645 - [Process 2/5] - INFO - res.shape is :torch.Size([53])
results:Avril asks Frank if he has plans for the weekend. Frank says no and Avril suggests mushroom picking, which Frank finds ridiculous. Avril then invites Frank to see horse racing and Frank agrees to come with her.
 65%|██████▌   | 26/40 [02:32<01:22,  5.93s/it]2024-12-22 02:14:30,760 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:30,935 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:30,935 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1945])
2024-12-22 02:14:31,015 - [Process 4/5] - DEBUG - predict_token:tensor([[476]], device='cuda:4')
2024-12-22 02:14:31,506 - [Process 4/5] - INFO - res.shape is :torch.Size([11])
results:
Dialogue: 





 55%|█████▌    | 22/40 [02:33<01:49,  6.10s/it]2024-12-22 02:14:31,734 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:32,493 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:32,493 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2064])
2024-12-22 02:14:32,566 - [Process 3/5] - DEBUG - predict_token:tensor([[349]], device='cuda:3')
2024-12-22 02:14:32,759 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
results:



 60%|██████    | 24/40 [02:34<01:44,  6.52s/it]2024-12-22 02:14:32,979 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:33,351 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:33,351 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2285])
2024-12-22 02:14:33,422 - [Process 0/5] - DEBUG - predict_token:tensor([[1260]], device='cuda:0')
2024-12-22 02:14:33,721 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:33,721 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2090])
2024-12-22 02:14:33,795 - [Process 1/5] - DEBUG - predict_token:tensor([[7861]], device='cuda:1')
2024-12-22 02:14:34,288 - [Process 1/5] - INFO - res.shape is :torch.Size([11])
results:










 62%|██████▎   | 25/40 [02:36<01:17,  5.15s/it]2024-12-22 02:14:34,536 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:34,580 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:34,580 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2139])
2024-12-22 02:14:34,654 - [Process 2/5] - DEBUG - predict_token:tensor([[8507]], device='cuda:2')
2024-12-22 02:14:35,087 - [Process 0/5] - INFO - res.shape is :torch.Size([38])
results:Elena wishes Dorothea a happy birthday and asks if she is going to celebrate. Dorothea says she is going to meet Tom and eat something in the town.
 62%|██████▎   | 25/40 [02:36<01:31,  6.08s/it]2024-12-22 02:14:35,272 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:35,363 - [Process 2/5] - INFO - res.shape is :torch.Size([17])
results:Finn wants to track his shipment and Jim helps him find the information.
 68%|██████▊   | 27/40 [02:37<01:12,  5.56s/it]2024-12-22 02:14:35,489 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:35,803 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:35,803 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2182])
2024-12-22 02:14:35,883 - [Process 4/5] - DEBUG - predict_token:tensor([[8251]], device='cuda:4')
2024-12-22 02:14:37,577 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:37,577 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2387])
2024-12-22 02:14:37,668 - [Process 3/5] - DEBUG - predict_token:tensor([[11783]], device='cuda:3')
2024-12-22 02:14:38,295 - [Process 4/5] - INFO - res.shape is :torch.Size([55])
results:Callan's Samsung S8 is not working properly and he thinks it might be due to overheating. He will take it to the store and let them see it since it's still under warranty. He always backs up his files.
 57%|█████▊    | 23/40 [02:40<01:47,  6.31s/it]2024-12-22 02:14:38,374 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:38,375 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2136])
2024-12-22 02:14:38,447 - [Process 1/5] - DEBUG - predict_token:tensor([[23738]], device='cuda:1')
2024-12-22 02:14:38,491 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:39,187 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:39,187 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 02:14:39,264 - [Process 2/5] - DEBUG - predict_token:tensor([[4335]], device='cuda:2')
2024-12-22 02:14:39,288 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:39,288 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2300])
2024-12-22 02:14:39,371 - [Process 0/5] - DEBUG - predict_token:tensor([[5322]], device='cuda:0')
2024-12-22 02:14:40,313 - [Process 1/5] - INFO - res.shape is :torch.Size([43])
results:Kate broke her arm and needs to go to the hospital. Greg is not sure if their medical insurance covers hospital costs. Mel asks what happened. Kate will call Linda or ask at the reception.
 65%|██████▌   | 26/40 [02:42<01:15,  5.41s/it]2024-12-22 02:14:40,428 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:40,492 - [Process 2/5] - INFO - res.shape is :torch.Size([30])
results:Tom and Ben have decided to meet at 2 pm in the Oval Room. Tom tells Ben to bring all his papers for a fight.
 70%|███████   | 28/40 [02:42<01:05,  5.43s/it]2024-12-22 02:14:40,615 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:40,856 - [Process 3/5] - INFO - res.shape is :torch.Size([69])
results:Adam is concerned about May's mental health and has talked to Karen about it. Karen agrees that it is a serious issue and suggests May should see a specialist, but May is not interested. Adam wants to help May but is unsure of how. Karen offers to call a psychologist friend for advice.
 62%|██████▎   | 25/40 [02:42<01:44,  7.00s/it]2024-12-22 02:14:41,098 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:42,153 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:42,154 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 02:14:42,228 - [Process 4/5] - DEBUG - predict_token:tensor([[2610]], device='cuda:4')
2024-12-22 02:14:42,734 - [Process 0/5] - INFO - res.shape is :torch.Size([73])
results:Charles and Camilla are planning to visit the German markets in Birmingham and Manchester. They will go for the sausage and beer. Camilla hasn't been to the markets for ages and Charles recommends trying the mulled wine. They plan to make their own mulled wine at Christmas but nobody else drank it last year.
 65%|██████▌   | 26/40 [02:44<01:31,  6.55s/it]2024-12-22 02:14:42,959 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:43,243 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:43,244 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1581])
2024-12-22 02:14:43,303 - [Process 1/5] - DEBUG - predict_token:tensor([[2627]], device='cuda:1')
2024-12-22 02:14:43,777 - [Process 4/5] - INFO - res.shape is :torch.Size([35])
results:Maya asks Boris to bring home the clothes that are hanging outside. Boris is not home and tells Maya to tell Brian to take care of it.
 60%|██████    | 24/40 [02:45<01:36,  6.06s/it]2024-12-22 02:14:43,973 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:44,642 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:44,642 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2300])
2024-12-22 02:14:44,726 - [Process 2/5] - DEBUG - predict_token:tensor([[3467]], device='cuda:2')
2024-12-22 02:14:45,196 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:45,196 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2222])
2024-12-22 02:14:45,277 - [Process 3/5] - DEBUG - predict_token:tensor([[1425]], device='cuda:3')
2024-12-22 02:14:46,657 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:46,657 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 02:14:46,695 - [Process 2/5] - INFO - res.shape is :torch.Size([47])
results:Karen is asking Peter about the road from the swimming pool to Waitrose, which is blocked with vans. Peter thinks it might be repairs workers. Karen suggests avoiding the road and offers help if needed.
 72%|███████▎  | 29/40 [02:48<01:02,  5.66s/it]2024-12-22 02:14:46,737 - [Process 0/5] - DEBUG - predict_token:tensor([[22264]], device='cuda:0')
2024-12-22 02:14:46,815 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:47,216 - [Process 0/5] - INFO - res.shape is :torch.Size([10])
results:
Dialogue: 




 68%|██████▊   | 27/40 [02:48<01:17,  5.93s/it]2024-12-22 02:14:47,330 - [Process 3/5] - INFO - res.shape is :torch.Size([46])
results:Erin wants to do an interview with Ashley, who is at a camp. They arrange to meet at the restaurant area. Ashley mentions that the wifi is good but can be spotty in other areas.
 65%|██████▌   | 26/40 [02:49<01:35,  6.84s/it]2024-12-22 02:14:47,483 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:47,568 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:47,738 - [Process 1/5] - INFO - res.shape is :torch.Size([108])
results:Janet is ashamed of who voted for Trump. Alison compares him to the Wizard of Oz. Nicole calls him a sissy boy. Cheryl uses the term "Pussy in Chief". Linda calls him selfish and inconsiderate. Roz misses Elsie and talks about Trump's hair. Leslie and Eric express their disgust at Trump. Sue says there is no excuse for missing the ceremony. Linda jokes about the umbrella.
 68%|██████▊   | 27/40 [02:49<01:18,  6.02s/it]2024-12-22 02:14:47,961 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:48,040 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:48,040 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2127])
2024-12-22 02:14:48,122 - [Process 4/5] - DEBUG - predict_token:tensor([[10447]], device='cuda:4')
2024-12-22 02:14:50,532 - [Process 4/5] - INFO - res.shape is :torch.Size([55])
results:Jane and Steven are planning to meet at 4:30 instead of 5 due to the long distance. Steven thinks they can make it in 2 hours but the road is new, so they might be late. Jane will wait at the main entrance.
 62%|██████▎   | 25/40 [02:52<01:34,  6.27s/it]2024-12-22 02:14:50,766 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:50,828 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:50,828 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2154])
2024-12-22 02:14:50,908 - [Process 2/5] - DEBUG - predict_token:tensor([[5043]], device='cuda:2')
2024-12-22 02:14:51,071 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:51,071 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 02:14:51,143 - [Process 0/5] - DEBUG - predict_token:tensor([[3457]], device='cuda:0')
2024-12-22 02:14:51,404 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:51,405 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 02:14:51,485 - [Process 3/5] - DEBUG - predict_token:tensor([[365]], device='cuda:3')
2024-12-22 02:14:51,800 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:51,800 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2139])
2024-12-22 02:14:51,873 - [Process 1/5] - DEBUG - predict_token:tensor([[17468]], device='cuda:1')
2024-12-22 02:14:52,152 - [Process 3/5] - INFO - res.shape is :torch.Size([14])
results:Lilly is running late and Gabriel suggests ordering food for her.
 68%|██████▊   | 27/40 [02:53<01:21,  6.23s/it]2024-12-22 02:14:52,393 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:52,748 - [Process 1/5] - INFO - res.shape is :torch.Size([20])
results:Randolph asks Maya to buy him some earplugs from the pharmacy.
 70%|███████   | 28/40 [02:54<01:08,  5.71s/it]2024-12-22 02:14:52,881 - [Process 0/5] - INFO - res.shape is :torch.Size([39])
results:Biwott asked Chloe if she watched the series he told her to watch but Chloe said she hasn't had time yet and will watch it during the weekend.
 70%|███████   | 28/40 [02:54<01:10,  5.85s/it]2024-12-22 02:14:52,963 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:53,096 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:53,130 - [Process 2/5] - INFO - res.shape is :torch.Size([54])
results:They are wishing each other a Merry Christmas, Happy Holidays, and a Happy New Year. They are excited for the summer and traveling adventures, including Cuba, Mexico, and Thailand. Jose offers to be Ricky's guide.
 75%|███████▌  | 30/40 [02:54<00:58,  5.90s/it]2024-12-22 02:14:53,245 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:54,863 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:54,864 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2193])
2024-12-22 02:14:54,946 - [Process 4/5] - DEBUG - predict_token:tensor([[9596]], device='cuda:4')
2024-12-22 02:14:56,480 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:56,481 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2195])
2024-12-22 02:14:56,561 - [Process 3/5] - DEBUG - predict_token:tensor([[19663]], device='cuda:3')
2024-12-22 02:14:57,019 - [Process 4/5] - INFO - res.shape is :torch.Size([47])
results:Ray is locked in a room and needs Max to open the door. Max is not in the room and can't open the door. Ray needs to pee and asks Max to ask someone else to open the door.
 65%|██████▌   | 26/40 [02:58<01:28,  6.33s/it]2024-12-22 02:14:57,074 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:57,074 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2320])
2024-12-22 02:14:57,158 - [Process 1/5] - DEBUG - predict_token:tensor([[19802]], device='cuda:1')
2024-12-22 02:14:57,284 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:57,351 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:57,351 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2405])
2024-12-22 02:14:57,434 - [Process 0/5] - DEBUG - predict_token:tensor([[306]], device='cuda:0')
2024-12-22 02:14:57,948 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:14:57,948 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2606])
2024-12-22 02:14:58,038 - [Process 2/5] - DEBUG - predict_token:tensor([[21116]], device='cuda:2')
2024-12-22 02:14:58,472 - [Process 3/5] - INFO - res.shape is :torch.Size([42])
results:Kathy shared a photo of her aunt getting a haircut and mentioned she might get something done today too. Olivia is just chilling and Kathy shared some photos of her hair.
 70%|███████   | 28/40 [03:00<01:15,  6.26s/it]2024-12-22 02:14:58,768 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:59,038 - [Process 1/5] - INFO - res.shape is :torch.Size([42])
results:Olivia needs to do her accounts, upload videos to YouTube and do post-production. Jake doesn't have time for post-production and has faced copyright strikes on his videos.
 72%|███████▎  | 29/40 [03:00<01:04,  5.89s/it]2024-12-22 02:14:59,113 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:14:59,301 - [Process 0/5] - INFO - res.shape is :torch.Size([40])
results:Iris, Ken, Luke, and Julia are discussing how to fix an expense issue in a group. They need to add a person back to the group to edit the expense.
 72%|███████▎  | 29/40 [03:01<01:06,  6.02s/it]2024-12-22 02:14:59,578 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:00,732 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:00,732 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 891])
2024-12-22 02:15:00,765 - [Process 1/5] - DEBUG - predict_token:tensor([[24703]], device='cuda:1')
2024-12-22 02:15:00,922 - [Process 2/5] - INFO - res.shape is :torch.Size([67])
results:Jen is fed up with her boyfriend who is abusive, irresponsible and disrespectful. Jen has lent him money but he refuses to pay her back. Jen is afraid to ask him to leave because it will cause another fight. Jane advises Jen to cut her losses and move on.
 78%|███████▊  | 31/40 [03:02<00:58,  6.46s/it]2024-12-22 02:15:01,070 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:01,145 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:01,146 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2143])
2024-12-22 02:15:01,220 - [Process 4/5] - DEBUG - predict_token:tensor([[26422]], device='cuda:4')
2024-12-22 02:15:02,432 - [Process 1/5] - INFO - res.shape is :torch.Size([38])
results:Mia and Steven are going to watch a movie at 9 pm and would like to have something to eat before that. They decide to meet at 8 pm at Chinese restaurant.
 75%|███████▌  | 30/40 [03:04<00:51,  5.14s/it]2024-12-22 02:15:02,605 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:02,605 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2037])
2024-12-22 02:15:02,677 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:02,686 - [Process 3/5] - DEBUG - predict_token:tensor([[315]], device='cuda:3')
2024-12-22 02:15:03,174 - [Process 4/5] - INFO - res.shape is :torch.Size([43])
results:Emma invites Abigail for a stroll with their kids, but Abigail declines as her smog alert app shows that the norms have been exceeded by 30%.
 68%|██████▊   | 27/40 [03:04<01:21,  6.28s/it]2024-12-22 02:15:03,185 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:03,185 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1909])
2024-12-22 02:15:03,265 - [Process 0/5] - DEBUG - predict_token:tensor([[379]], device='cuda:0')
2024-12-22 02:15:03,382 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:03,516 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
results:




 75%|███████▌  | 30/40 [03:05<00:54,  5.48s/it]2024-12-22 02:15:03,719 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:04,228 - [Process 3/5] - INFO - res.shape is :torch.Size([35])
results:Celine and Mia went skating. Mark doesn't know how to skate, but they had fun. Mia wishes she could be there with them.
 72%|███████▎  | 29/40 [03:06<01:07,  6.11s/it]2024-12-22 02:15:04,475 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:04,890 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:04,890 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2141])
2024-12-22 02:15:04,964 - [Process 2/5] - DEBUG - predict_token:tensor([[4667]], device='cuda:2')
2024-12-22 02:15:06,239 - [Process 2/5] - INFO - res.shape is :torch.Size([31])
results:Emma and William are in a queue and waiting for something. William has been waiting for 20 minutes and Emma is getting impatient.
 80%|████████  | 32/40 [03:08<00:48,  6.12s/it]2024-12-22 02:15:06,366 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:06,744 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:06,745 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 02:15:06,825 - [Process 1/5] - DEBUG - predict_token:tensor([[12828]], device='cuda:1')
2024-12-22 02:15:07,382 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:07,383 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2244])
2024-12-22 02:15:07,464 - [Process 4/5] - DEBUG - predict_token:tensor([[678]], device='cuda:4')
2024-12-22 02:15:07,721 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:07,722 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2244])
2024-12-22 02:15:07,804 - [Process 0/5] - DEBUG - predict_token:tensor([[323]], device='cuda:0')
2024-12-22 02:15:08,329 - [Process 1/5] - INFO - res.shape is :torch.Size([33])
results:Mike wants someone to do the washing up. Sara offers to do it. Sam is disappointed he can't join them to the cinema.
 78%|███████▊  | 31/40 [03:10<00:48,  5.37s/it]2024-12-22 02:15:08,355 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:08,356 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2158])
2024-12-22 02:15:08,430 - [Process 3/5] - DEBUG - predict_token:tensor([[25281]], device='cuda:3')
2024-12-22 02:15:08,559 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:10,021 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:10,021 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 02:15:10,069 - [Process 3/5] - INFO - res.shape is :torch.Size([35])
results:Pam needs Robert's help with Tom's birthday celebration. She needs him to pick up some balloons from a store in the city center.
 75%|███████▌  | 30/40 [03:11<01:00,  6.03s/it]2024-12-22 02:15:10,096 - [Process 2/5] - DEBUG - predict_token:tensor([[3760]], device='cuda:2')
2024-12-22 02:15:10,218 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:10,480 - [Process 2/5] - INFO - res.shape is :torch.Size([9])
results:








 82%|████████▎ | 33/40 [03:12<00:38,  5.56s/it]2024-12-22 02:15:10,593 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:11,143 - [Process 0/5] - INFO - res.shape is :torch.Size([73])
results:Tina and Steve are planning to have pasta for dinner with broccoli, ham, cheese, and cream. They will go shopping together after work. Steve is not good at shopping lists, so they will do it together. Tina wants to meet in the car park after work. They will leave work at the usual time.
 78%|███████▊  | 31/40 [03:12<00:55,  6.12s/it]2024-12-22 02:15:11,341 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:11,641 - [Process 4/5] - INFO - res.shape is :torch.Size([93])
results:Lesley wants Chloe to go for a walk with the dog when she gets home. Chloe doesn't know when she will be home as she plans to go to Megan's house that night. Lesley reminds Chloe that her dad has to work late and no one will be home for a long time, so someone needs to come home and let the dog out. Chloe agrees to do it.
 70%|███████   | 28/40 [03:13<01:23,  6.94s/it]2024-12-22 02:15:11,827 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:12,403 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:12,403 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 02:15:12,476 - [Process 1/5] - DEBUG - predict_token:tensor([[3739]], device='cuda:1')
2024-12-22 02:15:13,125 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:13,125 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1697])
2024-12-22 02:15:13,185 - [Process 3/5] - DEBUG - predict_token:tensor([[11783]], device='cuda:3')
2024-12-22 02:15:13,869 - [Process 1/5] - INFO - res.shape is :torch.Size([32])
results:Paul will be home later than expected, so Lena shouldn't wait for him. He will call her in 15 minutes to explain why.
 80%|████████  | 32/40 [03:15<00:43,  5.42s/it]2024-12-22 02:15:14,030 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:14,933 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:14,933 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 02:15:15,007 - [Process 0/5] - DEBUG - predict_token:tensor([[2819]], device='cuda:0')
2024-12-22 02:15:15,092 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:15,092 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2421])
2024-12-22 02:15:15,182 - [Process 2/5] - DEBUG - predict_token:tensor([[6015]], device='cuda:2')
2024-12-22 02:15:15,440 - [Process 3/5] - INFO - res.shape is :torch.Size([54])
results:Adam has a juicy gossip about Iga and her boyfriend. They had to cancel their weekend getaway because he couldn't convince his group to change the date of the presentation. Adam thinks Iga is upset about it.
 78%|███████▊  | 31/40 [03:17<00:52,  5.83s/it]2024-12-22 02:15:15,683 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:15,703 - [Process 0/5] - INFO - res.shape is :torch.Size([15])
results:Christie and Katie are tired after the party but had fun.
 80%|████████  | 32/40 [03:17<00:45,  5.65s/it]2024-12-22 02:15:15,899 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:16,159 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:16,159 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2390])
2024-12-22 02:15:16,241 - [Process 4/5] - DEBUG - predict_token:tensor([[2812]], device='cuda:4')
2024-12-22 02:15:17,276 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:17,276 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1892])
2024-12-22 02:15:17,342 - [Process 1/5] - DEBUG - predict_token:tensor([[1670]], device='cuda:1')
2024-12-22 02:15:17,695 - [Process 2/5] - INFO - res.shape is :torch.Size([59])
results:Claire and Aaron are discussing a conference about relations at school. Claire wants to attend the workshop on teamwork, while Aaron is giving two lectures and is also a host. Claire is interested in mindfulness workshop and Aaron recommends it.
 85%|████████▌ | 34/40 [03:19<00:36,  6.05s/it]2024-12-22 02:15:17,814 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:18,180 - [Process 4/5] - INFO - res.shape is :torch.Size([42])
results:Emily broke one of Linda's green tea cups and feels terrible about it. Linda is not bothered and offers Emily the whole green set of cups if she wants them.
 72%|███████▎  | 29/40 [03:19<01:14,  6.82s/it]2024-12-22 02:15:18,438 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:19,784 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:19,784 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2133])
2024-12-22 02:15:19,867 - [Process 3/5] - DEBUG - predict_token:tensor([[2994]], device='cuda:3')
2024-12-22 02:15:20,452 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:20,452 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2399])
2024-12-22 02:15:20,541 - [Process 0/5] - DEBUG - predict_token:tensor([[23010]], device='cuda:0')
2024-12-22 02:15:20,756 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
results:



 82%|████████▎ | 33/40 [03:22<00:38,  5.47s/it]2024-12-22 02:15:21,019 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:21,610 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:21,610 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2032])
2024-12-22 02:15:21,689 - [Process 2/5] - DEBUG - predict_token:tensor([[8081]], device='cuda:2')
2024-12-22 02:15:21,735 - [Process 3/5] - INFO - res.shape is :torch.Size([42])
results:Corbin reports school violence in Jungang high school to Dimitri, who is in charge of the department. Corbin is worried about his safety and gives his phone number to Dimitri.
 80%|████████  | 32/40 [03:23<00:47,  5.97s/it]2024-12-22 02:15:21,984 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:22,054 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:22,054 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2004])
2024-12-22 02:15:22,128 - [Process 4/5] - DEBUG - predict_token:tensor([[922]], device='cuda:4')
2024-12-22 02:15:22,608 - [Process 4/5] - INFO - res.shape is :torch.Size([10])
results:









 75%|███████▌  | 30/40 [03:24<01:00,  6.10s/it]2024-12-22 02:15:22,810 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:22,838 - [Process 1/5] - INFO - res.shape is :torch.Size([128])
results:



Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 


Dialogue: 

 82%|████████▎ | 33/40 [03:24<00:45,  6.48s/it]2024-12-22 02:15:23,053 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:24,597 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:24,598 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 02:15:24,671 - [Process 0/5] - DEBUG - predict_token:tensor([[19122]], device='cuda:0')
2024-12-22 02:15:24,949 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
results:





 85%|████████▌ | 34/40 [03:26<00:30,  5.09s/it]2024-12-22 02:15:25,056 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:25,876 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:25,876 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1493])
2024-12-22 02:15:25,978 - [Process 3/5] - DEBUG - predict_token:tensor([[21116]], device='cuda:3')
2024-12-22 02:15:26,859 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:26,859 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2153])
2024-12-22 02:15:26,902 - [Process 2/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 88%|████████▊ | 35/40 [03:28<00:35,  7.00s/it]2024-12-22 02:15:26,940 - [Process 4/5] - DEBUG - predict_token:tensor([[26422]], device='cuda:4')
2024-12-22 02:15:26,960 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:26,960 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2172])
2024-12-22 02:15:27,008 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:27,037 - [Process 1/5] - DEBUG - predict_token:tensor([[306]], device='cuda:1')
2024-12-22 02:15:27,385 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:27,385 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1301])
2024-12-22 02:15:27,431 - [Process 0/5] - DEBUG - predict_token:tensor([[498]], device='cuda:0')
2024-12-22 02:15:27,839 - [Process 4/5] - INFO - res.shape is :torch.Size([19])
results:



Please let me know if you have any questions or need further assistance!
 78%|███████▊  | 31/40 [03:29<00:52,  5.84s/it]2024-12-22 02:15:28,110 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:28,677 - [Process 1/5] - INFO - res.shape is :torch.Size([36])
results:Iris's husband is famous. Pete has one interview that doesn't make him famous. Iris's parents are happy that she married a decent man.
 85%|████████▌ | 34/40 [03:30<00:37,  6.29s/it]2024-12-22 02:15:28,796 - [Process 0/5] - INFO - res.shape is :torch.Size([30])
results:Thelma wants to look wonderful but has nothing to wear. Louisa offers her a red velvet dress and Thelma is grateful.
 88%|████████▊ | 35/40 [03:30<00:23,  4.72s/it]2024-12-22 02:15:28,897 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:29,070 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:30,635 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:30,635 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-22 02:15:30,710 - [Process 2/5] - DEBUG - predict_token:tensor([[4699]], device='cuda:2')
2024-12-22 02:15:31,601 - [Process 3/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 82%|████████▎ | 33/40 [03:33<00:49,  7.14s/it]2024-12-22 02:15:31,747 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:31,748 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 02:15:31,819 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:31,820 - [Process 4/5] - DEBUG - predict_token:tensor([[14227]], device='cuda:4')
2024-12-22 02:15:32,460 - [Process 2/5] - INFO - res.shape is :torch.Size([43])
results:David wants a new tattoo but has no idea what to get. Mike suggests a skull and rose but thinks it's too common. David wants something more personal but doesn't know what.
 90%|█████████ | 36/40 [03:34<00:26,  6.57s/it]2024-12-22 02:15:32,550 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:32,654 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:32,654 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 02:15:32,726 - [Process 0/5] - DEBUG - predict_token:tensor([[22354]], device='cuda:0')
2024-12-22 02:15:32,758 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:32,759 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2113])
2024-12-22 02:15:32,833 - [Process 1/5] - DEBUG - predict_token:tensor([[360]], device='cuda:1')
2024-12-22 02:15:33,429 - [Process 0/5] - INFO - res.shape is :torch.Size([16])
results:Tobias and Trevor want to have a beer after work.
 90%|█████████ | 36/40 [03:35<00:18,  4.69s/it]2024-12-22 02:15:33,666 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:33,926 - [Process 4/5] - INFO - res.shape is :torch.Size([47])
results:Celia and Mike are discussing where they want to go for their holiday. Mike suggests Egypt, but Celia thinks it's too hot. Mike then suggests Croatia, which Celia thinks is a good idea.
 80%|████████  | 32/40 [03:35<00:47,  5.91s/it]2024-12-22 02:15:34,092 - [Process 1/5] - INFO - res.shape is :torch.Size([27])
results:Derek asks Judy to feed his animals on Friday and Saturday, and he might give her his keys on Thursday.
 88%|████████▊ | 35/40 [03:35<00:30,  6.03s/it]2024-12-22 02:15:34,151 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:34,283 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:35,905 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:35,905 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2347])
2024-12-22 02:15:35,987 - [Process 3/5] - DEBUG - predict_token:tensor([[11254]], device='cuda:3')
2024-12-22 02:15:36,530 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:36,530 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2251])
2024-12-22 02:15:36,611 - [Process 2/5] - DEBUG - predict_token:tensor([[2259]], device='cuda:2')
2024-12-22 02:15:37,835 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:37,835 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 02:15:37,912 - [Process 4/5] - DEBUG - predict_token:tensor([[13727]], device='cuda:4')
2024-12-22 02:15:37,968 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:37,968 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2333])
2024-12-22 02:15:38,058 - [Process 0/5] - DEBUG - predict_token:tensor([[11230]], device='cuda:0')
2024-12-22 02:15:38,127 - [Process 3/5] - INFO - res.shape is :torch.Size([48])
results:Simon wants to sing at the school concert and Freddy agrees. Simon needs time to practice and the concert is on Friday. Freddy will send Simon three singing tracks and Simon will play guitar for one or two of them.
 85%|████████▌ | 34/40 [03:39<00:41,  6.95s/it]2024-12-22 02:15:38,310 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:38,311 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2260])
2024-12-22 02:15:38,369 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:38,390 - [Process 1/5] - DEBUG - predict_token:tensor([[27036]], device='cuda:1')
2024-12-22 02:15:40,133 - [Process 1/5] - INFO - res.shape is :torch.Size([37])
results:Wayne and Tommy had a good weekend together. Tommy enjoyed the angling and Wayne promised to send him pictures of their trip. They will talk later in the evening.
 90%|█████████ | 36/40 [03:41<00:24,  6.03s/it]2024-12-22 02:15:40,403 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:40,789 - [Process 0/5] - INFO - res.shape is :torch.Size([61])
results:Anna and Peter are discussing an app that helps people choose what to wear. Anna wants to know how many clothes Peter has and how it would be helpful for him to use the app. Peter is skeptical and doesn't think he needs an app to tell him what to wear.
 92%|█████████▎| 37/40 [03:42<00:16,  5.49s/it]2024-12-22 02:15:41,042 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:41,930 - [Process 2/5] - INFO - res.shape is :torch.Size([128])
results:John plays a tank game where he battles other players around the world. He can't be attacked by players higher ranked than him. Andrew and Brett discuss other RPG games, including World of Warcraft, where players can team up and do quests together, but higher ranked players can kill lower ranked ones. Brett's cousin met his wife through the game, and they are still together. Andrew's favorite RPG series is Final Fantasy, but Brett has not heard of it. Andrew explains that in the last part of the game, the player plays a prince whose kingdom has been destroyed and tries to
 92%|█████████▎| 37/40 [03:43<00:22,  7.44s/it]2024-12-22 02:15:41,960 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:41,960 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2108])
2024-12-22 02:15:42,030 - [Process 3/5] - DEBUG - predict_token:tensor([[8649]], device='cuda:3')
2024-12-22 02:15:42,099 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:43,159 - [Process 3/5] - INFO - res.shape is :torch.Size([26])
results:Millie is sick and won't come to the gathering today. Sal sends her well wishes and a hug.
 88%|████████▊ | 35/40 [03:44<00:31,  6.38s/it]2024-12-22 02:15:43,310 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:43,552 - [Process 4/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 82%|████████▎ | 33/40 [03:45<00:49,  7.03s/it]2024-12-22 02:15:43,766 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:44,281 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:44,281 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2108])
2024-12-22 02:15:44,362 - [Process 1/5] - DEBUG - predict_token:tensor([[323]], device='cuda:1')
2024-12-22 02:15:45,035 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:45,035 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2288])
2024-12-22 02:15:45,116 - [Process 0/5] - DEBUG - predict_token:tensor([[349]], device='cuda:0')
2024-12-22 02:15:46,077 - [Process 1/5] - INFO - res.shape is :torch.Size([39])
results:Tilly has to go to detention after school and will take 40 minutes to get home. Sam will go back to his house and will call Tilly when she is home.
 92%|█████████▎| 37/40 [03:47<00:18,  6.01s/it]2024-12-22 02:15:46,083 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:46,083 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2294])
2024-12-22 02:15:46,165 - [Process 2/5] - DEBUG - predict_token:tensor([[360]], device='cuda:2')
2024-12-22 02:15:46,324 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:46,524 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:46,524 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1804])
2024-12-22 02:15:46,588 - [Process 3/5] - DEBUG - predict_token:tensor([[19054]], device='cuda:3')
2024-12-22 02:15:47,657 - [Process 0/5] - INFO - res.shape is :torch.Size([56])
results:People are planning to meet at Jesus bar for a drink later. Clare can't make it as she is not in town. Annette is home sick with lung lurgy. Oli cycled around the bar but couldn't find anyone.
 95%|█████████▌| 38/40 [03:49<00:11,  5.91s/it]2024-12-22 02:15:47,868 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:47,868 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2236])
2024-12-22 02:15:47,885 - [Process 2/5] - INFO - res.shape is :torch.Size([41])
results:Dima's laptop broke and she needs to deliver a translation tomorrow. Nada lends her her old laptop. Dima is grateful. They discuss the importance of having a backup laptop.
 95%|█████████▌| 38/40 [03:49<00:13,  6.99s/it]2024-12-22 02:15:47,912 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:47,950 - [Process 4/5] - DEBUG - predict_token:tensor([[5918]], device='cuda:4')
2024-12-22 02:15:48,008 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:48,186 - [Process 3/5] - INFO - res.shape is :torch.Size([36])
results:Oscar invites Payne to meet at Tristano's for coffee in half an hour. Payne agrees and will be there in 15 minutes.
 90%|█████████ | 36/40 [03:49<00:23,  5.97s/it]2024-12-22 02:15:48,431 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:50,378 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:50,378 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2134])
2024-12-22 02:15:50,458 - [Process 1/5] - DEBUG - predict_token:tensor([[10630]], device='cuda:1')
2024-12-22 02:15:50,794 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:Max's sister is studying in China. Max and Rory are talking about how it's a good investment. Eliza thinks it's the best investment imaginable. Max's sister speaks Chinese and is in Shanghai, which has a population of 25 million inhabitants.
 85%|████████▌ | 34/40 [03:52<00:42,  7.09s/it]2024-12-22 02:15:50,895 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:51,110 - [Process 1/5] - INFO - res.shape is :torch.Size([14])
results:

Dialogue: 







 95%|█████████▌| 38/40 [03:52<00:11,  5.71s/it]2024-12-22 02:15:51,318 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:52,276 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:52,277 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2125])
2024-12-22 02:15:52,349 - [Process 3/5] - DEBUG - predict_token:tensor([[10750]], device='cuda:3')
2024-12-22 02:15:52,412 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:52,412 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2369])
2024-12-22 02:15:52,500 - [Process 0/5] - DEBUG - predict_token:tensor([[6502]], device='cuda:0')
2024-12-22 02:15:52,507 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:52,507 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2394])
2024-12-22 02:15:52,597 - [Process 2/5] - DEBUG - predict_token:tensor([[25292]], device='cuda:2')
2024-12-22 02:15:52,895 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:52,895 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1057])
2024-12-22 02:15:52,936 - [Process 4/5] - DEBUG - predict_token:tensor([[341]], device='cuda:4')
2024-12-22 02:15:53,607 - [Process 4/5] - INFO - res.shape is :torch.Size([17])
results:Maddie is in Asda and John wants white bread and apples.
 88%|████████▊ | 35/40 [03:55<00:29,  5.81s/it]2024-12-22 02:15:53,856 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:53,917 - [Process 3/5] - INFO - res.shape is :torch.Size([34])
results:Molly invites Hannah and Anna to a Muse concert in Cracow. Hannah doesn't like the band, but Anna is excited to go.
 92%|█████████▎| 37/40 [03:55<00:17,  5.90s/it]2024-12-22 02:15:54,174 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:54,756 - [Process 0/5] - INFO - res.shape is :torch.Size([48])
results:Martin has something important to tell Nicole and is afraid to tell her in person. He wants to be honest with her but doesn't know how she will react. Nicole is surprised but doesn't want to break up.
 98%|█████████▊| 39/40 [03:56<00:06,  6.26s/it]2024-12-22 02:15:54,935 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:54,980 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:54,980 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2086])
2024-12-22 02:15:55,054 - [Process 1/5] - DEBUG - predict_token:tensor([[4667]], device='cuda:1')
2024-12-22 02:15:55,884 - [Process 2/5] - INFO - res.shape is :torch.Size([77])
results:Stefano and Josie are talking about Foucault's Pendulum. They find it weird and nerdy, but Stefano thinks it's set in the 1980s, so the language is more comprehensible. They also talk about Salman Rushdie's review of the book and how he hated it.
 98%|█████████▊| 39/40 [03:57<00:07,  7.30s/it]2024-12-22 02:15:56,004 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:56,189 - [Process 1/5] - INFO - res.shape is :torch.Size([26])
results:William is making spaghetti and asks Olivia and Beth to buy fresh tomatoes and chocolate, respectively.
 98%|█████████▊| 39/40 [03:57<00:05,  5.52s/it]2024-12-22 02:15:56,447 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:15:57,692 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:57,693 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2117])
2024-12-22 02:15:57,765 - [Process 4/5] - DEBUG - predict_token:tensor([[20367]], device='cuda:4')
2024-12-22 02:15:58,050 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:58,050 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 02:15:58,130 - [Process 3/5] - DEBUG - predict_token:tensor([[922]], device='cuda:3')
2024-12-22 02:15:58,880 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:15:58,880 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2260])
2024-12-22 02:15:58,960 - [Process 0/5] - DEBUG - predict_token:tensor([[3951]], device='cuda:0')
2024-12-22 02:16:00,007 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:16:00,008 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2266])
2024-12-22 02:16:00,080 - [Process 3/5] - INFO - res.shape is :torch.Size([43])
results:Sean has decided that his spirit animal is a tortoise after much thought and reflection. Tiffany thinks it explains a lot about him. Sean suggests that Tiffany is a wasp.
 95%|█████████▌| 38/40 [04:01<00:11,  5.98s/it]2024-12-22 02:16:00,089 - [Process 2/5] - DEBUG - predict_token:tensor([[383]], device='cuda:2')
2024-12-22 02:16:00,300 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:16:00,891 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:16:00,892 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2537])
2024-12-22 02:16:00,982 - [Process 1/5] - DEBUG - predict_token:tensor([[951]], device='cuda:1')
2024-12-22 02:16:03,224 - [Process 1/5] - INFO - res.shape is :torch.Size([46])
results:Leah met a creepy guy at a poetry reading. He talked to her, named all of her friends, and tried to walk her home. He also googled her beforehand to find information about her.
100%|██████████| 40/40 [04:05<00:00,  5.98s/it]100%|██████████| 40/40 [04:05<00:00,  6.13s/it]
2024-12-22 02:16:03,435 - [Process 4/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 90%|█████████ | 36/40 [04:05<00:28,  7.01s/it]2024-12-22 02:16:03,670 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:16:03,936 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:16:03,936 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1987])
2024-12-22 02:16:04,011 - [Process 3/5] - DEBUG - predict_token:tensor([[24511]], device='cuda:3')
2024-12-22 02:16:04,534 - [Process 3/5] - INFO - res.shape is :torch.Size([11])
results:
Dialogue: 





 98%|█████████▊| 39/40 [04:06<00:05,  5.52s/it]2024-12-22 02:16:04,709 - [Process 0/5] - INFO - res.shape is :torch.Size([128])
results:

Dialogue: 


Dialogue: 


Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary: 

Dialogue: 

Summary:
100%|██████████| 40/40 [04:06<00:00,  7.37s/it]100%|██████████| 40/40 [04:06<00:00,  6.16s/it]
2024-12-22 02:16:04,730 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:16:05,364 - [Process 2/5] - INFO - res.shape is :torch.Size([128])
results:

Dialogue: 

Mia: Hey, can I ask you something?
Lily: Of course! What's up?
Mia: Do you think he likes me?
Lily: Who?
Mia: Jack.
Lily: Oh, him! *giggles* I think he might have a crush on you!
Mia: Really? *blushes*
Lily: Yeah, he's been looking at you a lot lately and he even asked you to sit with him at lunch today!
Mia: *sm
100%|██████████| 40/40 [04:07<00:00,  7.95s/it]100%|██████████| 40/40 [04:07<00:00,  6.18s/it]
2024-12-22 02:16:07,296 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:16:07,296 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 02:16:07,369 - [Process 4/5] - DEBUG - predict_token:tensor([[11546]], device='cuda:4')
2024-12-22 02:16:08,323 - [Process 4/5] - INFO - res.shape is :torch.Size([22])
results:Ron sent Josh something he wants him to have. Josh checked his email and found something he wanted.
 92%|█████████▎| 37/40 [04:10<00:19,  6.38s/it]2024-12-22 02:16:08,590 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:16:08,640 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:16:08,640 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:16:08,722 - [Process 3/5] - DEBUG - predict_token:tensor([[21776]], device='cuda:3')
2024-12-22 02:16:09,004 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
results:





100%|██████████| 40/40 [04:10<00:00,  5.21s/it]100%|██████████| 40/40 [04:10<00:00,  6.27s/it]
2024-12-22 02:16:12,203 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:16:12,203 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2037])
2024-12-22 02:16:12,277 - [Process 4/5] - DEBUG - predict_token:tensor([[6182]], device='cuda:4')
2024-12-22 02:16:17,731 - [Process 4/5] - INFO - res.shape is :torch.Size([128])
results:
































































































































 95%|█████████▌| 38/40 [04:19<00:14,  7.29s/it]2024-12-22 02:16:17,939 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:16:22,001 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:16:22,002 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2213])
2024-12-22 02:16:22,084 - [Process 4/5] - DEBUG - predict_token:tensor([[3561]], device='cuda:4')
2024-12-22 02:16:24,589 - [Process 4/5] - INFO - res.shape is :torch.Size([57])
results:Maggie and Lucy are talking about John who wants to date Lucy. Lucy is not interested and turns him down. Maggie is jealous. Lucy wants to pamper herself with her friends before the gig. They decide to meet at Lucy's house at 6 pm.
 98%|█████████▊| 39/40 [04:26<00:07,  7.16s/it]2024-12-22 02:16:24,851 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:16:28,463 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:16:28,464 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 02:16:28,538 - [Process 4/5] - DEBUG - predict_token:tensor([[17841]], device='cuda:4')
2024-12-22 02:16:29,108 - [Process 4/5] - INFO - res.shape is :torch.Size([13])
results:

Dialogue: 






100%|██████████| 40/40 [04:30<00:00,  6.37s/it]100%|██████████| 40/40 [04:30<00:00,  6.77s/it]
2024-12-22 02:16:29,141 - [Process 4/5] - DEBUG - datasets_name:samsum
2024-12-22 02:16:29,141 - [Process 0/5] - DEBUG - datasets_name:samsum
2024-12-22 02:16:29,141 - [Process 3/5] - DEBUG - datasets_name:samsum
2024-12-22 02:16:29,141 - [Process 2/5] - DEBUG - datasets_name:samsum
2024-12-22 02:16:29,141 - [Process 1/5] - DEBUG - datasets_name:samsum
Running evaluation for dataset: lcc
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.78s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.76s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:18:39,685 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 02:18:39,685 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 02:18:39,685 - [Process 0/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:18:39,690 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 02:18:39,691 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 02:18:39,691 - [Process 4/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:18:39,700 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 02:18:39,700 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 02:18:39,701 - [Process 2/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里!!!!!!!!!!!!!!!!!!!!!!!! 这里

2024-12-22 02:18:39,702 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 02:18:39,702 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 02:18:39,702 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 02:18:39,702 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 02:18:39,702 - [Process 3/5] - INFO - output_max_len: 64
2024-12-22 02:18:39,702 - [Process 1/5] - INFO - output_max_len: 64
2024-12-22 02:18:39,707 - [Process 0/5] - INFO - Max Length is 10029
2024-12-22 02:18:39,707 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 02:18:39,708 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]2024-12-22 02:18:39,726 - [Process 4/5] - INFO - Max Length is 10029
2024-12-22 02:18:39,727 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 02:18:39,728 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]2024-12-22 02:18:39,735 - [Process 2/5] - INFO - Max Length is 10029
2024-12-22 02:18:39,735 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 02:18:39,736 - [Process 2/5] - INFO - get_predicted begin
2024-12-22 02:18:39,736 - [Process 3/5] - INFO - Max Length is 10029
2024-12-22 02:18:39,736 - [Process 1/5] - INFO - Max Length is 10029
2024-12-22 02:18:39,737 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 02:18:39,737 - [Process 1/5] - INFO - Finish loading dataset
  0%|          | 0/100 [00:00<?, ?it/s]2024-12-22 02:18:39,738 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 02:18:39,738 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]2024-12-22 02:18:44,437 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:44,503 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:44,544 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:44,550 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:44,555 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:47,214 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:47,215 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1279])
2024-12-22 02:18:47,264 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:18:47,782 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:47,782 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1304])
2024-12-22 02:18:47,836 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:18:48,654 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:48,655 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1708])
2024-12-22 02:18:48,733 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:18:48,747 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:48,747 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1946])
2024-12-22 02:18:48,761 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:48,762 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 02:18:48,819 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:18:48,830 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:18:49,606 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       digest.update((byte) (ch[start]);
    }
    public void ignorableWhitespace(char ch[], int start, int length) throws SAXException {
    }
    public void processingInstruction(String target, String data) throws SAXException {

  1%|          | 1/100 [00:09<16:17,  9.87s/it]2024-12-22 02:18:49,692 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:50,635 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   OBD_PID00 = OBD_Packet('PID_00_PIDsSupported', fields_desc=fields_desc)
    OBD_PID01 = OBD_Packet('PID_01_MonitorStatusSinceDtcs
  1%|          | 1/100 [00:10<17:59, 10.90s/it]2024-12-22 02:18:50,836 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:51,614 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:				for ( int j = 0; j < m_Participants[i].Count; ++j )
				{
					Mobile mob = (Mobile)m_Participants[i][j];
					if ( mob
  1%|          | 1/100 [00:11<19:38, 11.91s/it]2024-12-22 02:18:51,759 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           {"commentable_id": "dummy", "course_id": unicode(self.course_id)},
            {"body": "foo"},
            mock_request
        )
    def test_update_thread_empty_title(self, mock_request):
        self._test
  1%|          | 1/100 [00:12<19:51, 12.03s/it]2024-12-22 02:18:51,767 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                   Radius = 120,
                    MissileSpeed = 20000,
                    FixedRange = true,
                    AddHitbox = true,
                    DangerValue = 3,
                    IsDangerous = true,

  1%|          | 1/100 [00:12<19:51, 12.03s/it]2024-12-22 02:18:51,783 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:51,893 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:51,907 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:52,492 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:52,492 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1631])
2024-12-22 02:18:52,550 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:18:54,062 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:54,062 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1331])
2024-12-22 02:18:54,104 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:18:54,313 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:54,313 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 02:18:54,382 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:18:54,490 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:54,491 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1510])
2024-12-22 02:18:54,539 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:18:55,011 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   return AE.AECreateRange(dict)
def mkcomparison(dict):
    return AE.AECreateComparison(dict)
def mklogical(dict):
    return AE.AECreateLogical(dict)
def mkenum(dict):
    return
  2%|▏         | 2/100 [00:15<11:49,  7.24s/it]2024-12-22 02:18:55,090 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:55,320 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:55,320 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1774])
2024-12-22 02:18:55,400 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:18:56,939 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:	@Produces
	@Mock
	protected ModelImportService modelImportService;
	protected ModelImportServiceStub modelImportServiceStub;
	@Produces
	@Mock
	protected CodeListService codeListService;
	protected CodeListServiceStub codeListServiceStub
  2%|▏         | 2/100 [00:17<13:03,  8.00s/it]2024-12-22 02:18:57,068 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:57,240 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:			bus = dbus.SessionBus()
		self.bus_name = dbus.service.BusName(name, allow_replacement=True, replace_existing=True)
		dbus.service.Object.__init__(self, self.bus_name,
  2%|▏         | 2/100 [00:17<13:40,  8.37s/it]2024-12-22 02:18:57,392 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           env.FindIxes(source, "WINDOWSDEFPREFIX", "WINDOWSDEFSUFFIX")
            )
        )
    return (embedManifestExeAction, embedManifestDllAction)
def _dllEmitter(target, source, env
  2%|▏         | 2/100 [00:17<13:30,  8.27s/it]2024-12-22 02:18:57,438 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:57,476 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:57,477 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1460])
2024-12-22 02:18:57,524 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:18:57,528 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:58,272 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   B, n, supernodes, exposed_nodes, M, C = self.contracting_phase(B, n, supernodes, exposed_nodes, M, C, root)

I am not able to understand what this code is doing, can someone please explain it to me?

  2%|▏         | 2/100 [00:18<14:24,  8.82s/it]2024-12-22 02:18:58,396 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:18:59,468 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:18:59,468 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1350])
2024-12-22 02:18:59,516 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:18:59,898 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                           sample = formatter.CreateSample(sampleObject, mediaType);
                            samples.Add(mediaType, sample);
                        }
                    }
            }
            return type;
        }
        private object GetActionSample(string controllerName, string actionName, IEnumerable
  3%|▎         | 3/100 [00:20<09:58,  6.17s/it]2024-12-22 02:18:59,959 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:00,114 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:00,114 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1519])
2024-12-22 02:19:00,162 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:19:00,936 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:00,936 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1473])
2024-12-22 02:19:00,984 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:19:00,994 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:00,994 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1551])
2024-12-22 02:19:01,083 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:19:01,960 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:01,960 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1197])
2024-12-22 02:19:01,998 - [Process 3/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:3')
2024-12-22 02:19:02,318 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       context = testlib.TestContext()
        self.assertRaises(IOError, lambda: os.popen('/nonexistingfile', 'r'))

    @testlib.with_context
    def test_popen_creates_file_with_read_mode
  3%|▎         | 3/100 [00:22<10:59,  6.80s/it]2024-12-22 02:19:02,426 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:02,982 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                   // Call the operation on the resource
                    invokeOperation(resource, new SubProgressMonitor(monitor, 1000));
                } catch (CoreException e) {
                    errors = recordError(errors, e);
                }
            }
        } finally
  3%|▎         | 3/100 [00:23<11:23,  7.05s/it]2024-12-22 02:19:03,099 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:03,902 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       def test_int_io(self, dtype):
            ofilename = join(TEST_DATA_DIR, 'test.wav')
            rfd, fd, cfilename = open_tmp_file('pysndfiletest.wav')
            try:
               
  3%|▎         | 3/100 [00:24<11:54,  7.36s/it]2024-12-22 02:19:04,018 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:04,018 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           }
            else
            {
                index = this.AddCraft(typeof(EnchantedSwitch), 1044294, 1072893, 45.0, 95.0, typeof(BlankScroll), 10
  3%|▎         | 3/100 [00:24<12:21,  7.64s/it]2024-12-22 02:19:04,162 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:04,256 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:column.name, txt)
        else:
            self.on_quit_cell(record, column.name, value)
        return True
    def on_key_press_event(self, event):
        if event.keyval in self.leaving_events:
           
  4%|▍         | 4/100 [00:24<08:43,  5.45s/it]2024-12-22 02:19:04,331 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:04,444 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:04,444 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1100])
2024-12-22 02:19:04,486 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:19:05,437 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:05,437 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1289])
2024-12-22 02:19:05,483 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:19:06,173 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:06,173 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1320])
2024-12-22 02:19:06,215 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:19:06,790 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:06,791 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1429])
2024-12-22 02:19:06,839 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:19:06,897 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:06,897 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1653])
2024-12-22 02:19:06,951 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:19:07,212 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:				PropertyService.Start();
			ResourceService.Start();
		}
	}
}

Please complete the code by adding the necessary methods and properties to the CoreStartup class.

Note:

* AddInTree is not a built-in class
  4%|▍         | 4/100 [00:27<09:40,  6.05s/it]2024-12-22 02:19:07,306 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:08,228 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:			// Check if we can find the input stream in the step
			if (meta.getInputStream()==null)
			{
				logError(BaseMessages.getString(PKG, "AggregateRows.Log.NoInputStreamSpecified
  4%|▍         | 4/100 [00:28<10:08,  6.33s/it]2024-12-22 02:19:08,406 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:08,894 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:08,894 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 934])
2024-12-22 02:19:08,925 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:19:08,994 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       self.select_option_by_text(self._SUB_LANGUAGES, sub_lang)
        #Choose the file
        self.select_option_by_text(self._SUBTITLES_FILE, sub_file)
        #Choose the audio
  4%|▍         | 4/100 [00:29<10:20,  6.47s/it]2024-12-22 02:19:09,145 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:09,196 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           MutablePropertyValues hibernateProperties = processLocations(beanFactory, globalPropertyLocations, LegacyConfigPostProcessor.BEAN_NAME_HIBERNATE_PROPERTIES,
                    new String[] { "classpath:alfresco/domain/hibernate-cfg.properties
  5%|▌         | 5/100 [00:29<08:20,  5.27s/it]2024-12-22 02:19:09,254 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:09,767 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def __getattr__(self, name):
        return getattr(self.app, name)
    def __setattr__(self, name, value):
        setattr(self.app, name, value)
    def __delattr__(self, name):
        delattr(
  4%|▍         | 4/100 [00:30<11:02,  6.90s/it]2024-12-22 02:19:09,990 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:11,049 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:11,049 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1044])
2024-12-22 02:19:11,083 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:19:11,394 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:				public virtual void AddToConstraintCollection (ConstraintCollection collection)
		{
				throw new NotImplementedException ();
		}
}

I have tried to complete the code by adding the missing methods and properties, but I am not sure if I have
  5%|▌         | 5/100 [00:31<08:30,  5.38s/it]2024-12-22 02:19:11,570 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:11,840 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:11,840 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1584])
2024-12-22 02:19:11,897 - [Process 0/5] - DEBUG - predict_token:tensor([[268]], device='cuda:0')
2024-12-22 02:19:11,976 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:11,977 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1762])
2024-12-22 02:19:12,057 - [Process 4/5] - DEBUG - predict_token:tensor([[308]], device='cuda:4')
2024-12-22 02:19:13,311 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       os.makedirs(os.path.dirname(dest))
        with open(dest, "w") as f:
            f.write("")
    f.close()
    return ("Created %s" % dest, True)
def absent(dest, username, check_
  6%|▌         | 6/100 [00:33<07:38,  4.88s/it]2024-12-22 02:19:13,382 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:13,407 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:13,407 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1880])
2024-12-22 02:19:13,481 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:19:14,616 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   							OLStyleValue styleValue = (OLStyleValue) value;
						writer.addAttribute("name", styleValue.getName());
						writer.addAttribute("value", styleValue.getValue());
				
  5%|▌         | 5/100 [00:34<09:45,  6.16s/it]2024-12-22 02:19:14,784 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   # test_bounds_normalize_group_with_timezone
    assert f('12:30..+5', last, tz='America/New_York') == (
        d(2014,1,31, 12,30,
  5%|▌         | 5/100 [00:35<10:09,  6.41s/it]2024-12-22 02:19:14,818 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:14,972 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:15,015 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:15,015 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 02:19:15,080 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:19:15,681 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:15,682 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1327])
2024-12-22 02:19:15,727 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:19:16,302 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   response = _pubsub_consumer(session, cluster_id, needs_columns=needs_columns)
    if topic_name:
        response = response.filter(PubSubConsumer.topic_id==topic_name)
    return response
# ###################################################################
  5%|▌         | 5/100 [00:36<10:42,  6.77s/it]2024-12-22 02:19:16,457 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:17,802 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.verticalLayout_3.addWidget(self.grab_no_background_option)
        self.verticalLayout_3.addWidget(self.monochrome_option)
        self.verticalLayout_2.addWidget(self.verticalLayout_3)
        self
  6%|▌         | 6/100 [00:38<08:58,  5.73s/it]2024-12-22 02:19:17,936 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:18,063 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   public class MFExternAlt
    {
        [DllImport("MFPlat.dll", ExactSpelling = true), SuppressUnmanagedCodeSecurity]
        public static extern IMFMediaEventQueueAlt MFCreateEventQueueAlt(
            out IMFMediaEvent
  7%|▋         | 7/100 [00:38<07:29,  4.84s/it]2024-12-22 02:19:18,125 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:18,201 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:18,201 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2101])
2024-12-22 02:19:18,266 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:19:18,449 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:18,449 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1953])
2024-12-22 02:19:18,520 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:19:19,404 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:19,405 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1851])
2024-12-22 02:19:19,459 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:19:20,161 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:20,161 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1153])
2024-12-22 02:19:20,202 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:19:20,697 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:20,697 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1500])
2024-12-22 02:19:20,752 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:19:21,123 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   questions_type"):
            d[a] = (lambda x: x, lambda x: x, lambda x: x)
        #strings
        for a in ("anagrams_suffix_blacklist", "anagrams_prefix_blacklist", "selected_questions
  6%|▌         | 6/100 [00:41<09:50,  6.28s/it]2024-12-22 02:19:21,277 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:21,281 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       }
        private static void SaveByHand()
        {
            //TODO: Save the spawns by hand
        }
        private static void SaveByCoord(Mobile from, int x1, int y1, int x2, int y2)
        {
            //
  6%|▌         | 6/100 [00:41<10:05,  6.44s/it]2024-12-22 02:19:21,473 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:22,261 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       return isMatchingPastRelease;
    }
    public boolean isBestMatchingFutureReleaseTime(Long bestMatchingReleaseTime, long releaseInstallationTime, long currentTime) {
        boolean isMatchingFutureRelease = false;
        if (releaseInstallationTime >= currentTime
  6%|▌         | 6/100 [00:42<10:10,  6.49s/it]2024-12-22 02:19:22,391 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:22,464 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           spm.addPrincipalToAcl( directedAclKeys.getAclKey(), aclKey );
        }
    }
}

I am trying to complete the code by adding the missing methods and annotations. Can someone please help me with that?

I have gone through
  8%|▊         | 8/100 [00:42<07:12,  4.70s/it]2024-12-22 02:19:22,529 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:23,367 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           }
            }
        }
    }
    private final class TafReceiver extends BroadcastReceiver {
        @Override
        public void onReceive( Context context, Intent intent ) {
            String action = intent.getAction();
            if ( action.equals( mAction ) ) {
  7%|▋         | 7/100 [00:43<08:47,  5.67s/it]2024-12-22 02:19:23,483 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:24,010 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:24,010 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1657])
2024-12-22 02:19:24,066 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:19:24,611 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:24,611 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1156])
2024-12-22 02:19:24,653 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:19:24,861 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:24,861 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1438])
2024-12-22 02:19:24,911 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:19:24,952 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:24,952 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1977])
2024-12-22 02:19:25,023 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:19:25,713 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:25,713 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1370])
2024-12-22 02:19:25,754 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:19:26,874 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                   mob.Prompt = new SR_NewRunePrompt(RuneAcc, mob.Location, mob.Map);
                    Send(mob, SR_Utilities.FetchInfo(mob.Account));
                    break;
                case 3:
                    mob
  7%|▋         | 7/100 [00:47<09:27,  6.11s/it]2024-12-22 02:19:26,934 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       if self.expected_config.get('namespaces') is not None:
            self.gen_full_tags()
            self.expected_data = self.gen_expected_data()
            self.add_callback('NOTIFY', self.packet_handler)
           
  9%|▉         | 9/100 [00:47<07:01,  4.63s/it]2024-12-22 02:19:27,070 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:27,094 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:27,706 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   axis_axis_in_detector = ...
    rotation_direction_in_detector = ...
    fbp_filter = ...
    parker_weighting = ...
    tam_danielson_window = ...

I'm not sure what the code is doing,
  7%|▋         | 7/100 [00:47<09:31,  6.15s/it]2024-12-22 02:19:27,887 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       s, o = self._check_guest_suspend_log(session, driver)
        if not s:
            raise exceptions.TestFail("Guest suspend log is not found.")
    else:
        s, o = self._check_guest_sus
  7%|▋         | 7/100 [00:48<10:04,  6.50s/it]2024-12-22 02:19:27,892 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:27,974 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:28,554 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:						Assert.AreEqual(1, stats.QueryExecutionCount);
					Assert.AreEqual(1, stats.QueryFetchCount);
					// ...and the same for the second query
					stats.Clear();

  8%|▊         | 8/100 [00:48<08:27,  5.52s/it]2024-12-22 02:19:28,690 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:29,578 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:29,578 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 883])
2024-12-22 02:19:29,609 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:19:30,554 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:30,554 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-22 02:19:30,621 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:30,621 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 02:19:30,622 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:19:30,690 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:19:31,380 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:31,380 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1557])
2024-12-22 02:19:31,380 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:31,381 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 02:19:31,433 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:19:31,447 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:19:32,193 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       pub.connect(host);
        assertThat(pub.connect(host), is(true));
        pub.send("test");
        assertThat(pub.send("test"), is(true));
        pub.close();
        sub.recv();
        assertThat(sub
  8%|▊         | 8/100 [00:52<08:53,  5.80s/it]2024-12-22 02:19:32,382 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:33,256 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           #    raise xml.dom.NotFoundErr()
            pass
        else:
            raise xml.dom.HierarchyRequestErr(
                "normalize() called on non-element node")
        return self
    def _get_id_cache(self):
        return self
 10%|█         | 10/100 [00:53<07:43,  5.15s/it]2024-12-22 02:19:33,360 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:33,501 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       extensions.add(new ServerNameExtension(serverNames));
    }
    // add signature_algorithm extension
    void addSignatureAlgorithmsExtension(
            Collection<SignatureAndHashAlgorithm> algorithms) {
        HelloExtension signatureAlgorithm =
                new SignatureAlgorithmsExtension
  8%|▊         | 8/100 [00:53<09:37,  6.27s/it]2024-12-22 02:19:33,644 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:34,087 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           }
            return (ushort)crc;
        }
        public ushort crctable(byte[] p)
        {
            // 
            // 
            // 
            // 
            // 
            // 
            // 
            // 
  9%|▉         | 9/100 [00:54<08:22,  5.52s/it]2024-12-22 02:19:34,277 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:34,336 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def getEpisode(self, season, episode):
        return self.cache.getEpisode(season, episode)
    def getSeason(self, season):
        return self.cache.getSeason(season)
    def getAllEpisodes(self, season):
  8%|▊         | 8/100 [00:54<09:39,  6.30s/it]2024-12-22 02:19:34,433 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:35,770 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:35,770 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2087])
2024-12-22 02:19:35,832 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:19:36,186 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:36,186 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1561])
2024-12-22 02:19:36,235 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:19:36,279 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:36,279 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 951])
2024-12-22 02:19:36,320 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:19:36,873 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:36,873 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 02:19:36,943 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:19:37,671 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:37,671 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1873])
2024-12-22 02:19:37,744 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:19:38,595 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               this.storage = (CheckpointServer) (Naming.lookup(urlCheckpoint));
                this.location = (LocationServer) (Naming.lookup(urlLocation));
                this.recovery = (RecoveryProcess) (Naming.lookup(urlRecovery));
            }
  9%|▉         | 9/100 [00:58<09:04,  5.99s/it]2024-12-22 02:19:38,727 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:39,146 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   public synchronized void notifyListeners(K key, V value) {
        for (SpaceListener sl : sl) {
            try {
                sl.spaceChanged (key, value);
            } catch (Throwable t) {
                Log.error (t);
            }
  9%|▉         | 9/100 [00:59<09:12,  6.08s/it]2024-12-22 02:19:39,224 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:			if (type == FontSize.AsUnit)
				return unit.ToString (fmt);
			else
				return font_size_names [type];
		}
	}
}

Please complete the code by writing the ToString
  9%|▉         | 9/100 [00:59<08:53,  5.86s/it]2024-12-22 02:19:39,275 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:39,373 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:39,507 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:						@Override
			protected boolean validate(EObject objectToTest, BasicDiagnostic diagnostician, Map<Object, Object> map) {
					return EMSSceneSectionOperations.validateEMSSceneSectionCode(
		
 11%|█         | 11/100 [00:59<08:08,  5.49s/it]2024-12-22 02:19:39,627 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:40,528 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:		return shootingSpeed;
	}
	public void setShootingSpeed(int shootingSpeed) {
		this.shootingSpeed = shootingSpeed;
	}
}

I'm having trouble understanding how to implement the code for the Sentry class. I'm
 10%|█         | 10/100 [01:00<08:42,  5.81s/it]2024-12-22 02:19:40,676 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:41,107 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:41,107 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1251])
2024-12-22 02:19:41,162 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:19:41,625 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:41,625 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1180])
2024-12-22 02:19:41,679 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:19:42,148 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:42,148 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1635])
2024-12-22 02:19:42,204 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:19:43,068 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:43,069 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1896])
2024-12-22 02:19:43,144 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:19:43,496 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:43,496 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1896])
2024-12-22 02:19:43,546 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:19:43,695 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:																																																																
 10%|█         | 10/100 [01:03<08:34,  5.71s/it]2024-12-22 02:19:43,933 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:44,508 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:						//TODO also allow alchemical catalyst? Or save that for an elixir/brew?
						//RedButton btnAlchemize = new RedButton(Messages.get(this, "alchemize")) {
			
 10%|█         | 10/100 [01:04<08:47,  5.86s/it]2024-12-22 02:19:44,712 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:45,061 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:																																																																
 10%|█         | 10/100 [01:05<08:46,  5.85s/it]2024-12-22 02:19:45,213 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:45,715 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:			if (decoder.peekType(TlvTypeCodes.KeyLocator, endOffset)) {
				decodeKeyLocator
				  (TlvTypeCodes.KeyLocator, keyLocator, decoder);
			}
 12%|█▏        | 12/100 [01:05<08:22,  5.71s/it]2024-12-22 02:19:45,782 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:46,170 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       reconciler.setDamageRepairer(damageRepairer);
        return reconciler;
    }
    
    public IInformationControlCreator getInformationControlCreator(
            ISourceViewer sourceViewer) {
        return new CeylonInformation
 11%|█         | 11/100 [01:06<08:32,  5.76s/it]2024-12-22 02:19:46,347 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:47,340 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:47,340 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1910])
2024-12-22 02:19:47,414 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:19:47,991 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:47,991 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1236])
2024-12-22 02:19:48,039 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:19:48,178 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:48,178 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1972])
2024-12-22 02:19:48,249 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:19:48,385 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:48,385 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1789])
2024-12-22 02:19:48,448 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:19:49,833 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:49,834 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1977])
2024-12-22 02:19:49,905 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:19:50,152 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       self.fm.env.status.append(descr)
        self.fm.env.status.append("Extracting...")
        self.fm.env.status.append("")
        self.fm.env.status.append("")
        self.fm.env
 11%|█         | 11/100 [01:10<08:48,  5.94s/it]2024-12-22 02:19:50,259 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:50,355 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       for(int i = 0; i < myWeapons.length; i++)
        {
            WeaponType w = myWeapons[i];
            if(w.offCD()){
                w.updateCooldown(delta);
            }
       
 13%|█▎        | 13/100 [01:10<07:48,  5.38s/it]2024-12-22 02:19:50,427 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:51,179 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       request = SimpleXMLElement(environ['REQUEST_BODY'])
        # parse request message and get local method
        method = request.find('//soap:body/soap:method')
        if method:
            # method name = action
            name = method.attrib
 11%|█         | 11/100 [01:11<09:03,  6.11s/it]2024-12-22 02:19:51,340 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:51,367 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					var inner = (JObject)obj.Properties[0].Value;
				Assert.That(inner["The outermost value"].ToString(), Is.EqualTo("must be an object or array."));
				Assert.That(inner["In this
 11%|█         | 11/100 [01:11<08:53,  5.99s/it]2024-12-22 02:19:51,538 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:52,266 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:52,266 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1100])
2024-12-22 02:19:52,306 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:19:52,665 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.l.addWidget(self.drawingModeWidget, 0, 0)
        self.connect(self.drawingModeWidget,
                     qt.SIGNAL("drawingModeChanged(int)"),
                     self._slot)
        self.
 12%|█▏        | 12/100 [01:12<08:46,  5.98s/it]2024-12-22 02:19:52,792 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:52,798 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:52,798 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1408])
2024-12-22 02:19:52,846 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:19:54,172 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:54,172 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1689])
2024-12-22 02:19:54,230 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:19:54,849 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:54,849 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1922])
2024-12-22 02:19:54,855 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:			public override MethodInfo[] GetMethods (BindingFlags bindingAttr)
		{
			throw new NotImplementedException ();
		}
}

I'm getting an error in the line "public override MethodInfo[] GetMethods (BindingFlags bindingAttr)" that
 12%|█▏        | 12/100 [01:15<08:09,  5.56s/it]2024-12-22 02:19:54,913 - [Process 1/5] - DEBUG - predict_token:tensor([[29908]], device='cuda:1')
2024-12-22 02:19:54,998 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:55,189 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   m_companySelection.addChangeHandler( new ChangeHandler()
    {
      @Override
      public void onChange(ChangeEvent p_event)
      {
        Company company = Company.valueOf( m_companySelection.getValue( m_companySelection
            .getSelectedIndex()
 14%|█▍        | 14/100 [01:15<07:28,  5.22s/it]2024-12-22 02:19:55,216 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:55,216 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1158])
2024-12-22 02:19:55,253 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:55,272 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:19:57,087 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       if status is not None:
            self.base.show_update_box_for_reply(self.account_id, status)
            self.loader.setVisible(False)
            self.webview.set_status(status)
            self.last_id =
 12%|█▏        | 12/100 [01:17<08:51,  6.04s/it]2024-12-22 02:19:57,281 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:57,332 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:57,333 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1186])
2024-12-22 02:19:57,373 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:19:57,745 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:19:57,746 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1544])
2024-12-22 02:19:57,766 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:					case 3: // Set location
						{
							toSet = new Point2D( m_Mobile.Location );
							shouldSet = true;
							should
 13%|█▎        | 13/100 [01:18<08:17,  5.71s/it]2024-12-22 02:19:57,798 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           " + edges.size() + " edges found");
            for (T2 edge : edges) {
                log("Edge: " + edge.getSubject() + " -> " + edge.getObject());
            }
        }
    }
}







 12%|█▏        | 12/100 [01:18<08:59,  6.13s/it]2024-12-22 02:19:57,802 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:19:57,880 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:57,973 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:19:59,650 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   if "total" in json_total:
        total = json_total["total"]
    else:
        total = default_total_fn(json_total)
    if total == 0:
        raise RuntimeError("No results found")
    # Now we know the total
 15%|█▌        | 15/100 [01:19<07:04,  4.99s/it]2024-12-22 02:19:59,738 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:00,060 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:00,061 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1347])
2024-12-22 02:20:00,098 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:20:00,433 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertTupleEqual(
            _actions4appversion(self.old_av, {da.id}, None, 100),
            ({}, {da.id}))
        self.assertTupleEqual(
            _actions4appversion(self.new_
 13%|█▎        | 13/100 [01:20<08:04,  5.57s/it]2024-12-22 02:20:00,643 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:00,669 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:00,670 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-22 02:20:00,734 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:20:01,480 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:01,480 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1969])
2024-12-22 02:20:01,552 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:20:02,633 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:itValue);
               convertedValuesList.add(singleValue);
            }
         }
         else
         {
            convertedValues = Sets.newHashSet(value);
         }
      }
      else
      {
         convertedValues = Collections.emptySet();

 14%|█▍        | 14/100 [01:22<07:49,  5.46s/it]2024-12-22 02:20:02,740 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:02,740 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1526])
2024-12-22 02:20:02,804 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:20:02,809 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:03,630 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           }
            progress = {
                "total": 0,
                "completed": 0
            }
            pid = None
            pname = None
            if data:
                operation.set_attributes(data)
            else:
                operation.set_status(
 13%|█▎        | 13/100 [01:23<08:59,  6.20s/it]2024-12-22 02:20:03,757 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:04,151 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:04,151 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 02:20:04,220 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:20:04,446 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   isoel = i1.get(col1="deform",
                   col2="area_um",
                   lut_identifier="test-LE-2D-ana-18",
                   channel_width=20,
                   flow_rate=0.
 13%|█▎        | 13/100 [01:24<09:06,  6.28s/it]2024-12-22 02:20:04,612 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:05,255 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:							response = service.MergeOrder(request);
							mergedOrder = response.MergedOrder;
							failureReason = response.ErrorMessage;
						});
		
 16%|█▌        | 16/100 [01:25<07:14,  5.17s/it]2024-12-22 02:20:05,340 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:06,060 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:06,060 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1314])
2024-12-22 02:20:06,107 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:20:06,216 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:06,217 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1874])
2024-12-22 02:20:06,290 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:20:06,991 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           mess.session = self.session
            mess.channel_id = self.channel_id
            mess.type = PYMUMBLE_MSG_TYPES_REQUESTBLOB
            mess.data = self.blobs.get_blob(self.session, self
 14%|█▍        | 14/100 [01:27<08:24,  5.87s/it]2024-12-22 02:20:07,127 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:07,812 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:07,812 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1880])
2024-12-22 02:20:07,875 - [Process 1/5] - DEBUG - predict_token:tensor([[268]], device='cuda:1')
2024-12-22 02:20:08,070 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:08,070 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1573])
2024-12-22 02:20:08,126 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:20:08,742 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:				ZoneIdentityPermission zip = (ZoneIdentityPermission) a.Union (b);
				Assert.IsTrue (Same (a, zip), "Trusted+Untrusted");
				Assert.IsFalse (Object.ReferenceEquals (a, zip),
 14%|█▍        | 14/100 [01:29<08:24,  5.87s/it]2024-12-22 02:20:08,878 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:09,064 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:					if (i!=j)
						for (int k=0; k<maxChangesRecorded; k++) {
							fractionWithAmount[i][j][k] = fractionWithAmount[i][j][k
 15%|█▌        | 15/100 [01:29<08:08,  5.75s/it]2024-12-22 02:20:09,218 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:09,530 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:09,530 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1498])
2024-12-22 02:20:09,573 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:20:10,547 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   for i in range(len(grid)):
        with model:
            for rxn in reactions:
                point = grid.at[i, rxn.id]
                rxn.bounds = point, point
            obj_val = model.slim_optim
 17%|█▋        | 17/100 [01:30<07:12,  5.21s/it]2024-12-22 02:20:10,606 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:10,626 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:    * 
     * 
     * @param actual
     *            The actual triple store.
     * 
     * @param expected
     *            The expected triple store.
     * 
     * @return An iterator visiting the statements not found in <i>actual</
 14%|█▍        | 14/100 [01:30<08:57,  6.25s/it]2024-12-22 02:20:10,776 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:11,225 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:11,226 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1453])
2024-12-22 02:20:11,271 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:20:12,085 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   public static final void onPlayerJoin(Player player) {
        PlayerGroup group = player.getPlayerGroup2();
        if (group != null) {
            group.onEvent(new PlayerConnectedEvent(group, player));
        }
    }
    public static final void
 15%|█▌        | 15/100 [01:32<07:58,  5.63s/it]2024-12-22 02:20:12,185 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:12,185 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1795])
2024-12-22 02:20:12,203 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:12,242 - [Process 2/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:2')
2024-12-22 02:20:12,626 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:12,626 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1094])
2024-12-22 02:20:12,667 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:20:13,666 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:13,667 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1718])
2024-12-22 02:20:13,725 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:20:13,801 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                   certs.append(cert_converter.getCertificate(obj))
    return private_key, certs

def get_ssl_context(keyfile, certfile, ca_certs):
    log.debug("Loading SSL context")
    private_key, cert
 15%|█▌        | 15/100 [01:34<07:58,  5.62s/it]2024-12-22 02:20:13,931 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:14,548 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:14,548 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1334])
2024-12-22 02:20:14,596 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:20:14,888 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:               //call the API to send the purchase order data to the supplier
                APIv1EndpointResponse endpointResponse = apiOrgSession.sendPurchaseOrderToSupplier(esDocumentOrderSale);
                //check the result of sending the purchase order to the supplier
                if (endpoint
 16%|█▌        | 16/100 [01:35<08:04,  5.77s/it]2024-12-22 02:20:14,916 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def get_for_path(self, pootle_path, user):
        """Returns units that fall below the `pootle_path` umbrella.
        :param pootle_path: An internal pootle path.
        :param user: The user
 18%|█▊        | 18/100 [01:35<06:46,  4.96s/it]2024-12-22 02:20:14,975 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:15,010 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:16,303 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:16,303 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1351])
2024-12-22 02:20:16,349 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:20:16,364 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:                   assert not alert.check_for_alert(history, {'name': 'Just 1a', 'threshold': '0.05', 'region': 'us-east-1', 'zone': 'us-east-1a', 'instance_type': 'g2.2
 15%|█▌        | 15/100 [01:36<08:38,  6.10s/it]2024-12-22 02:20:16,446 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:16,509 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:16,510 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 818])
2024-12-22 02:20:16,543 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:20:17,251 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def highlight(self, toks, formatter=None, outfile=None):
        formatter = formatter or Formats.get(conf.UI.formatter)
        if isinstance(formatter, str):
            formatter = Formats[formatter]
        out
 16%|█▌        | 16/100 [01:37<07:41,  5.49s/it]2024-12-22 02:20:17,379 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:17,956 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:17,956 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 820])
2024-12-22 02:20:17,988 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:20:18,185 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:18,185 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1757])
2024-12-22 02:20:18,249 - [Process 3/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:3')
2024-12-22 02:20:19,016 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   private void doAliases(HttpServletRequest request, MailManager manager, String domain)
    {
        List aliases = manager.getAliases(domain);
        request.setAttribute("aliases", aliases);
    })
    {
        List accounts = manager.get
 16%|█▌        | 16/100 [01:39<07:42,  5.50s/it]2024-12-22 02:20:19,122 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:			public void AddParameterInspector (IParameterInspector inspector)
			{
				inspectors.Add (inspector);
			}
		}
}

Please complete the code by adding the missing methods and properties.
 17%|█▋        | 17/100 [01:39<07:20,  5.31s/it]2024-12-22 02:20:19,212 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:19,307 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:19,625 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:19,625 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1269])
2024-12-22 02:20:19,674 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:20:20,635 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:				URL url = new URL(Strings.DownloaderYartaLink);
			URLConnection connection = url.openConnection();
			connection.connect();
			long lastModified = connection.getLastModified();
			String filePath = get
 16%|█▌        | 16/100 [01:40<07:46,  5.55s/it]2024-12-22 02:20:20,735 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           ],
        ),
        migrations.CreateModel(
            name='Lecturer',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('name', models
 19%|█▉        | 19/100 [01:40<07:02,  5.22s/it]2024-12-22 02:20:20,748 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:20,809 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:22,167 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       column, tree_model, tree_iter = self._setup_column(1, False)
        tree_model.add_map(tree_iter, self._create_store_map(1, False, 15, 2))
        column.quantity_renderer.set
 17%|█▋        | 17/100 [01:42<07:21,  5.32s/it]2024-12-22 02:20:22,302 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:22,729 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:22,729 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 02:20:22,800 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:20:22,804 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:22,805 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1982])
2024-12-22 02:20:22,877 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:20:22,994 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:22,994 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1138])
2024-12-22 02:20:23,043 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:20:23,263 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:23,263 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1470])
2024-12-22 02:20:23,313 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:20:25,089 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:25,090 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1534])
2024-12-22 02:20:25,144 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:20:25,615 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       //expect(mockSecurityManager.getGroup(ownerGroupId)).andReturn(null);
        replay(mockSecurityManager, mockDatabase, mockCurrentSubject);
        SimpleACLPermission permission = new SimpleACLPermission(mockSecurityManager);
        assertEquals(0, permission
 18%|█▊        | 18/100 [01:45<07:44,  5.67s/it]2024-12-22 02:20:25,677 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def _get_table_entry(self, dpid):
        return self.pox_switch.get_table_entry(dpid)
    def get_table_entry(self, dpid):
        return self.pox_switch.get_table_entry(d
 20%|██        | 20/100 [01:45<06:50,  5.13s/it]2024-12-22 02:20:25,724 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:```

This code is a Python implementation of a renderer for a multimedia application, specifically a music player. It uses the FFmpeg library to encode audio files in various formats. The code is divided into several classes, each responsible for a specific part of the rendering process:

1. `Renderer`: This
 17%|█▋        | 17/100 [01:46<08:06,  5.86s/it]2024-12-22 02:20:25,743 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:25,770 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:25,888 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:25,943 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:			SDL.SDL_GL_DeleteContext(context);
			context = IntPtr.Zero;
			window.Dispose();
		}
	}
}

I have written this code for a graphics context class in C# but I am unable
 17%|█▋        | 17/100 [01:46<07:34,  5.48s/it]2024-12-22 02:20:26,120 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:27,092 - [Process 4/5] - INFO - res.shape is :torch.Size([47])
results:   public void doEndTag() throws PageException {
	// provide to child
	Tag child = this.getNext();
	if (child != null) child.doStartTag();
    }
}
 18%|█▊        | 18/100 [01:47<07:06,  5.20s/it]2024-12-22 02:20:27,192 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:28,078 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:28,078 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1395])
2024-12-22 02:20:28,124 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:20:28,813 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:28,813 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1676])
2024-12-22 02:20:28,867 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:28,867 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1703])
2024-12-22 02:20:28,878 - [Process 3/5] - DEBUG - predict_token:tensor([[259]], device='cuda:3')
2024-12-22 02:20:28,929 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:20:29,173 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:29,173 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1078])
2024-12-22 02:20:29,214 - [Process 4/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:4')
2024-12-22 02:20:29,633 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:29,633 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1966])
2024-12-22 02:20:29,705 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:20:30,847 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:						vector.X = Math.Abs (vector.X);
				vector.Y = Math.Abs (vector.Y);
				return vector;
		}
	}
}
}

I am trying to complete the
 19%|█▉        | 19/100 [01:51<07:28,  5.54s/it]2024-12-22 02:20:30,927 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:31,349 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   m_RecentItems.add(item);
    updateMenu();
  }
  protected void removeRecentItem(T item) {
    m_RecentItems.remove(item);
    updateMenu();
  }
  protected void clearRecentItems() {
   
 21%|██        | 21/100 [01:51<06:58,  5.30s/it]2024-12-22 02:20:31,401 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:31,762 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:							if (option1) {
						jumpID = (int) store.GetValue (iter, columnJumpID);
						//do something with jumpID
					}
			}
 18%|█▊        | 18/100 [01:52<08:05,  5.92s/it]2024-12-22 02:20:31,934 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               // ReadElement(element, "Name");
        }
        private void ParseStatus(InventoryEntry invEntry, string status)
        {
            string[] statusParts = status.Split(':');
            switch (statusParts[0]) {
                case "Health":
 19%|█▉        | 19/100 [01:52<06:52,  5.09s/it]2024-12-22 02:20:31,953 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:32,108 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:32,423 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:32,423 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 880])
2024-12-22 02:20:32,450 - [Process 2/5] - DEBUG - predict_token:tensor([[321]], device='cuda:2')
2024-12-22 02:20:32,581 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       (projected onto the *y* axis).
        :rtype: numpy.ndarray or None
        """
        return self._project_cov_mat(
            self.x_data_cov_mat, self.y_data_cov_mat, self.x_model
 18%|█▊        | 18/100 [01:52<07:57,  5.82s/it]2024-12-22 02:20:32,657 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:33,065 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:33,066 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 986])
2024-12-22 02:20:33,098 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:20:34,118 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:34,118 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 831])
2024-12-22 02:20:34,149 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:20:34,907 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   key = endpoint_key(vlan=10, mac_hi=0x1234, mac_lo=0x5678)
    packed_key = key.pack()
    unpacked_key = endpoint_key.unpack(packed_
 20%|██        | 20/100 [01:55<06:47,  5.09s/it]2024-12-22 02:20:35,100 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:35,305 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                   if (foundAnyone)
                    {
                        src.SendLocalizedMessage(5008150); // You see someone hiding nearby.
                    }
            }
        }
        public static bool CanDetect(Mobile src, Item item)
       
 22%|██▏       | 22/100 [01:55<06:21,  4.89s/it]2024-12-22 02:20:35,412 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:35,425 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:35,425 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2023])
2024-12-22 02:20:35,493 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:20:35,708 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:35,709 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1845])
2024-12-22 02:20:35,789 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:20:36,831 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       CmsLink link = new CmsLink(m_structureId, m_target, m_query, m_anchor, m_type, m_internal);
        return link;
    }
}

I need help in completing the code by writing the method to convert the C
 19%|█▉        | 19/100 [01:57<07:13,  5.35s/it]2024-12-22 02:20:37,078 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:38,333 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       if (message == null) {
            message = defaultMessages.getMessage(key);
        }
        */
        return message;
    }
}

Please complete the code by filling in the missing parts.

Note: The code is incomplete and needs to be completed.


 19%|█▉        | 19/100 [01:58<08:15,  6.11s/it]2024-12-22 02:20:38,428 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:38,522 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:						m_key[i] = new ExodusActivation();
						break;
					}
				}
			}
		}
	}
}

Please help me complete this code
 20%|██        | 20/100 [01:58<07:23,  5.54s/it]2024-12-22 02:20:38,604 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:38,604 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:20:38,647 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:38,673 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:20:39,071 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:39,071 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2235])
2024-12-22 02:20:39,135 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:20:40,253 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:40,254 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1017])
2024-12-22 02:20:40,294 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:20:40,724 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:40,725 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2201])
2024-12-22 02:20:40,788 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:20:41,233 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:41,233 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1297])
2024-12-22 02:20:41,288 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:20:41,399 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:						    // ... bind the listening socket to the port number ...
					    m_mainSocket.Bind(new IPEndPoint(IPAddress.Any, PortNumber));
					    // ... and listen for incoming connections ...
		
 21%|██        | 21/100 [02:01<07:15,  5.51s/it]2024-12-22 02:20:41,563 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:41,704 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   // Deserialized from a stream.
    // Called when deserialized from a stream.
    // Called when deserialized from a stream.
    // Called when deserialized from a stream.
    // Called when deserialized from a stream.
   
 23%|██▎       | 23/100 [02:01<06:51,  5.35s/it]2024-12-22 02:20:41,816 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:43,136 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:					DataManager.timedData.put(timedData.getId(), timedData);
			}
	}
}

Please help me complete this code. I am new to Java and I am having trouble understanding how to complete this code.

Thank
 20%|██        | 20/100 [02:03<07:37,  5.72s/it]2024-12-22 02:20:43,323 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:43,695 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:         // Add grants for the owner and IAM users
          Grant grant = new Grant();
          grant.setGrantee(new Grantee(AccountIdentifiers.forAccountId(owningAccount.getId())));
          grant.setPermission(Grant.Permission.READ
 20%|██        | 20/100 [02:03<07:44,  5.81s/it]2024-12-22 02:20:43,757 - [Process 4/5] - INFO - res.shape is :torch.Size([62])
results:		for(int x = 0; x < beans.length; x++)
		{
			coll.add((ChangeOfServiceVo)beans[x].buildVo());
		}
		return coll;
	}
}

 21%|██        | 21/100 [02:04<07:10,  5.45s/it]2024-12-22 02:20:43,789 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:43,876 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:44,566 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:44,567 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1729])
2024-12-22 02:20:44,625 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:20:45,386 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:45,386 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1730])
2024-12-22 02:20:45,468 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:20:45,611 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:45,612 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 991])
2024-12-22 02:20:45,650 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:20:46,159 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:46,159 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1436])
2024-12-22 02:20:46,201 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:20:46,878 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:46,878 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1865])
2024-12-22 02:20:46,957 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:20:47,336 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                               mGooglePortal.notifyDataReceived(type, requestId, list);
                            }
                        }
    }
    @Override
    public void onError(int requestId, Exception e) {
        if(mRequestMap != null && mRequestMap.containsKey
 22%|██▏       | 22/100 [02:07<07:19,  5.64s/it]2024-12-22 02:20:47,512 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:48,033 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				if (a != null) return a;
				a = LeadingAny(p.down);
			}
			return a;
		}
}
}
//---------------------------------------------------------------------
// Utility routines
//----------------
 24%|██▍       | 24/100 [02:08<07:08,  5.64s/it]2024-12-22 02:20:48,195 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:48,329 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           add { _selectedFolderChanged += value; }
            remove { _selectedFolderChanged -= value; }
        }
        }
        #endregion
        #region Private methods
        private void Initialize()
        {
				_initializationState = InitializationState.Initial
 21%|██        | 21/100 [02:08<07:10,  5.45s/it]2024-12-22 02:20:48,502 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:48,856 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           targetVersion);
            if (bitsDifference < bestDifference) {
                bestDifference = bitsDifference;
                bestVersion = i + 7;
            }
        }
        return null;
    }
}











 22%|██▏       | 22/100 [02:09<06:56,  5.34s/it]2024-12-22 02:20:49,049 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:49,802 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			generalNode.setPage(new FieldEditorPreferencePage() {
				@Override
				protected void createFieldEditors() {
					addField(new StringFieldEditor(ROUND_COLOR_DEFAULT,
				
 21%|██        | 21/100 [02:10<07:54,  6.00s/it]2024-12-22 02:20:49,887 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:50,716 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:50,716 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1867])
2024-12-22 02:20:50,781 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:20:51,632 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:51,633 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1025])
2024-12-22 02:20:51,664 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:20:51,681 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:51,682 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1849])
2024-12-22 02:20:51,745 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:20:51,770 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:51,770 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1741])
2024-12-22 02:20:51,852 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:20:52,637 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:52,638 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1580])
2024-12-22 02:20:52,726 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:20:53,505 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results: PrintLongLine('typename ReturnType%s%s%s = typenames[0];' % (optional_comma, typenames[0], optional_comma))
  PrintLongLine('BaseCallback<ReturnType%s%s> %s;' % (number_
 23%|██▎       | 23/100 [02:13<07:26,  5.80s/it]2024-12-22 02:20:53,598 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:54,418 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				HIDImports.HidD_SetOutputReport(this.mHandle.DangerousGetHandle(), buff, (uint)buff.Length);
				if(mAltWriteMethod)
					HIDImports.HidD_
 25%|██▌       | 25/100 [02:14<07:19,  5.86s/it]2024-12-22 02:20:54,479 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:54,570 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:val > 0)
						{
							for (int i = 0; i < targets.Count; i++)
							{
								Mobile m = targets[i];
	
 22%|██▏       | 22/100 [02:14<07:19,  5.63s/it]2024-12-22 02:20:54,662 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           StorageType storageType) {
        // ...
    }
    private void assertInvalidVolumeInfoCombination(VolumeFormat volumeFormat,
            VolumeType volumeType,
            StorageType storageType) {
        // ...
    }
}
```

In this code, the
 22%|██▏       | 22/100 [02:14<07:25,  5.72s/it]2024-12-22 02:20:54,827 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:54,833 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:55,254 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:55,254 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 979])
2024-12-22 02:20:55,285 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:20:55,527 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:				kdf.Compute(pbData, pbSalt, pbExpc);
				if(!MemUtil.ArraysEqual(kdf.Hash, pbExpc))
					throw new SecurityException("Argon2d");

 23%|██▎       | 23/100 [02:15<07:22,  5.74s/it]2024-12-22 02:20:55,686 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:56,486 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:56,487 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 983])
2024-12-22 02:20:56,532 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:20:57,795 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   'csv', the result is delivered as a CSV.
    If format is 'json', the result is delivered as JSON.
    """
    if format == 'csv':
        return HttpResponse(
            csv.writerow([
                ['id', 'title', 'start', 'end
 24%|██▍       | 24/100 [02:18<06:46,  5.35s/it]2024-12-22 02:20:57,870 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:57,870 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1720])
2024-12-22 02:20:57,912 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:20:57,934 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:20:58,295 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:58,295 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 02:20:58,364 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:20:58,563 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:20:58,563 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1687])
2024-12-22 02:20:58,622 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:20:58,812 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				if (order == SortOrder.DESCENDING)
				{
					direction = -1;
				}
			}
		public int compare(Object o1, Object o2)
		{

 26%|██▌       | 26/100 [02:19<06:41,  5.42s/it]2024-12-22 02:20:58,874 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:00,170 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:00,170 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1461])
2024-12-22 02:21:00,212 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:21:00,817 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					w10.XOptions = ((global::Gtk.AttachOptions)(4));
				w10.YOptions = ((global::Gtk.AttachOptions)(4));
				// Container child vbox3.Gtk.Box
 23%|██▎       | 23/100 [02:21<07:30,  5.85s/it]2024-12-22 02:21:00,913 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:00,913 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1103])
2024-12-22 02:21:00,955 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:21:01,015 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:01,283 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       return self.query(clause, params or {})
    def scalar(self, clause, *args, **kwargs):
        return self.query(clause, *args, **kwargs)
    def scalar(self, *args, **kwargs):
        return self.query(*
 23%|██▎       | 23/100 [02:21<07:38,  5.96s/it]2024-12-22 02:21:01,353 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:				for(int i = 0; i < mainInv.getSizeInventory(); i++) {
				ItemStack stackInSlot = mainInv.getStackInSlot(i);
				if(stackInSlot == stack)
	
 24%|██▍       | 24/100 [02:21<07:18,  5.77s/it]2024-12-22 02:21:01,465 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:01,467 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:02,870 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       final QueryRoot query = new QueryRoot(QueryType.SELECT);
        query.setProjection(new ProjectionNode(new VarNode("subj"), new VarNode("score")));
        query.setWhereClause(new JoinGroupNode(new StatementPatternNode(new
 25%|██▌       | 25/100 [02:23<06:34,  5.26s/it]2024-12-22 02:21:02,969 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:03,241 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:					Email.Send( Email.FromAddress, Email.CrashAddresses, "Crash Report", "Crash Report for " + e.ServerName + " on " + DateTime.Now.ToString() );
					Console.WriteLine( "sent" );

 27%|██▋       | 27/100 [02:23<06:14,  5.12s/it]2024-12-22 02:21:03,297 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:03,515 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:03,516 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1139])
2024-12-22 02:21:03,558 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:21:04,454 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:04,454 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1901])
2024-12-22 02:21:04,528 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:21:04,980 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:04,980 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1094])
2024-12-22 02:21:05,020 - [Process 2/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:2')
2024-12-22 02:21:05,020 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:05,020 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1841])
2024-12-22 02:21:05,095 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:05,096 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1001])
2024-12-22 02:21:05,100 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:21:05,134 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:21:05,416 - [Process 1/5] - INFO - res.shape is :torch.Size([20])
results:   self.main()

Please provide the output of the script when you run it.
 24%|██▍       | 24/100 [02:25<06:55,  5.47s/it]2024-12-22 02:21:05,556 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:06,218 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   from . import settings_local

Please provide the complete code for the settings_local.py file.

I have provided the code for the settings_local.py file below:

from django.conf import settings

# Local time zone for this installation. Choices can be found here
 25%|██▌       | 25/100 [02:26<06:52,  5.50s/it]2024-12-22 02:21:06,321 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:07,363 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:						IList list = session.CreateCriteria(typeof(Item))
						.Add(Expression.Gt("Id", 2))
						.SetCacheable(true)
						.List();
 28%|██▊       | 28/100 [02:27<05:47,  4.82s/it]2024-12-22 02:21:07,468 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:07,667 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       return super(UserDetailView, self).post(request, *args, **kwargs)


I need help in completing the code given above. Please provide me with the complete code.


Answer: Sure, here is the complete code for the `UserDetailView` and `User
 26%|██▌       | 26/100 [02:27<06:19,  5.12s/it]2024-12-22 02:21:07,818 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:07,854 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:		for(IField field : fields) { 
			final String name = field.getDeclaringType().getName().toString() + "." + field.getName().toString();
			List<IField> named = name2Field.get(name);
		
 24%|██▍       | 24/100 [02:28<07:46,  6.14s/it]2024-12-22 02:21:08,118 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:08,182 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:08,182 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1001])
2024-12-22 02:21:08,223 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:21:08,347 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:08,347 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1489])
2024-12-22 02:21:08,402 - [Process 1/5] - DEBUG - predict_token:tensor([[2528]], device='cuda:1')
2024-12-22 02:21:10,693 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:10,694 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1672])
2024-12-22 02:21:10,719 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:							"but was given {1})",
						LightCount, NextFrame.PixelCount));
			OutputQueue.Enqueue (NextFrame);
		}
	}
}

Please complete the code by adding the necessary methods
 26%|██▌       | 26/100 [02:30<06:24,  5.20s/it]2024-12-22 02:21:10,752 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:21:10,889 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:10,958 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:						mb.GetILGenerator().Emit(OpCodes.Ldarg_0);
				mb.GetILGenerator().Emit(OpCodes.Call, BaseType.GetConstructor(BindingFlags.Instance | BindingFlags.Public | BindingFlags.
 25%|██▌       | 25/100 [02:31<06:52,  5.49s/it]2024-12-22 02:21:10,997 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:10,997 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1973])
2024-12-22 02:21:11,070 - [Process 3/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:3')
2024-12-22 02:21:11,158 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:11,615 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:11,615 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2056])
2024-12-22 02:21:11,683 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:21:13,369 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:     get { return this.ResolvedAssembly.PublicKeyToken; }
    }
    }
    readonly IName moduleName;
  }
}
```

Please help me complete the code by implementing the `GetAssemblyAttributes` method and `IAssemblyReference.ContainsForeignTypes` property
 27%|██▋       | 27/100 [02:33<06:26,  5.30s/it]2024-12-22 02:21:13,491 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:13,639 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           for (p = s0.GetNext(); p != s0.Address; p = p.GetNext())
            {
                // while ((p1=MBPtr(p,p->NU))->Stamp == 0xFFFF && int(p->NU)+p1->NU
 29%|██▉       | 29/100 [02:33<06:13,  5.26s/it]2024-12-22 02:21:13,709 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:13,944 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:13,944 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1890])
2024-12-22 02:21:14,004 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:21:14,412 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       and all other exceptions as errors with stack traces (on the
        `tornado.application` logger).
        """
        if typ is HTTPError:
            self.application.log_error(value, tb)
        else:
            self.application.log_error(
 25%|██▌       | 25/100 [02:34<07:49,  6.27s/it]2024-12-22 02:21:14,517 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:14,658 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:14,659 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2308])
2024-12-22 02:21:14,717 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:21:15,897 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:15,897 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1486])
2024-12-22 02:21:15,898 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:15,898 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1330])
2024-12-22 02:21:15,939 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:21:15,940 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:21:16,480 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:16,480 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1024])
2024-12-22 02:21:16,525 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:21:16,732 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       propertyMap.put("state", PropertyDescriptorSupport.createEnumeration(STATE, ManualState.class, 19, PROPERTY_CONSTRAINTS[0], false));
        propertyMap.put("userMessage", PropertyDescriptorSupport.createBasetype(USERME
 27%|██▋       | 27/100 [02:37<06:37,  5.44s/it]2024-12-22 02:21:16,930 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:17,518 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   public override SendInvitationRequest DefaultInstance {
      get { return defaultInstance; }
    }
    
    public override SendInvitationRequest BuildPartial() {
      return new SendInvitationRequest(this);
    }
    
    public static SendInvitationRequest Create(pb
 26%|██▌       | 26/100 [02:37<07:10,  5.81s/it]2024-12-22 02:21:17,641 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:18,257 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       'Add')
    @cached_property
    def rates(self):
        return self.browser.get_all_by_css(
            './/table/tr',
            parent=self.browser.get_active_view())
    def __init__(self, *
 30%|███       | 30/100 [02:38<05:54,  5.07s/it]2024-12-22 02:21:18,321 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:18,552 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       /// <list type="number">
        /// <item>It is possible to set the desired convergence limits.</item>
        /// <item>
        /// It is possible to check the reason for which the solver finished 
        /// the iterative procedure by calling the <see cref="
 28%|██▊       | 28/100 [02:38<06:18,  5.26s/it]2024-12-22 02:21:18,664 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:19,174 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			base.Render (writer);
		}
	}
}

Please complete the code by adding the necessary methods and properties to the MultiView class.

Note:

* The code given is just a part of the complete MultiView class, you may need to add more
 26%|██▌       | 26/100 [02:39<07:10,  5.82s/it]2024-12-22 02:21:19,334 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:19,860 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:19,860 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1283])
2024-12-22 02:21:19,902 - [Process 1/5] - DEBUG - predict_token:tensor([[268]], device='cuda:1')
2024-12-22 02:21:20,453 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:20,453 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1964])
2024-12-22 02:21:20,522 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:20,522 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1109])
2024-12-22 02:21:20,525 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:21:20,570 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:21:20,862 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:20,863 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1279])
2024-12-22 02:21:20,910 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:21:22,395 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:    *            the geometry.
     * @param indices
     *            the indices of the geometry.
     */
    }
    public void setVertices(Vector3f[] vertices, Vector3f[] normal, Vector3f[] color, IntBuffer indices) {
        super.set
 27%|██▋       | 27/100 [02:42<06:43,  5.53s/it]2024-12-22 02:21:22,495 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:22,496 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1864])
2024-12-22 02:21:22,558 - [Process 0/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:0')
2024-12-22 02:21:22,583 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:22,855 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				if ( useShadowBuffer )
				{
					IntPtr destData = shadowBuffer.Lock( offset, length, BufferLocking.WriteOnly );
					Memory.Copy( src, destData, length );
		
 31%|███       | 31/100 [02:43<05:39,  4.93s/it]2024-12-22 02:21:22,960 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:23,415 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           dc.number = p.readString();
            response.add(dc);
        }
        return response;
    }
}

I'm not sure what the code is doing, but it seems to be related to handling RIL requests and responses. The code is using a
 28%|██▊       | 28/100 [02:43<06:58,  5.81s/it]2024-12-22 02:21:23,544 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:23,765 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:					Apfloat[] medians = new Apfloat[1];
				medians[0] = sortedNumbers[left];
				return medians;
			} else {
				Apfloat[] medians = new Apfloat[
 29%|██▉       | 29/100 [02:44<06:12,  5.25s/it]2024-12-22 02:21:23,965 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:25,223 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   self.downloader = WigleDownloader(user, password, coordfile, outpath)
    self.downloader.run()
    print >> sys.stdout, 'Finished downloading.'
except KeyboardInterrupt:
    print >> sys.stderr, 'Stopping the
 27%|██▋       | 27/100 [02:45<07:09,  5.88s/it]2024-12-22 02:21:25,411 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:25,952 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:25,952 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1359])
2024-12-22 02:21:26,000 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:21:26,039 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:26,040 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 02:21:26,106 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:21:26,500 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:26,501 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1931])
2024-12-22 02:21:26,575 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:21:27,468 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:27,468 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2029])
2024-12-22 02:21:27,537 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:21:28,658 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:						if (licence.isApplication()) {
					relevantLicences.add(licence);
				}
			}
			}
		}
	}
	private void getLicencesfor
 29%|██▉       | 29/100 [02:48<06:40,  5.64s/it]2024-12-22 02:21:28,804 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:28,827 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   }
    public void doDispose() {
        // nothing to do
    }
    public void doGet(PageContext pageContext) throws Throwable {
        // nothing to do
    }
    public void doPost(PageContext pageContext) throws Throwable {

 28%|██▊       | 28/100 [02:49<06:57,  5.80s/it]2024-12-22 02:21:28,935 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:29,003 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:29,003 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2204])
2024-12-22 02:21:29,066 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:21:29,143 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				self.list.append(self.satfinder_scan_nims)
				self.list.append(self.tuning_type)
				self.list.append(self.orbital_position)
				self
 32%|███▏      | 32/100 [02:49<06:02,  5.33s/it]2024-12-22 02:21:29,215 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:30,340 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       tables.edges.add_row(left=edge.left, right=edge.right, parent=edge.parent, child=edge.child)
        for u in edge.siblings:
            tables.edges.add_row(left=u, right=edge.
 30%|███       | 30/100 [02:50<06:35,  5.65s/it]2024-12-22 02:21:30,439 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:31,132 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:31,132 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1093])
2024-12-22 02:21:31,179 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:21:31,419 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:31,419 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1436])
2024-12-22 02:21:31,473 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:21:31,571 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:31,572 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1338])
2024-12-22 02:21:31,620 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:21:31,853 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                           else
                                context.Imbue_ModInt = max;
                        }
                        else
                        {
                            context.Imbue_ModInt = m_Definition.MaxIntensity;
                        }
                        return GetNameForAttribute(m
 28%|██▊       | 28/100 [02:52<07:19,  6.11s/it]2024-12-22 02:21:31,972 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:32,462 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:32,462 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1135])
2024-12-22 02:21:32,503 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:21:33,407 - [Process 3/5] - INFO - res.shape is :torch.Size([49])
results:       }
    }
}

I am unable to complete the code as the last line of code is incomplete and there are some unresolved external variables.
Please provide me with the complete code and the resolved external variables.
 33%|███▎      | 33/100 [02:53<05:35,  5.01s/it]2024-12-22 02:21:33,569 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:33,782 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:				yield return new WaitForSeconds (animationDuration);
				animDeltaTime = Time.realtimeSinceStartup - animStartTime;
				if (animDeltaTime <= animationDuration)
				{
					state
 29%|██▉       | 29/100 [02:54<06:33,  5.55s/it]2024-12-22 02:21:33,888 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:34,183 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:34,183 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1291])
2024-12-22 02:21:34,225 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:21:34,225 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           for (int x = 0; x < xSizeP; x++)
            {
                // Get the color at the current position
                Color32 color = colorMapTexture.GetPixel(x, y);
                // Calculate the texture coordinates
                Vector2 uv = new
 30%|███       | 30/100 [02:54<06:33,  5.62s/it]2024-12-22 02:21:34,339 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:35,232 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       cls.ocean_backend = get_elastic(self.es_con, self.ocean_index, clean=True,
                                    backend=self.connectors[self.connector][1])
        cls.enrich_backend = get_elastic(
 31%|███       | 31/100 [02:55<06:13,  5.42s/it]2024-12-22 02:21:35,354 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:35,973 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:35,974 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1128])
2024-12-22 02:21:36,016 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:21:36,407 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:36,407 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1110])
2024-12-22 02:21:36,449 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:21:36,856 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   # Build the download link
    link = download_urls['direct'] + '?'.join([
        download_urls['transition'],
        'product={prod}&os={plat}&lang={locale}'.format(
            prod=product, plat=platform, locale=locale),

 29%|██▉       | 29/100 [02:57<06:50,  5.78s/it]2024-12-22 02:21:37,041 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:37,103 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:37,103 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2046])
2024-12-22 02:21:37,173 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:21:37,767 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:37,767 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1274])
2024-12-22 02:21:37,822 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:21:38,639 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       base.OnSizeChanged(e);
      }
      else
      {
        // adjust the arrow position
        Point p = new Point(LEFT_MARGIN, TOP_MARGIN);
        SizeF size = Size;
        p.X += (size
 30%|███       | 30/100 [02:58<06:13,  5.34s/it]2024-12-22 02:21:38,765 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:39,102 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:				client.add(API_METHOD_ADD, new Object[] {testInt1});
				
			rawResult = client.call(API_METHOD_ADD, new Object[] {testInt1});
			}
		
 31%|███       | 31/100 [02:59<06:12,  5.40s/it]2024-12-22 02:21:39,176 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:39,735 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:               if (type == PRTokeniser.TK_OBJ) {
                    obj = ReadOject();
                    if (obj != null) {
                        if (obj.IsStream()) {
                            // read the stream
                            int len = obj.GetDirect()
 34%|███▍      | 34/100 [02:59<05:56,  5.41s/it]2024-12-22 02:21:39,819 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:40,407 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:40,407 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1915])
2024-12-22 02:21:40,478 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:21:40,501 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:												var bot = botController.CreateBot(slot.Value.Name);
											slot.Value.AddBot(bot);
										}
 32%|███▏      | 32/100 [03:00<06:05,  5.37s/it]2024-12-22 02:21:40,543 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:40,543 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 675])
2024-12-22 02:21:40,574 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:21:40,595 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:41,338 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:41,338 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1487])
2024-12-22 02:21:41,385 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:21:42,472 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:42,472 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 974])
2024-12-22 02:21:42,512 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:21:42,670 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:42,670 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1525])
2024-12-22 02:21:42,730 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:21:43,142 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:				return new int[0];
			}
			@Nonnull
			public int[] getBlockSelectionEnds()
			{
				return new int[0];
			}
			public void set
 32%|███▏      | 32/100 [03:03<05:39,  4.99s/it]2024-12-22 02:21:43,247 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:43,349 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			List<TStateBean> stateBeans = StatusBL.loadByObjectID(objectID);
		if (stateBeansList==null || stateBeansList.isEmpty()) {
			stateBeans = stateBeansList;
		}
		
 30%|███       | 30/100 [03:03<06:59,  5.99s/it]2024-12-22 02:21:43,498 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:44,187 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:               TLogging.LogAtLevel(1, "TProcessDataChecks.CheckModule: " + AModule + " errors:");
                foreach (DataRow r in errors.Rows)
                {
                    string message = "Error in " + AModule + " table: " +
 31%|███       | 31/100 [03:04<06:12,  5.40s/it]2024-12-22 02:21:44,290 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:45,035 - [Process 2/5] - INFO - res.shape is :torch.Size([63])
results:			}
			return base.ToString();
		}
	}
}

Please complete the code by writing the missing methods and properties.

Note: The code is written in C# and it is a part of a library for working with SMB protocol.
 33%|███▎      | 33/100 [03:05<05:43,  5.12s/it]2024-12-22 02:21:45,060 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:45,060 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1082])
2024-12-22 02:21:45,094 - [Process 4/5] - DEBUG - predict_token:tensor([[268]], device='cuda:4')
2024-12-22 02:21:45,143 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:45,154 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertEqual(types.Interface('my.interface', 'my.interfacetype', 'my.signal'),
                         {'my.interface': 'my.interfacetype', 'my.signal': 'my.signal'})
        self.assertEqual(types.Interface
 35%|███▌      | 35/100 [03:05<05:51,  5.41s/it]2024-12-22 02:21:45,262 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:46,282 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:46,282 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1523])
2024-12-22 02:21:46,337 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:21:46,369 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:46,369 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1122])
2024-12-22 02:21:46,411 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:21:47,305 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:47,305 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1316])
2024-12-22 02:21:47,345 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:21:47,614 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:    * @param clazz      the class of the object.
     * @param method     the method to be called.
     * @param args      the arguments to be passed to the method.
     * @return         the bytecode of the proxy class.
     */
    public byte[]
 33%|███▎      | 33/100 [03:07<05:23,  4.83s/it]2024-12-22 02:21:47,837 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:48,793 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:48,793 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1934])
2024-12-22 02:21:48,866 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:21:49,265 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       }
    }
}

Please help me to complete this code by adding the necessary methods and classes to convert the AutoIt syntax to C# syntax.

I have added some of the methods and classes as per the code snippet you provided but I am not sure if I have done it correctly.
 31%|███       | 31/100 [03:09<06:51,  5.97s/it]2024-12-22 02:21:49,330 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:						directions |= ScrollDirection.Right;
					if (Viewport.LastMousePos.Y >= Game.Renderer.Resolution.Height - EdgeScrollThreshold)
						directions |= ScrollDirection.Down;
	
 32%|███▏      | 32/100 [03:09<06:02,  5.33s/it]2024-12-22 02:21:49,504 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:49,540 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:49,865 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       frame_header = FrameHeader.from_frame(self).serialize()
        padding_bytes = ((self.pad_high * 256) + self.pad_low) * struct.pack('!x')
        return frame_header + padding_bytes
    def deser
 34%|███▍      | 34/100 [03:10<05:32,  5.03s/it]2024-12-22 02:21:49,985 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:51,285 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:51,285 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2324])
2024-12-22 02:21:51,342 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:21:51,440 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def make_resolution(self):
        massey_out = self.p_operator(self.get_E_2_page().level_one_m_product(
            h3, 1, h04, 4, "x", "y", self.get
 36%|███▌      | 36/100 [03:11<06:03,  5.67s/it]2024-12-22 02:21:51,551 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:52,352 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:52,353 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1327])
2024-12-22 02:21:52,401 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:21:52,987 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:52,987 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1972])
2024-12-22 02:21:53,059 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:21:53,064 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:53,064 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-22 02:21:53,136 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:21:54,192 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                           dirty_videoram_b[offs] = 0;
                            dirty_chardata_b[char_number] = 0;
                            Machine.visible_area = 0;
                            TRANSPARENCY_NONE = 0
 34%|███▍      | 34/100 [03:14<05:53,  5.36s/it]2024-12-22 02:21:54,404 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:55,012 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:55,013 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2107])
2024-12-22 02:21:55,079 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:21:55,164 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           var b = cluster.BoundingBox;
            var b = new Rectangle(b.Left, b.Bottom, b.Width, b.Height);
            b.Inflate(margins, margins);
            return b;
        }
        internal void UpdateBounding
 35%|███▌      | 35/100 [03:15<05:32,  5.11s/it]2024-12-22 02:21:55,361 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:56,007 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       split_test = self._update_partition_id(0)
        # Verify that the child verticals are updated to use the new group configuration.
        self.assertEqual(2, len(split_test.children))
        vertical_0 = self.get_item_from
 32%|███▏      | 32/100 [03:16<07:01,  6.20s/it]2024-12-22 02:21:56,080 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           }
        }
    }
}
\end{code}

I have a problem with the code above. The code is not able to find the `PcDiscount` table in the `FMainDS` dataset.

I have checked the connection string and the table name in
 33%|███▎      | 33/100 [03:16<06:25,  5.75s/it]2024-12-22 02:21:56,174 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:56,192 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:57,652 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       public void ParseExtensions()
        {
            TestAllExtensions.Builder builder = TestAllExtensions.CreateBuilder();
            TextFormat.Merge(AllExtensionsSetText,
                             TestUtil.CreateExtensionRegistry(),
                             builder);
            TestUtil.AssertAllExtensionsSet(builder
 37%|███▋      | 37/100 [03:17<06:07,  5.84s/it]2024-12-22 02:21:57,692 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:57,692 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 958])
2024-12-22 02:21:57,705 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:21:57,721 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:21:57,928 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:57,928 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2011])
2024-12-22 02:21:57,997 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:21:58,863 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:58,864 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1939])
2024-12-22 02:21:58,935 - [Process 2/5] - DEBUG - predict_token:tensor([[29912]], device='cuda:2')
2024-12-22 02:21:59,351 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:59,351 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 883])
2024-12-22 02:21:59,379 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:21:59,379 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1885])
2024-12-22 02:21:59,385 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:21:59,444 - [Process 0/5] - DEBUG - predict_token:tensor([[308]], device='cuda:0')
2024-12-22 02:22:00,339 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           var file = e.Args.Next ("file");
            if (file != null) {
                // Loading file and saving to stream.
                LoadFile (context, e.Args, file, outStream);
            }
        }
        private static void LoadFile (
           
 34%|███▍      | 34/100 [03:20<05:50,  5.30s/it]2024-12-22 02:22:00,442 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:00,859 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           result.Hitchance = HitChance.OutOfRange;
            return result;
        }
        }
    }
}

I'm trying to create a custom spell for Yasuo that will predict where the skillshot will hit based on the distance and angle between the c
 35%|███▌      | 35/100 [03:21<06:13,  5.75s/it]2024-12-22 02:22:01,013 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:01,582 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:			return map.remove( key );
	}
	@Override
	public boolean isDirty() {
		return dirty;
	}
	@Override
	public void setDirty(boolean dirty) {
		this.dirty = dirty;
	}
 38%|███▊      | 38/100 [03:21<05:26,  5.26s/it]2024-12-22 02:22:01,653 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:01,799 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:void btKinematicCharacterController::setGroundPlane(btCollisionShape* groundShape)
{
	m_groundShape = groundShape;
}
void btKinematicCharacterController::setGroundNormal(btVector3 groundNormal)
{
	m_
 36%|███▌      | 36/100 [03:22<05:56,  5.57s/it]2024-12-22 02:22:01,941 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:02,211 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           if (vertical) {
                // Create a new CIDFontType2 dictionary
                PdfDictionary cidFont = new PdfDictionary(PdfName.FONT);
                // Set the encoding to vertical
                cidFont.Put(PdfName.SUBTYPE, P
 33%|███▎      | 33/100 [03:22<06:55,  6.20s/it]2024-12-22 02:22:02,375 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:02,388 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:02,388 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1054])
2024-12-22 02:22:02,426 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:22:03,763 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:03,764 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1559])
2024-12-22 02:22:03,822 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:22:04,103 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:04,103 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1347])
2024-12-22 02:22:04,152 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:22:04,373 - [Process 1/5] - INFO - res.shape is :torch.Size([50])
results:       :param reason: reason for adding the package
        :type reason: str
        :param strong: is the requirement strong
        :type strong: bool
        """

Please complete the code by adding the missing parts.
 35%|███▌      | 35/100 [03:24<05:19,  4.92s/it]2024-12-22 02:22:04,549 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:04,661 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:04,662 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1587])
2024-12-22 02:22:04,716 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:22:05,171 - [Process 4/5] - INFO - res.shape is :torch.Size([32])
results:       List<Node> path = graph.getPath(x, y);
        return !path.isEmpty();
    }
}
}
 36%|███▌      | 36/100 [03:25<05:40,  5.32s/it]2024-12-22 02:22:05,264 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:05,735 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:05,735 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1737])
2024-12-22 02:22:05,808 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:22:06,491 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:						}
				catch {}
			}
		}
	}
}

This code is for an auction system in a private server, and it logs various events related to the auctions, such as new auctions, bids, and
 39%|███▉      | 39/100 [03:26<05:14,  5.16s/it]2024-12-22 02:22:06,557 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:06,889 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:06,889 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 924])
2024-12-22 02:22:06,920 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:22:07,391 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       for (ExpressionTree init : initializers) {
            checkUnique(init);
        }
        return super.visitNewArray(node, p);
    }
    private void checkUnique(ExpressionTree init) {
        // Check if the expression is a reference to a
 37%|███▋      | 37/100 [03:27<05:51,  5.58s/it]2024-12-22 02:22:07,515 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:08,162 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:08,162 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1855])
2024-12-22 02:22:08,244 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:22:08,533 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:							GraphNode cgn = graphNodes.get(cell);
							cgn.x = (int)(xScale * (cell.getX() - graphCell.getX()));
							cgn.y =
 34%|███▍      | 34/100 [03:28<06:51,  6.24s/it]2024-12-22 02:22:08,639 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:08,639 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1202])
2024-12-22 02:22:08,645 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:08,682 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:22:09,472 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                   var child = pop.Children.Find(Name);
                    if (child != null)
                        return child;
                }
            }
        }
    }
}

Please help me complete this code. I am new to C# and 3DML programming.
 37%|███▋      | 37/100 [03:29<05:15,  5.01s/it]2024-12-22 02:22:09,515 - [Process 1/5] - INFO - res.shape is :torch.Size([29])
results:				return Enabled && CopyCustom;
		}
		#endregion
	}
}


 36%|███▌      | 36/100 [03:29<05:19,  4.99s/it]2024-12-22 02:22:09,570 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:09,613 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:09,752 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:09,753 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1279])
2024-12-22 02:22:09,801 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:22:10,681 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:10,682 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1192])
2024-12-22 02:22:10,722 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 02:22:10,977 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   attendance_ids = self.env['hr.attendance'].search([('employee_id', '=', self.id)])
    for attendance in attendance_ids:
        attendance.write({'check_in': attendance.check_in, 'check_out
 40%|████      | 40/100 [03:31<04:57,  4.96s/it]2024-12-22 02:22:11,064 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:11,488 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:11,488 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1017])
2024-12-22 02:22:11,531 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:22:11,578 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:11,578 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1134])
2024-12-22 02:22:11,616 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:22:12,401 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:				if (nbttagcompound != null)
			{
				NBTTagCompound nbttagcompound1 = nbttagcompound.getCompoundTag("display");
				if (nbttagcompound1
 38%|███▊      | 38/100 [03:32<05:35,  5.41s/it]2024-12-22 02:22:12,512 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:13,508 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results: private Drawable getEmojiDrawable(int codePoint) {
    Drawable drawable = offsets.get(codePoint);
    if (drawable != null) {
      return drawable;
    }
    // ...
  }
}

Please complete the code by
 35%|███▌      | 35/100 [03:33<06:20,  5.86s/it]2024-12-22 02:22:13,658 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:13,905 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:13,905 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1444])
2024-12-22 02:22:13,972 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:22:14,222 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   public void setUpdateTimestamp(Timestamp updateTimestamp) {
        this.updateTimestamp = updateTimestamp;
    }
    public String getUpdateUserFullName() {
        return updateUserFullName;
    }
    public void setUpdateUserFullName(String updateUserFullName) {
 38%|███▊      | 38/100 [03:34<05:05,  4.93s/it]2024-12-22 02:22:14,325 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:				get {
					return Path.Combine (XdgBaseDirectorySpec.GetUserDirectory ("XDG_DATA_HOME",
					                                                               Path.Combine (".local", "share")),
					
 37%|███▋      | 37/100 [03:34<05:10,  4.94s/it]2024-12-22 02:22:14,348 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:14,416 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:14,564 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:14,564 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1181])
2024-12-22 02:22:14,604 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:22:16,105 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:16,105 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1020])
2024-12-22 02:22:16,140 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:22:16,368 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:16,368 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1494])
2024-12-22 02:22:16,422 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:22:16,432 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:		grdResultsRow row = form.grdResults().getRows().newRow();
		row.setValue(orderInvestigationLiteVo);
		return row;
	}
	private void addResult(OrderInvestigationLiteVo orderInvestig
 41%|████      | 41/100 [03:36<05:01,  5.11s/it]2024-12-22 02:22:16,495 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:16,793 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:16,793 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1427])
2024-12-22 02:22:16,841 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:22:17,114 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:										_methodHeader = string.Format(
									HEADER_PATTERN,
									ItemName.ToString(true),
									_
 39%|███▉      | 39/100 [03:37<05:17,  5.20s/it]2024-12-22 02:22:17,246 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:18,581 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:18,581 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1139])
2024-12-22 02:22:18,624 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:22:18,691 - [Process 1/5] - INFO - res.shape is :torch.Size([57])
results:           outputStream.print(bugAnnotation.toString());
        }
    }
    }
}

I need help in completing the code by filling the missing braces and statements.
Please help me in completing the code.

Thank you.
 38%|███▊      | 38/100 [03:38<04:55,  4.76s/it]2024-12-22 02:22:18,883 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:19,275 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:               if (ke.getKeyCode() == KeyEvent.VK_DOWN) {
                    final String down = cmdHistory.goDown();
                    if (!msgTF.getText().equals(down)) {
                        msgTF.setText(down);
                    }
               
 36%|███▌      | 36/100 [03:39<06:13,  5.83s/it]2024-12-22 02:22:19,367 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:19,409 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               m_Item.Explode_Callback(this, p, from.Map);
            }
        }
        }
    }
}

Please help me to complete the code.

I have tried to understand the code but it seems to be incomplete and I am not able to
 39%|███▉      | 39/100 [03:39<05:05,  5.01s/it]2024-12-22 02:22:19,561 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:19,683 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:19,683 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1374])
2024-12-22 02:22:19,733 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:22:20,219 - [Process 2/5] - INFO - res.shape is :torch.Size([12])
results:			}
	}
}


 40%|████      | 40/100 [03:40<04:34,  4.57s/it]2024-12-22 02:22:20,390 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:20,904 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:               return new SystemListNext(sw, c.ReadToStructure(0, typeof(LVCOLUMN)));
            }
        }
        internal static SystemListViewItem[] GetItems(SystemWindow sw, int start, int count)
        {
            SystemListViewItem[] items = new
 42%|████▏     | 42/100 [03:41<04:45,  4.92s/it]2024-12-22 02:22:20,975 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:21,011 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:21,011 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 999])
2024-12-22 02:22:21,044 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:22:22,413 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:22,413 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 02:22:22,482 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:22:22,559 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:22,559 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1623])
2024-12-22 02:22:22,625 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:22:23,330 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:23,331 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1299])
2024-12-22 02:22:23,377 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:22:23,718 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   @navigator.register(Node, 'ManagePolicies', view=ManagePolicies)
    """
    def step(self, *args, **kwargs):
        self.prerequisite_view.policy.item_select('Manage Policies')

 37%|███▋      | 37/100 [03:44<05:41,  5.41s/it]2024-12-22 02:22:23,826 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:23,900 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:23,901 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-22 02:22:23,970 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:22:25,156 - [Process 2/5] - INFO - res.shape is :torch.Size([26])
results:					assertThat(result, is(nullValue()));
		}
	}
}

 41%|████      | 41/100 [03:45<04:36,  4.68s/it]2024-12-22 02:22:25,302 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:			{
			this.Weight = 1.0;
			this.FillFactor = 4;
		}
		public FriedEggs( Serial serial ) : base( serial )
		{
		}
		public override
 39%|███▉      | 39/100 [03:45<05:24,  5.32s/it]2024-12-22 02:22:25,344 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:25,368 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:				History.Add(new WorldChatMessage(user, text, user.ToMapPoint(), DateTime.Now));
			}
			return true;
		}
	}
}

Please complete the code by adding the necessary methods and properties to the
 40%|████      | 40/100 [03:45<05:17,  5.29s/it]2024-12-22 02:22:25,483 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:25,494 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:25,700 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       new_tab = maker.new_tab(self, cwd, profile)
        return new_tab
    def on_hide_window(self, widget, event):
        """Called when the hide window key is pressed"""
        if self.get_property('visible
 43%|████▎     | 43/100 [03:45<04:38,  4.88s/it]2024-12-22 02:22:25,815 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:25,878 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:25,878 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1191])
2024-12-22 02:22:25,918 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:22:28,088 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:28,088 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1349])
2024-12-22 02:22:28,143 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 02:22:28,411 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:				column_definitions, LabanSequenceGenerator sequence_generator, long pose_buffer_size,
			long beats_per_measure, long beat_duration, ETimeUnit time_unit,
			double framerate, boolean debug) {
		
 38%|███▊      | 38/100 [03:48<05:22,  5.20s/it]2024-12-22 02:22:28,497 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:28,850 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:28,850 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1938])
2024-12-22 02:22:28,922 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:22:28,937 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:28,937 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 02:22:29,003 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:22:29,261 - [Process 4/5] - INFO - res.shape is :torch.Size([27])
results:   return mock.getAvailCompoIds(sClientSpaceId, sUserId);
}
}
}
 41%|████      | 41/100 [03:49<04:47,  4.87s/it]2024-12-22 02:22:29,278 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:29,278 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 02:22:29,346 - [Process 3/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:3')
2024-12-22 02:22:29,366 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:30,144 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:30,145 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 976])
2024-12-22 02:22:30,175 - [Process 0/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:0')
2024-12-22 02:22:31,367 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:31,367 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1133])
2024-12-22 02:22:31,405 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:22:31,682 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:			if (postdominators == null) {
				if (other.postdominators != null)
					return false;
			} else if (!postdominators.equals(other.postdominators))
				return false;
 42%|████▏     | 42/100 [03:51<05:03,  5.23s/it]2024-12-22 02:22:31,812 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:31,852 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:                           E.Cast(bigmob);
                        }
                        }
                        else if (Menu.GetBool("ComboR") && R.IsReady() && GetRCount >= Menu.GetSlider("ComboRLimit"))
                        {
                            R.
 40%|████      | 40/100 [03:52<05:41,  5.69s/it]2024-12-22 02:22:31,911 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           new[] {
                new AttributeArgument( "LayoutKind", (int)LayoutKind.Sequential )
            } );
            return Type.GetType( customAttributeBuilder.CreateAttributes() );
        }
        private StructTypeInfo GetTypeInfo( string typeName ) {
            return
 44%|████▍     | 44/100 [03:52<04:55,  5.28s/it]2024-12-22 02:22:31,939 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:32,017 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:32,929 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:               String s = (String)ViewState["ClickedImageURL"];
                return (s ?? String.Empty);
            }
            set
            {
                ViewState["ClickedImageURL"] = inspectURL(value);
            }
        }
        #endregion Public Properties

 39%|███▉      | 39/100 [03:53<05:04,  4.99s/it]2024-12-22 02:22:33,032 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:33,557 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:33,557 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 912])
2024-12-22 02:22:33,589 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:22:33,943 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                   for (int i = 0; i < checkedItemSize; i++) {
                        final int key = items.keyAt(i);
                        if (items.get(key)) {
                            files[++index] = (String) mListView.getItemAtPosition
 42%|████▏     | 42/100 [03:54<04:39,  4.82s/it]2024-12-22 02:22:34,050 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:34,183 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:34,183 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1399])
2024-12-22 02:22:34,231 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:22:35,045 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:35,046 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1115])
2024-12-22 02:22:35,087 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:22:35,683 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:35,683 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2227])
2024-12-22 02:22:35,748 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:22:36,113 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:36,113 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1218])
2024-12-22 02:22:36,153 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:22:36,255 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					int height, double wx, double wy, char fill, String anchor)
	{
		return get(x, y, width, height, wx, wy, fill, anchor);
	}
}

I need help in completing the code by adding the remaining
 41%|████      | 41/100 [03:56<05:12,  5.30s/it]2024-12-22 02:22:36,384 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:36,819 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                       //      if (spillGuts) System.out.println("Skipping rule " + i + " because of constraint " + c);
                        continue;
                      }
                    }
                  }
                  if (Test.lengthNormalization) {
                
 43%|████▎     | 43/100 [03:57<04:56,  5.21s/it]2024-12-22 02:22:37,039 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:37,764 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			smite = compound.getInteger("smite");
			sharpness = compound.getInteger("sharpness");
			fire = compound.getInteger("fire");
			blast = compound.getInteger("blast");
			b
 40%|████      | 40/100 [03:58<04:56,  4.95s/it]2024-12-22 02:22:37,845 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:38,320 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:					if (value) {
						attributes = attributes.SetMaskedAttributes ((uint) TypeAttributes.VisibilityMask, (uint) TypeAttributes.Public, value);
					} else {
						attributes = attributes.
 45%|████▌     | 45/100 [03:58<05:08,  5.62s/it]2024-12-22 02:22:38,365 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:38,591 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:38,592 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1258])
2024-12-22 02:22:38,639 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:22:38,745 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   at net.minecraft.server.NetworkManager.a(NetworkManager.java:105)
    at net.minecraft.server.NetworkManager.channelActive(NetworkManager.java:83)
    at io.netty.channel.AbstractChannel.fireChannelActive(
 43%|████▎     | 43/100 [03:59<04:34,  4.81s/it]2024-12-22 02:22:38,971 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:39,447 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:39,448 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 873])
2024-12-22 02:22:39,478 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:22:39,817 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:39,817 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 839])
2024-12-22 02:22:39,846 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:22:40,546 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:40,546 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1965])
2024-12-22 02:22:40,618 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:22:41,427 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       fb = self._retr_file(filename)
        try:
            year, month, day = re.findall(r"\d\d\d\d\-\d\d\-\d\d", fb)[0].split('-')
        except Exception:
 42%|████▏     | 42/100 [04:01<05:05,  5.26s/it]2024-12-22 02:22:41,553 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:42,031 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       /// </summary>
        public void Evaluate()
        {
            //evaluate the rules
            foreach (string key in models.Keys)
            {
                //evaluate the model
                XmlDocument model = models[key];
                //evaluate the rules
                foreach
 46%|████▌     | 46/100 [04:02<04:32,  5.05s/it]2024-12-22 02:22:42,136 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:42,268 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       return super.toString() + "probability: " + probability + " scopes: " + (scopes != null ? scopes : "none");
    }
}

I am unable to complete the code as the method signature is not complete and there are some missing curly braces.
 41%|████      | 41/100 [04:02<04:44,  4.81s/it]2024-12-22 02:22:42,385 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:42,603 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:42,603 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2141])
2024-12-22 02:22:42,667 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:22:43,380 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   'rsync', '--pgdata', 'src', 'dst'
],
shell=False, env=None,
stdout=PIPE, stderr=PIPE, stdin=PIPE,
preexec_fn=mock.ANY, close_fds=True
)
 44%|████▍     | 44/100 [04:03<05:14,  5.61s/it]2024-12-22 02:22:43,571 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:43,749 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:43,750 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1324])
2024-12-22 02:22:43,792 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:22:44,765 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:44,765 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1312])
2024-12-22 02:22:44,813 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:22:45,455 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                       h_key = '{0}_{1:02}'.format('theta', theta_count-1)
                        h_value = float(h_list[h_index+1])
                        header_dict[h_key].append(h_value)
                
 44%|████▍     | 44/100 [04:05<05:01,  5.38s/it]2024-12-22 02:22:45,583 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:45,583 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2081])
2024-12-22 02:22:45,597 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:45,648 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:22:46,557 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       title:
            creator: marc, '245__', value
        bar:
            marc, '245__', value
        baz:
            marc, '245__', value
        '''
        tmp_file_5.write(config_5)
 43%|████▎     | 43/100 [04:06<04:57,  5.22s/it]2024-12-22 02:22:46,777 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:46,998 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:46,998 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1912])
2024-12-22 02:22:47,072 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:22:47,590 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   i = store.handle_indicators_create(t, {
        'indicator': 'example.com',
        'group': 'staff',
        'provider': 'example.com',
        'tags': ['test'],
        'itype': 'fqdn
 42%|████▏     | 42/100 [04:07<04:48,  4.97s/it]2024-12-22 02:22:47,753 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:48,210 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   # We need to convert the dNSName to the standard library format.
    # This is a bit tricky, because the standard library uses PyUnicode_FromStringAndSize
    # to decode the dNSName, but we got it as a bytes object from OpenSSL.
    # So
 47%|████▋     | 47/100 [04:08<04:45,  5.39s/it]2024-12-22 02:22:48,277 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:48,278 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1712])
2024-12-22 02:22:48,318 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:48,327 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:22:49,906 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:				process.IsRunningChanged -= DbgProcess_IsRunningChanged;
				process.DelayedIsRunningChanged -= DbgProcess_DelayedIsRunningChanged;
				process.ThreadsChanged -= DbgProcess_ThreadsChanged;
			
 45%|████▌     | 45/100 [04:10<05:23,  5.89s/it]2024-12-22 02:22:50,109 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:50,308 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:50,308 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1930])
2024-12-22 02:22:50,381 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:22:50,919 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:50,920 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1867])
2024-12-22 02:22:50,983 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:22:51,104 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   GroupDatabase.GroupRecord groupRecord;
    while ((groupRecord = groupDatabase.getNext()) != null) {
      if (groupRecord.getMembers().contains(recipient.getAddress()) && groupRecord.isActive()) {
        SignalServiceGroup group = new Sign
 45%|████▌     | 45/100 [04:11<05:00,  5.46s/it]2024-12-22 02:22:51,213 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:51,771 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:51,772 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1886])
2024-12-22 02:22:51,847 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:22:52,782 - [Process 1/5] - INFO - res.shape is :torch.Size([53])
results:                   new[] {new object[] {"E1", 10L, ">E1<", "?E1?"}});
                env.UndeployAll();
            }
        }
    }
}


 44%|████▍     | 44/100 [04:13<05:09,  5.52s/it]2024-12-22 02:22:52,995 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:53,287 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:53,287 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1257])
2024-12-22 02:22:53,330 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:22:53,543 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:53,544 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2098])
2024-12-22 02:22:53,609 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:22:53,836 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertEqual(person.owns[0], organization)
        self.assertEqual(organization.owner, person)
        self.assertEqual(person.employer, organization)
        self.assertEqual(organization.employees[0], employee)
       
 43%|████▎     | 43/100 [04:14<05:04,  5.35s/it]2024-12-22 02:22:54,020 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:54,418 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				this.hbox3.Add (this.label8);
				global::Gtk.Box.BoxChild w28 = ((global::Gtk.Box.BoxChild)(this.vbox5 [this.hbox3]));
				w2
 48%|████▊     | 48/100 [04:14<04:52,  5.63s/it]2024-12-22 02:22:54,528 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:56,197 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       def get_binary_path(self, release, dev, android=False):
            base_path = self.get_target_dir()
            if android:
                base_path = path.join(base_path, self.config["android"]["target"])
            release_
 46%|████▌     | 46/100 [04:16<04:48,  5.35s/it]2024-12-22 02:22:56,301 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:56,512 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:																																																																
 46%|████▌     | 46/100 [04:16<05:29,  6.10s/it]2024-12-22 02:22:56,518 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:56,519 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1936])
2024-12-22 02:22:56,591 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:22:56,625 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:22:57,502 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:57,502 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2023])
2024-12-22 02:22:57,570 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:22:58,061 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:58,061 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1942])
2024-12-22 02:22:58,134 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:22:58,247 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:58,248 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1052])
2024-12-22 02:22:58,286 - [Process 4/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:4')
2024-12-22 02:22:58,828 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:22:58,828 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1207])
2024-12-22 02:22:58,874 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:22:59,460 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def __div__(self, other):
        if not isinstance(other, Particle):
            return NotImplemented
        c = self.copy()
        return c.__idiv__(other)
    
    def __idiv__(self, other):
        if not is
 45%|████▌     | 45/100 [04:19<05:22,  5.87s/it]2024-12-22 02:22:59,577 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:00,447 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       bins=50, alpha=0.5, label='Waveform')
        plt.xlabel('Time (s)')
        plt.ylabel('Amplitude')
        plt.title('Waveform')
        plt.show()
        plt.close()
 44%|████▍     | 44/100 [04:20<05:20,  5.73s/it]2024-12-22 02:23:00,534 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:00,697 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   headbook.set_tab_reorderable(True)
    headbook.connect("tab-reordered", page_reordered)
    headbook.show_all()
    mainvbox.show()
    widgets["window1"].show()
    gtk.main
 49%|████▉     | 49/100 [04:20<04:57,  5.83s/it]2024-12-22 02:23:00,803 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:01,081 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               changed = True
        return self, changed, reftest_nodes, reftest_changes
    def _load_tests(self, tests_root):
        self.url_base = os.path.join(tests_root, self.url_base)
        self.
 47%|████▋     | 47/100 [04:21<04:36,  5.21s/it]2024-12-22 02:23:01,189 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:01,679 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 02:23:01,682 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:01,682 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1192])
results:		if ( (proxyThat instanceof Contact) && (((Contact)proxyThat).getId().equals(this.getId())) ) {
			return true;
		}
		return false;
	}
	/** HashCode implementation.
	 * @see java.
 47%|████▋     | 47/100 [04:21<05:08,  5.82s/it]2024-12-22 02:23:01,724 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:23:01,812 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:02,156 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:02,156 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1009])
2024-12-22 02:23:02,187 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:23:03,213 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:03,214 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1100])
2024-12-22 02:23:03,255 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:23:04,334 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:04,334 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1962])
2024-12-22 02:23:04,407 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:23:04,433 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:04,433 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1425])
2024-12-22 02:23:04,490 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:23:04,572 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   view = view(cr, uid, 'view_id', context=context)
    arch = view.arch
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
   
 46%|████▌     | 46/100 [04:24<05:04,  5.64s/it]2024-12-22 02:23:04,682 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:05,041 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           var rootFolder = message.Model;
            var path = rootFolder.Path;
            if (_changedPaths.TryGetValue(path, out var watcher))
            {
                DisposeWatcher(watcher, true);
                _changedPaths.Remove(path);
 45%|████▌     | 45/100 [04:25<04:56,  5.39s/it]2024-12-22 02:23:05,137 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:05,936 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               if (compiledScript != null) {
                    compiledScript.clearCache();
                }
            }
    }
    private void trustedCompileAndCache(PrintStream outStream) throws Throwable {
        try {
            if (errorsInScript != null) {
 48%|████▊     | 48/100 [04:26<04:25,  5.10s/it]2024-12-22 02:23:06,017 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:06,675 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:06,675 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1122])
2024-12-22 02:23:06,712 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:23:06,973 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   "Move-Tab-to-left": QKeySequence(Qt.CTRL + Qt.SHIFT + Qt.Key_0),
}

# End of code

I have a question regarding this code, how to use the "os.path.join" function in this code
 50%|█████     | 50/100 [04:27<04:58,  5.96s/it]2024-12-22 02:23:06,997 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:06,997 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1003])
2024-12-22 02:23:07,018 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:07,038 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:23:07,216 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   # Calculate forces and torques
    LSolv = LS(bodies, a, eta, cutoff, L, debye_length=firm_delta)
    LSolv.dt = dt
    LSolv.kT = read.kT

 48%|████▊     | 48/100 [04:27<04:58,  5.74s/it]2024-12-22 02:23:07,433 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:07,434 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 840])
2024-12-22 02:23:07,458 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:23:07,561 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:08,475 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:08,475 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 794])
2024-12-22 02:23:08,506 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:23:09,575 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:                                 int w, int h) {
        View view = c.getView();
        if (view != null) {
            int baseline = getHTMLBaseline(view, w, h);
            if (baseline != -1) {
                return y + baseline
 47%|████▋     | 47/100 [04:29<04:48,  5.45s/it]2024-12-22 02:23:09,593 - [Process 0/5] - INFO - res.shape is :torch.Size([56])
results:		return read(key, computed);
	}
}

I have a problem with the last line of code, I am not sure how to write it. Can someone please help me?

Please let me know if you need any more information.
 46%|████▌     | 46/100 [04:29<04:37,  5.14s/it]2024-12-22 02:23:09,722 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:09,799 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:09,895 - [Process 4/5] - INFO - res.shape is :torch.Size([63])
results:           public void Reconnect()
            {
                throw new NotImplementedException();
            }
        }
    }
}

I am unable to complete the code as it is incomplete and lacks proper documentation. Can someone please help me complete this code and provide proper documentation?
 49%|████▉     | 49/100 [04:30<04:02,  4.76s/it]2024-12-22 02:23:10,031 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:10,587 - [Process 3/5] - INFO - res.shape is :torch.Size([61])
results:			return super.toString();
	}
}

I have to complete the code by adding the missing methods and classes.
Please help me with that.

Note: I have already completed the first 4 methods and the last method is already provided in the code.
 51%|█████     | 51/100 [04:30<04:17,  5.26s/it]2024-12-22 02:23:10,639 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:11,069 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:11,069 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1963])
2024-12-22 02:23:11,141 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:23:12,305 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:12,305 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 912])
2024-12-22 02:23:12,333 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:12,334 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1551])
2024-12-22 02:23:12,340 - [Process 3/5] - DEBUG - predict_token:tensor([[418]], device='cuda:3')
2024-12-22 02:23:12,386 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:23:12,655 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:12,655 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1406])
2024-12-22 02:23:12,714 - [Process 4/5] - DEBUG - predict_token:tensor([[418]], device='cuda:4')
2024-12-22 02:23:13,258 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:13,258 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 02:23:13,323 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:23:13,807 - [Process 1/5] - INFO - res.shape is :torch.Size([34])
results:   assert read_script_metadata(BytesIO(content), js_meta_re)

Please provide the content of the file you want to test.
 48%|████▊     | 48/100 [04:34<04:24,  5.08s/it]2024-12-22 02:23:13,918 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:13,938 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   if cdata is None:
        return
    # We have a Subtensor, try to replace it with an AdvancedSubtensor1
    # that operates inplace
    if not any(isinstance(e, slice) and e.start is None and e.stop is None and

 49%|████▉     | 49/100 [04:34<05:07,  6.03s/it]2024-12-22 02:23:14,062 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:14,196 - [Process 3/5] - INFO - res.shape is :torch.Size([54])
results:   return super.isFileReadOnly();
  }
}

I have a problem with the last line of code, I am not sure how to implement it. Can someone please help me with this?

Thank you,
Sarah
 52%|█████▏    | 52/100 [04:34<03:48,  4.76s/it]2024-12-22 02:23:14,289 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:15,434 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   for i in range(energy.size):
      Etf = energy[i]
      grad_Etf = grad[i].flatten()
      grad_E = Etf*grad_var+energy_var*grad_Etf+grad_means
      grad_E
 50%|█████     | 50/100 [04:35<04:09,  4.99s/it]2024-12-22 02:23:15,553 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:15,972 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:15,972 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1225])
2024-12-22 02:23:16,013 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:23:16,105 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:16,106 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1268])
2024-12-22 02:23:16,106 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                                   for eachdisc in marker_discussion:
                                        strDiscussion += eachdisc.id + ','
                                        strTags += eachdisc.get('tags') + ','
                                        discussiontitle += eachdisc.title + ','
                                
 47%|████▋     | 47/100 [04:36<04:54,  5.55s/it]2024-12-22 02:23:16,147 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:23:16,294 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:17,486 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:17,486 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1752])
2024-12-22 02:23:17,552 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:23:17,937 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:17,937 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1147])
2024-12-22 02:23:17,991 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:23:18,531 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:				mapping.sort = HbmSort.True;
		}
		public void Sort(string sortClause)
		{
			mapping.sort = sortClause;
		}
		public void Join(string joinClause)

 49%|████▉     | 49/100 [04:38<04:13,  4.98s/it]2024-12-22 02:23:18,639 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:18,732 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.fields['groups'].label = "Select Permission Groups"
        self.helper_class = SubmitCancelFormHelper
        self.helper_cancel_href = "{% url 'view_community_page' community.slug %}"
    def clean_groups(self):

 50%|█████     | 50/100 [04:39<04:43,  5.66s/it]2024-12-22 02:23:19,056 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:19,852 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:19,852 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1809])
2024-12-22 02:23:19,932 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:23:20,050 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				this.butAttach.Location = new System.Drawing.Point(768, 65);
				this.butAttach.Name = "butAttach";
				this.butAttach.Size = new System.Drawing.Size
 53%|█████▎    | 53/100 [04:40<03:59,  5.09s/it]2024-12-22 02:23:20,107 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:20,616 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:		if (this.getClass().equals(obj.getClass()))
		{
			OrderInvestigationForStatusChangeVo other = (OrderInvestigationForStatusChangeVo)obj;
			return this.getOrdInvCurrentStatus().compareTo(
 51%|█████     | 51/100 [04:40<04:07,  5.05s/it]2024-12-22 02:23:20,717 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:20,873 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:20,874 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1114])
2024-12-22 02:23:20,922 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:23:22,052 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:22,052 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1076])
2024-12-22 02:23:22,091 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:23:22,493 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:22,493 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2069])
2024-12-22 02:23:22,559 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:23:22,727 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:22,727 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1168])
2024-12-22 02:23:22,765 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:23:22,807 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:				set { m_ID = value; }
			}
		}
}

I am unable to understand the code provided, as it seems to be incomplete and contains errors. Here are some suggestions on how to improve the code:

1. Use meaningful variable
 48%|████▊     | 48/100 [04:43<05:06,  5.90s/it]2024-12-22 02:23:23,011 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:23,710 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:			writer.WriteEncodedInt( (int) m_Members.Count );
			foreach ( PlayerState pl in m_Members )
			{
				pl.Serialize( writer );
			}
			m_E
 50%|█████     | 50/100 [04:43<04:11,  5.04s/it]2024-12-22 02:23:23,910 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:24,329 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       out.write(encode(contentTitle == null ? "" : contentTitle));
        }
    }
    public static String encode(String string)
    {
        if (string == null)
        {
            return "";
        }
        else
        {
            try

 54%|█████▍    | 54/100 [04:44<03:42,  4.85s/it]2024-12-22 02:23:24,438 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:25,464 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   def init(self, manager, cls):
        """Called after the first instance of a class has been initialized.
        This event is called after the first instance of a class has been
        initialized, and before any attribute events are fired.
        """
    def first_attribute_
 51%|█████     | 51/100 [04:45<04:53,  5.98s/it]2024-12-22 02:23:25,588 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:25,654 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   tion.class)
    public void shouldThrowConflictExceptionWhenCreatingStackWithSameNameAsExistingOne() throws Exception {
        final StackImpl existingStack = stacks[0];
        final StackImpl newStack = createStack(existingStack.getName(), existingStack.
 52%|█████▏    | 52/100 [04:45<04:02,  5.05s/it]2024-12-22 02:23:25,789 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:26,472 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:26,473 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 02:23:26,538 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:23:27,369 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:27,370 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 02:23:27,435 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:23:27,633 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:27,634 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1273])
2024-12-22 02:23:27,675 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:23:28,213 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:28,213 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1536])
2024-12-22 02:23:28,308 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:23:28,573 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:28,573 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1524])
2024-12-22 02:23:28,628 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:23:29,427 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                   m_log.DebugFormat("[FreeSwitchVoice][PROVISIONVOICE]: ProvisionVoiceAccountRequest: {0}", request);
                    //m_log.DebugFormat("[FreeSwitchVoice][PROVISIONVOICE]: ProvisionVoice
 49%|████▉     | 49/100 [04:49<05:11,  6.11s/it]2024-12-22 02:23:29,571 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:30,120 - [Process 3/5] - INFO - res.shape is :torch.Size([45])
results:			Check ("HMACSHA512-N-RFC4231-TC1", hmac, data, digest);
		}
	}
}
}
 55%|█████▌    | 55/100 [04:50<03:50,  5.13s/it]2024-12-22 02:23:30,190 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:30,317 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   }
    private void moveForward() {
        assert graphQueue.size() > 0;
        CallsiteHolderExplorable callsiteHolder = (CallsiteHolderExplorable) graphQueue.peek();
        assert callsiteHolder != null;
        In
 51%|█████     | 51/100 [04:50<04:29,  5.51s/it]2024-12-22 02:23:30,402 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                   return etree.tostring(xml_object, encoding='unicode', method='xml')
                    return self.system.save_item(xml_object, resource_fs)
        else:
            return None
    def is_condition_satisfied(self):

 52%|█████▏    | 52/100 [04:50<04:32,  5.67s/it]2024-12-22 02:23:30,497 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:30,539 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:31,403 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       Parameter depthParam = pf.addParameter(new Parameter(DEPTH_PARAM_KEY, depth));
        crawlParameters.add(depthParam);
        //EXCLUSION REGEX
        Parameter excludeRegexpParam = pf.addParameter(new Parameter(EX
 53%|█████▎    | 53/100 [04:51<04:07,  5.26s/it]2024-12-22 02:23:31,584 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:32,228 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:32,228 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1432])
2024-12-22 02:23:32,286 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:23:32,467 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:32,467 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1176])
2024-12-22 02:23:32,518 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:23:33,121 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:33,121 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1348])
2024-12-22 02:23:33,176 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:23:34,075 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:34,075 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1986])
2024-12-22 02:23:34,147 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:23:34,850 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:	    sum += current_align;
	    current_sum += 4;
	    
	    switch(this.m_d) {
		    case option_1:
			    sum += CDRSerializer.alignment(current_sum, 4); //
 56%|█████▌    | 56/100 [04:55<03:40,  5.01s/it]2024-12-22 02:23:34,934 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:34,955 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:					MessageBox.Show("Error: " + textDateStart.errorProvider1.GetError(textDateStart) + Environment.NewLine + " " + textDateStop.errorProvider1.GetError(textDateStop) + Environment.NewLine + " " + textDate
 50%|█████     | 50/100 [04:55<04:56,  5.94s/it]2024-12-22 02:23:35,173 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:35,220 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:35,220 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2119])
2024-12-22 02:23:35,286 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:23:35,747 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       for change in data_model_changes:
            self.update_filtered_data()
            self.update_axes(self.get_axes(self.filtered_data))
            self.update_vlabels(self.get_vlabels())
            self.
 53%|█████▎    | 53/100 [04:56<04:21,  5.57s/it]2024-12-22 02:23:35,967 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:36,936 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def get_pr_metadata(self, pr):
        r = requests.get(
            'https://api.github.com/repos/{}/{}/pulls/{}/issues'.format(
                self.args['gh_owner'], self.args['gh_repo'], pr
 52%|█████▏    | 52/100 [04:57<04:40,  5.84s/it]2024-12-22 02:23:37,061 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:37,695 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:37,695 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1615])
2024-12-22 02:23:37,750 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:23:38,060 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:																																																																
 54%|█████▍    | 54/100 [04:58<04:21,  5.68s/it]2024-12-22 02:23:38,169 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:38,685 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:38,686 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 02:23:38,754 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:23:39,394 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:39,394 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1872])
2024-12-22 02:23:39,468 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:23:39,481 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:39,481 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1433])
2024-12-22 02:23:39,527 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:23:40,179 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       arguments.Append(" -server ");
        arguments.Append(ResinArgs.Server);
      arguments.Append(" -start-resin ");
      arguments.Append(ResinArgs.StartResin);
      arguments.Append(" -stop-resin ");
      arguments.Append(
 57%|█████▋    | 57/100 [05:00<03:39,  5.11s/it]2024-12-22 02:23:40,234 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:40,234 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1239])
2024-12-22 02:23:40,275 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:23:40,285 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:41,649 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def _generate_cubes(header, column_headings, coords, data_arrays, cell_methods):
        cubes = []
        for i, (name, unit) in enumerate(column_headings.items()):
            if name == 'Time':

 51%|█████     | 51/100 [05:01<05:02,  6.16s/it]2024-12-22 02:23:41,762 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:42,332 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:				if (Class != null)
				{
					ilgen.Emit(OpCodes.Ldtoken, Class);
				}
				else if (Method != null)
				{
	
 54%|█████▍    | 54/100 [05:02<04:30,  5.88s/it]2024-12-22 02:23:42,351 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:               if (deleted) {
                    state.killReadCache(write.location());
                }
            }
        }
    }
    private void processIdentity(ReadEliminationBlockState state, LocationIdentity identity) {
        if (identity == ANY_LOCATION)
 53%|█████▎    | 53/100 [05:02<04:28,  5.71s/it]2024-12-22 02:23:42,451 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:42,480 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:43,077 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                       q.Cast(predictionInput);
                    }
                }
            }
        }
    }
}

Please help me complete the code.
I have provided the necessary imports and the code for the ZedShadows class.
I am not sure how to complete the
 55%|█████▌    | 55/100 [05:03<04:06,  5.48s/it]2024-12-22 02:23:43,270 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:43,818 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:43,818 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 02:23:43,833 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:43,833 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1186])
2024-12-22 02:23:43,875 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:23:43,888 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:23:44,451 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:44,451 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1145])
2024-12-22 02:23:44,489 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:23:45,093 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:45,093 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1399])
2024-12-22 02:23:45,152 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:23:46,456 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   hour: "12"
    day: "monday"
    month: "january"
    year: "2015"
    job: "yum update -y"
    backup: yes
    state: present
    insertafter: "PATH"
 58%|█████▊    | 58/100 [05:06<03:49,  5.46s/it]2024-12-22 02:23:46,567 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:46,697 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       return authz.is_authorized('group_create', context, data_dict)
    else:
        return {'success': False, 'msg': _('User %s not authorized to create groups') % user}
def organization_create(context, data_dict):
   
 52%|█████▏    | 52/100 [05:06<04:39,  5.83s/it]2024-12-22 02:23:46,714 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:46,714 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1911])
2024-12-22 02:23:46,789 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:23:46,799 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:47,311 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           mHandler.obtainMessage(MSG_SET_CAMERA, cameraId).sendToTarget();
        }
        public void setPreviewSurface(Surface previewSurface) {
            mProviderHandler.obtainMessage(MSG_SET_PREVIEW
 54%|█████▍    | 54/100 [05:07<04:12,  5.49s/it]2024-12-22 02:23:47,452 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:47,833 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:			this.objYLabel.Size = new System.Drawing.Size(200, 32);
			this.objYLabel.TabIndex = 13;
			this.objYLabel.Text = "Description...";
			//
 55%|█████▌    | 55/100 [05:08<04:19,  5.76s/it]2024-12-22 02:23:48,004 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:48,583 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:48,583 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1083])
2024-12-22 02:23:48,616 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:23:49,597 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:			super.addControl(factory.getControl(Menu.class, new Object[] { control, new Integer(startControlID.intValue() + 1000), new Integer(0), new Integer(0), new Integer(0), null, null, new Integer(0),
 56%|█████▌    | 56/100 [05:09<04:14,  5.79s/it]2024-12-22 02:23:49,735 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:50,098 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:50,098 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-22 02:23:50,171 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:23:50,300 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:50,300 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1533])
2024-12-22 02:23:50,359 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:23:51,180 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:51,180 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1849])
2024-12-22 02:23:51,242 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 02:23:51,244 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
results:				ExampleSet exampleSet, Node root) {
			super(exampleSet);
			this.root = root;
		}
		@Override
		public void train() throws OperatorException {
			// nothing to do
		
 53%|█████▎    | 53/100 [05:11<04:15,  5.44s/it]2024-12-22 02:23:51,383 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:52,508 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:52,508 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1569])
2024-12-22 02:23:52,564 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:23:52,735 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       modifier_json['use_object_transform'] = modifier.use_object_transform
        modifier_json['use_max_distance'] = modifier.use_max_distance
        modifier_json['use_poly_data'] = modifier.use_poly_
 59%|█████▉    | 59/100 [05:12<03:53,  5.70s/it]2024-12-22 02:23:52,828 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:53,056 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					from.SendLocalizedMessage(11566901); // You examine the wall map of Eodon.
				}
				else
				{
					from.SendLocalizedMessage(11
 55%|█████▌    | 55/100 [05:13<04:10,  5.56s/it]2024-12-22 02:23:53,153 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:53,774 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:53,774 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1462])
2024-12-22 02:23:53,821 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:23:54,006 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertEqual(set(c.files), set(["Twisted/web/woven/form.py",
                                   "Twisted/python/formmethod.py"]))
        self.assertEqual(c.comments,
                         "submit formmethod now subclass of
 56%|█████▌    | 56/100 [05:14<04:18,  5.89s/it]2024-12-22 02:23:54,126 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:55,134 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:55,134 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1058])
2024-12-22 02:23:55,174 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:23:55,299 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   proc = subprocess.Popen(npm_command, stderr=subprocess.PIPE, universal_newlines=True)
    retcode = proc.wait()
    if retcode == 0:
        # Success!
        print("Node prerequisites installed successfully!
 57%|█████▋    | 57/100 [05:15<04:07,  5.76s/it]2024-12-22 02:23:55,471 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:55,879 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:55,879 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1837])
2024-12-22 02:23:55,939 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:23:56,199 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:56,199 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1195])
2024-12-22 02:23:56,241 - [Process 2/5] - DEBUG - predict_token:tensor([[334]], device='cuda:2')
2024-12-22 02:23:56,546 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       public virtual SearchResults[] SearchSegments(SearchSettings settings, Segment[] segments)
        {
            return new SearchResults[0];
        }
        #endregion // Methods
    }
}
```

Please complete the code by implementing the `AddOrUpdateTranslation
 54%|█████▍    | 54/100 [05:16<04:08,  5.40s/it]2024-12-22 02:23:56,822 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:57,880 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   from invenio.models import Record, RecordMetadata
    from invenio.models import BibrecBib01x, BibrecBib02x, BibrecBib03x, ...
    from invenio.models import BibrecBib10x, Bib
 56%|█████▌    | 56/100 [05:18<03:55,  5.34s/it]2024-12-22 02:23:57,981 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:58,424 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       return INVALID_PEP_LEN;
    }
    private static int extractGene(String allele)
    {
        // A, B or C
        return allele.substring(0, 1);
    }
    private static String extractTwoDigitType
 60%|██████    | 60/100 [05:18<03:47,  5.70s/it]2024-12-22 02:23:58,480 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:58,708 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       if (termData == null) {
            termData = new ArrayListValuedHashMap<>();
            this.data.put(row.get(ID_KEY), termData);
        }
        termData.add(row.get(ID_KEY), csvData.get(ID_
 57%|█████▋    | 57/100 [05:18<03:57,  5.53s/it]2024-12-22 02:23:58,826 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:23:58,993 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:23:58,993 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:23:59,063 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:24:00,019 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:00,019 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1132])
2024-12-22 02:24:00,060 - [Process 1/5] - DEBUG - predict_token:tensor([[29879]], device='cuda:1')
2024-12-22 02:24:00,359 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:00,359 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1102])
2024-12-22 02:24:00,394 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:24:00,411 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:00,411 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2216])
2024-12-22 02:24:00,473 - [Process 0/5] - DEBUG - predict_token:tensor([[418]], device='cuda:0')
2024-12-22 02:24:01,160 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:01,160 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1326])
2024-12-22 02:24:01,207 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:24:01,824 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           Sub.create({'name': 'a'}))
        self.assertEqual(r.count, 3)
        r.m2o.remove(index=0)
        self.assertEqual(r.count, 2)
        r.m2o.remove(
 58%|█████▊    | 58/100 [05:22<04:11,  5.99s/it]2024-12-22 02:24:02,052 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:02,624 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       vertex.setCenter(3, 4));
        assertEquals(3, vertex.getCenter().getX());
        assertEquals(4, vertex.getCenter().getY());
    }
    @Test
    @DisplayName("setCenter() should throw an exception if the coordinates
 61%|██████    | 61/100 [05:22<03:24,  5.25s/it]2024-12-22 02:24:02,685 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:02,913 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:sessions', set()) if d.resource != resource])
        self.module.set(user, data)
        all_sessions = self.module.get('all_sessions', set())
        all_sessions = set([s for s in all_sessions if s.
 57%|█████▋    | 57/100 [05:23<03:45,  5.25s/it]2024-12-22 02:24:03,122 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:03,392 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:     if (offset < strlen) {
         char ch = string.charAt(offset);
         int value = Character.getType(ch);
         if (value == Character.COMBINING_SPACING_MARK
                 || value == Character.ENCLOSING
 55%|█████▌    | 55/100 [05:23<04:22,  5.84s/it]2024-12-22 02:24:03,546 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:03,797 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:			if(isPassiveFlower()) {
				// Add mana to the linked collector
				if(linkedCollector != null) {
					int manaToAdd = getMana();
					if
 58%|█████▊    | 58/100 [05:24<03:46,  5.40s/it]2024-12-22 02:24:03,891 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:04,772 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:04,772 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1173])
2024-12-22 02:24:04,813 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:24:05,675 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:05,675 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1789])
2024-12-22 02:24:05,680 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:05,681 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1015])
2024-12-22 02:24:05,718 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:24:05,757 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:24:06,513 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:06,514 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1865])
2024-12-22 02:24:06,569 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:24:06,572 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:06,573 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 02:24:06,638 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:24:07,089 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           self.fil_ = self.arg.filter
        else:
            self.fil_ = None
        if self.arg.num:
            self.num_ = self.arg.num
        else:
            self.num_ = None
        ###If --read###
 62%|██████▏   | 62/100 [05:27<03:10,  5.01s/it]2024-12-22 02:24:07,184 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:08,596 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           float belongingStrength = this.getBelongingStrength(component, c);
            intrinsicCohesion += belongingStrength * this.getRepresentativity(component, c);
        }
        return intrinsicCohesion;
    }
    private float get
 59%|█████▉    | 59/100 [05:28<03:33,  5.22s/it]2024-12-22 02:24:08,677 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   assert simplify(Sum(x, (x, 1, 3))) == Sum(x, (1, 2, 3))
    assert simplify(Sum(x, (x, 1, 3), (x, 2, 4))) == Sum(x, (
 59%|█████▉    | 59/100 [05:28<04:16,  6.25s/it]2024-12-22 02:24:08,748 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:08,803 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:09,515 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			// Get the largest accession number from the database
			String largestAccessionNumber = sampleDAO.getLargestAccessionNumber();
			if (largestAccessionNumber != null) {
				// Check if the largest accession number starts
 56%|█████▌    | 56/100 [05:29<04:20,  5.92s/it]2024-12-22 02:24:09,589 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   """
    # detect_encoding {{{
    try:
        # try to read the file's magic number
        with open(fpath, 'rb') as f:
            magic = f.read(4)
        if magic == b'MO\0':
            #
 58%|█████▊    | 58/100 [05:29<03:58,  5.68s/it]2024-12-22 02:24:09,661 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:09,709 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:10,387 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:10,387 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1779])
2024-12-22 02:24:10,452 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:24:11,249 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:11,249 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1369])
2024-12-22 02:24:11,299 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:24:11,509 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:11,509 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1527])
2024-12-22 02:24:11,563 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:24:11,756 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:11,757 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1243])
2024-12-22 02:24:11,798 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:24:12,210 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:12,210 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1398])
2024-12-22 02:24:12,266 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:24:12,958 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           this.rptComboBox.Location = new System.Drawing.Point(91, 161);
            this.rptComboBox.Name = "rptComboBox";
            this.rptComboBox.Size = new System.Drawing.Size(264, 21);
 63%|██████▎   | 63/100 [05:33<03:15,  5.27s/it]2024-12-22 02:24:13,069 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:14,141 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:						UnityEngine.WWWW self=(UnityEngine.WWWW)checkSelf(l);
					System.String a1;
					checkType(l,2,out a1);
				
 60%|██████    | 60/100 [05:34<04:00,  6.01s/it]2024-12-22 02:24:14,255 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:14,413 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   if are_equal:
        logger.info("No changes needed. Skipping upgrade.")
    else:
        do_upgrade()
        estimate()
        post_upgrade()
        logger.info("Upgrade completed.")

I have a problem with the code, I am
 60%|██████    | 60/100 [05:34<03:35,  5.40s/it]2024-12-22 02:24:14,632 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:14,634 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   for transDict in transDicts:
      transID = transDict['TransformationID']
      res = self.integrityClient.setTransformationStatus( transID, 'ValidatedOutput' )
      if not res['OK']:
        gLogger.error( "Failed to
 59%|█████▉    | 59/100 [05:34<03:45,  5.49s/it]2024-12-22 02:24:14,833 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:15,135 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:						SendOrderTo(newConn, "ServerError", message);
						DropClient(newConn);
						return;
					}
					if (handshake.Version !=
 57%|█████▋    | 57/100 [05:35<04:10,  5.83s/it]2024-12-22 02:24:15,294 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:16,336 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:16,336 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1151])
2024-12-22 02:24:16,379 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:24:16,599 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:16,600 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1976])
2024-12-22 02:24:16,673 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:24:18,186 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:18,186 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1736])
2024-12-22 02:24:18,267 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:24:18,353 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:18,354 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1981])
2024-12-22 02:24:18,425 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:24:18,427 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:18,427 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1719])
2024-12-22 02:24:18,495 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:24:18,963 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               this.InitializeAdd(action, changedItems, startingIndex);
            }
            else if (action == NotifyCollectionChangedAction.Remove)
            {
                this.InitializeRemove(action, changedItems, startingIndex);
            }
            else
            {
                throw
 61%|██████    | 61/100 [05:39<03:40,  5.66s/it]2024-12-22 02:24:19,163 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:19,238 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           'Meta': {'object_name': 'Scan'},
            'content': ('editorsnotes.main.fields.XHTMLField', [], {'null': 'True', 'blank': 'True'}),
            'created': ('django.db.models.fields.DateTimeField', [], {'auto
 64%|██████▍   | 64/100 [05:39<03:20,  5.57s/it]2024-12-22 02:24:19,347 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:21,038 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:			e.Cancel = true;
		}
		#endregion
	}
}
```

Please help me to complete this code by adding the necessary methods and properties to make it functional.

I have added some comments to explain what each section of the code does.
 61%|██████    | 61/100 [05:41<03:44,  5.77s/it]2024-12-22 02:24:21,181 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:21,369 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:						}
			// else {
				// allow links to plugins
				if(action.startsWith(PLUGINS_PREFIX)) {
					String plugin = action.substring(PLUGINS_PRE
 60%|██████    | 60/100 [05:41<03:54,  5.86s/it]2024-12-22 02:24:21,439 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:		for(int x = 0; x < beans.length; x++)
		{
			coll.add((CatsReferralPendingEmergencyNonEDAdmissionListVo)beans[x].buildVo());
		}
		
 58%|█████▊    | 58/100 [05:41<04:10,  5.97s/it]2024-12-22 02:24:21,564 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:21,567 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:22,697 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:22,697 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 02:24:22,766 - [Process 4/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:4')
2024-12-22 02:24:22,803 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:22,803 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 02:24:22,870 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:24:23,722 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:23,722 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1255])
2024-12-22 02:24:23,767 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:24:24,111 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:24,111 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1729])
2024-12-22 02:24:24,165 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:24:25,009 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:25,009 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2071])
2024-12-22 02:24:25,075 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:24:25,438 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                   }
                }
            }
        }
        }
        return returnList;
    }
    private void processSchema(Schema schema) {
        // Initialize the List of Types before we process the schema
        java.util.List<Type> returnList = new ArrayList<
 65%|██████▌   | 65/100 [05:45<03:21,  5.76s/it]2024-12-22 02:24:25,552 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:25,587 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               XMLMarshaller marshaller = createMarshaller();
                marshaller.setFormattedOutput(false);
                InputStream inputStream = new ByteArrayInputStream(xmlRoot.getObject().getBytes());
                try {
                    marshaller.marshal(xmlRoot
 62%|██████▏   | 62/100 [05:45<03:46,  5.95s/it]2024-12-22 02:24:25,741 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:26,473 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   xbmc.executebuiltin("Reboot")

Please help me to complete this code.

I have tried to understand the code but I am not able to understand the logic of the code.
Please help me to complete this code.

I have also tried to
 59%|█████▉    | 59/100 [05:46<03:53,  5.69s/it]2024-12-22 02:24:26,626 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:26,919 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   lastDayMonth.set(java.util.Calendar.SECOND, 0);
    lastDayMonth.set(java.util.Calendar.MILLISECOND, 0);
    lastDayMonth.add(java.util.Calendar.DATE, 1);

 62%|██████▏   | 62/100 [05:47<03:40,  5.80s/it]2024-12-22 02:24:27,026 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:27,913 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       gsp = yield self.setupGerritStatusPush(summaryCB=sampleSummaryCBDeferred)
        yield gsp.buildStarted(None, builds[0])
        yield gsp.buildComplete(None, builds[0])
        result = yield self.run_f
 61%|██████    | 61/100 [05:48<03:56,  6.07s/it]2024-12-22 02:24:28,052 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:28,470 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:28,471 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1548])
2024-12-22 02:24:28,526 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:24:29,059 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:29,059 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1908])
2024-12-22 02:24:29,136 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:24:29,142 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:29,142 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1037])
2024-12-22 02:24:29,188 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:24:29,352 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:29,352 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1569])
2024-12-22 02:24:29,408 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:24:30,797 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:30,798 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1549])
2024-12-22 02:24:30,855 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:24:31,338 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                   c.Campus = Token2Csv();
                    break;
                    case ">":
                        c.SetComparisonType(CompareType.Greater);
                        break;
                    case "<":
                        c.SetComparisonType(Comp
 63%|██████▎   | 63/100 [05:51<03:37,  5.89s/it]2024-12-22 02:24:31,554 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:31,552 - [Process 0/5] - INFO - res.shape is :torch.Size([51])
results:           int [] ret = new int[A.Dimensions.NumberOfDimensions]; 
            A.GetValueSeq(seqindex,ref ret); 
            return ret; 
        }
    }
}


 60%|██████    | 60/100 [05:51<03:40,  5.51s/it]2024-12-22 02:24:31,699 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       self.holidays_first_validate(cr, uid, ids, context=context)
        return True
    def holidays_first_validate_notificate(self, cr, uid, ids, context=None):
        obj_emp = self.pool.get
 66%|██████▌   | 66/100 [05:51<03:20,  5.91s/it]2024-12-22 02:24:31,708 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:31,816 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:31,983 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:		sourceEventBinding.bind( sourceComponent, sourceEventName, OnSourceEvent );
	}
	private void bindTargetEvent()
	{
		targetEventBinding = gameObject.AddComponent<dfEventBinding>();
		targetEventBinding.hideFlags = HideFlags.
 63%|██████▎   | 63/100 [05:52<03:26,  5.58s/it]2024-12-22 02:24:32,190 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:33,524 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   self.category = atom.Category()
    
  def testCategoryToAndFromString(self):
    self.category.term = 'http://example.com/category'
    self.assert_(self.category.term == 'http://example.com/category')
    new
 62%|██████▏   | 62/100 [05:53<03:45,  5.93s/it]2024-12-22 02:24:33,723 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:34,487 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:34,487 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1576])
2024-12-22 02:24:34,546 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:24:35,163 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:35,164 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1834])
2024-12-22 02:24:35,243 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:24:35,271 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:35,271 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2068])
2024-12-22 02:24:35,338 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:24:35,623 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:35,624 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2091])
2024-12-22 02:24:35,689 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:24:37,148 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   plotName = pD[ 'plotName' ]
    del( pD[ 'plotName' ] )
    if plotName:
      retVal = self.__queryForPlot( plotName )
      if not retVal[ 'OK' ]:
        return retVal
     
 61%|██████    | 61/100 [05:57<03:35,  5.53s/it]2024-12-22 02:24:37,171 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:37,171 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-22 02:24:37,237 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:24:37,271 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:37,902 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           if (businessObject == null)
            {
                return null;
            }
            var row = GridBaseManager.GetRowByValue(businessObject);
            if (row == null)
            {
                return null;
            }
            return row;
       
 67%|██████▋   | 67/100 [05:58<03:17,  6.00s/it]2024-12-22 02:24:37,966 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:38,141 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:					if (ide.planes == 1) {
						// read the palette
						ushort palSize = reader.ReadUInt16 ();
						ushort palEntries = reader.Read
 64%|██████▍   | 64/100 [05:58<03:41,  6.16s/it]2024-12-22 02:24:38,348 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:38,593 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                           resultBean.setStatus(true);
                            resultBean.setMessage("Added port to subnet gateway address successfully");
                            for (String port : toAdd) {
                                DaylightWebUtil.auditlog("Port", userName, "added",
                                
 64%|██████▍   | 64/100 [05:58<03:31,  5.89s/it]2024-12-22 02:24:38,871 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:39,803 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:39,803 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1282])
2024-12-22 02:24:39,857 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:24:39,972 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   // * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
 63%|██████▎   | 63/100 [06:00<03:45,  6.09s/it]2024-12-22 02:24:40,031 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:40,031 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1249])
2024-12-22 02:24:40,073 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:24:40,188 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:41,797 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:41,797 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 02:24:41,863 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:24:42,359 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                   // sources
                    case MixerLineComponentType.SourceUndefined:
                        return "Undefined Source";
                    case MixerLineComponentType.SourceDigital:
                        return "Digital Source";
                    case MixerLineComponentType.SourceLine:
                       
 68%|██████▊   | 68/100 [06:02<02:57,  5.54s/it]2024-12-22 02:24:42,409 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:				if (HasAdequateRefineryCount)
				{
					foreach (var building in Info.NewProductionCashThresholdBuildings)
						bot.AddRallyPoint(building, 1);
 62%|██████▏   | 62/100 [06:02<03:27,  5.45s/it]2024-12-22 02:24:42,448 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:42,449 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1819])
2024-12-22 02:24:42,476 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:42,517 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:42,528 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:24:43,634 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:43,635 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1894])
2024-12-22 02:24:43,709 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:24:44,695 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:44,695 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1293])
2024-12-22 02:24:44,735 - [Process 0/5] - DEBUG - predict_token:tensor([[268]], device='cuda:0')
2024-12-22 02:24:44,743 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       // Adds a forward reference to this label.
        Label l = new Label();
        l.status = RESOLVED;
        l.position = position;
        l.line = line;
        l.referenceCount = referenceCount;
        l.next = stack
 65%|██████▌   | 65/100 [06:05<03:40,  6.29s/it]2024-12-22 02:24:44,901 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:45,413 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:				super(id, path, width, height);
			}
		}
		public Images(ims.framework.Context context)
		{
			super(context);
		}
		public ims.framework.utils.
 65%|██████▌   | 65/100 [06:05<03:35,  6.17s/it]2024-12-22 02:24:45,490 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:46,097 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:46,098 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1835])
2024-12-22 02:24:46,179 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:24:46,577 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def draw(self, context):
        layout = self.layout
        mat = context.material  # don't use node material
        ob = context.object
        slot = context.material_slot
        space = context.space_data
        is_sortable = (
 64%|██████▍   | 64/100 [06:06<03:44,  6.24s/it]2024-12-22 02:24:46,779 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:46,998 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:46,998 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 860])
2024-12-22 02:24:47,029 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:24:47,526 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:    */
    public CompiledPlan compileAdHocPlan(String sql, DeterminismMode detModNext)
    {
        compile(sql, 0, null, null, true, false, detModNext);
        return m_currentPlan;
    }
    private
 63%|██████▎   | 63/100 [06:07<03:18,  5.35s/it]2024-12-22 02:24:47,680 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:47,904 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:47,904 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1533])
2024-12-22 02:24:47,968 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:24:48,745 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:			value10 = domainObject.getCareContext();
			if (value10 != null)
			{
				ims.core.admin.domain.objects.CareContext value10 = (ims.core.admin.domain.objects
 69%|██████▉   | 69/100 [06:09<02:59,  5.79s/it]2024-12-22 02:24:48,809 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:49,762 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           return (int) rnd.nextInt(size);
        }
    }
}

I need help with the code given above. I am not able to understand how to complete the code. Can someone please help me?

The code is for a class called OMEData, which
 66%|██████▌   | 66/100 [06:10<03:11,  5.62s/it]2024-12-22 02:24:49,957 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:50,391 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:50,391 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1852])
2024-12-22 02:24:50,472 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:24:50,620 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:50,621 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1596])
2024-12-22 02:24:50,686 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:24:50,774 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           ptr_of_this_method = ILIntepreter.Minus(__esp, 1);
            UnityEngine.Ray instance_of_this_method = (UnityEngine.Ray)typeof(UnityEngine.Ray).CheckCLRTypes(StackObject.To
 66%|██████▌   | 66/100 [06:11<03:31,  6.21s/it]2024-12-22 02:24:50,803 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:50,803 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1237])
2024-12-22 02:24:50,841 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:24:50,935 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:53,114 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   help='Target platform (e.g. "linux", "win32", "macosx")')
    def build(self, target):
        build_done = notify_build_done(time() - self.start_time)
        if build_done:
            print
 70%|███████   | 70/100 [06:13<02:40,  5.36s/it]2024-12-22 02:24:53,220 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:53,408 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           }
        }
    }
    public class Mapper01 : Mapper
    {
        public Mapper01(int address, int size)
        {
            base.Address = address;
            base.Size = size;
        }
        public override void Write
 65%|██████▌   | 65/100 [06:13<03:44,  6.42s/it]2024-12-22 02:24:53,515 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:53,573 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:53,573 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2113])
2024-12-22 02:24:53,615 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			// 
			// textBox2
			// 
			this.textBox2.Anchor = ((System.Windows.Forms.AnchorStyles.Bottom | System.Windows.Forms.AnchorStyles.Left) 
				| System
 64%|██████▍   | 64/100 [06:13<03:20,  5.57s/it]2024-12-22 02:24:53,639 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:24:53,764 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:54,253 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:54,253 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1929])
2024-12-22 02:24:54,317 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:24:55,395 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:55,395 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1163])
2024-12-22 02:24:55,428 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:24:56,511 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                                                                                                   la_obj.subject))
                            ok = False
                        else:
                            ok = self.replace_hierarchy(predicate, new_predicate)
                            if ok is False:
                                ok = self.replace_subject_
 67%|██████▋   | 67/100 [06:16<03:16,  5.96s/it]2024-12-22 02:24:56,733 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:56,751 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:56,751 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1937])
2024-12-22 02:24:56,758 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:24:56,758 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1763])
2024-12-22 02:24:56,816 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:24:56,824 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:24:57,184 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:																																																																
 67%|██████▋   | 67/100 [06:17<03:27,  6.27s/it]2024-12-22 02:24:57,391 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:58,105 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       m_jInspectorDialog.pack();
        m_jInspectorDialog.setVisible(true);
    }
    private void insertMenus() {
        JMenu jMnuFile = new JMenu("File");
        jMnuFile.add(getSave
 66%|██████▌   | 66/100 [06:18<03:20,  5.90s/it]2024-12-22 02:24:58,281 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:59,393 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:					double estN = counts[3];
				if (estP > 0) {
					return true; // indicates pruning
				}
				// else fall through to next line
			}

 71%|███████   | 71/100 [06:19<02:43,  5.64s/it]2024-12-22 02:24:59,451 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:24:59,587 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:						ReportIOError (null);
			}
			public override int WriteTimeout {
				get {
					return write_timeout;
				}
				set {
					
 65%|██████▌   | 65/100 [06:19<03:19,  5.69s/it]2024-12-22 02:24:59,763 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:00,161 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:00,161 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2108])
2024-12-22 02:25:00,227 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:25:00,924 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:00,925 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 02:25:00,994 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:25:01,400 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:01,400 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1038])
2024-12-22 02:25:01,438 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:25:01,602 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:01,602 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1949])
2024-12-22 02:25:01,665 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:25:03,097 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       public short RightBorderPaletteIdx
        {
            get{return _right_border_palette_idx
                .GetShortValue(field_7_palette_options);}
            set {
                field_7_palette_options =
                    _right_
 68%|██████▊   | 68/100 [06:23<03:16,  6.15s/it]2024-12-22 02:25:03,253 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:03,253 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1930])
2024-12-22 02:25:03,258 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:03,326 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:25:03,681 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:               var stash = Stashes.SelectedItem as GitStash;
                if (stash is not null)
                {
                    // ...
                }
            }
        }
        private void StashedSelectedIndexChanged(object sender, EventArgs e)
        {
           
 72%|███████▏  | 72/100 [06:23<02:26,  5.23s/it]2024-12-22 02:25:03,773 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:03,872 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       # driver.execute_script( "arguments[0].scrollIntoView(true);", stepIncrementText)
        # driver.find_element_by_xpath( "//div[@qxclass='skel.boundWidgets.Animator']/div/div[text()
 68%|██████▊   | 68/100 [06:24<03:24,  6.40s/it]2024-12-22 02:25:04,066 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:04,434 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:																																																																
 67%|██████▋   | 67/100 [06:24<03:18,  6.03s/it]2024-12-22 02:25:04,567 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:06,148 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       page = self.get_page(self.user.get_profile().get_url(), login_as=admin)
        self.assertEquals(page.context['cobrand'], cobrand)
    def test_user_with_cobrand_and_approved(
 66%|██████▌   | 66/100 [06:26<03:22,  5.95s/it]2024-12-22 02:25:06,217 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:06,217 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1586])
2024-12-22 02:25:06,249 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:06,282 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:25:06,968 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:06,968 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1787])
2024-12-22 02:25:07,013 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:07,013 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1350])
2024-12-22 02:25:07,032 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:25:07,063 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:25:07,514 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:07,514 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2112])
2024-12-22 02:25:07,580 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:25:08,259 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:08,259 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1141])
2024-12-22 02:25:08,299 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:25:09,045 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:				// Half-width kana.
				value = (ch - 0xFF60) * 2;
				value = ((int) (cjkToJis [value])) |
						(((int) (
 69%|██████▉   | 69/100 [06:29<03:08,  6.09s/it]2024-12-22 02:25:09,352 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:09,521 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   job.run_job()
    assert lattice.cbuffer.get_object(bm0_index).out_address == 0
    assert lattice.cbuffer.get_object(bm1_index).out_address == 0
    assert lattice.cbuffer.get_object
 73%|███████▎  | 73/100 [06:29<02:26,  5.42s/it]2024-12-22 02:25:09,583 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:09,789 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:				return base.SizeHeightToFit (min);
		}
	}
}
}

Please help me complete the code. I am new to Android development and I am trying to create a custom view that has a rounded rectangle with a border. I want to be able
 68%|██████▊   | 68/100 [06:30<03:06,  5.83s/it]2024-12-22 02:25:09,942 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:10,414 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       self.transport.send_message(m)
        self.transport._expect_packet(MSG_KEXGSS_GROUPREQ,
                                      MSG_KEXGSS_GROUP,
                                      MSG_KEXGSS_COMPLETE,

 69%|██████▉   | 69/100 [06:30<03:19,  6.44s/it]2024-12-22 02:25:10,523 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:11,028 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:r = subprocess.check_output(["python",
                   "fetchphotos.py",
                   "-c",
                   self.cfgfile,
                   os.path.join(self.tempdir,
                              u"src",
                              u"IMG
 67%|██████▋   | 67/100 [06:31<03:05,  5.63s/it]2024-12-22 02:25:11,203 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:11,575 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:11,575 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1087])
2024-12-22 02:25:11,618 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:25:12,593 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:12,593 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1217])
2024-12-22 02:25:12,636 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:25:12,707 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:12,708 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1565])
2024-12-22 02:25:12,764 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:25:12,772 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:12,773 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1899])
2024-12-22 02:25:12,846 - [Process 2/5] - DEBUG - predict_token:tensor([[29900]], device='cuda:2')
2024-12-22 02:25:13,893 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       if (this == o) {
            return true;
        }
        if (o instanceof AuthScope) {
            final AuthScope that = (AuthScope) o;
            return LangUtils.equals(this.host, that.host) &&
                   LangUtils.equals(
 74%|███████▍  | 74/100 [06:34<02:12,  5.10s/it]2024-12-22 02:25:14,003 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:14,598 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:14,598 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1871])
2024-12-22 02:25:14,671 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:25:15,195 - [Process 4/5] - INFO - res.shape is :torch.Size([57])
results:                   treeModel.setRoot(tree);
                }
            }
        }
    }
    private void removeNode(DefaultMutableTreeNode node, String title) {
        treeModel.removeNodeFromParent(node);
    }
}
 70%|███████   | 70/100 [06:35<02:58,  5.94s/it]2024-12-22 02:25:15,338 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:15,416 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       return self.is_published() and self.author == request.user
    can_see_preview.boolean = True
    can_see_preview.short_description = _('Can see preview')
    def get_absolute_url(self):
        return reverse('announ
 69%|██████▉   | 69/100 [06:35<02:58,  5.77s/it]2024-12-22 02:25:15,603 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:15,737 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   obj.operands = [env.A[b], env.cst(off2,8)]
    obj.type = type_data_processing
    if obj.mnemonic=="LD_A":
        obj.mode = "Long-offset"
    elif obj.m
 70%|███████   | 70/100 [06:36<03:08,  6.27s/it]2024-12-22 02:25:15,865 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:17,472 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:self.newAPList = sorted(self.newAPList, key=lambda x: x.essid)

Please help me complete the code.

I have tried to complete the code but I am not able to complete the last line of code.

Please help me to complete the code
 68%|██████▊   | 68/100 [06:37<03:07,  5.87s/it]2024-12-22 02:25:17,543 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:17,543 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 02:25:17,613 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:25:17,618 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:17,982 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:17,983 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1437])
2024-12-22 02:25:18,037 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:25:18,483 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:18,483 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1490])
2024-12-22 02:25:18,534 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:25:19,035 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:19,035 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1861])
2024-12-22 02:25:19,110 - [Process 1/5] - DEBUG - predict_token:tensor([[29937]], device='cuda:1')
2024-12-22 02:25:20,182 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       /// to use for encryption.
        /// </summary>
        private class StoreDialog : System.Windows.Forms.Form
        {
            private System.Windows.Forms.Button _okBtn;
            private System.Windows.Forms.Button _cancelBtn;
            private X50
 75%|███████▌  | 75/100 [06:40<02:16,  5.46s/it]2024-12-22 02:25:20,248 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:20,547 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:20,548 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1808])
2024-12-22 02:25:20,602 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:25:20,876 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           slip_lines = payslip.simulate_sheet(cr, uid, context=context)
            slip_line_pool.create(cr, uid, slip_lines, context=context)
            #slip_line_pool.write(cr, uid
 71%|███████   | 71/100 [06:41<02:50,  5.86s/it]2024-12-22 02:25:21,065 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:21,376 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:	 * is null.
	 * 
	 * @param propName the name of the property
	 * @return whether the map contains the property
	 */
	public boolean hasProperty(String propName)
	{
		return hasOwnProperty(propName) || base !=
 71%|███████   | 71/100 [06:41<02:56,  6.08s/it]2024-12-22 02:25:21,443 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:21,931 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   lexers.add_lexer('php', PhpLexer())

I have tried to complete the code by adding the missing lines of code, but I am not sure if I have done it correctly. Please let me know if there are any errors in my code or if there is anything else
 70%|███████   | 70/100 [06:42<02:59,  5.99s/it]2024-12-22 02:25:22,101 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:22,631 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:22,631 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1034])
2024-12-22 02:25:22,688 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:25:22,692 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:22,692 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 751])
2024-12-22 02:25:22,716 - [Process 2/5] - DEBUG - predict_token:tensor([[268]], device='cuda:2')
2024-12-22 02:25:23,383 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       }
    }
}

Please complete the code by writing the GetBars method and the GetChecksum method.

Note: The GetBars method should take a string as input and return an array of bytes representing the bars of the barcode.

Note: The GetChecksum
 69%|██████▉   | 69/100 [06:43<03:02,  5.89s/it]2024-12-22 02:25:23,477 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:24,591 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:24,592 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1942])
2024-12-22 02:25:24,664 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:25:25,007 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				}
			}
		}
		return s;
	}
}







































 76%|███████▌  | 76/100 [06:45<02:06,  5.27s/it]2024-12-22 02:25:25,078 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:25,137 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:25,137 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1723])
2024-12-22 02:25:25,202 - [Process 1/5] - DEBUG - predict_token:tensor([[29937]], device='cuda:1')
2024-12-22 02:25:25,255 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 02:25:25,257 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:25,257 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 999])
results:    * disabled.
     */
    public boolean getMapFeaturesEnabled()
    {
        return this.mapFeaturesEnabled;
    }
    public WorldType getTerrainType()
    {
        return this.terrainType;
    }
}

Please
 72%|███████▏  | 72/100 [06:45<02:31,  5.42s/it]2024-12-22 02:25:25,295 - [Process 0/5] - DEBUG - predict_token:tensor([[259]], device='cuda:0')
2024-12-22 02:25:25,402 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:27,434 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   cl1h, cl2h, cl = integrate_halo(ell, lnzarr, chiarr, dVdzdOm, marr, mf, BDarr, rhobarr, rho_crit_arr, bias, Darr, pk, zsarr
 72%|███████▏  | 72/100 [06:47<02:50,  6.07s/it]2024-12-22 02:25:27,454 - [Process 0/5] - INFO - res.shape is :torch.Size([47])
results:   return results;
  }
}

I need help in completing the code for the method getStringArray(Properties props, String key) which is incomplete.
Please provide the complete code for the method.
 70%|███████   | 70/100 [06:47<02:40,  5.34s/it]2024-12-22 02:25:27,533 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:27,556 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:27,556 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1357])
2024-12-22 02:25:27,609 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:25:27,626 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:28,070 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   time_return = time_add(row[4], row[5])
    print "Time added is: ", time_return
    #db.commit()
    # Phase 3.  This sums the flight durations for each of the flight groups
    # hence resulting in the
 71%|███████   | 71/100 [06:48<02:55,  6.04s/it]2024-12-22 02:25:28,121 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:28,122 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1582])
2024-12-22 02:25:28,176 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:25:28,225 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:29,442 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:29,442 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1173])
2024-12-22 02:25:29,477 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:25:29,967 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:								EType = (EClassifier)value;
						break;																			
					case "eGenericType" : 
			
 77%|███████▋  | 77/100 [06:50<01:59,  5.18s/it]2024-12-22 02:25:30,032 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:30,790 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:30,790 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1779])
2024-12-22 02:25:30,855 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:25:30,896 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:					CommonSecurityDescriptor csd = new CommonSecurityDescriptor (false, false, ControlFlags.None, null, null, null, null, null);
				csd.DiscretionaryAcl = new DiscretionaryAcl (true, true, 0
 73%|███████▎  | 73/100 [06:51<02:28,  5.49s/it]2024-12-22 02:25:31,041 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:31,254 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:31,254 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1694])
2024-12-22 02:25:31,319 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:25:32,110 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:32,111 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1187])
2024-12-22 02:25:32,153 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:25:32,158 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       setJustification(JUSTIFICATION_CENTER);
        setBorder(null);
        setMargin(new Insets(0, 0, 0, 0));
        setFont(new Font(Font.SANS_SERIF, Font.BOLD
 73%|███████▎  | 73/100 [06:52<02:33,  5.67s/it]2024-12-22 02:25:32,336 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:33,747 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       y_points = np.arange(-90, 90, 10)
        x_points, y_points = np.meshgrid(x_points, y_points)
        sample_points = [
            ("grid_longitude", x_points.
 71%|███████   | 71/100 [06:54<02:43,  5.63s/it]2024-12-22 02:25:33,806 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:33,806 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1506])
2024-12-22 02:25:33,836 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:33,860 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:25:34,203 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       def run_on_minion(v, *args, **kwds):
            if isinstance(v, str):
                v = "ssh " + minion_ip() + " '%s'"%v
            else:
                v = ['ssh', minion_ip()
 72%|███████▏  | 72/100 [06:54<02:49,  6.06s/it]2024-12-22 02:25:34,384 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:34,441 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   if args.input_file is not None:
        input_strm = get_input_strm(args)
        archive_index = load_archive_index(args.input_file)
        encrypt_archive(archive_index, args)
    else:
        log
 78%|███████▊  | 78/100 [06:54<01:49,  4.97s/it]2024-12-22 02:25:34,548 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:35,381 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:35,381 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 902])
2024-12-22 02:25:35,412 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:25:35,781 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:35,781 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2112])
2024-12-22 02:25:35,847 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:25:36,480 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   for i, result in enumerate(results):
        song = result.song
        # ...
        # ...
        # ...
        # ...
        # ...
        # ...
        # ...
        # ...
        # ...
        # ...
        # ...
        #
 74%|███████▍  | 74/100 [06:56<02:23,  5.52s/it]2024-12-22 02:25:36,711 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:37,886 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:37,887 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:25:37,903 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           return _packets[_index];
        }
        }
        private void toolStripButtonSave_Click(object sender, EventArgs e)
        {
            if (_newStyleLogViewer)
            {
                SavePackets();
            }
            else
            {

 72%|███████▏  | 72/100 [06:58<02:25,  5.19s/it]2024-12-22 02:25:37,952 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:25:38,080 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:38,080 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 02:25:38,090 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:38,150 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:25:38,628 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       #calculate the distance between the center point and the three known points
        dist_list=[]
        for i in range(3):
            dist_list.append(np.sqrt(np.sum((self.center_point-self.face[i,:])**2)))
 74%|███████▍  | 74/100 [06:58<02:33,  5.91s/it]2024-12-22 02:25:38,725 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:40,216 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:40,216 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:25:40,285 - [Process 2/5] - DEBUG - predict_token:tensor([[1769]], device='cuda:2')
2024-12-22 02:25:40,663 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:40,663 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1084])
2024-12-22 02:25:40,683 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					break;
			}
		}
	}
}

Please help me complete the code by adding the missing lines of code for each keyword.

Note: I have provided the keywords that the NPC will respond to, but you will need to add the
 73%|███████▎  | 73/100 [07:00<02:47,  6.19s/it]2024-12-22 02:25:40,700 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:25:40,718 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       # self.putx([10, ['CMD: %s' % self.cmd_str]])
        # self.putx([11, ['CMD token: 0x%02x' % self.cmd_token[0]]])
        # self.put
 79%|███████▉  | 79/100 [07:00<01:52,  5.36s/it]2024-12-22 02:25:40,837 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:40,871 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:41,486 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:41,487 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1887])
2024-12-22 02:25:41,560 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:25:43,169 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       }

    }
}
}
```

Please help me complete this code by filling in the missing parts.

I have provided the necessary information for the code to work, but I am not sure what the code is supposed to do.

Please let me know if you need any
 75%|███████▌  | 75/100 [07:03<02:26,  5.87s/it]2024-12-22 02:25:43,286 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:43,553 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:					window.IsVisible = !window.IsVisible;
		}
	}
}

I have written this code for a windows form application using c# and xaml. I am trying to connect to a rithmic server using the rithmic trader library.
 75%|███████▌  | 75/100 [07:03<02:20,  5.61s/it]2024-12-22 02:25:43,743 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:44,280 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:44,281 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1913])
2024-12-22 02:25:44,291 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   elif color == 'yellow':
        print '\033[1;32;40m'
    else:
        print '\033[1;30m'

def main():
    # slice_wavs_dirs('mir1k-W
 73%|███████▎  | 73/100 [07:04<02:29,  5.55s/it]2024-12-22 02:25:44,356 - [Process 3/5] - DEBUG - predict_token:tensor([[418]], device='cuda:3')
2024-12-22 02:25:44,372 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:44,416 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:44,416 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1924])
2024-12-22 02:25:44,490 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:25:45,482 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:45,482 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1299])
2024-12-22 02:25:45,522 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:25:45,867 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:45,868 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 832])
2024-12-22 02:25:45,899 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:25:46,926 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:     }
    }
    return true;
  }
  private static final int[] a_0 = {0, 1, 2, 3, 4, 5};
  private static final int[] a_1 = {6, 7, 8, 9
 80%|████████  | 80/100 [07:07<01:52,  5.61s/it]2024-12-22 02:25:46,990 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:47,187 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:47,188 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1887])
2024-12-22 02:25:47,262 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:25:47,324 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       self.codestr = 'VOTE_REVOKED'
        self.codehead = ()
        self.codetail = ()
        self.coderep = ()
        self.datalines = datalines
        self.parse()
        self.resolve
 74%|███████▍  | 74/100 [07:07<02:44,  6.32s/it]2024-12-22 02:25:47,451 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:48,103 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       fullAnswerBtn.setCompoundDrawablesWithIntrinsicBounds(null, new IconicsDrawable(getActivity(), GoogleMaterial.Icon.gmd_reply).sizeDp(20).color(secondaryColor), null, null);
        fullAnswerBtn.setCompoundDrawable
 76%|███████▌  | 76/100 [07:08<02:14,  5.59s/it]2024-12-22 02:25:48,258 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:48,615 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       public virtual void Remove(TK key)
        {
            if (ReferenceEquals(key, null)) {
                RemoveNull();
            } else {
                RemoveNonNull(key);
            }
        }
        public virtual void RemoveNonNull(TK key)
       
 74%|███████▍  | 74/100 [07:08<02:14,  5.18s/it]2024-12-22 02:25:48,803 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:49,069 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:49,069 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1227])
2024-12-22 02:25:49,111 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:25:49,698 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:49,698 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1267])
2024-12-22 02:25:49,747 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:25:50,053 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       for i in xrange(self.nbins(axis)):
            self.xedgesl(i) = self.xedgesl(i) + self.xedgesh(i) * i / self.nbins(axis)
        for i in xrange(self
 76%|███████▌  | 76/100 [07:10<02:21,  5.88s/it]2024-12-22 02:25:50,160 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:51,246 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:51,246 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1512])
2024-12-22 02:25:51,309 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:25:51,403 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:plugins)
        {
            plugins.add(new SpyPlugin(Next line of code:
                Next line of code:
                Next line of code:
                Next line of code:
                Next line of code:
                Next line of code:
                Next line of code:

 81%|████████  | 81/100 [07:11<01:40,  5.27s/it]2024-12-22 02:25:51,524 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:52,159 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:52,159 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1259])
2024-12-22 02:25:52,196 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:25:52,248 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def _build_test_case(self, task_data, host_data):
        """
        Build a TestCase from the given TaskData and HostData.
        """
        name = '[%s] %s: %s' % (host_data.name, task
 75%|███████▌  | 75/100 [07:12<02:27,  5.90s/it]2024-12-22 02:25:52,284 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:52,285 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1957])
2024-12-22 02:25:52,356 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:25:52,357 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:54,097 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           op_cc = np.dot(op_cc, self.symmetry.op_scc[s])
            op_cc = np.dot(op_cc, np.linalg.inv(self.symmetry.op_scc[s]))
            if
 77%|███████▋  | 77/100 [07:14<02:11,  5.71s/it]2024-12-22 02:25:54,287 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:54,403 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:54,403 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1243])
2024-12-22 02:25:54,444 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:25:54,961 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       if (contact.getLastMessageTransmitted() > timestamp) {
            timestamp = contact.getLastMessageTransmitted();
        }
        return timestamp;
    }
    private void queryReverse(XmppConnection connection, Contact contact, long start) {
        long
 77%|███████▋  | 77/100 [07:15<02:08,  5.59s/it]2024-12-22 02:25:55,053 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:55,054 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 02:25:55,079 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:55,124 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:25:55,141 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           this.DummySolutionExplorer.ImageIndex = 0;
            this.DummySolutionExplorer.Name = "DummySolutionExplorer";
            this.DummySolutionExplorer.SelectedImageIndex = 0;
            this.DummySolutionEx
 75%|███████▌  | 75/100 [07:15<02:19,  5.58s/it]2024-12-22 02:25:55,254 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:57,060 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           if (useUnshortenIt) {
                String unshortenedUrl = unshortenIt.unshorten(url);
                if (unshortenedUrl != null) {
                    // If the unshortened URL is not null, we have a valid redirect.
                   
 76%|███████▌  | 76/100 [07:17<02:13,  5.58s/it]2024-12-22 02:25:57,101 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:57,101 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1238])
2024-12-22 02:25:57,134 - [Process 0/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:0')
2024-12-22 02:25:57,219 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:57,326 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:57,326 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1256])
2024-12-22 02:25:57,375 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:25:57,695 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   result = [
        crypto_utils.pyopenssl_normalize_name(usage.strip()) for usage in to_text(extension, errors='surrogate_or_strict').split(',')
    ]
    return result
subject_alt_name_critical =
 82%|████████▏ | 82/100 [07:17<01:40,  5.58s/it]2024-12-22 02:25:57,774 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:25:57,774 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:25:57,777 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:25:57,840 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:25:59,629 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:               argTypes = [ (t,) for t in argTypes ]
        func = ctypesloader.load(
            dll,
            functionName,
            resultType,
            argTypes,
            doc,
            argNames,
            extension = extension,
            deprecated = deprecated
 76%|███████▌  | 76/100 [07:19<02:06,  5.25s/it]2024-12-22 02:25:59,811 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:00,213 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       im1L = im1.convert("L", h0)
        return self.tmpl_out("wait.html", s1=s1, s2=s2)
    def run(self, s1="0", s2="3.0"):
        """
 78%|███████▊  | 78/100 [07:20<02:00,  5.49s/it]2024-12-22 02:26:00,262 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:00,263 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1780])
2024-12-22 02:26:00,322 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:26:00,330 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:00,521 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:00,521 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1553])
2024-12-22 02:26:00,576 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:26:00,731 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       when(hsaEmployeeService.getHsaEmployee(anyString(), any())).thenReturn(new HsaEmployee(1L, "HsaEmployee", "hsaEmployee", "hsa@example.com", "Hsa", "Employee", "hsa", "employee", "h
 78%|███████▊  | 78/100 [07:20<02:11,  5.99s/it]2024-12-22 02:26:00,927 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:02,411 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:02,411 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1214])
2024-12-22 02:26:02,453 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:26:02,952 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       repodata_do_not_differ(primary, primary_zck, filelists_zck, other_zck)
        keys_do_not_differ(primary, filelists, other)
        # Checksums
        checksum_of_file(primary
 77%|███████▋  | 77/100 [07:23<02:10,  5.67s/it]2024-12-22 02:26:02,979 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   print("    f (ir%s.value);", file=f)
    print("    return;", file=f)
    print("}", file=f)
    print("\t}", file=f)
    print("}", file=f)
    print("", file=
 83%|████████▎ | 83/100 [07:23<01:33,  5.49s/it]2024-12-22 02:26:03,054 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:03,056 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:03,295 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:03,295 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 02:26:03,364 - [Process 0/5] - DEBUG - predict_token:tensor([[268]], device='cuda:0')
2024-12-22 02:26:03,860 - [Process 4/5] - INFO - res.shape is :torch.Size([36])
results:					tag.selectByID(DFI_MF);
		}

Please provide the complete code for the above line of code.
 79%|███████▉  | 79/100 [07:24<01:43,  4.93s/it]2024-12-22 02:26:03,997 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:04,520 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:04,520 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1850])
2024-12-22 02:26:04,600 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:26:05,098 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:05,099 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1145])
2024-12-22 02:26:05,139 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:26:05,616 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:05,617 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1294])
2024-12-22 02:26:05,672 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:26:06,175 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:    * we don't want to include that in the archive.
     *
     * @param baos the output stream to write to
     * @throws IOException on error
     */
            }
        }
    }
    private void writeFileEmptyFiles(final DataOutput header
 77%|███████▋  | 77/100 [07:26<02:09,  5.64s/it]2024-12-22 02:26:06,275 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:06,623 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:06,623 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1626])
2024-12-22 02:26:06,673 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:26:07,368 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:					foreach (ILNode child in block.Body) {
						if (child is ILBasicBlock) {
							ILBasicBlock childBlock = child as ILBasicBlock;
							if (prevChild
 79%|███████▉  | 79/100 [07:27<02:09,  6.18s/it]2024-12-22 02:26:07,572 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:07,789 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       # Build the Python modules.
        self.build_py()
        self.build_ext()
        # Build the package data.
        self.build_data()
        # Build the documentation.
        self.build_docs()
        # Build the test suite.
       
 78%|███████▊  | 78/100 [07:28<01:59,  5.42s/it]2024-12-22 02:26:08,015 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:08,042 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   # Split the interval into a range below and above 0xFFFF. This corresponds
    # to the basic idea of the function.
    interval_1word, intervals_2word = get_contigous_intervals(X)
    if interval_1word is not None:

 84%|████████▍ | 84/100 [07:28<01:25,  5.36s/it]2024-12-22 02:26:08,059 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:08,059 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1072])
2024-12-22 02:26:08,092 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:26:08,145 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:09,334 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:size", "Comment size"))
        yield RawBytes(self, "comment", self["comment_size"].value, "Comment data")
class ACE(FieldSet):
    TAG_INFO = {
        0: ("header", "ACE header", markerFlags, markerHeader, None),
 80%|████████  | 80/100 [07:29<01:41,  5.10s/it]2024-12-22 02:26:09,457 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:10,583 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:hints);
            g2.setColor(getBackground());
            g2.fillRect(0, 0, getWidth(), getHeight());
            for (int i = 0; i < ticker.length; i++)
            {
                Area tick = ticker[i
 78%|███████▊  | 78/100 [07:30<01:55,  5.27s/it]2024-12-22 02:26:10,760 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:11,080 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:11,080 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1972])
2024-12-22 02:26:11,152 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:26:11,641 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:11,641 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1749])
2024-12-22 02:26:11,723 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:26:11,786 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:11,786 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1778])
2024-12-22 02:26:11,836 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:11,836 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1283])
2024-12-22 02:26:11,868 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:26:11,883 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:26:13,385 - [Process 4/5] - INFO - res.shape is :torch.Size([33])
results:		// // tidy up the socket adapter
		socketAdapter.disconnect();
		socketAdapter = null;
	}
}
 81%|████████  | 81/100 [07:33<01:30,  4.78s/it]2024-12-22 02:26:13,578 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:13,977 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:	    if (ragdoll) {
		//do this regardless of network state
		//this will ensure a smooth transition even if the updates don't happen very often
		if (!IsRagdoll) {
		    mDistanceSinceLastFootstep = 0
 80%|████████  | 80/100 [07:34<02:06,  6.31s/it]2024-12-22 02:26:14,098 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:14,157 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:14,158 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1871])
2024-12-22 02:26:14,231 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:26:14,435 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				Expect(27);
			} else {
				SemErr("bad attribute");
			}
		}
	}
	void ExpectWeak(int n, int follow) {
		if (la.kind !=
 85%|████████▌ | 85/100 [07:34<01:25,  5.67s/it]2024-12-22 02:26:14,461 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:			for (int j = 0; j < disassembledValues.Length; j++)
			{
				object propValue = disassembledValues[j];
				if (propValue != null)
				{

 79%|███████▉  | 79/100 [07:34<02:01,  5.80s/it]2024-12-22 02:26:14,518 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:14,647 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:16,495 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:16,496 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1411])
2024-12-22 02:26:16,542 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:26:16,991 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           var item = Subject.GetItems().Single();
            item.CanBeRemoved.Should().BeTrue();
            item.CanMoveFiles.Should().BeTrue();
        }
        [Test]
        public void should_not_be_removable_if_max
 79%|███████▉  | 79/100 [07:37<01:57,  5.61s/it]2024-12-22 02:26:17,030 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:17,030 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1880])
2024-12-22 02:26:17,105 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:26:17,157 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:17,308 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:17,308 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1536])
2024-12-22 02:26:17,364 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:26:18,111 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:18,111 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1972])
2024-12-22 02:26:18,180 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:26:19,344 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.surface_ids = [ int ]
        """List of surface indices of the topology"""
        self.interface_ids = [ int ]
        """List of interface indices of the topology"""
        self.region_ids = [ int ]
        """List of region indices
 81%|████████  | 81/100 [07:39<01:54,  6.03s/it]2024-12-22 02:26:19,480 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:19,774 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       edi_doc_list.append(edi_doc)
        return edi_doc_list
    def edi_import(self, cr, uid, edi_document, context=None):
        """Overridden to provide sale order line fields with the expected names

 86%|████████▌ | 86/100 [07:40<01:17,  5.57s/it]2024-12-22 02:26:19,845 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:19,984 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       path_info = environ['PATH_INFO']
        for app, (prefix, handler) in self.apps:
            if path_info.startswith(prefix):
                break
        else:
            return []
        return handler(environ, start_response)




 82%|████████▏ | 82/100 [07:40<01:35,  5.33s/it]2024-12-22 02:26:20,159 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:20,443 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:20,444 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1924])
2024-12-22 02:26:20,507 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:26:20,914 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       # self.cursor.execute("UPDATE job SET job_state='running' WHERE job_id=%s", (r['job_id'],))
        # we can check the job state and avoid to start a job that is already running
        with db.connection(self):
            q =
 80%|████████  | 80/100 [07:41<01:59,  5.99s/it]2024-12-22 02:26:21,083 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:22,022 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:22,022 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1551])
2024-12-22 02:26:22,070 - [Process 2/5] - DEBUG - predict_token:tensor([[268]], device='cuda:2')
2024-12-22 02:26:22,235 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:22,235 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1338])
2024-12-22 02:26:22,284 - [Process 3/5] - DEBUG - predict_token:tensor([[268]], device='cuda:3')
2024-12-22 02:26:23,234 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       private static async Task OptimizedRecyclePotions(ISession session, CancellationToken cancellationToken)
        {
            var potionCount = await session.Inventory.GetItemAmountByType(ItemId.ItemPotion);
            var superPotionCount = await session
 80%|████████  | 80/100 [07:43<01:56,  5.80s/it]2024-12-22 02:26:23,419 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:23,683 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:23,684 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 02:26:23,716 - [Process 3/5] - INFO - res.shape is :torch.Size([39])
results:t.setHttpService(new HttpService(httpService));
}
}

I am unable to understand how to complete the code. Can someone please help me with this?
 87%|████████▋ | 87/100 [07:43<01:06,  5.08s/it]2024-12-22 02:26:23,752 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:26:23,826 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:24,120 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:24,120 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1808])
2024-12-22 02:26:24,179 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:26:24,716 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:    * DataLengthException if the input data is too long or too short.
     * @exception IllegalArgumentException if the IV is too short.
     */
    public void init(
        boolean     encrypting,
        CipherParameters params)
        throws IllegalArgumentException, Data
 82%|████████▏ | 82/100 [07:44<01:44,  5.83s/it]2024-12-22 02:26:24,839 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:26,555 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:			self.network = network
		self.topological = topo_order
		self.bicvalues = dict( [ ( field , {} ) for field in self.data.fields ] )
		self.entropyvalues = dict( [ ( field , {} )
 83%|████████▎ | 83/100 [07:46<01:36,  5.70s/it]2024-12-22 02:26:26,733 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:26,817 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       (int id)
        {
            var errors = new List<IModelError>();
            var model = new DelegateViewModel();
            var result = service.TrySave(model, errors);
            if (result)
            {
                service.AddEventDelegate(model.Id
 81%|████████  | 81/100 [07:47<01:53,  5.97s/it]2024-12-22 02:26:26,904 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:26,905 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 02:26:26,938 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:26,973 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:26:27,012 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:27,013 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1321])
2024-12-22 02:26:27,054 - [Process 2/5] - DEBUG - predict_token:tensor([[268]], device='cuda:2')
2024-12-22 02:26:27,280 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:27,280 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2112])
2024-12-22 02:26:27,347 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:26:29,312 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:29,312 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1377])
2024-12-22 02:26:29,359 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:26:29,521 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:    * the given {@link PatternNode} starts matching at the given {@link Node}.
     *
     * @param start the {@link PatternNode} to start matching at.
     * @param startNode the {@link Node} to start matching at.
     * @param objectVariables mapping
 83%|████████▎ | 83/100 [07:49<01:33,  5.52s/it]2024-12-22 02:26:29,664 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:29,728 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:
```

This code is a Python script that appears to be a GALFIT configuration generator. It takes in a set of input parameters and generates a configuration file for GALFIT based on those parameters. The script appears to be designed to handle a variety of different input parameters, including the following:
 81%|████████  | 81/100 [07:50<01:54,  6.01s/it]2024-12-22 02:26:29,904 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:29,914 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       assertEquals(AccessStatus.DENIED,
                     hasPermissionAs(filingFolderNodeRef,
                                     PermissionService.DELETE_CHILDREN,
                                     user1));
        assertEquals(AccessStatus.DENIED,
                    
 88%|████████▊ | 88/100 [07:50<01:05,  5.42s/it]2024-12-22 02:26:29,987 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:30,193 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:30,193 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2306])
2024-12-22 02:26:30,251 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:26:31,933 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           Builder.AppendLine("[Build]   : {0}", Build.BuildNumber));
            Builder.AppendLine("[Date]   : {0}", Build.BuildDate));
            Builder.AppendLine("--------------------------------------");
        }
        private static void GenerateGame
 82%|████████▏ | 82/100 [07:52<01:42,  5.71s/it]2024-12-22 02:26:32,039 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:32,403 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:32,404 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1343])
2024-12-22 02:26:32,456 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:26:32,504 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:32,504 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1685])
2024-12-22 02:26:32,559 - [Process 2/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:2')
2024-12-22 02:26:33,008 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                       res = EnhanceResult.Success;
                        break;
                    }
                case EnhanceResult.NoResources:
                    {
                        resMessage = "You don't have enough resources to enhance the item.";
                        break;
                   
 84%|████████▍ | 84/100 [07:53<01:34,  5.93s/it]2024-12-22 02:26:33,077 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:33,078 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1875])
2024-12-22 02:26:33,142 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:26:33,228 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:34,081 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:34,081 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1261])
2024-12-22 02:26:34,122 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:26:34,804 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def test_match_tag(self, context):
        enumerator = context.list_devices()
        funcname = 'udev_enumerate_add_match_tag'
        spec = lambda e, t: None
        with mock.patch.object(enumerator._
 89%|████████▉ | 89/100 [07:55<00:57,  5.26s/it]2024-12-22 02:26:34,853 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:35,191 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:               child_code, child_json = transform_node_xml_json_to_json(child_xml_json, root = False)
                child_json_by_code[child_code] = child_json
    if comments:
        node_json['comment'] = u
 84%|████████▍ | 84/100 [07:55<01:29,  5.57s/it]2024-12-22 02:26:35,308 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:35,982 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                   2);
                    i += ((b & 0x08) << 2);
                    i += ((b & 0x04) << 5);
                    i &= 0xFF;
                    break;
                case 0x08:
 82%|████████▏ | 82/100 [07:56<01:49,  6.08s/it]2024-12-22 02:26:36,095 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:36,474 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:36,474 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 891])
2024-12-22 02:26:36,507 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:26:36,759 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:36,760 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1953])
2024-12-22 02:26:36,832 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:26:36,898 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   captcha = ReCaptcha(self)
    captcha_challenge = re.search(self.CAPTCHA_KEY_PATTERN, self.html).group(1)
    res = captcha.challenge(captcha_challenge)
    self
 83%|████████▎ | 83/100 [07:57<01:33,  5.49s/it]2024-12-22 02:26:37,013 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:37,502 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:37,502 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1258])
2024-12-22 02:26:37,549 - [Process 2/5] - DEBUG - predict_token:tensor([[418]], device='cuda:2')
2024-12-22 02:26:38,141 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:38,141 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1151])
2024-12-22 02:26:38,183 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:26:38,693 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   app = Menu(device)
    device = Device("123456789012345678901234567890123456789012345678901
 90%|█████████ | 90/100 [07:58<00:48,  4.85s/it]2024-12-22 02:26:38,806 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:39,256 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:39,256 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1252])
2024-12-22 02:26:39,305 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:26:39,701 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           return comp.Addrmap(
                type_name,
                entry_name,
                entry_value,
                displayname,
                desc,
                self.ns,
                self.compiler
            )
        # If the addressBlock has a 'baseAddress' attribute,
 85%|████████▌ | 85/100 [07:59<01:32,  6.16s/it]2024-12-22 02:26:39,785 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:40,195 - [Process 2/5] - INFO - res.shape is :torch.Size([60])
results: }
}
}

I am getting an error in the line "btn.setSelected(true);" as it is giving a compile error saying that "cannot find symbol"
Please help me to resolve this issue.

Thank you,
Suresh.
 85%|████████▌ | 85/100 [08:00<01:20,  5.40s/it]2024-12-22 02:26:40,300 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:40,906 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       return self.studio_course_outline_as_json


I'm not sure what the code is doing, but it seems to be defining a fixture for a course and some of its properties, like `start_date` and `end_date`. It also defines a method `
 83%|████████▎ | 83/100 [08:01<01:37,  5.74s/it]2024-12-22 02:26:41,051 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:41,248 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:41,249 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 911])
2024-12-22 02:26:41,274 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:26:42,049 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					if (path != null && path.Count > 0)
				{
					// If we found a path, follow it:
					foreach (var p in path)
						QueueChild(self
 84%|████████▍ | 84/100 [08:02<01:26,  5.39s/it]2024-12-22 02:26:42,182 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:42,386 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:42,386 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1176])
2024-12-22 02:26:42,428 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:26:42,446 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:42,446 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1770])
2024-12-22 02:26:42,529 - [Process 3/5] - DEBUG - predict_token:tensor([[361]], device='cuda:3')
2024-12-22 02:26:43,598 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:43,598 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1388])
2024-12-22 02:26:43,654 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:26:43,959 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                   FOG_PF_DIR = c:\program files\fog
                    has not been set correctly. Please correct this and try again.
                    Press OK to continue", "Error", MessageBoxButtons.OK, MessageBoxIcon.Error);
                    this.Close();
               
 86%|████████▌ | 86/100 [08:04<01:18,  5.59s/it]2024-12-22 02:26:44,154 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:44,807 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:44,807 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1455])
2024-12-22 02:26:44,862 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:26:45,099 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   # Calculate the frequency configuration.
    freqconfig = determine_array(srcname, freq)
    # Calculate the time variation.
    timevar = timevariation_cal(freqconfig)
    # Calculate the uv index.
    uvindex =
 91%|█████████ | 91/100 [08:05<00:47,  5.32s/it]2024-12-22 02:26:45,115 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   socket = getSocket()
    try:
        # do something with the socket
    except SDSSError as e:
        print "Error:", e
        raise
    finally:
        socket.close()

I need help in completing the code by filling in the blanks
 86%|████████▌ | 86/100 [08:05<01:13,  5.25s/it]2024-12-22 02:26:45,147 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:45,261 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:46,408 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:		if (splitNumber.length == 2) {
			chance = Float.parseFloat(splitNumber[1]);
		}
		return (int) (chance * 100);
	}
}

Please help me complete this code by
 84%|████████▍ | 84/100 [08:06<01:30,  5.67s/it]2024-12-22 02:26:46,506 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:46,767 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:46,767 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 869])
2024-12-22 02:26:46,799 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:26:47,619 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def write_dhcp(self):
        self.logger.info("writing DHCP files")
        self.dhcp.write_dhcp_file()
        self.dhcp.regen_ethers()

I need help with the code above, I'm not
 85%|████████▌ | 85/100 [08:07<01:21,  5.44s/it]2024-12-22 02:26:47,662 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:47,662 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 02:26:47,700 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:47,728 - [Process 4/5] - DEBUG - predict_token:tensor([[15329]], device='cuda:4')
2024-12-22 02:26:48,175 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:48,175 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 973])
2024-12-22 02:26:48,208 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:26:48,264 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:48,264 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1670])
2024-12-22 02:26:48,327 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:26:48,616 - [Process 3/5] - INFO - res.shape is :torch.Size([53])
results:           cont.DropItem(from);
        }
    }
    }
}

Please help me complete this code. I am new to C# programming and I am having trouble understanding how to complete this code.

Thank you.
 92%|█████████▏| 92/100 [08:08<00:38,  4.78s/it]2024-12-22 02:26:48,732 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:49,222 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:49,222 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 917])
2024-12-22 02:26:49,250 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:26:50,444 - [Process 1/5] - INFO - res.shape is :torch.Size([26])
results:   }
}

I am unable to understand how to complete this code. Can someone please help me with this?
 86%|████████▌ | 86/100 [08:10<01:05,  4.66s/it]2024-12-22 02:26:50,610 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       MultiSelect")]
        [DefaultValue("The maximum length for this field is {maxLength}")]
        [Description("Error text to display if the maximum length validation fails (defaults to 'The maximum length for this field is {maxLength}').")]
        public virtual string MaxLengthErrorText
 87%|████████▋ | 87/100 [08:10<01:16,  5.91s/it]2024-12-22 02:26:50,649 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:50,811 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:50,894 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   if cert_type is None:
        cert_type = "key"
    else:
        cert_type = "cert"
    # Download the file
    try:
        response, info = fetch_url(module, "https://%s/%s/meters/%s
 85%|████████▌ | 85/100 [08:11<01:19,  5.31s/it]2024-12-22 02:26:51,081 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:51,185 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   move(out, Util.px2mm(offset.x, dpi), Util.px2mm(offset.y, dpi));
    for (int y = 0; y < p.getRasterHeight(); y += toolDiameterInPx/2)
   
 87%|████████▋ | 87/100 [08:11<01:11,  5.50s/it]2024-12-22 02:26:51,303 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:52,264 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:52,264 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 02:26:52,334 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:26:53,704 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:53,704 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1288])
2024-12-22 02:26:53,753 - [Process 2/5] - DEBUG - predict_token:tensor([[268]], device='cuda:2')
2024-12-22 02:26:54,393 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:54,394 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1990])
2024-12-22 02:26:54,430 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:54,430 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2447])
2024-12-22 02:26:54,466 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:26:54,486 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:26:54,533 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:54,533 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1906])
2024-12-22 02:26:54,608 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:26:54,906 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       }
        // DRS 20120315 - Added 2 - externalTuner
        // Count external tuners
        List<Tuner> externalTunerList = countTunersExternal(addDevice);
        for (Tuner tuner
 93%|█████████▎| 93/100 [08:15<00:36,  5.23s/it]2024-12-22 02:26:55,002 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:56,539 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:						db.update(Db.Table1.TABLE_NAME, new ContentValues(), "ID_GROUP = " + deleteId, null);
					
					//Update budget items
					db.update(Db.Table
 88%|████████▊ | 88/100 [08:16<01:05,  5.46s/it]2024-12-22 02:26:56,632 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:57,336 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       if(this.trianglePoint >= 0)
            throw new IllegalStateException("endTriangle called without beginTriangle in between");
        this.trianglePoint = -1;
        this.textureArray[this.currentTextureHash] = null;
        this.triangles
 88%|████████▊ | 88/100 [08:17<01:13,  6.15s/it]2024-12-22 02:26:57,427 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:                   numberStatistics = context.NumberStatistics;
                    ns1 = context.NumberStatistics;
                    cf = (uint)(2 * numberStatistics * (foundStateFrequency + 6));
                    sf = s0 = (ushort)(foundStateFrequency
 87%|████████▋ | 87/100 [08:17<01:09,  5.35s/it]2024-12-22 02:26:57,449 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:57,560 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           this.lblPrivacy.AutoSize = true;
            this.lblPrivacy.Location = new System.Drawing.Point(6, 9);
            this.lblPrivacy.Name = "lblPrivacy";
            this.lblPrivacy.Size =
 86%|████████▌ | 86/100 [08:17<01:20,  5.72s/it]2024-12-22 02:26:57,578 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:57,784 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:26:58,221 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:58,221 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1827])
2024-12-22 02:26:58,288 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:26:58,291 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:58,292 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 967])
2024-12-22 02:26:58,324 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:26:59,508 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:26:59,508 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1246])
2024-12-22 02:26:59,550 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:27:00,381 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:00,382 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1623])
2024-12-22 02:27:00,437 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:27:00,807 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   }
}
  private static boolean isLegacyUpdate(Context context) {
    return TextSecurePreferences.getAppMigrationVersion(context) < LEGACY_CANONICAL_VERSION;
  }
  private static boolean isUpdate(Context context) {

 94%|█████████▍| 94/100 [08:21<00:32,  5.43s/it]2024-12-22 02:27:00,896 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:01,008 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       None = 0,
        SignatureVerified = 1,
        SignatureVerificationFailed = 2,
        CertificateVerificationFailed = 3,
        CertificateChainVerificationFailed = 4,
        CertificateRevocationListVerificationFailed = 5
 89%|████████▉ | 89/100 [08:21<00:56,  5.16s/it]2024-12-22 02:27:01,213 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:01,283 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:01,283 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2053])
2024-12-22 02:27:01,351 - [Process 0/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:0')
2024-12-22 02:27:02,258 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   return ensure_valid_course_key(handle_5000(template_path='certificates/server-error.html', context={'error-info': 'Internal Server Error'}, test_func=lambda request: request.GET.get('preview', None)))
    """

 89%|████████▉ | 89/100 [08:22<01:03,  5.78s/it]2024-12-22 02:27:02,379 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:03,249 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           true, ' ', 1,
            delegate(DicomTag tag, ByteBuffer bb)
            {
                if (bb == null) return new DicomAttributeDS(tag);
                return new DicomAttributeDS(tag, bb);
            });
        /// <
 88%|████████▊ | 88/100 [08:23<01:05,  5.49s/it]2024-12-22 02:27:03,458 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:03,734 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:03,734 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1612])
2024-12-22 02:27:03,794 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:27:04,236 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def _exit(self, status):
        self.soslog.fatal("sosreport exited with status %d" % status)
        sys.exit(status)
    def _format_list(self, first_line, items, indent=False):
        lines
 87%|████████▋ | 87/100 [08:24<01:18,  6.00s/it]2024-12-22 02:27:04,358 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:04,595 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:04,596 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1395])
2024-12-22 02:27:04,638 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:27:04,728 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:04,728 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2051])
2024-12-22 02:27:04,797 - [Process 2/5] - DEBUG - predict_token:tensor([[29937]], device='cuda:2')
2024-12-22 02:27:06,240 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:     if (split.length != 4) {
        throw new RuntimeIOException("Bad line " + lineCount + " in " + mapping + ": expected 4 fields, got " + split.length);
      }
      Entry entry = new Entry(split[1], split[2], split
 95%|█████████▌| 95/100 [08:26<00:27,  5.43s/it]2024-12-22 02:27:06,329 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:06,696 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:06,697 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1383])
2024-12-22 02:27:06,743 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:27:06,979 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:06,980 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-22 02:27:07,052 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:27:07,530 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       private void EnqueueIdle(Driver driver)
        {
            lock (idlePool.SyncRoot)
            {
                idlePool.Enqueue(driver);
            }
        }
        private Driver CreateNewPooledConnection()
        {
            Driver driver = Driver
 90%|█████████ | 90/100 [08:27<00:56,  5.63s/it]2024-12-22 02:27:07,711 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:TIME_ZONE_FORMAT = '%Y-%m-%d %H:%M:%S'
DATE_FORMAT = '%d %B %Y'
DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S'
FUTURE_DATE_FORMAT
 90%|█████████ | 90/100 [08:27<00:56,  5.62s/it]2024-12-22 02:27:07,760 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:07,890 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:09,303 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:09,303 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1783])
2024-12-22 02:27:09,359 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:27:09,637 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   from MaKaC.webinterface.urlHandlers import UHAbstractAttachmentFileAccess
    from MaKaC.webinterface.urlHandlers import UHMaterialModification
    from MaKaC.webinterface.urlHandlers import UHFileAccess
    from Ma
 88%|████████▊ | 88/100 [08:29<01:09,  5.82s/it]2024-12-22 02:27:09,741 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:09,978 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					sessionValuesArray.Add(tFull[3]);
		}
		//now we have the list of sessions that have this test
		//now we have to check if the test is in the list of sessions
		//if it is, we can delete
 89%|████████▉ | 89/100 [08:30<01:04,  5.86s/it]2024-12-22 02:27:10,153 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:11,290 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:11,290 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1942])
2024-12-22 02:27:11,299 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:11,299 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1702])
2024-12-22 02:27:11,363 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:27:11,380 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:27:11,583 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:11,583 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1132])
2024-12-22 02:27:11,617 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:27:11,815 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   env['PATH'] for "includegraphics" keyword
    env['BSTINPUTS'] for "bibliography" keyword
    env['BIBINPUTS'] for "bibliographystyle" keyword
    env['TEXINPUTS'] for all other keywords
    """
    def
 96%|█████████▌| 96/100 [08:32<00:21,  5.48s/it]2024-12-22 02:27:11,933 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:13,775 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:13,775 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1859])
2024-12-22 02:27:13,855 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:27:14,142 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       noteLabel.setToolTipText("<html><i>Note: RECEIVED, QUEUED, or PENDING messages will be set to ERROR upon import.</i></html>");
        importButton.setToolTipText("<html>Import messages from the selected file/
 89%|████████▉ | 89/100 [08:34<00:59,  5.43s/it]2024-12-22 02:27:14,251 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:14,298 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       var = [var] if len(var) == 1 else var
    return var
def _capture_subarguments(params, name, *args):
    """Capture subarguments for a parameter.
    Args:
        params: dict
            Dictionary of parameters.

 91%|█████████ | 91/100 [08:34<00:53,  5.97s/it]2024-12-22 02:27:14,317 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:			if (queue.Any(pi => pi.Item == itemName && pi.RemainingCost > numberToCancel))
			{
				var toCancel = queue.FirstOrDefault(pi => pi.Item == itemName && pi.RemainingCost > number
 91%|█████████ | 91/100 [08:34<00:53,  5.92s/it]2024-12-22 02:27:14,446 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:14,510 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:15,393 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:15,393 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2082])
2024-12-22 02:27:15,460 - [Process 3/5] - DEBUG - predict_token:tensor([[418]], device='cuda:3')
2024-12-22 02:27:16,118 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:16,118 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1013])
2024-12-22 02:27:16,160 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:27:16,647 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       o = self.file(b'abcdefghij')
        insert_bytes(o, 4, 9)
        self.assertEquals(b'abcdefghij\x00\x00\x00\x00', self.read(o))
 90%|█████████ | 90/100 [08:36<01:01,  6.11s/it]2024-12-22 02:27:16,748 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:17,089 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:17,089 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1718])
2024-12-22 02:27:17,137 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:27:18,014 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:18,015 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:27:18,030 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self, *args, **kwargs):
        Box2d.__init__(self, *args, **kwargs)
    def expand_to_include(self, other):
        """
        Expand this envelope to include all points in the given other envelope.

 97%|█████████▋| 97/100 [08:38<00:17,  5.70s/it]2024-12-22 02:27:18,083 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:27:18,138 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:18,551 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:18,551 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1068])
2024-12-22 02:27:18,584 - [Process 1/5] - DEBUG - predict_token:tensor([[268]], device='cuda:1')
2024-12-22 02:27:18,758 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:						layer = (int)((Scriptable)value).get(LAYER, -1);
					} else {
						layer = (int)value;
					}
		    	}
    	
 90%|█████████ | 90/100 [08:39<00:51,  5.18s/it]2024-12-22 02:27:18,897 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:19,910 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:```

I'm not sure what you're asking, but it seems like you want to complete the code for the `StolenDignity` quest in the `com.l2scoria.gameserver.model.quest` module. The code you provided is a partial implementation of
 92%|█████████▏| 92/100 [08:40<00:46,  5.86s/it]2024-12-22 02:27:20,102 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:20,935 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       action = menu.addAction(QtGui.QAction(self.tr("Add Tag"), self))
        action.triggered.connect(self.callAddTag)
        menu.addSeparator()
        menu.addAction(action)
        menu.addSeparator()

 92%|█████████▏| 92/100 [08:41<00:49,  6.13s/it]2024-12-22 02:27:21,114 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results: var:ndk.lNext line of code:
  var:ndk.lNext line of code:
  var:ndk.lNext line of code:
  var:ndk.lNext line of code:
  var:ndk.lNext line of code:
 91%|█████████ | 91/100 [08:41<00:50,  5.61s/it]2024-12-22 02:27:21,121 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:21,217 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:21,262 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:21,262 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1464])
2024-12-22 02:27:21,308 - [Process 0/5] - DEBUG - predict_token:tensor([[268]], device='cuda:0')
2024-12-22 02:27:21,674 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:21,674 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1939])
2024-12-22 02:27:21,747 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 02:27:23,289 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:23,289 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1150])
2024-12-22 02:27:23,331 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:27:23,733 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:23,733 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2129])
2024-12-22 02:27:23,798 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:27:23,921 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:    * Session.getActiveSession().setPublishPermissions(new String[] { "read_stream" });
     * 
     * @param permissions the permissions to use
     */
    public void setPermissions(String[] permissions) {
        loginButtonProperties.setPermissions(
 91%|█████████ | 91/100 [08:44<00:46,  5.18s/it]2024-12-22 02:27:24,028 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:24,319 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   if self.want.interval != self.have.interval:
        raise F5ModuleError(
            "The interval must be the same for all monitors"
        )
    if self.want.timeout != self.have.timeout:
        raise F5ModuleError(
 98%|█████████▊| 98/100 [08:44<00:11,  5.87s/it]2024-12-22 02:27:24,389 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:24,548 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:24,548 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1894])
2024-12-22 02:27:24,622 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:27:25,922 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   add_result_set(results, 'results', 'startup', 'mocha', 'test', 'duration', first_repetition=1, last_repetition=5)
    add_result_set(results, 'results', 'startup', 'mocha
 92%|█████████▏| 92/100 [08:46<00:42,  5.37s/it]2024-12-22 02:27:26,057 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:26,057 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1121])
2024-12-22 02:27:26,097 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:27:26,122 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:26,671 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:			{
				throw;
			}
			return playTime;
		}
	}
}
}

I am trying to complete the code for the PlayTime class in C#. The code is given below:

public struct Play
 93%|█████████▎| 93/100 [08:46<00:42,  6.13s/it]2024-12-22 02:27:26,759 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:26,772 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:26,772 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1283])
2024-12-22 02:27:26,820 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:27:27,497 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       code = re_m4.sub(r'@(\w+)@', lambda x: self.env.get(x[1], x[0]), code)
        self.outputs.append(self.path.join('%.pc'))
        self.outputs.append(
 93%|█████████▎| 93/100 [08:47<00:43,  6.26s/it]2024-12-22 02:27:27,605 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:28,325 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:28,325 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 898])
2024-12-22 02:27:28,356 - [Process 4/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:4')
2024-12-22 02:27:28,582 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			for (Emotion emotion : emotions) {
			sum += emotion.strength;
		}
		for (Emotion emotion : emotions) {
			emotion.strength /= sum;
		}
	}
 92%|█████████▏| 92/100 [08:48<00:40,  5.02s/it]2024-12-22 02:27:28,768 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:29,143 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   method_copy_view = MethodCopyView(context={'object': method})
    method_add_view = MethodAddView()
    method_add_view.fill(method.inputs)
    method_add_view.read()
    method_add_view.fill
 99%|█████████▉| 99/100 [08:49<00:05,  5.56s/it]2024-12-22 02:27:29,200 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:29,641 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:29,641 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1231])
2024-12-22 02:27:29,648 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:29,648 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 02:27:29,683 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:27:29,719 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:27:31,024 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:31,024 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1085])
2024-12-22 02:27:31,035 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               + Offset + ", Divisor: " + Divisor + ", InputOffset: " + InputOffset + ", PointerType: " + PointerType + ", Normalize: " + Normalize;
            }
        }
        public void Dispose()
        {
            if
 94%|█████████▍| 94/100 [08:51<00:33,  5.60s/it]2024-12-22 02:27:31,059 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:27:31,206 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:32,256 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:32,256 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2036])
2024-12-22 02:27:32,324 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:27:32,363 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       result = self._planningExecutorClient.get_result()
        rospy.loginfo("Received the result from PlanExecutorServer:")
        print result
        self._result = result
        rospy.loginfo("Sending result to client ...")
        self.send
 94%|█████████▍| 94/100 [08:52<00:35,  5.84s/it]2024-12-22 02:27:32,443 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def test_gid(self):
        self.test_gidFromNumericString()
        self.test_gidFromGroupnameString()
        self.test_gidFromUsernameString()
        self.test_gidFromNumericString()
        self.
 93%|█████████▎| 93/100 [08:52<00:40,  5.72s/it]2024-12-22 02:27:32,501 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:32,547 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:33,288 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       if (key == null) {
            key = "default";
        }
        // check if the key is valid
        if (this._dbManager.containsKey(key)) {
            result = this._dbManager.get(key);
        } else {
            // if
100%|██████████| 100/100 [08:53<00:00,  5.13s/it]100%|██████████| 100/100 [08:53<00:00,  5.34s/it]
2024-12-22 02:27:34,249 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:34,250 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1818])
2024-12-22 02:27:34,308 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:27:34,523 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:34,523 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1057])
2024-12-22 02:27:34,564 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:27:35,112 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           'started_at': constants.SPLIT_TIMESTAMP1
        }
        ]
    def test_get_stage_by_name(self):
        """Test getting a stage by name."""
        # add a stage
        stage = Stage()
        stage
 93%|█████████▎| 93/100 [08:55<00:38,  5.48s/it]2024-12-22 02:27:35,270 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:35,271 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1627])
2024-12-22 02:27:35,282 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:35,327 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:27:37,121 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def add_genes(self, gene):
        """Will add a cobra.Gene object to the model, if gene.id is not in self.genes.
        gene: A :class:`~cobra.core.Gene` object
        """
       
 95%|█████████▌| 95/100 [08:57<00:28,  5.75s/it]2024-12-22 02:27:37,150 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:						}else if(bDrawCheck && bDrawCross){
						g.setColor(MetalLookAndFeel.getControlColor());
						g.fillRect(x, y, controlSize - 1,
 94%|█████████▍| 94/100 [08:57<00:32,  5.41s/it]2024-12-22 02:27:37,257 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:37,259 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:38,123 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:															- 4 * chemical[ 0 ][ idx ] + chemical[ 0 ][ idx + 1 ]
											+ 2 * ( chemical[ 0 ][ idx -
 95%|█████████▌| 95/100 [08:58<00:29,  5.82s/it]2024-12-22 02:27:38,240 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:38,451 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:38,451 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1920])
2024-12-22 02:27:38,514 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:27:39,385 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:39,385 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1040])
2024-12-22 02:27:39,430 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:27:39,675 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:39,676 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1267])
2024-12-22 02:27:39,734 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:27:40,444 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:40,444 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1403])
2024-12-22 02:27:40,485 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:27:41,366 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   UTMNorthing = (k0*N*(A+(1-T+C)*A*A*A/6 + (5-18*T+T*T+72*C-58*eccPrimeSquared)*A*A*A*A
 94%|█████████▍| 94/100 [09:01<00:34,  5.71s/it]2024-12-22 02:27:41,493 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:42,227 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   # Add more test cases here
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...
    # ...

 95%|█████████▌| 95/100 [09:02<00:26,  5.31s/it]2024-12-22 02:27:42,323 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:42,538 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:		for(int i = 0; i < beans.size(); i++)
		{
			ims.clinicaladmin.vo.beans.TumourGroupListVoBean bean = (ims.clinicaladmin.vo.beans.TumourGroup
 96%|█████████▌| 96/100 [09:02<00:22,  5.65s/it]2024-12-22 02:27:42,741 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:43,282 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                       throw JsonException.$(position, "Expecting array, but found " + tag);
                }
                state = S_NEED_ARRAY_ELEMENT;
                break;
            case JsonLexer.EVT_OBJ_START:
                if (
 96%|█████████▌| 96/100 [09:03<00:22,  5.62s/it]2024-12-22 02:27:43,386 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:43,884 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:43,884 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1350])
2024-12-22 02:27:43,932 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:27:44,129 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:44,130 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1086])
2024-12-22 02:27:44,163 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:27:45,438 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:45,438 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1173])
2024-12-22 02:27:45,479 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:27:46,377 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:46,377 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1776])
2024-12-22 02:27:46,458 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:27:46,829 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       for album in user.get_profile().albums.all():
            yield album
    else:
        user = request.user
        albums = user.get_profile().albums.all()
    return list_detail.object_list(
        request=request,
       
 95%|█████████▌| 95/100 [09:07<00:28,  5.63s/it]2024-12-22 02:27:47,070 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   public final void writeArray(ObjectArray array, Accessor accessor) {
        // ...
    }
}

Please complete the code by writing the remaining lines of code for the method 'writeArray' and the constructor 'OhmArrayLayout'

Note: The code you provide should
 96%|█████████▌| 96/100 [09:07<00:20,  5.17s/it]2024-12-22 02:27:47,097 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:47,187 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:48,206 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:					Param = qs.NewParameter();
				Param.DbType = DbType.Int32;
				Param.ParameterName = "@obj_id";
				Param.Size = 4;
				Param.
 97%|█████████▋| 97/100 [09:08<00:16,  5.41s/it]2024-12-22 02:27:48,396 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:49,313 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:					current.x=lines[selectedLine].x;
					current.y=lines[selectedLine].y;
					current.width=lines[selectedLine].width;
					current.height=lines[selectedLine
 97%|█████████▋| 97/100 [09:09<00:17,  5.99s/it]2024-12-22 02:27:49,432 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:49,432 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1294])
2024-12-22 02:27:49,461 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:49,474 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:27:50,688 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:50,688 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2181])
2024-12-22 02:27:50,752 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:27:51,901 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:51,901 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1967])
2024-12-22 02:27:51,972 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:27:52,176 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       public override void GetReferencedAssemblies(Configuration solutionConfiguration, Hashtable referencedAssemblies) {
            string assemblyFile = ResolveAssemblyReference();
            if (assemblyFile != null) {
                base.GetReferencedAssemblies(solutionConfiguration, referencedAss
 97%|█████████▋| 97/100 [09:12<00:15,  5.15s/it]2024-12-22 02:27:52,297 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:52,467 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:52,467 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1530])
2024-12-22 02:27:52,531 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:27:53,598 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def get_context(self):
        """See `LaunchpadFormView`."""
        context = super(ProductAddSeriesView, self).get_context()
        context.title = 'Add Series'
        return context
    def get_form_class(self):
       
 96%|█████████▌| 96/100 [09:13<00:23,  5.98s/it]2024-12-22 02:27:53,680 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:54,346 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:54,347 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1245])
2024-12-22 02:27:54,388 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:27:54,856 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:					throw new NotImplementedException();
		}
		public object Replace(object original, object target, ISessionImplementor session)
		{
			throw new NotImplementedException();
		}
	}
}
}
 98%|█████████▊| 98/100 [09:15<00:11,  5.78s/it]2024-12-22 02:27:55,048 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:55,337 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:55,337 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 813])
2024-12-22 02:27:55,375 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:27:55,397 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       getListView().setAnimation(set);
    }
	
	private void fillData() {
		mDbAdapter.open();
		
		String whereClause = "(" + WeaveColumns.IS_FOLDER + " = 0)";
		
 98%|█████████▊| 98/100 [09:15<00:12,  6.02s/it]2024-12-22 02:27:55,527 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:57,139 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   if input:
        return _input(string, *args, **kwargs)
    else:
        return input()





































 98%|█████████▊| 98/100 [09:17<00:10,  5.10s/it]2024-12-22 02:27:57,324 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:58,104 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:58,105 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1525])
2024-12-22 02:27:58,133 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:				form.Invalidate();
			}
		}
#endif
	}
}

Please complete the code by adding the missing methods and properties.

Note:

* In the code given above, the `KPTranslation` class has several properties
 97%|█████████▋| 97/100 [09:18<00:16,  5.54s/it]2024-12-22 02:27:58,151 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:27:58,221 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:27:58,650 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:58,650 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1860])
2024-12-22 02:27:58,730 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 02:27:59,973 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:27:59,973 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1031])
2024-12-22 02:28:00,004 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:28:00,814 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:28:00,814 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1903])
2024-12-22 02:28:00,889 - [Process 1/5] - DEBUG - predict_token:tensor([[418]], device='cuda:1')
2024-12-22 02:28:00,985 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       private void LoadDataOnDemand()
        {
            // Load Partner Interests, if not already loaded
            if (FMainDS.PPartnerInterest == null)
            {
                FMainDS.Tables.Add(new PartnerEditTDSPartnerInter
 99%|█████████▉| 99/100 [09:21<00:05,  5.89s/it]2024-12-22 02:28:01,088 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:28:01,460 - [Process 1/5] - INFO - res.shape is :torch.Size([12])
results:     }
    }
  }
}

 99%|█████████▉| 99/100 [09:21<00:04,  4.86s/it]2024-12-22 02:28:01,617 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   ce_ntp_auth = NtpAuth(argument_spec=argument_spec)
    ce_ntp_auth.module = module
    ce_ntp_auth.init_module()
    ce_ntp_auth.get_ntp_auth_exist_config
 99%|█████████▉| 99/100 [09:21<00:06,  6.08s/it]2024-12-22 02:28:01,631 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:28:01,892 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:28:02,594 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           int size = 0;
            for (int i = 0; i < value.length; i++)
            {
                size += Byte.SIZE;
            }
            return size;
        }
    }
    public void readFromFile(File file) throws InvalidData
 98%|█████████▊| 98/100 [09:22<00:10,  5.22s/it]2024-12-22 02:28:02,750 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:28:02,963 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:28:02,963 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1015])
2024-12-22 02:28:03,006 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:28:04,846 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:28:04,846 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1780])
2024-12-22 02:28:04,912 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:28:05,401 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:28:05,401 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2017])
2024-12-22 02:28:05,470 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:28:05,489 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:		this.predictScores(pssm, scoresSol);
	}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

100%|██████████| 100/100 [09:25<00:00,  5.47s/it]100%|██████████| 100/100 [09:25<00:00,  5.66s/it]
2024-12-22 02:28:05,902 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:28:05,903 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1756])
2024-12-22 02:28:05,965 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:28:07,746 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					str += "	" + s.name + " : " + s.GetType().ToString() + "\n";
				}
				return str;
			}
		}
	}
}






100%|██████████| 100/100 [09:28<00:00,  5.29s/it]100%|██████████| 100/100 [09:28<00:00,  5.68s/it]
2024-12-22 02:28:08,185 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       # Create a course with one item
        course = self.store.create_course(
            course_key,
            display_name='Course 1',
            display_name_with_user=None,
            category='Course 1',
            category_display_
100%|██████████| 100/100 [09:28<00:00,  6.22s/it]100%|██████████| 100/100 [09:28<00:00,  5.68s/it]
2024-12-22 02:28:08,788 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def define_post_processors(config):
        return [
            set_default_build_dir,
            fix_verbosity_hack,
            threads_as_int,
            test_threads_as_int,
            default_isa,
            default_
 99%|█████████▉| 99/100 [09:29<00:05,  5.51s/it]2024-12-22 02:28:08,851 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:28:10,031 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:28:10,032 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 649])
2024-12-22 02:28:10,055 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:28:12,008 - [Process 0/5] - INFO - res.shape is :torch.Size([51])
results:           return new object[0];
        }
        }
    }
}

I am trying to complete the code by adding the implementation for the GetCustomAttributes method.
Please help me with that.

Thank you.
100%|██████████| 100/100 [09:32<00:00,  4.82s/it]100%|██████████| 100/100 [09:32<00:00,  5.72s/it]
2024-12-22 02:28:12,029 - [Process 4/5] - DEBUG - datasets_name:lcc
2024-12-22 02:28:12,029 - [Process 2/5] - DEBUG - datasets_name:lcc
2024-12-22 02:28:12,029 - [Process 0/5] - DEBUG - datasets_name:lcc
2024-12-22 02:28:12,029 - [Process 1/5] - DEBUG - datasets_name:lcc
2024-12-22 02:28:12,029 - [Process 3/5] - DEBUG - datasets_name:lcc
Running evaluation for dataset: repobench-p
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:30:09,414 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 02:30:09,414 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 02:30:09,414 - [Process 3/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:30:09,424 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 02:30:09,425 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 02:30:09,425 - [Process 4/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:30:09,435 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 02:30:09,435 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 02:30:09,435 - [Process 2/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:30:09,436 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 02:30:09,436 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 02:30:09,436 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 02:30:09,436 - [Process 0/5] - INFO - output_max_len: 64
2024-12-22 02:30:09,436 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 02:30:09,436 - [Process 1/5] - INFO - output_max_len: 64
2024-12-22 02:30:09,488 - [Process 3/5] - INFO - Max Length is 18754
2024-12-22 02:30:09,489 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 02:30:09,489 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]2024-12-22 02:30:09,544 - [Process 4/5] - INFO - Max Length is 18754
2024-12-22 02:30:09,545 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 02:30:09,546 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]2024-12-22 02:30:09,551 - [Process 1/5] - INFO - Max Length is 18754
2024-12-22 02:30:09,551 - [Process 2/5] - INFO - Max Length is 18754
2024-12-22 02:30:09,552 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 02:30:09,552 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 02:30:09,553 - [Process 1/5] - INFO - get_predicted begin
2024-12-22 02:30:09,553 - [Process 2/5] - INFO - get_predicted begin
2024-12-22 02:30:09,554 - [Process 0/5] - INFO - Max Length is 18754
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]2024-12-22 02:30:09,554 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 02:30:09,555 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]2024-12-22 02:30:14,177 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:14,249 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:14,284 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:14,288 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:14,288 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:19,584 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:19,584 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2692])
2024-12-22 02:30:19,652 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:19,652 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2547])
2024-12-22 02:30:19,672 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:30:19,694 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:19,695 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2375])
2024-12-22 02:30:19,749 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:30:19,790 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:30:19,805 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:19,805 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2805])
2024-12-22 02:30:19,909 - [Process 4/5] - DEBUG - predict_token:tensor([[268]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:30:22,113 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:22,113 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3906])
2024-12-22 02:30:22,248 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:30:22,640 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:               //initMiPush();
                //Log.e(MYTAG, "使用MiPush推送");
                break;
            }
            default:
                Log.e(MYTAG, "不支持的推送类型");
                break;
  1%|          | 1/100 [00:13<21:36, 13.09s/it]2024-12-22 02:30:22,809 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:    */
    private QuerySetConfig parseQuerySetConfigTag(Element element) {
        Validate.notNull(element, "querySetConfig tag cannot be null");
        Set<String> querySets = Sets.newHashSet();
        NodeList nodeList = element.get
  1%|          | 1/100 [00:13<21:53, 13.26s/it]2024-12-22 02:30:22,865 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:22,873 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   private final transient JKademliaStorageEntry jke;
    private final transient JKademliaRoutingTable jrt;

    private final transient Timer timer;

    private final transient int k;

    private final transient int maxConcurrentMessages
  1%|          | 1/100 [00:13<21:58, 13.32s/it]2024-12-22 02:30:22,896 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   client['room'].send('pw', client['id'], puffleById[puffle.id], 27) if sendPacket else None #TODO: Play type

I'm not sure what the code is doing but it seems to be related to a puffle ad
  1%|          | 1/100 [00:13<22:01, 13.34s/it]2024-12-22 02:30:22,913 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:23,107 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:23,192 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:25,649 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:			if model_item.data.name == "Sketch":
				default_flags |= Qt.ItemIsEditable
			elif model_item.data.name == "Part":
				default_flags |= Qt.ItemIsEditable
  1%|          | 1/100 [00:16<26:40, 16.16s/it]2024-12-22 02:30:25,836 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:26,877 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:26,877 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2183])
2024-12-22 02:30:26,959 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:30:27,120 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:27,120 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2132])
2024-12-22 02:30:27,201 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:30:28,622 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:28,622 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3023])
2024-12-22 02:30:28,737 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:30:29,287 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:29,287 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3078])
2024-12-22 02:30:29,411 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:30:29,801 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   private final KadConfiguration config;

    private int connectAttempts = 0;

    public ConnectOperation(KadServer server, KademliaNode localNode, Node bootstrapNode, KadConfiguration config)
    {
        this.server = server;
        this
  2%|▏         | 2/100 [00:20<15:40,  9.60s/it]2024-12-22 02:30:30,008 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   image = runner.run(P, H, I, tile, **kwargs)














































  2%|▏         | 2/100 [00:20<15:48,  9.68s/it]2024-12-22 02:30:30,057 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:30,217 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:31,875 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def CheckIt(self,S,x):
        pass

    def FindAndUpdate(self):
        pass

    def GetUpdateSQL(self,table,where,clause):
        pass

    def ExeSQL(self,sql):
        pass


  2%|▏         | 2/100 [00:22<17:37, 10.80s/it]2024-12-22 02:30:32,047 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:32,047 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3412])
2024-12-22 02:30:32,130 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:32,173 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:30:32,438 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       // Load the default effects from the asset file
        // Load the default effects from the asset file
        AssetManager assetManager = activity.getAssets();
        InputStream is = assetManager.open("effects.txt");
        BufferedReader reader = new BufferedReader(new InputStream
  2%|▏         | 2/100 [00:22<18:08, 11.11s/it]2024-12-22 02:30:32,588 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:35,423 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       array.items = [object1, object2, object3]


        root_dto = RootDto()
        root_dto.versions = versions
        root_dto.categories = {}
        root_dto.methods = {}
        root_dto
  2%|▏         | 2/100 [00:25<20:15, 12.40s/it]2024-12-22 02:30:35,722 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:36,497 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:36,497 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3325])
2024-12-22 02:30:36,623 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:30:36,730 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:36,731 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2557])
2024-12-22 02:30:36,809 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:30:37,160 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:37,161 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4089])
2024-12-22 02:30:37,292 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:30:39,510 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   class MyCipher(Cipher):  # noqa: D101
        key_schedule = KeySchedule1
        encryption = MyFunction
        rounds = 1

        def __new__(cls):
            return super().__new__(cls, "
  3%|▎         | 3/100 [00:29<14:58,  9.27s/it]2024-12-22 02:30:39,652 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:39,652 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3849])
2024-12-22 02:30:39,689 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:39,802 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:30:39,865 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def compile_information_arguments(self) -> List[Tuple[Any, ...]]:
        """
        Generates arguments for the compile_information function.

        See Also
        --------
        :func:`~montreal_forced_aligner.alignment.multip
  3%|▎         | 3/100 [00:30<15:46,  9.76s/it]2024-12-22 02:30:40,140 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:40,885 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:40,886 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2899])
2024-12-22 02:30:40,975 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   return output


def main():
    cli(database='', host='localhost', port=5433, user='',
        prompt_passwd=False, password='', version=False, vclirc='~/.vclirc')



def test():

  3%|▎         | 3/100 [00:31<16:41, 10.32s/it]2024-12-22 02:30:40,989 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:30:41,166 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:43,316 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self, stances, robot, swing_height, cycle=False):
        super(MultiContactWalkingFSM, self).__init__()
        self.cur_phase = stances[0].label
        self.cur_stance = stances[
  3%|▎         | 3/100 [00:33<17:55, 11.09s/it]2024-12-22 02:30:43,564 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:44,049 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:         //TODO: D,  build the pipe reader
          PipeReader reader = buildPipeReader(catBytes, clientConfig, templateSource, dataSource);
          
          //TODO: E,  build the decoder
          FASTDecoder decoder = buildDecoder(
  3%|▎         | 3/100 [00:34<17:15, 10.68s/it]2024-12-22 02:30:44,268 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:45,314 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:45,314 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2910])
2024-12-22 02:30:45,436 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:30:45,964 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:45,964 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2591])
2024-12-22 02:30:46,062 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:30:46,801 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:46,801 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3571])
2024-12-22 02:30:46,937 - [Process 1/5] - DEBUG - predict_token:tensor([[2341]], device='cuda:1')
2024-12-22 02:30:47,602 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:47,602 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2324])
2024-12-22 02:30:47,684 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:30:48,403 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   public EffectManager(Activity activity) {
        mActivity = activity;
        mParameterListView = (ViewGroup) mActivity.findViewById(R.id.parameter_list);
        mParameterListAdapter = new EffectParameterListAdapter(mActivity, R.layout.parameter_list_item
  4%|▍         | 4/100 [00:38<14:35,  9.12s/it]2024-12-22 02:30:48,566 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:49,121 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   @app.route('/api/words/<token>')
    @as_json
    def words_api_route(token):
        return words_api(wordnik_api, token)



















  4%|▍         | 4/100 [00:39<15:08,  9.46s/it]2024-12-22 02:30:49,358 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:49,485 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:49,486 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2902])
2024-12-22 02:30:49,591 - [Process 3/5] - DEBUG - predict_token:tensor([[517]], device='cuda:3')
2024-12-22 02:30:50,285 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   platepar = alignPlatepar(config, platepar, calstars_time, calstars_coords, scale_update=True, show_plot=True)

I'm not sure what the code is doing, but it seems to be trying to align the platepar with
  4%|▍         | 4/100 [00:40<16:02, 10.02s/it]2024-12-22 02:30:50,418 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:50,652 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   class TestOperation(unittest.TestCase):
    def test_operation(self):
        for op in simple_op:
            self.assertIsInstance(op, Operation)

    def test_BvAnd(self):
        self.assertEqual(BvAnd(
  4%|▍         | 4/100 [00:41<15:22,  9.61s/it]2024-12-22 02:30:50,857 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:52,711 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results: to_map_and_back(DFO, DopplerPoly(DopplerBase.UNITS_DOPPLER, (1,)))

  ...
  to_map_and_back(MFO, Message(b'Hello, world!'))
 
  4%|▍         | 4/100 [00:43<15:48,  9.88s/it]2024-12-22 02:30:52,945 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:54,015 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:54,015 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2400])
2024-12-22 02:30:54,111 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:30:54,866 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:54,867 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2270])
2024-12-22 02:30:54,949 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:30:56,061 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:56,061 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4039])
2024-12-22 02:30:56,210 - [Process 0/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:0')
2024-12-22 02:30:56,406 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:56,407 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3103])
2024-12-22 02:30:56,523 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:30:57,152 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:	public TaskListByJQLActivity() {
		super();
	}

	@Override
	protected void onCreate(Bundle savedInstanceState) {
		super.onCreate(savedInstanceState);
		setContentView(R.layout.task_list_by_jql);

  5%|▌         | 5/100 [00:47<14:09,  8.95s/it]2024-12-22 02:30:57,359 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:57,955 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:				aOutput.writeInt16(SegmentMarker.APP0.CODE);
			aICCProfile(aJPEG.mICCProfile).encode(aOutput).log(aLog);
		}

		new DACSegment(
  5%|▌         | 5/100 [00:48<13:53,  8.78s/it]2024-12-22 02:30:58,210 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:58,763 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:30:58,763 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3030])
2024-12-22 02:30:58,880 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:30:59,527 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:               new EventCommandHandler<>(UpdateFrequencyChangeCommand.class, UpdateFrequencyChangeEvent::fromCommand, this::queueEvent));
    }

    private void queueEvent(Event event) {
        this.eventQueue.offer(event);
    }

    public void
  5%|▌         | 5/100 [00:49<15:34,  9.84s/it]2024-12-22 02:30:59,684 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:30:59,703 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       contentValues.put(JobStorage.COLUMN_END_MS, 120_000L);
        contentValues.put(JobStorage.COLUMN_BACKOFF_MS, 100L);
        contentValues.put(JobStorage.COLUMN_
  5%|▌         | 5/100 [00:50<15:31,  9.80s/it]2024-12-22 02:30:59,938 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:01,888 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:01,888 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2442])
2024-12-22 02:31:01,978 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:31:02,073 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       containsString("_source"));
    }

    @Test
    public void storeWithId() throws Exception {
        Collection<SourceRecord> records = SourceRecordHelper.loadGetRecordsResponse(Resources.asByteSource(Resources.getResource("responses/dab-records
  5%|▌         | 5/100 [00:52<15:20,  9.69s/it]2024-12-22 02:31:02,313 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:03,795 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:03,795 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2304])
2024-12-22 02:31:03,882 - [Process 0/5] - DEBUG - predict_token:tensor([[268]], device='cuda:0')
2024-12-22 02:31:04,644 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:04,644 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2642])
2024-12-22 02:31:04,735 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:31:04,889 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertIsNotNone(handle)

    def test_create_event_invalid_handle(self):
        handle = CreateEvent(bManualReset=False, bInitialState=False)
        self.assertRaises(WindowsAPIError, CloseHandle, handle)
  6%|▌         | 6/100 [00:55<13:22,  8.53s/it]2024-12-22 02:31:05,067 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:05,838 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:05,838 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4032])
2024-12-22 02:31:05,993 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:31:06,601 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   private LocalRateLimiter localRateLimiter;

    public TaskRunnerContainer(TaskProperties taskProperties, TaskFactory taskFactory, TaskAPI taskAPI, GlobalRateLimiter globalRateLimiter, LocalRateLimiter localRateLimiter) {
        this.taskFactory = taskFactory;

  6%|▌         | 6/100 [00:57<13:56,  8.90s/it]2024-12-22 02:31:06,705 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:07,685 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       // Get the list of users from the DAO
        users = UserDAO.getInstance().getAll();
        // Initialize the adapter
        mDelAdapter = new UserAdapter(getActivity(), users);
        // Set the adapter for the list view
        ListView listView = getListView();
  6%|▌         | 6/100 [00:58<14:23,  9.18s/it]2024-12-22 02:31:07,896 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:08,867 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:08,867 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3421])
2024-12-22 02:31:09,004 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:31:09,525 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           if (sl != null) {
                HibiscusExporter exporter = new HibiscusExporter(sl);
                exporter.export();
            }
        }
        }
    }

    private void getSelectedSammellast() {
        //
  6%|▌         | 6/100 [00:59<15:14,  9.73s/it]2024-12-22 02:31:09,747 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:09,933 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:09,934 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2714])
2024-12-22 02:31:10,030 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:31:11,406 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:11,406 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2843])
2024-12-22 02:31:11,495 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:31:12,273 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       }[self._pkalg in _c][1](self)

    def __init__(self):
        super(PubKeyV4, self).__init__()
        self.created = datetime.utcnow()
        self.pkalg = PubKeyAlgorithm.R
  6%|▌         | 6/100 [01:02<15:27,  9.87s/it]2024-12-22 02:31:12,544 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:13,016 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       this.deployments = new SpringDeployments(restTemplate, root, tasks);
        this.vms = new SpringVms(restTemplate, root, tasks);
        this.jobs = new SpringJobs(restTemplate, root, tasks);
    }

    @
  7%|▋         | 7/100 [01:03<13:01,  8.40s/it]2024-12-22 02:31:13,162 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:13,679 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:13,680 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3020])
2024-12-22 02:31:13,795 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:31:14,317 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			Set<Problem> problems = integrityCheck.check(args.path(), passphrase, args.checkFileIntegrity());
			if (!problems.isEmpty()) {
				printNoNewline(format("Integrity check failed: %s", problems));

  7%|▋         | 7/100 [01:04<13:11,  8.51s/it]2024-12-22 02:31:14,483 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:15,544 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:15,545 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3189])
2024-12-22 02:31:15,660 - [Process 4/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:4')
2024-12-22 02:31:16,737 - [Process 1/5] - INFO - res.shape is :torch.Size([60])
results:       model = Progress
        fields = ['indicator', 'target', 'area', 'code']


I have tried to implement the code according to the provided code but I am getting an error.

Please help me to resolve this issue.

Thanks in advance.
  7%|▋         | 7/100 [01:07<14:10,  9.14s/it]2024-12-22 02:31:16,929 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:18,829 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   from montreal_forced_aligner.alignment.pretrained import PretrainedAligner









































  7%|▋         | 7/100 [01:09<14:51,  9.59s/it]2024-12-22 02:31:18,883 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:18,884 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3074])
2024-12-22 02:31:18,992 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:31:19,008 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:19,384 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:19,384 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3517])
2024-12-22 02:31:19,522 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:31:20,151 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:20,151 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3151])
2024-12-22 02:31:20,266 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:31:22,108 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                   Collection<ParsedPath> parsedPaths = parserHolder.parse(roundEnv, annotation, originatingElements);
                    return parsedPaths != null ? parsedPaths : Collections.emptyList();
                })
                .collect(Collectors.toList());

       
  8%|▊         | 8/100 [01:12<13:13,  8.62s/it]2024-12-22 02:31:22,297 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:22,828 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:			JPEParserManager.getInstance().registerJPEParser(new XMLJPEParser());

		// Registers a GeoJSON writer to return the GeoJSON document
		GeoJSONWriter geoJSONWriter = new GeoJSONWriter();
		serviceManager.
  7%|▋         | 7/100 [01:13<15:38, 10.09s/it]2024-12-22 02:31:23,195 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:23,264 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           // notify the observer
            // notify the observer
            // notify the observer
            // notify the observer
            // notify the observer
            // notify the observer
            // notify the observer
            // notify the observer
            // notify the observer
            // notify the observer
            // notify the
  8%|▊         | 8/100 [01:13<13:15,  8.65s/it]2024-12-22 02:31:23,404 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:24,599 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:24,600 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4050])
2024-12-22 02:31:24,753 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:31:25,828 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:25,828 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3545])
2024-12-22 02:31:25,963 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:31:26,974 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:26,975 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2419])
2024-12-22 02:31:27,071 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:31:28,299 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           fab.setOnClickListener(new View.OnClickListener() {
                @Override
                public void onClick(View v) {
                    presenter.loadData();
                }
            });
        }
    }

    private AdapterView.OnItemClickListener itemClickListener(ProduceData
  8%|▊         | 8/100 [01:18<15:11,  9.91s/it]2024-12-22 02:31:28,490 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:29,408 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       LoreProperties properties = entityPlayer.getLoreProperties();
        properties.addLore(key);
        PacketSyncLore.updateLore(entityPlayer);

        entityPlayer.addChatComponentMessage(new ChatComponentText(String.format("Gave %
  8%|▊         | 8/100 [01:19<15:11,  9.90s/it]2024-12-22 02:31:29,643 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:30,130 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   protected ExecutorService executorService = Executors.newFixedThreadPool(crawlerConfig.getThreadCount());

    public Crawler(CrawlerConfig crawlerConfig) {
        this.crawlerConfig = crawlerConfig;
        this.pageFetcher = new
  9%|▉         | 9/100 [01:20<12:47,  8.43s/it]2024-12-22 02:31:30,372 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:31,643 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:31,643 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4165])
2024-12-22 02:31:31,810 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:31:32,181 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:32,181 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4283])
2024-12-22 02:31:32,370 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:31:34,213 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:34,213 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3119])
2024-12-22 02:31:34,328 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:31:34,555 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:34,555 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2627])
2024-12-22 02:31:34,654 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:31:35,654 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                   res = Schema.search(
                    context.put(
                    Schema.ENTITY_NAMES.get(context.get(Schema.ENTITY_NAME).toLowerCase()
                    .trim()
                    .toLowerCase()
                    .trim
  8%|▊         | 8/100 [01:26<16:48, 10.96s/it]2024-12-22 02:31:35,856 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:36,049 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:
































































  9%|▉         | 9/100 [01:26<15:04,  9.94s/it]2024-12-22 02:31:36,218 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:36,333 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:36,333 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3096])
2024-12-22 02:31:36,448 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:31:37,557 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           @Override
            protected void onSuccess(final Channel channel) {
                context.setConnectionState(new ConnectedConnectionState(channel, false);
                deferred.setSuccess(null);
            }

            @Override
            protected void onFailure(final Throwable cause
  9%|▉         | 9/100 [01:28<14:43,  9.71s/it]2024-12-22 02:31:37,717 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   public List<AccountSummary> getAccountSummaries(Long userId) {
        List<AccountSummary> summaries = getSummaries(userId);
        return summaries;
    }

    private List<AccountSummary> getSummaries(Long userId) {

  9%|▉         | 9/100 [01:28<14:15,  9.41s/it]2024-12-22 02:31:37,775 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:37,978 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:39,653 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           actionPopupHandler = PopupHandler.getPopupHandler();
            actionPopupGroup.add(new AddKeyAction(mongoDocumentOperations));
            actionPopupGroup.add(new AddValueAction(mongoDocumentOperations));
            actionPopupGroup.add(
 10%|█         | 10/100 [01:30<13:09,  8.77s/it]2024-12-22 02:31:39,919 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:41,078 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:41,078 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2916])
2024-12-22 02:31:41,183 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:31:41,850 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:41,850 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2066])
2024-12-22 02:31:41,934 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:31:43,252 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:43,252 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3194])
2024-12-22 02:31:43,356 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:31:44,299 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:44,299 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2350])
2024-12-22 02:31:44,322 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   url(r'^users/list/$', users_list, name='users_list'),
    url(r'^users/new/$', new_user, name='new_user'),
    url(r'^users/edit/(?P<user_id>[-\w]+
  9%|▉         | 9/100 [01:34<15:32, 10.25s/it]2024-12-22 02:31:44,392 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:31:44,488 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:44,489 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4190])
2024-12-22 02:31:44,651 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:31:44,674 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:44,791 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:	private static final Logger logger = LogUtil.getLogger(Util.class);

	private Util() {}

	public static Util getInstance() {
		return instance;
	}

	public static String format(long milliseconds) {
		return TimeUtil.format
 10%|█         | 10/100 [01:35<13:01,  8.69s/it]2024-12-22 02:31:45,041 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:46,536 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           new RegisterFragment().showProgressBar(true);
            new RegisterFragment().showRegistrationSuccessful(true);
        );
    }
}































 10%|█         | 10/100 [01:36<14:13,  9.48s/it]2024-12-22 02:31:46,755 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:47,305 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   private GuiButton saveButton;
    private GuiButton loadButton;

    public GuiSettingsChannel(Channel channel) {
        super(channel);
        this.channel = channel;
        this.channels = new GuiScrollingPanel();
        this.panel =
 11%|█         | 11/100 [01:37<12:30,  8.43s/it]2024-12-22 02:31:47,512 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:47,999 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def placeOriginate(self, route):
        # Check that we got necessary result from Radius
        if len(route) != 0:
            self.uaA.recvEvent(event)
            return
        else:
            self.uaA.recvEvent(
 10%|█         | 10/100 [01:38<15:50, 10.56s/it]2024-12-22 02:31:48,105 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:51,718 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:51,718 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3608])
2024-12-22 02:31:51,746 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:51,746 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2728])
2024-12-22 02:31:51,849 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:31:51,862 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:31:52,105 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:52,105 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3768])
2024-12-22 02:31:52,247 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:31:52,497 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:52,498 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2567])
2024-12-22 02:31:52,578 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:31:53,601 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:53,601 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3086])
2024-12-22 02:31:53,722 - [Process 2/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:2')
2024-12-22 02:31:55,036 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   private final SpatialOperator spatial;

    private final int k;
    private final int t;

    private final AtomicInteger syncChildren = new AtomicInteger(1);
    private final AtomicBoolean cancelled = new AtomicBoolean(false);

    private final Map
 11%|█         | 11/100 [01:45<13:37,  9.18s/it]2024-12-22 02:31:55,224 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:55,291 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self, *args, **kwargs):
        super(URI, self).__init__(*args, **kwargs)
        self.type_ = 'uri'


class FlagList(Signature):
Next line of code:
 
    def __init__(
 11%|█         | 11/100 [01:45<14:11,  9.56s/it]2024-12-22 02:31:55,357 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                           }
                        }
                    });
                    }
                    dialogFragment.show(getActivity().getFragmentManager(), "play_session_settings");
                }
            }
        }
    }

    private void startLearnActivity() {
        Lear
 10%|█         | 10/100 [01:45<15:44, 10.49s/it]2024-12-22 02:31:55,398 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:55,585 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:55,717 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   FadeOutUp(FadingOutUp.class),
    FadeOutLeft(FadingOutLeft.class),
    FadeOutRight(FadingOutRight.class),

    ZoomIn(ZoomIn.class),
    ZoomInDown(ZoomIn
 11%|█         | 11/100 [01:46<13:54,  9.37s/it]2024-12-22 02:31:56,119 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:56,980 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:               return new BtnQuickNotice();
            case ConstantStr.FUNC_SCREEN_OFF_CODE:
                return new BtnScreenOff();
            case ConstantStr.FUNC_CLEAR_NOTIFICATIONS_CODE:
                return new Btn
 12%|█▏        | 12/100 [01:47<12:55,  8.81s/it]2024-12-22 02:31:57,172 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:31:59,252 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:31:59,252 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2262])
2024-12-22 02:31:59,334 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:32:00,340 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:00,341 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2439])
2024-12-22 02:32:00,436 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:32:01,357 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:01,357 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3256])
2024-12-22 02:32:01,473 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:32:01,996 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:01,996 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3303])
2024-12-22 02:32:02,105 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:32:02,295 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   private List<String>[] data;
    private List<FilterType> filterTypes;
    private List<String>[] checkedItems;
    private int menuCount;

    public DropMenuAdapter(Context context, OnFilterDoneListener onFilterDoneListener, String[] titles, List<
 12%|█▏        | 12/100 [01:52<12:36,  8.60s/it]2024-12-22 02:32:02,573 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:03,440 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   private void initData() {
        // 请求数据
        OkhttpManager.getCookBookManager().getCookBookList(new Request.Builder().url(FilterUrl.getFilterUrl(getApplicationContext(), "mentality_health")).build(), new OkhttpManager.
 11%|█         | 11/100 [01:53<14:27,  9.75s/it]2024-12-22 02:32:03,734 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:03,734 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3638])
2024-12-22 02:32:03,801 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:03,859 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:32:04,491 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   if cstr_node.kind == Expr.CONSTANT:
      cstr_node.data = convert_ast_constraint(ast_node)
    else:
      cstr_node.data = ast_node.data

    self.tree = cstr_node
 12%|█▏        | 12/100 [01:54<13:51,  9.45s/it]2024-12-22 02:32:04,625 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:05,368 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       problem = SearchSkCh(ch, der_mode=der_mode, search_mode=search_mode)

    if check:
        problem = problem.check()

    if solver_name is not None:
        problem = problem.solve(solver_name
 12%|█▏        | 12/100 [01:55<13:52,  9.46s/it]2024-12-22 02:32:05,566 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:07,199 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       Composite webModuleComposite = toolkit.createComposite(form.getForm());
        GridData gridData = new GridData(SWT.FILL, SWT.FILL, true, true);
        webModuleComposite.setLayoutData(gridData
 13%|█▎        | 13/100 [01:57<13:23,  9.23s/it]2024-12-22 02:32:07,355 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:09,437 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:09,438 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2080])
2024-12-22 02:32:09,519 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:32:09,647 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:09,647 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3010])
2024-12-22 02:32:09,765 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:32:10,299 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:10,299 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3844])
2024-12-22 02:32:10,455 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 02:32:10,481 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:10,482 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3101])
2024-12-22 02:32:10,596 - [Process 0/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:0')
2024-12-22 02:32:11,061 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:11,061 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 02:32:11,136 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:32:12,407 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def test_hosts_container(self):
        hc = HostsContainer(hosts=self.get_hosts())
        self.assertEqual(hc.roles, ['localhost1', 'localhost2', 'localhost3', 'localhost4'])
        self.assertEqual(h
 13%|█▎        | 13/100 [02:02<12:39,  8.72s/it]2024-12-22 02:32:12,627 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:13,038 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       #obj.updateBuffImg()























































 12%|█▏        | 12/100 [02:03<14:14,  9.71s/it]2024-12-22 02:32:13,280 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:13,570 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:               Uri uri = Uri.parse(url);
                intent.setData(uri);
                activity.startActivity(intent);
            }
        }
    }

    private static boolean useInternPlayer(TDActivity activity) {
        return activity != null && activity.getApplicationContext
 13%|█▎        | 13/100 [02:04<13:32,  9.34s/it]2024-12-22 02:32:13,679 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:14,004 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       type = Type(name="my_type", format="json")
        type.get_sample()


    def test_type_get_sample_return_default_sample(self):
        type = Type(name="my_type", format="json")
        type.
 14%|█▍        | 14/100 [02:04<12:11,  8.50s/it]2024-12-22 02:32:14,014 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:

def test_parameters_msgtype1():
  '''
  All One message test
  '''
  parser = prepareArgsParser()
  params = [
      '--gps-sv', '1',
      '--message-type', 'one']
  args = parser
 13%|█▎        | 13/100 [02:04<13:50,  9.54s/it]2024-12-22 02:32:14,212 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:14,277 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:18,861 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:18,862 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3431])
2024-12-22 02:32:18,988 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:32:19,584 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:19,585 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3347])
2024-12-22 02:32:19,712 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:32:20,850 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:20,850 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3317])
2024-12-22 02:32:20,987 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:32:21,101 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:21,101 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3904])
2024-12-22 02:32:21,251 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:32:22,248 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           return command.startswith("change") or command.startswith("add")

        return [
            command for command in self.tc_command_output.commands if tc_command_filter(command)
        ]

    def get_shaper(self):
        return
 14%|█▍        | 14/100 [02:12<12:59,  9.06s/it]2024-12-22 02:32:22,507 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:22,807 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:22,807 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4432])
2024-12-22 02:32:22,976 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:32:23,055 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def test_derivative_BvAdd(self):
        self.assertEqual(XDA(BvAdd(Constant(0, 4), Constant(0, 4))).derivative(XorDiff(Constant(0, 4))), XDA(
 13%|█▎        | 13/100 [02:13<14:12,  9.80s/it]2024-12-22 02:32:23,240 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:24,347 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def _render_stroke(self):
        stroke = self.style.stroke
        stroke_width = self.style.stroke_width

        is_miter = self.style.stroke_linejoin == 'miter'

        miter_limit = self.style.
 14%|█▍        | 14/100 [02:14<14:01,  9.78s/it]2024-12-22 02:32:24,514 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       PostLocationDataBody postLocationDataBody = new PostLocationDataBody(AppSettings.sUserLogin, latitude, longitude,
                AppSettings.sFindPeopleMessage, AppSettings.sEmail, AppSettings.sName, AppSettings.sAvatarUrl,
                AppSettings.
 14%|█▍        | 14/100 [02:14<14:04,  9.82s/it]2024-12-22 02:32:24,616 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:24,711 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:26,817 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           }
            }
            }
            }
            // AddTag(Event.
            // AddTag(Event.
            // AddTag(Event.
            // AddTag(Event.
            // AddTag(Event.
            // AddTag(Event.
            // AddTag
 15%|█▌        | 15/100 [02:17<13:53,  9.80s/it]2024-12-22 02:32:27,000 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:28,118 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:28,119 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2561])
2024-12-22 02:32:28,218 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:32:28,956 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:28,956 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2623])
2024-12-22 02:32:29,029 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:32:30,679 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:30,679 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3109])
2024-12-22 02:32:30,807 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:32:30,906 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:30,906 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4005])
2024-12-22 02:32:31,088 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 02:32:31,178 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:			public static Node getTransform(Node node, NodeTransformer transformer) {
			if(node == null) {
				return null;
			}
			if(node instanceof ConstantNode) {
				return transformer.
 14%|█▍        | 14/100 [02:21<13:19,  9.29s/it]2024-12-22 02:32:31,456 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:31,729 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertRaises(TypeError, InterLayerPipeline, *[1, 2, 3])

        self.assertRaises(TypeError, InterLayerPipeline, *['a', 'b', 'c'])

        self.assertRaises(
 15%|█▌        | 15/100 [02:22<12:48,  9.04s/it]2024-12-22 02:32:31,841 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:32,133 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:32,134 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2827])
2024-12-22 02:32:32,237 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:32:34,016 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					try {
						a = IconPackManager.getApp(data);
						if (a != null) {
							apps.add(a);
							needSave =
 15%|█▌        | 15/100 [02:24<13:48,  9.75s/it]2024-12-22 02:32:34,270 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:35,106 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


GPS


























 15%|█▌        | 15/100 [02:25<14:27, 10.21s/it]2024-12-22 02:32:35,323 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:35,444 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.set_cluster_type( _cluster_type )
        return self

    def set_cluster_type(self, _cluster_type):
        self._cluster_type = _cluster_type
        return self

    def set_cluster_location(self, _
 16%|█▌        | 16/100 [02:25<13:13,  9.45s/it]2024-12-22 02:32:35,628 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:36,299 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:36,300 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2624])
2024-12-22 02:32:36,383 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:32:36,682 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:36,682 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2637])
2024-12-22 02:32:36,791 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:32:39,133 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   class MyModel(BaseModel):
        id = Column(Integer, hash_key=True)
        name = Column(String)

    engine.bind(MyModel)

    obj = MyModel(id=1, name="hello")
    assert obj.id == 
 16%|█▌        | 16/100 [02:29<11:57,  8.55s/it]2024-12-22 02:32:39,294 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:39,780 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:		// Register the commands
		getCommand("ancientgates").setExecutor(new CommandExecutor() {
			@Override
			public boolean onCommand(CommandSender sender, Command cmd, String label, String[] args) {
				BaseCommand command =
 15%|█▌        | 15/100 [02:30<12:52,  9.08s/it]2024-12-22 02:32:39,985 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:41,980 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:41,980 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4074])
2024-12-22 02:32:42,134 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:32:42,728 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:42,729 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3710])
2024-12-22 02:32:42,872 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:32:44,992 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:44,992 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3174])
2024-12-22 02:32:45,108 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:32:45,390 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:45,390 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4993])
2024-12-22 02:32:45,589 - [Process 4/5] - DEBUG - predict_token:tensor([[403]], device='cuda:4')
2024-12-22 02:32:45,705 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:               if (isOpsNeed2Add) {
                    // do something
                }

                if (isPlayNeed2Add) {
                    // do something
                }

                if (isAppSetNeed2Add) {
                    // do something

 16%|█▌        | 16/100 [02:36<14:27, 10.33s/it]2024-12-22 02:32:45,891 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:46,275 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   query = {
        'api_key': settings.FLICKR_KEY[0],
        'perms': 'write',
        'frob': frob,
    }
    sign_flickr_query(query)
    url = urlunparse(('
 17%|█▋        | 17/100 [02:36<13:38,  9.86s/it]2024-12-22 02:32:46,517 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:46,800 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:46,800 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3397])
2024-12-22 02:32:46,947 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:32:48,114 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def evaluate_union_operator(self, context=None):
    ...
    def select_simple_map_operator(self, context=None):
    ...
    def evaluate_parenthesized_expression(self, context=None):
    ...
    def nud_parent
 17%|█▋        | 17/100 [02:38<12:00,  8.68s/it]2024-12-22 02:32:48,254 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:49,709 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:




























site
site


site
site
site
name

















name
name

 16%|█▌        | 16/100 [02:40<16:08, 11.53s/it]2024-12-22 02:32:49,934 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:50,248 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       connection.sendChallenge(PacketParserUtils.parseChallenge(doc));
    }

    private void parseSuccess(Element doc) throws IOException {
        String username = PacketParserUtils.parseUsername(doc);
        String password = PacketParserUtils.parsePassword(doc
 16%|█▌        | 16/100 [02:40<13:18,  9.50s/it]2024-12-22 02:32:50,462 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:51,445 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:51,445 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2699])
2024-12-22 02:32:51,571 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:32:53,693 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:53,693 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3199])
2024-12-22 02:32:53,798 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:32:54,665 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def greatCircle(self):
        """ Compute great circle parameters. """

        self.gc_beg_phase = angularSeparation(self.beg_vect, self.end_vect)
        self.gc_end_phase = angularSeparation(self
 17%|█▋        | 17/100 [02:45<13:43,  9.92s/it]2024-12-22 02:32:54,840 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:54,987 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:54,988 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2853])
2024-12-22 02:32:55,079 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:55,079 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2829])
2024-12-22 02:32:55,086 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:32:55,161 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:32:55,182 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:32:55,182 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4152])
2024-12-22 02:32:55,362 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:32:56,756 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                   new HSBAdjustTransformation(),
                    new InvertTransformation(),
                    new LevelsTransformation(),
                    new LookupTransformation(),
                    new MapColorsTransformation(),
                    new MarbleTransformation(),
                    new MaskTransformation
 18%|█▊        | 18/100 [02:47<11:50,  8.67s/it]2024-12-22 02:32:56,875 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:58,090 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   assert cls.set_rounds(24)



















































 17%|█▋        | 17/100 [02:48<12:27,  9.00s/it]2024-12-22 02:32:58,247 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   public MonthCalendar(Context context, AttributeSet attrs) {
        super(context, attrs);
        layoutInflater = (LayoutInflater) context.getSystemService(Context.LAYOUT_INFLATER_SERVICE);
        monthHeaderView = (ViewGroup) layoutInflater.inflate
 17%|█▋        | 17/100 [02:48<14:42, 10.63s/it]2024-12-22 02:32:58,369 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:58,443 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:32:59,394 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:						
			
			
			
				
				
				
			
			
			
			
			
			
			
		
 18%|█▊        | 18/100 [02:49<14:49, 10.84s/it]2024-12-22 02:32:59,637 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:00,131 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:00,131 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2784])
2024-12-22 02:33:00,235 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:33:01,842 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:01,842 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2890])
2024-12-22 02:33:01,937 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:33:03,267 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:				String name = fhead.ID().getText();
		FunctionSymbol s = (FunctionSymbol)currentScope.resolve(name);
		MMethod m = new MMethod(s);
		classInFile.addMember(m);
	}

	@
 18%|█▊        | 18/100 [02:53<13:00,  9.52s/it]2024-12-22 02:33:03,396 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:03,397 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2814])
2024-12-22 02:33:03,467 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:03,496 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:33:04,784 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   private Movie movie;
    private MovieDetails movieDetails;
    private List<Trailer> trailers;
    private List<Rating> ratings;
    private List<String> similarMovies;
    private Unbinder unbinder;
    private MovieInfoView movieInfo
 19%|█▉        | 19/100 [02:55<11:26,  8.47s/it]2024-12-22 02:33:04,836 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:06,462 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:06,462 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3487])
2024-12-22 02:33:06,511 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   private SongListAdapter mAdapter;



    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {
        View view = inflater.inflate(R.layout.song_list_fragment, container, false);

        mActivity
 18%|█▊        | 18/100 [02:57<12:03,  8.83s/it]2024-12-22 02:33:06,599 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:33:06,750 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:06,751 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4124])
2024-12-22 02:33:06,810 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:06,917 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:33:08,872 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:08,873 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2980])
2024-12-22 02:33:08,982 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:33:10,039 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   if op in (opcode.HAVE_ARGUMENT, opcode.HAVE_KW_ARGUMENT):
      pop, push = get_stack_effect(op, arg)
      cond_stack_size -= 1
      if pop > 
 19%|█▉        | 19/100 [03:00<14:33, 10.78s/it]2024-12-22 02:33:10,319 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:10,588 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def test_white_noise(self, dur):
    my_stream = white_noise(dur)
    assert isinstance(my_stream, Stream)


  def test_saw_noise(self, dur):
    my_stream = saw_
 18%|█▊        | 18/100 [03:01<15:13, 11.14s/it]2024-12-22 02:33:10,764 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:11,778 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:11,778 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3649])
2024-12-22 02:33:11,914 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:33:12,084 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   void add(NodeAdditionVisitor visitor) {
        visitor.add(this);
    }
}







































 19%|█▉        | 19/100 [03:02<12:34,  9.31s/it]2024-12-22 02:33:12,229 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:14,950 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:14,950 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4180])
2024-12-22 02:33:15,084 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:
                writeResponse(SUCCESS, correlationId);
            } catch (InstanceAlreadyExistsException e) {
                writeResponse(FAILURE, correlationId);
            } catch (MBeanException e) {
                writeResponse(FAILURE, correlationId);
            } catch (
 20%|██        | 20/100 [03:05<12:01,  9.02s/it]2024-12-22 02:33:15,107 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:33:15,204 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:15,321 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:15,322 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2814])
2024-12-22 02:33:15,419 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:33:16,654 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:16,655 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2337])
2024-12-22 02:33:16,749 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:33:16,993 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:16,994 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3347])
2024-12-22 02:33:17,119 - [Process 4/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:4')
2024-12-22 02:33:18,498 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   //Create a new instance of the fragment
    public OverviewFragment() {
        //Required empty public constructor
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {
        //Inflate the layout for this fragment

 20%|██        | 20/100 [03:08<13:26, 10.08s/it]2024-12-22 02:33:18,690 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:18,778 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           // Create a new flow abs effect instance
            FlowAbsEffect flowAbsEffect = new FlowAbsEffect();
            flowAbsEffect.setEffect(effect);
            // Add the flow abs effect to the effect list
            mSpectaculumView.addEffect(flow
 19%|█▉        | 19/100 [03:09<13:18,  9.86s/it]2024-12-22 02:33:19,092 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:19,752 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   @Override
    public void init(ProcessingEnvironment processingEnvironment) {
        super.init(processingEnvironment);
        this.typeUtils = processingEnvironment.getTypeUtils();
        this.elementUtils = processingEnvironment.getElementUtils();
        this.messager = processingEnvironment.getMess
 20%|██        | 20/100 [03:10<11:45,  8.82s/it]2024-12-22 02:33:20,021 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:20,439 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   unify(t1, t2, backref)


I'm not sure what this code is doing, but it seems to be related to type unification. The code is trying to unify two types, `t1` and `t2`, and it's using a function
 19%|█▉        | 19/100 [03:10<14:31, 10.76s/it]2024-12-22 02:33:20,526 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:20,527 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2951])
2024-12-22 02:33:20,633 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:20,634 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:33:23,140 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:23,141 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2423])
2024-12-22 02:33:23,230 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:33:23,534 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       await wild_report.add_reaction(emoji.emojis.get(MyEmojis.WILD_SPAWN))



    @group(pass_context=True, category='Bot Info', aliases=["wild"])
    @channel
 21%|██        | 21/100 [03:13<11:39,  8.85s/it]2024-12-22 02:33:23,697 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:25,053 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:25,053 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2699])
2024-12-22 02:33:25,060 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:25,060 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2443])
2024-12-22 02:33:25,143 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:33:25,158 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:33:25,963 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:25,964 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3505])
2024-12-22 02:33:26,101 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:33:26,201 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   class PackageInventoryForm(forms.ModelForm):
        class Meta:
            model = Package
            fields = (
                'package_name', 'package_version', 'package_build_system',
                'package_build_tags', 'package_build_tags_last_
 21%|██        | 21/100 [03:16<12:20,  9.37s/it]2024-12-22 02:33:26,507 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:28,107 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       account_id=None,
        client_id=None,
        client_secret=None,
        refresh_token=None,
        region_id=None,
        token=None,
        username=None,
        password=None,
        host=None,

 20%|██        | 20/100 [03:18<13:06,  9.83s/it]2024-12-22 02:33:28,303 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       bind(GerritCheckoutProvider.class).to(GerritCheckoutProvider.class);
        bind(GerritHttpAuthDataProvider.class).to(GerritHttpAuthDataProvider.class);
        bind(GerritRestModule.class).to(G
 21%|██        | 21/100 [03:18<11:30,  8.74s/it]2024-12-22 02:33:28,399 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:28,519 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:29,255 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:29,255 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2614])
2024-12-22 02:33:29,381 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:33:29,515 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   Bit('ESM', ReprName='ESM Information', Pt=4, BitLen=4, Repr='hum'),
    ]

    def __init__(self, CallName='', ReprName='', Pt=0, BitLen=0, EBT=
 20%|██        | 20/100 [03:20<13:29, 10.12s/it]2024-12-22 02:33:29,792 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:32,300 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   if c['vocab_text']:
        vocab_text = vocab.get_text_vocabulary()
    else:
        vocab_text = None

    # Load dictionary
    if c['dict']:
        dictionary = Dictionary(c
 22%|██▏       | 22/100 [03:22<11:28,  8.83s/it]2024-12-22 02:33:32,457 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:35,439 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:35,440 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4500])
2024-12-22 02:33:35,494 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:35,494 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3757])
2024-12-22 02:33:35,613 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:33:35,636 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:33:36,076 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:36,076 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3451])
2024-12-22 02:33:36,203 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:33:37,391 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:37,391 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4553])
2024-12-22 02:33:37,565 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:33:39,204 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:		def create_model_item(self, parent_item, data):
			model_item = DocumentModelItem(data, self, parent_item)
			return model_item


I have tried to implement the code according to the provided documentation, but I am
 21%|██        | 21/100 [03:29<13:26, 10.21s/it]2024-12-22 02:33:39,383 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:		start(FeedItem.start(FeedItem.start(FeedItem.start(Feedback(Feed)
	start(rating.place(FeedItem.start(Feedback(Feedback(Feedback(rating(Feedback(Feedback(Feedback(Feedback(Feed
	start
 22%|██▏       | 22/100 [03:29<13:40, 10.51s/it]2024-12-22 02:33:39,417 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:39,557 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           // Update the song list adapter with the new tags
            mSongListAdapter.notifyDatasetChanged();
        }
    };


    public ArtistFragment() {
        // Required empty public constructor
    }


    @Override
    public View onCreateView(
 21%|██        | 21/100 [03:30<13:17, 10.10s/it]2024-12-22 02:33:39,585 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:39,881 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:40,665 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:40,665 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4239])
2024-12-22 02:33:40,830 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 02:33:41,570 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           //snaphots.read()
            //read()
            FileName.
            //read()
            //read()
            //read()
            //read()
            //read()
            //read()
            //read()
            //read()
            //read()
 22%|██▏       | 22/100 [03:32<13:07, 10.10s/it]2024-12-22 02:33:41,760 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:43,587 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:43,587 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2283])
2024-12-22 02:33:43,669 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:33:44,193 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results: assert isinstance(doppler, DopplerSine)
  assert doppler.distance0_m == 0.
  assert doppler.tec_epm2 == 50.
  assert doppler.coeffs == (50, 
 23%|██▎       | 23/100 [03:34<12:30,  9.75s/it]2024-12-22 02:33:44,386 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:46,295 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:46,295 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3490])
2024-12-22 02:33:46,431 - [Process 4/5] - DEBUG - predict_token:tensor([[418]], device='cuda:4')
2024-12-22 02:33:46,508 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   def parse(self, ping_message: Sequence[str]) -> PingStats:
        return self.__parser.parse(ping_message)


Expected output:

    def parse(self, ping_message: Sequence[str]) -> PingStats:
       
 23%|██▎       | 23/100 [03:36<12:11,  9.50s/it]2024-12-22 02:33:46,825 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:47,282 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:47,282 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3957])
2024-12-22 02:33:47,428 - [Process 3/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:3')
2024-12-22 02:33:48,974 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:48,974 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3716])
2024-12-22 02:33:49,120 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:33:49,745 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   self.next_sink.AsyncProcessRequest(sink_stack, buf, headers)


  def _OnTimeout(self, tag):
    if tag:
      self.AsyncProcessRequest(None, self._CreateDiscardMessage(tag))

  def _Process
 22%|██▏       | 22/100 [03:40<13:24, 10.31s/it]2024-12-22 02:33:50,012 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:50,933 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def _load_corpus_from_source(self) -> None:
        """
        Load a corpus without using multiprocessing
        """
        begin_time = time.time()
        sanitize_function = None
        if hasattr(self, "san
 22%|██▏       | 22/100 [03:41<13:37, 10.48s/it]2024-12-22 02:33:51,201 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:51,778 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:51,778 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3876])
2024-12-22 02:33:51,927 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:33:52,613 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def _parse_search_query(self, query_string):
        query = re.search(r'(\b(?:^| )\w+(?: [^ ]+)?\b)', query_string)
        return query

    def parse_search_query(
 23%|██▎       | 23/100 [03:43<13:19, 10.38s/it]2024-12-22 02:33:52,893 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:53,044 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:53,044 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3436])
2024-12-22 02:33:53,168 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:33:55,188 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:               startActivityForResult(new Intent(getActivity(), ParticipantFortunateActivity.class), 0, BaseActivity.ActivityAnimation.SLIDE_LEFT);
            }
        }
    }
    }

    private void checkPermission() {
        if (Build.
 24%|██▍       | 24/100 [03:45<12:49, 10.12s/it]2024-12-22 02:33:55,309 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:55,531 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:55,531 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2861])
2024-12-22 02:33:55,649 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:33:56,438 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           //load latest message from server
            getLatestMessageFromServer();
        }
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {
        View view = inflater.inflate(R.layout.chat_fragment
 24%|██▍       | 24/100 [03:46<12:11,  9.63s/it]2024-12-22 02:33:56,805 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:58,258 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:58,258 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3838])
2024-12-22 02:33:58,395 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:33:58,802 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       // assertThat(out.toString(), equalTo(""));
        // assertThat(out.toString(), startsWith("Thread dump"));
    }

    @Test
    public void cliNoSuchThread() {
        run("deadlocks", "--in", "threaddump
 23%|██▎       | 23/100 [03:49<12:44,  9.93s/it]2024-12-22 02:33:59,020 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:33:59,866 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:33:59,866 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3548])
2024-12-22 02:34:00,004 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:34:01,819 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:				if (userName != null) {
					UserManagement.addUser(userName, request.getRequest());
				}

				return true;

			} catch (Exception e) {
				
 23%|██▎       | 23/100 [03:52<13:36, 10.60s/it]2024-12-22 02:34:01,998 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:01,999 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3650])
2024-12-22 02:34:02,000 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:02,124 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:34:03,422 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   # 0xC0 : ('Start Of Frame (Baseline DCT)', 'SOF0'),
    # 0xC1 : ('Start Of Frame (Extended Sequential DCT)', 'S0F1'),
    # 0xC2 : ('Start
 24%|██▍       | 24/100 [03:53<13:18, 10.51s/it]2024-12-22 02:34:03,707 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:04,895 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:04,896 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3674])
2024-12-22 02:34:05,073 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:34:05,261 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def test_release(self):
        assert not self.redis.exists(self.redlock.key)
        assert self.redlock.acquire()
        assert self.redis.exists(self.redlock.key)
        assert self.redlock.release()
 25%|██▌       | 25/100 [03:55<12:37, 10.11s/it]2024-12-22 02:34:05,378 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:08,566 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:	private
	public
	private
	private
	public
	public
	private
		public
		public
		public
	private
		public
	public
	private
	public
	public
		private
	public
	public
	public
 25%|██▌       | 25/100 [03:59<12:58, 10.38s/it]2024-12-22 02:34:08,743 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:08,744 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4828])
2024-12-22 02:34:08,750 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:08,940 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:34:09,736 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:09,737 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2494])
2024-12-22 02:34:09,825 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:34:12,354 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:12,354 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4458])
2024-12-22 02:34:12,527 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:34:12,588 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       return get_class("ConeSettings", self._dll, "FMOD_THREED_CONE_SETTINGS")

































 26%|██▌       | 26/100 [04:03<11:26,  9.27s/it]2024-12-22 02:34:12,718 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:






















































 Hinweis








 24%|██▍       | 24/100 [04:03<14:05, 11.13s/it]2024-12-22 02:34:12,735 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:12,904 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:15,223 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:15,223 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 6134])
2024-12-22 02:34:15,485 - [Process 3/5] - DEBUG - predict_token:tensor([[30488]], device='cuda:3')
2024-12-22 02:34:16,108 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:16,108 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3785])
2024-12-22 02:34:16,162 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					.set(String.queryText() -> {
						.set(String.query() -> {
					.set(String -> {
					.query() -> {
							.set
 25%|██▌       | 25/100 [04:06<13:58, 11.18s/it]2024-12-22 02:34:16,261 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:34:16,413 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:19,529 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:19,529 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3465])
2024-12-22 02:34:19,666 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:34:19,701 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   account = account_for_facebook_user(fb_user, person=person)
    if request.user.is_anonymous():
        person = account.person
        if person.user is None:
            # AGH
            random_name = ''.join(choice(string
 26%|██▌       | 26/100 [04:10<13:04, 10.60s/it]2024-12-22 02:34:19,898 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:19,987 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:Љ.Љ.Љ.Љ.Љ.Љ.Љ.Љ.Љ.Љ.Љ..............Љ.Љ.Љ.Љ.Љ.Љ.Љ.Љ.Љ.Љ...Љ.Љ..


 24%|██▍       | 24/100 [04:10<16:18, 12.87s/it]2024-12-22 02:34:20,025 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:20,025 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3762])
2024-12-22 02:34:20,169 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:34:20,172 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:22,793 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self, *, calg: int):
        self.calg = calg
        super().__init__()

    def parse(self, packet):
        super(CompressedData, self).parse(packet)
        self.calg = packet
 27%|██▋       | 27/100 [04:13<11:37,  9.55s/it]2024-12-22 02:34:22,917 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:23,572 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       MediaLoader.getLoader().loadVideos(this, new OnVideoLoaderCallBack() {
            @Override
            public void onResult(VideoResult result) {
                tv_video_info.setText("视频: " + result.getItems().size() + " 
 25%|██▌       | 25/100 [04:14<13:48, 11.05s/it]2024-12-22 02:34:23,751 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:26,020 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:26,021 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4807])
2024-12-22 02:34:26,212 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:34:30,108 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:30,108 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3696])
2024-12-22 02:34:30,141 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:30,141 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 5021])
2024-12-22 02:34:30,186 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:
































































 26%|██▌       | 26/100 [04:20<14:50, 12.03s/it]2024-12-22 02:34:30,258 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:34:30,349 - [Process 2/5] - DEBUG - predict_token:tensor([[29892]], device='cuda:2')
2024-12-22 02:34:30,368 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:31,659 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:31,659 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4218])
2024-12-22 02:34:31,813 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:34:33,498 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   #    out.write(timer.elapsedTime(head = 'Cell Size'));
    
    #    cellIntensity = findCellIntensity(imgshape, imgmax, findCellIntensityParameter = findCellIntensityParameter, method = 'Sum', verbose = verbose
 28%|██▊       | 28/100 [04:23<11:52,  9.90s/it]2024-12-22 02:34:33,637 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:34,593 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:parameter_parameter_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_param_
 27%|██▋       | 27/100 [04:25<14:28, 11.89s/it]2024-12-22 02:34:34,691 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:35,448 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       RoxanaProperties mockedRoxanaProperties = mock(RoxanaProperties.class);
        when(mockedRoxanaProperties.getLocale()).thenReturns("pt_rooting_roxana_translator_messages_base_name");
        when(mock
 26%|██▌       | 26/100 [04:25<13:55, 11.29s/it]2024-12-22 02:34:35,646 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:36,361 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:36,362 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 7270])
2024-12-22 02:34:36,679 - [Process 3/5] - DEBUG - predict_token:tensor([[1]], device='cuda:3')
2024-12-22 02:34:36,696 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:36,696 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3279])
2024-12-22 02:34:36,821 - [Process 1/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:1')
2024-12-22 02:34:39,878 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:39,879 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3474])
2024-12-22 02:34:39,995 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:34:40,093 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:40,094 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2522])
2024-12-22 02:34:40,149 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   result, index = apply_fast_and(left.values, right.values, left.index, right.index)
    return Column(result, index)


def apply_or(left: Column, right):
    if type(right) == Column:
        result,
 27%|██▋       | 27/100 [04:30<13:53, 11.41s/it]2024-12-22 02:34:40,184 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:34:40,504 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:41,264 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:41,264 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3545])
2024-12-22 02:34:41,390 - [Process 2/5] - DEBUG - predict_token:tensor([[418]], device='cuda:2')
2024-12-22 02:34:41,678 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:ЉЉЉЉЉЉЉЉЉЉЉЉЉЉ Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis HinweisЉ Hinweis HinweisЉЉЉЉЉЉЉЉЉЉ
 25%|██▌       | 25/100 [04:32<19:23, 15.52s/it]2024-12-22 02:34:41,868 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:43,048 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           Music.INSTANCE.play(Assets.tapSound, 0.1f, 0.1f);
        } else {
            Music.INSTANCE.play(Assets.tapSound, 0.1f, 0.1f);
        }

 29%|██▉       | 29/100 [04:33<11:35,  9.79s/it]2024-12-22 02:34:43,162 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:43,231 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   self.SendMessage(self._ping_msg)


  def SendMessage(self, message):
    """Sends a message to the remote server."""
    self._log.debug('Sending message.')
    self.socket.send(message)

  def
 27%|██▋       | 27/100 [04:33<12:27, 10.24s/it]2024-12-22 02:34:43,433 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:44,765 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   } else {
      log("server response is: unknown " + getFileNames() + " " + responseTxt, null);
      successful = false;
      cancelUpload(i18nStrs.uploaderServerError() + "\nAction: " + getServletPath() +
 28%|██▊       | 28/100 [04:35<13:39, 11.38s/it]2024-12-22 02:34:44,945 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:46,386 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:46,387 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2584])
2024-12-22 02:34:46,471 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:34:46,622 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:46,622 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3312])
2024-12-22 02:34:46,740 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:34:48,909 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:48,910 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2839])
2024-12-22 02:34:49,028 - [Process 4/5] - DEBUG - predict_token:tensor([[418]], device='cuda:4')
2024-12-22 02:34:49,529 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   pass


class TestGammatone(object):

  @p(("n", "Hz"),
     [(4, 1000),
      (4, 3000),
      (5, 1000),
      (5,
 26%|██▌       | 26/100 [04:40<16:18, 13.22s/it]2024-12-22 02:34:49,813 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:50,084 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:                   Request request = RequestFragment.sSelectedRequests.get(i);
                    if (request.getIcon() == null) {
                        mError = Extras.Error.ICON_REQUEST_NULL;
                        return false;
                    }
                    if (
 28%|██▊       | 28/100 [04:40<13:09, 10.97s/it]2024-12-22 02:34:50,287 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:50,798 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:50,798 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4064])
2024-12-22 02:34:50,950 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:34:52,149 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:     assert freq2lag(lag2freq(v)) == v


class TestAlmostEqual(object):
  def test_almost_equal_with_float_numbers(self):
    for v in [1.0, 1.000001
 28%|██▊       | 28/100 [04:42<11:48,  9.84s/it]2024-12-22 02:34:52,324 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:53,384 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:53,384 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4141])
2024-12-22 02:34:53,555 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:34:54,239 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           while (transactionIterator.hasNext()) {
                printer.print(transactionIterator.next());
                System.out.println(print);
                print.setLength(0);
            }
        }
    }

    private boolean checkDirectoryValid(File directory) throws Interrupted
 30%|███       | 30/100 [04:44<11:54, 10.21s/it]2024-12-22 02:34:54,373 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:56,839 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:56,840 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3387])
2024-12-22 02:34:56,977 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:34:57,108 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   #     #     #
    #     #
    #
    #
    #     #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
 29%|██▉       | 29/100 [04:47<13:48, 11.67s/it]2024-12-22 02:34:57,297 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:57,298 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3854])
2024-12-22 02:34:57,318 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:34:57,443 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:34:58,616 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:34:58,617 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3445])
2024-12-22 02:34:58,744 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:35:00,416 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:```
    def __subclasscheck__(cls, subclass: type) -> bool:
        return issubclass(subclass, (int, float, Decimal, AbstractDateTime, Duration))
```

I'm not sure what you're trying to achieve, but it seems like you
 29%|██▉       | 29/100 [04:50<12:45, 10.78s/it]2024-12-22 02:35:00,624 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:00,951 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   return Sequence(seq)

def read_sequence_item(fp, is_implicit_VR, is_little_endian, encoding, offset=0):
    """Read and return a single item from a sequence (i.e. a single dataset)"""


 27%|██▋       | 27/100 [04:51<15:25, 12.68s/it]2024-12-22 02:35:01,168 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:02,021 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           Proxy14Service.class,
            Proxy15Service.class,
            Proxy16Service.class,
            Proxy17Service.class,
            Proxy18Service.class,
            Proxy19Service.class,
            Proxy2
 29%|██▉       | 29/100 [04:52<11:39,  9.85s/it]2024-12-22 02:35:02,231 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:02,561 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:02,561 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4229])
2024-12-22 02:35:02,725 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:35:03,532 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:03,532 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3357])
2024-12-22 02:35:03,658 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:35:05,058 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:05,058 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2467])
2024-12-22 02:35:05,149 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:35:06,090 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:
    }
    }
    return pgm.getAllStmnts();
}


    public void isValid() {
        for (BaseStatementMeta stmnt : getAllStmts()) {
            if (stmnt.getJsonObject().get("timestamp
 31%|███       | 31/100 [04:56<12:18, 10.70s/it]2024-12-22 02:35:06,199 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:06,650 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:06,650 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2215])
2024-12-22 02:35:06,744 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:35:06,830 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:06,830 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3074])
2024-12-22 02:35:06,921 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   api.add_resource(GyroscopeResource, "/aircraft/sensors/gyroscope", resource_class_args=(sensors.gyroscope,))

I'm not sure what the code is doing, but it seems to be defining some kind of web service for a
 30%|███       | 30/100 [04:57<12:57, 11.11s/it]2024-12-22 02:35:06,933 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:35:07,147 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:08,138 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       return SearchMenuPage(self.service(context, response))


















































 30%|███       | 30/100 [04:58<11:30,  9.86s/it]2024-12-22 02:35:08,367 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:09,627 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   @ConfigCog.command(name="config", help="Configure various settings for the bot")
    async def config(self, ctx: commands.Context):
        ...

























 30%|███       | 30/100 [05:00<10:42,  9.18s/it]2024-12-22 02:35:09,879 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:10,085 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   for (ICommentProcessor commentProcessor : commentProcessors) {
      try {
        commentProcessor.commitChanges(document);
      } catch (UnresolvedExpressionException | SpelEvaluationException | SpelParseException e) {
        logger.error("Error while running comment processor
 28%|██▊       | 28/100 [05:00<13:56, 11.62s/it]2024-12-22 02:35:10,290 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:10,857 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:10,857 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2523])
2024-12-22 02:35:10,957 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:35:13,750 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def decrypt(self, message: FinTSMessage):
        raise NotImplemented()


class PinTanDummyAuthenticationMechanism(AuthenticationMechanism):
    def __init__(self, security_method_version=1):
        super().__init__()
 32%|███▏      | 32/100 [05:04<11:05,  9.79s/it]2024-12-22 02:35:13,892 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:14,245 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:14,245 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3811])
2024-12-22 02:35:14,388 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:35:15,020 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:15,021 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3494])
2024-12-22 02:35:15,148 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:35:16,466 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:16,466 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3474])
2024-12-22 02:35:16,594 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:35:16,934 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:16,934 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3527])
2024-12-22 02:35:17,062 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:35:17,847 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           fields={ key.capitalize():value for key, value in config.items()}, msg_color=discord.Color.blue(),
            inline=True)

        return await ctx.send(embed=embed)



    @staticmethod
    async def send_
 31%|███       | 31/100 [05:08<12:42, 11.05s/it]2024-12-22 02:35:18,124 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:18,490 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       return rvalue



    def getByPath(self, path):
        """
        Get a parameter value by a path.

        path -- a full path to a parameter, e.g., x.y.z[i].a
        return -- the parameter value or
 31%|███       | 31/100 [05:08<11:30, 10.01s/it]2024-12-22 02:35:18,735 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:19,950 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   return evaluate_comma_operator(self, context=None)



###
# Function call expression
@method('(')
def led_function_call_expression(self, function):
    if self.parser.next_token.symbol == ')':
        self
 31%|███       | 31/100 [05:10<10:57,  9.52s/it]2024-12-22 02:35:20,196 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:20,427 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   protected void loadData(int page) {
        if (getActivity() == null) {
            return;
        }
        loadingStarted();
        AbstractRavelryGetRequest<PatternsResult> request = getRequest(page);
        spiceManager.execute(request,
 29%|██▉       | 29/100 [05:10<13:17, 11.23s/it]2024-12-22 02:35:20,603 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:20,948 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:20,948 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3688])
2024-12-22 02:35:21,093 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:35:23,265 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:23,266 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2422])
2024-12-22 02:35:23,356 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:35:24,277 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                       if (env.combExpRegs[i] != null) {
                            env.numCombExpCheck = 0;
                            break;
                        }
                        }
                    }
                } // USE_SUBEXP_CALL
            }
 33%|███▎      | 33/100 [05:14<11:10, 10.01s/it]2024-12-22 02:35:24,446 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:25,724 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:25,725 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4007])
2024-12-22 02:35:25,877 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:35:26,244 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   serializer_class = PlanSerializer




class GoalViewSet(ModelViewSet):
    queryset = Goal.objects.all()
    serializer_class = GoalSerializer
    filter_class = GoalFilter
    ordering_fields = ('id', '
 32%|███▏      | 32/100 [05:16<10:34,  9.33s/it]2024-12-22 02:35:26,511 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:26,975 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:26,975 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3232])
2024-12-22 02:35:27,104 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:35:27,258 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:27,258 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3541])
2024-12-22 02:35:27,402 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:35:29,417 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:								if (!level.isInstalled()) {
								showConfirm(
										getString(R.string.install_level),
										getString(
 32%|███▏      | 32/100 [05:19<12:42, 11.21s/it]2024-12-22 02:35:29,669 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:30,313 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def flags_int(self, val):
        self._flags += NotationDataFlags(val)

    @flags.register(bytearray)
    def flags_bytearray(self, val):
        self.flags = self.bytes_to_int(val)


 30%|███       | 30/100 [05:20<12:38, 10.83s/it]2024-12-22 02:35:30,520 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:30,891 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       welcomeString.setText("Welcome, " + getActivity().getSharedPreferences(getSharedPreferences(getActivity())).getString(
                Constants.PREF_USER_NAME, ""));

        return header;
    }

    private void initializeEvents() {
        // Load
 32%|███▏      | 32/100 [05:21<11:16,  9.95s/it]2024-12-22 02:35:31,123 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:31,644 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:31,644 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3891])
2024-12-22 02:35:31,785 - [Process 0/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:0')
2024-12-22 02:35:31,848 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:31,848 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2942])
2024-12-22 02:35:31,958 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:35:35,026 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   url(r'^new-language$', staff_member_required(NewLanguageView.as_view(), login_url=LOGIN_URL), name="new-language"),
    url(r'^update-language$', staff_member_required(UpdateLanguageView.as_
 34%|███▍      | 34/100 [05:25<11:15, 10.23s/it]2024-12-22 02:35:35,058 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   private StarItemAdapter starItemAdapter;
    private List<StarItem> starItemList = new ArrayList<StarItem>();


    private CommitTask commitTask;
    private StarTask starTask;
    private BookmarkTask bookmarkTask;
    private RepoContentTask repo
 33%|███▎      | 33/100 [05:25<10:14,  9.18s/it]2024-12-22 02:35:35,225 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:35,268 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:36,953 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:36,953 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3827])
2024-12-22 02:35:37,103 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:35:37,368 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:37,368 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3333])
2024-12-22 02:35:37,494 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:35:37,868 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:37,868 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3511])
2024-12-22 02:35:38,023 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:35:40,090 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:40,091 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2622])
2024-12-22 02:35:40,187 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:35:40,257 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:40,257 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2597])
2024-12-22 02:35:40,359 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:35:40,634 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   private WeeklyCaptionProvider weeklyCaptionProvider;


    private final DesignContext designContext;


    private final KeyMapper<Action> actionKeyMapper;

    private final ContentMode contentMode;

    private final DesignAttributeHandler designAttributeHandler;

   
 33%|███▎      | 33/100 [05:31<12:31, 11.21s/it]2024-12-22 02:35:40,824 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:40,926 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           // Fragment to notify
            InviteToChatFragment fragment = (InviteToChatFragment) getActivity().findFragmentByTag(InviteToChatFragment.TAG);
            fragment.showChatActivity(newChatId);
        }
    }
}
}
 33%|███▎      | 33/100 [05:31<11:08,  9.97s/it]2024-12-22 02:35:41,167 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:41,422 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def addMessage(self, message, isnew):
        if isnew:
            self.messages.append(message)
        else:
            self.messages.insert(self.messages.index(message), message)

    def removeMessage(self, message):
        self
 31%|███       | 31/100 [05:31<12:33, 10.91s/it]2024-12-22 02:35:41,613 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:42,984 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:				
		public ResourceData (Meter meter, EnumMap<SRSLevel, String> tags, EnumMap<SRSLevel, Integer> colors, String notEnoughData) {
				this.meter = meter;
				this.
 35%|███▌      | 35/100 [05:33<10:20,  9.55s/it]2024-12-22 02:35:43,103 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:43,392 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:			File inputFile = new File(basePathDDI2FO + "/input.ddi");
		File outputFO = ddi2fo.process(inputFile, null, "survey");
		
		
		
	}
}

}

 34%|███▍      | 34/100 [05:33<09:48,  8.92s/it]2024-12-22 02:35:43,624 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:45,829 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:45,829 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2622])
2024-12-22 02:35:45,931 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:35:46,226 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:46,227 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2909])
2024-12-22 02:35:46,325 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:35:48,231 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:48,232 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3453])
2024-12-22 02:35:48,368 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:35:49,118 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       this.vms = new SpringVms(restTemplate, root, tasks);
    }

    @Override
    public Info info() {
        return this.info;
    }

    @Override
    public Releases releases() {
        return this.releases;
 34%|███▍      | 34/100 [05:39<11:25, 10.39s/it]2024-12-22 02:35:49,233 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:49,233 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3344])
2024-12-22 02:35:49,242 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:49,355 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:35:49,522 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       UnifiedOrderResponse response = wxPayClient.unifiedOrder(request);
        System.out.println(JSON.toJSONString(response));
    }

    @Test
    public void refund() throws WXPayApiException {

        String nonceStr = SDK
 34%|███▍      | 34/100 [05:39<10:30,  9.56s/it]2024-12-22 02:35:49,698 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:51,647 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def check_arg(arg, type_, env):
        arg = check(arg, env)
        with env.errors.location(arg.location):
            unify(arg.__type__, type_)
        return arg

    def check(arg, env):
        if
 32%|███▏      | 32/100 [05:42<12:08, 10.71s/it]2024-12-22 02:35:51,712 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:51,713 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4025])
2024-12-22 02:35:51,862 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:51,882 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:35:52,406 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def set_client_id(self, client_id):
        self._client_id = client_id
        return self

    def set_user(self, user):
        self._user = user
        return self

    def set_pass(self, pass_
 36%|███▌      | 36/100 [05:42<10:08,  9.51s/it]2024-12-22 02:35:52,514 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:54,800 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:54,801 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2847])
2024-12-22 02:35:54,901 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:35:55,377 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:                       // Save the user to the DB
                        DatabaseManager.INSTANCE.saveUser(user);

                        // Send the email
                        // ...
                        // ...
                        // ...
                        // ...
                        // ...
                        // ...
                       
 35%|███▌      | 35/100 [05:45<10:39,  9.84s/it]2024-12-22 02:35:55,652 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:57,657 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:57,657 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2929])
2024-12-22 02:35:57,760 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:35:57,914 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       List<Parameter> parameters = new ArrayList<>();
        parameters.add(new Parameter(STRING, STRING_PARAMETER_NAME_01, RANDOM_STRING_PARAMETER_VALUE_01));
        parameters.add(new Parameter(STRING, STRING_PARAME
 35%|███▌      | 35/100 [05:48<09:58,  9.21s/it]2024-12-22 02:35:58,238 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:35:58,238 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3227])
2024-12-22 02:35:58,256 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:35:58,367 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:36:00,332 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:00,333 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 5404])
2024-12-22 02:36:00,553 - [Process 2/5] - DEBUG - predict_token:tensor([[29992]], device='cuda:2')
2024-12-22 02:36:00,679 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   public <$SystemUnderTest> GivenWhenThenDsl<$SystemUnderTest> prepareFixtures() {
        preparation.prepareFixtures();
        return new GivenWhenThenDsl<>(preparation);
    }

    public static class GivenTwoArgumentsWhenSte
 37%|███▋      | 37/100 [05:51<09:35,  9.14s/it]2024-12-22 02:36:00,841 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:01,580 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   if isinstance(value, (GregorianMonthDay, Date10, DateTime10)):
        return GregorianMonthDay(value.month, value.day, value.tzinfo)
    try:
        if isinstance(value, UntypedAtomic):
 33%|███▎      | 33/100 [05:52<11:41, 10.47s/it]2024-12-22 02:36:01,782 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:04,746 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:O
O





























































 35%|███▌      | 35/100 [05:55<12:57, 11.96s/it]2024-12-22 02:36:04,788 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:04,789 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4626])
2024-12-22 02:36:04,940 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:04,971 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:36:05,550 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:05,550 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3708])
2024-12-22 02:36:05,701 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:36:07,943 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:07,944 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3744])
2024-12-22 02:36:08,086 - [Process 0/5] - DEBUG - predict_token:tensor([[418]], device='cuda:0')
2024-12-22 02:36:08,540 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:08,540 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3603])
2024-12-22 02:36:08,659 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:      
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       

 36%|███▌      | 36/100 [05:59<11:35, 10.87s/it]2024-12-22 02:36:08,675 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:36:08,854 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:09,118 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   public int readSmallIntValue(int d1) throws IOException {
		int value = d1 & 0x07;
		if (value == CODEINT4_TAG)
			return readSmallIntValue(d1);
		else

 36%|███▌      | 36/100 [05:59<10:27,  9.81s/it]2024-12-22 02:36:09,293 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:09,400 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:09,400 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2524])
2024-12-22 02:36:09,490 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:36:11,304 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:     try {
        process.await();
      } catch (CommandFailureException e) {
        throw new IosDeviceException(RealDeviceImpl.this, "Command failed: " + e.getMessage());
      } catch (InterruptedException e) {
        throw new IosDeviceException
 38%|███▊      | 38/100 [06:01<09:54,  9.59s/it]2024-12-22 02:36:11,455 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:11,989 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   feature_ = _get_sample_condition_dicts(dbf, comps, phase_name, configuration, symmetry, datasets, ridge_alpha=ridge_alpha, aicc_feature_factors=aicc_feature_factors)



def _get
 34%|███▍      | 34/100 [06:02<11:30, 10.45s/it]2024-12-22 02:36:12,212 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:12,396 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   protected EasyOnItemChildTouchListener easyOnItemChildTouchListener;
    protected EasyOnViewAttachedToWindowListener easyOnViewAttachedToWindowListener;
    protected EasyOnViewDetachedFromWindowListener easyOnViewDetachedFromWindowListener;

    public MultiItem
 36%|███▌      | 36/100 [06:02<11:22, 10.67s/it]2024-12-22 02:36:12,914 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:14,870 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:14,870 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3032])
2024-12-22 02:36:14,976 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:36:15,739 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:15,740 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3697])
2024-12-22 02:36:15,876 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 02:36:16,883 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:16,883 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2579])
2024-12-22 02:36:16,973 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:36:18,087 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def __repr__(self):
        return self.name

    def __str__(self):
        return self.name

    def __eq__(self, other):
        return self.name == other

    def __ne__(self, other):
        return self.
 37%|███▋      | 37/100 [06:08<10:02,  9.56s/it]2024-12-22 02:36:18,273 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:18,273 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3506])
2024-12-22 02:36:18,286 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:18,410 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:36:19,332 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results: encoder = GPSL1L2BitEncoder(NormalRateConfig)
  assert isinstance(encoder, EncoderBase)
  assert isinstance(encoder, TwoBandsBitEncoder)
  assert encoder.bandIndexes[0] == NormalRateConfig.
 37%|███▋      | 37/100 [06:09<11:21, 10.81s/it]2024-12-22 02:36:19,521 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:19,826 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:19,826 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3645])
2024-12-22 02:36:19,967 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:36:20,042 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   private void pausePlayerAndShowVpaid(PlayerUIController controller, PlayerComponentController componentController, FsmPlayer fsmPlayer, MediaModel mediaModel) {

    //TODO: API level lower that certain, will disable vpaid.
    }

    private void pausePlayer
 35%|███▌      | 35/100 [06:10<10:32,  9.73s/it]2024-12-22 02:36:20,318 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:21,546 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:				if (textColor == Color.BLACK) {
				textColor = Themer.getColor(contextRef.get(), R.attr.colorAccent);
			} else {
				textColor = Color.BLACK;
	
 39%|███▉      | 39/100 [06:11<09:56,  9.78s/it]2024-12-22 02:36:21,656 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:23,027 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:23,028 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2649])
2024-12-22 02:36:23,118 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:36:23,338 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                   assertTrue(client.deleteObject(containerName, fileName));
                } catch (FilesException e) {
                    // Ignore
                }
            }
		} catch (Exception e) {
			e.printStackTrace();
			fail(e.getMessage
 37%|███▋      | 37/100 [06:13<11:17, 10.75s/it]2024-12-22 02:36:23,594 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:24,261 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:24,262 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2458])
2024-12-22 02:36:24,356 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:36:26,094 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       )

        for parser_class in parser_class_list:
            try:
                ping_stats = parser_class(self.timezone)._parse(ping_lines)
                break
            except ParseError as e:
                logger.debug(f"{e.
 38%|███▊      | 38/100 [06:16<09:23,  9.09s/it]2024-12-22 02:36:26,318 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:26,897 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:26,897 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2960])
2024-12-22 02:36:27,002 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:36:27,259 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   return tgAsDict["tier"][0]["entries"]




def test_save_textgrid(tg):
    # Save the textgrid to a file
    run_save(tg)
    # Read the saved file back in
    saved_
 38%|███▊      | 38/100 [06:17<10:16,  9.95s/it]2024-12-22 02:36:27,525 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:27,525 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3719])
2024-12-22 02:36:27,561 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:27,668 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:36:29,931 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:			try {
			AbstractReceivedMessage message = AbstractReceivedMessage.parse(xml);
			if (message instanceof ReceivedTextMessage) {
				ReceivedTextMessage textMessage = (ReceivedTextMessage) message;
				String text = textMessage
 40%|████      | 40/100 [06:20<09:21,  9.36s/it]2024-12-22 02:36:30,041 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:31,035 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           PressureSensorCollector.flushDBCache(deviceID);
        }
        if(type == 7 || type == 0) {
            TemperatureSensorCollector.flushDBCache(deviceID);
        }
        if(type == 8 || type ==
 36%|███▌      | 36/100 [06:21<10:47, 10.11s/it]2024-12-22 02:36:31,164 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:31,553 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:31,553 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2896])
2024-12-22 02:36:31,611 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:31,611 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4033])
2024-12-22 02:36:31,659 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:36:31,775 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:36:34,110 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:34,111 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3208])
2024-12-22 02:36:34,247 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:36:34,902 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       m.setDefaultVideoFrameRate(Integer.parseInt(m.videoSetFrameRate.getText()));
        }

        // set video width and height
        if (m.videoSetWidth.getText().equals("")) {// set default width and height
        m.setVideoWidth(
 39%|███▉      | 39/100 [06:25<09:09,  9.01s/it]2024-12-22 02:36:35,100 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:35,410 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           });
        }
    }

        }
    }

    @Override
    public void onPlayerServiceStateChanged(int state) {
        switch (state) {
            case PLAYER_SERVICE_STATE_PLAYING:
                // Do something when playing
 38%|███▊      | 38/100 [06:25<11:31, 11.15s/it]2024-12-22 02:36:35,685 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:37,472 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def __del__(self):
        pass

    def __init__(self, *args, **kwargs):
        pass

    def __str__(self):
        pass

    def __repr__(self):
        pass

    def __eq__(self, other):
 39%|███▉      | 39/100 [06:27<10:11, 10.03s/it]2024-12-22 02:36:37,705 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:37,760 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:37,761 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3442])
2024-12-22 02:36:37,898 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:36:38,314 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:38,314 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4282])
2024-12-22 02:36:38,481 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:36:40,602 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:40,602 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2594])
2024-12-22 02:36:40,699 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:36:41,171 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   Call<SuggestionResponse> getSuggestions(@QueryMap Map<String, String> query);

}






































 37%|███▋      | 37/100 [06:31<10:37, 10.12s/it]2024-12-22 02:36:41,403 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:41,437 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:41,437 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3282])
2024-12-22 02:36:41,563 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:36:41,891 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   # 
    import 
    import 
    import 
    import 
    import 
    import 
    import 
    import 
    import 
    import
    import
   
    import
    import
    import
    import
    import
   
 41%|████      | 41/100 [06:32<09:58, 10.14s/it]2024-12-22 02:36:42,039 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:42,644 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:42,644 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2592])
2024-12-22 02:36:42,745 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:36:43,780 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   @ViewById(R.id.drawer_layout)
    DrawerLayout drawerLayout;


    @ViewById(R.id.nav_view)
    View navView;

    @ViewById(R.id.patterns_fragment)
    Fragment patternsFragment;
 39%|███▉      | 39/100 [06:34<10:29, 10.31s/it]2024-12-22 02:36:44,076 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:44,919 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           self.probe = probe
        else:
            self.probe = _probe_adjacency_list(probe_adjacency_list=probe_adjacency_list)
        self.channels_per_group = _channels_per_
 40%|████      | 40/100 [06:35<09:18,  9.31s/it]2024-12-22 02:36:45,108 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:45,704 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       dataset = UAV123(root_dir, version=version)
        self._check_dataset(dataset)

    def _check_dataset(self, dataset):
        # check sequence names
        expected_names = ['seq_001', 'seq_00
 40%|████      | 40/100 [06:36<09:29,  9.49s/it]2024-12-22 02:36:45,964 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:47,942 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:47,943 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3136])
2024-12-22 02:36:48,067 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:36:48,783 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:48,784 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3785])
2024-12-22 02:36:48,936 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:36:50,886 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:50,887 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2526])
2024-12-22 02:36:50,996 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:36:51,105 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   async def cmd_raidcity(self, ctx, *city):
        """
        **!raid-city [city]** - sets the city for the raid party.

        """
        pass



    @command(pass_context=True, category='Bot
 42%|████▏     | 42/100 [06:41<09:32,  9.86s/it]2024-12-22 02:36:51,215 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:51,372 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:51,373 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3379])
2024-12-22 02:36:51,501 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:36:52,417 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 02:36:52,419 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:52,420 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4157])
results:       stream = Stream(
            engine=self,
            model=model,
            position=position,
            session=self.session,
            coordinator=Coordinator(
                session=self.session,
                stream_arn=model.Meta.stream["arn"])
           
 38%|███▊      | 38/100 [06:42<10:48, 10.46s/it]2024-12-22 02:36:52,584 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:36:52,634 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:54,090 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   void add(FixTag tag, Object value);
    void add(FixTag tag, String value);
    void add(FixTag tag, Boolean value);
    void add(FixTag tag, Double value);
    void add(FixTag tag, Int value);
 41%|████      | 41/100 [06:44<09:00,  9.16s/it]2024-12-22 02:36:54,276 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:54,879 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   protected final Promise<NegotiationResponse> negotiate(final TransportContext context) {
        // ...
    }

    @Override
    public final Promise<PingResponse> ping(final TransportContext context) {
        if (context == null) {
            throw new Il
 41%|████      | 41/100 [06:45<09:20,  9.50s/it]2024-12-22 02:36:55,133 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:56,194 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                       ImageView image = view.findViewById(R.id.image);Next line of code:
                        ImageLoader imageLoader = ImageConfig.getImageLoader(mContext);Next line of code:
                        imageLoader.loadImage(mImageConfig.getImageUrl(), new Runnable
 40%|████      | 40/100 [06:46<10:56, 10.94s/it]2024-12-22 02:36:56,381 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:36:57,904 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:57,905 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3239])
2024-12-22 02:36:58,047 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:36:59,543 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:36:59,543 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3749])
2024-12-22 02:36:59,678 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:37:00,690 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:00,690 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2881])
2024-12-22 02:37:00,692 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:00,692 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3312])
2024-12-22 02:37:00,807 - [Process 4/5] - DEBUG - predict_token:tensor([[418]], device='cuda:4')
2024-12-22 02:37:00,819 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:37:01,164 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertTrue(date_string_to_date("25/11/2015") == datetime.date(2015, 11, 25))
        self.assertTrue(date_string_to_date("11/12/
 43%|████▎     | 43/100 [06:51<09:25,  9.92s/it]2024-12-22 02:37:01,318 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:02,966 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:02,966 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3550])
2024-12-22 02:37:03,093 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:37:03,092 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       self.log.info("confusion matrix:")
        self.log.info(confusion_matrix)

        self.log.info("accuracy on all classes: %2.2f%% (+/- %2.2f%%)" % (100 * accuracy,
 39%|███▉      | 39/100 [06:53<10:41, 10.52s/it]2024-12-22 02:37:03,347 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:03,956 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results: def analyze(self):
    self.compute_conditions()
    self.compute_dominators()
    self.compute_control_dependence()
    self.compute_frames()
    self.compute_block_intervals()
    self.compute_block_constraint
 42%|████▏     | 42/100 [06:54<09:03,  9.38s/it]2024-12-22 02:37:04,159 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:04,198 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           yield self.iter_chunks(n_samples=n_samples, chunk_size=chunk_size,
                        chunk_overlap=overlap, group=group,
                        s_start=s_start, keep_start=keep_start,
                        keep_
 42%|████▏     | 42/100 [06:54<09:07,  9.44s/it]2024-12-22 02:37:04,420 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:06,422 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   converter = UrlConverter(
        logger=logger,
        con=con,
        symbol_replace_value=ctx.obj[Context.SYMBOL_REPLACE_VALUE],
        add_pri_key_name=ctx.obj[Context.ADD_PRIMARY
 41%|████      | 41/100 [06:56<10:33, 10.73s/it]2024-12-22 02:37:06,754 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:08,014 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:08,015 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2495])
2024-12-22 02:37:08,093 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:08,094 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3558])
2024-12-22 02:37:08,113 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:37:08,235 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:37:08,608 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:08,608 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2544])
2024-12-22 02:37:08,699 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:37:11,034 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:	private BannedIpServices bannedIpServices;
	private CategoryServices categoryServices;
	private ChallengeServices challengeServices;
	private CountryServices countryServices;
	private SubmissionServices submissionServices;
	private SubmissionServices submissionServices;
	private CountryServices countryServices;

 40%|████      | 40/100 [07:01<09:44,  9.75s/it]2024-12-22 02:37:11,258 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:11,409 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       persistence_actor = ConfigActor(config_file)
        persistence_actor.start()

        if honeypot_first:
            # send honeypot message first
            honeypot_session = HoneypotSession(source_ip='1
 44%|████▍     | 44/100 [07:01<09:21, 10.02s/it]2024-12-22 02:37:11,538 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:11,632 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       IntentIntegrator integrator = new IntentIntegrator(this);
        integrator.setTarget(QRCodeFragment.this);
        integrator.setFormContent(R.layout.qr_code_form);
        integrator.setFormMode(IntentIntegrator.FORM_
 43%|████▎     | 43/100 [07:02<08:25,  8.87s/it]2024-12-22 02:37:11,825 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:13,142 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:13,143 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4305])
2024-12-22 02:37:13,315 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:37:13,393 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:13,393 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3399])
2024-12-22 02:37:13,536 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:37:16,869 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   def items(self):
        """ Returns a generator of :class:`~plexapi.media.PlaylistItem` objects. """
        return self.findItems(self.key, media.PlaylistItem)

    def findItems(self, key, cls):
        """
 42%|████▏     | 42/100 [07:07<10:17, 10.64s/it]2024-12-22 02:37:16,911 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:			@Override
		@Override
	public ExpansionResult visit(AlvisIRAndQueryNode expanderOptions expanderOptions expanderOptions expanderOptions expanderOptions expander(AlvisIRAndQueryNode.INSTANCE expanderOptions expanderOptions expander(fieldOptions expander
 43%|████▎     | 43/100 [07:07<09:54, 10.42s/it]2024-12-22 02:37:17,087 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:17,164 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:18,059 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:18,059 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3595])
2024-12-22 02:37:18,196 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:37:19,048 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:19,048 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3977])
2024-12-22 02:37:19,198 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:37:19,408 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:19,408 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3930])
2024-12-22 02:37:19,560 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:37:21,397 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:21,397 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2251])
2024-12-22 02:37:21,488 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:37:21,505 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def __getitem__(self, key):
        return self.graph.get_vertex(key)
```

I'm not sure what you're trying to achieve, but it seems that you're trying to modify the code of the `pyorient` library. I would advise
 41%|████      | 41/100 [07:12<09:47,  9.97s/it]2024-12-22 02:37:21,715 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:22,467 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                           ShareUtil.sharePicture(mActivity, picUrl, picture.getComment_ID());
                            break;
                        case 1:
                            //分享到微博
                            ShareUtil.shareToWeibo(mActivity, picUrl, picture.
 45%|████▌     | 45/100 [07:12<09:28, 10.33s/it]2024-12-22 02:37:22,602 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:23,004 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results: public void visit(final InExp inExp) throws QueryException {
    final Column column = inExp.getColumn();
    if (inExp.getValues() == null)
      throw new QueryGrammarException("Cannot apply IN with no values");

    result.append(
 44%|████▍     | 44/100 [07:13<08:58,  9.62s/it]2024-12-22 02:37:23,194 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:23,910 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:23,910 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3530])
2024-12-22 02:37:24,046 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:37:24,374 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:	public ParameterizedGenerationService(ValorizatorParameters valorizatorParameters) {
		this.valorizatorParameters = valorizatorParameters;
		this.pipelineGenerator = new PipeLineGeneratorImpl();
	}

	public void generate() throws Eno
 44%|████▍     | 44/100 [07:14<08:53,  9.54s/it]2024-12-22 02:37:24,456 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:27,327 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:27,327 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2337])
2024-12-22 02:37:27,354 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertRaises(TypeError, _environment_to_string, "a")

    def test_type_check_for_environment_value(self):
        self.assertRaises(TypeError, _environment_to_string, "a=b")

 43%|████▎     | 43/100 [07:17<10:04, 10.60s/it]2024-12-22 02:37:27,411 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:37:27,539 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:28,156 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:28,156 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3283])
2024-12-22 02:37:28,284 - [Process 3/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:3')
2024-12-22 02:37:30,291 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:					File baseFile = new File(basePath + "/input.xml");
				File expectedFile = new File(basePath + "/expected/input.xforms");
			
			Preprocessor preprocessor = new DDIMarkdown2Xhtml
 45%|████▌     | 45/100 [07:20<08:10,  8.92s/it]2024-12-22 02:37:30,501 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:30,767 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:30,767 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4303])
2024-12-22 02:37:30,922 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:37:31,502 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   @scoped_subgraph_initializers(klass=RNNArchitecture)
def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.encoder = self.encoder_initializer(self.encoder
 42%|████▏     | 42/100 [07:22<09:38,  9.97s/it]2024-12-22 02:37:31,685 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:31,757 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:31,757 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3680])
2024-12-22 02:37:31,901 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:37:34,062 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:34,062 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3489])
2024-12-22 02:37:34,187 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:37:34,288 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:					np.column_stack((image_array.shape[0].astype(masking_array.shape[0].astype(np.astype(np.astype(np.astype(np.astype(np.astype(np.astype(
 46%|████▌     | 46/100 [07:24<09:42, 10.78s/it]2024-12-22 02:37:34,412 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:34,413 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2287])
2024-12-22 02:37:34,489 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:37:34,535 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:35,264 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       registry.put((byte) (REMOVE_NOTIFICATION_LISTENER ^ RESPONSE_MASK), new MarshalledResponseHandler<Void>(VOID));
        registry.put((byte) (SEND_NOTIFICATION ^ RESPONSE_MA
 45%|████▌     | 45/100 [07:25<09:06,  9.94s/it]2024-12-22 02:37:35,479 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:35,972 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:35,972 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2315])
2024-12-22 02:37:36,064 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:37:37,484 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       # Test the discrete distribution
        dist = DiscreteDistribution(
            pmf=np.array([0.3, 0.5, 0.2]),
            X=[0, 1, 2],
            seed=42
        )
        draws =
 46%|████▌     | 46/100 [07:27<07:33,  8.40s/it]2024-12-22 02:37:37,619 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   try:
      result = cmd.Run(gopts, argv)
    except DownloadError as e:
      print(e, file=sys.stderr)
      result = 1
    except InvalidProjectGroupsError as e:
      print(e, file=sys.
 44%|████▍     | 44/100 [07:28<09:47, 10.50s/it]2024-12-22 02:37:37,710 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:37,888 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:38,942 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   for platform in Platform.objects.all():
        all_platform_slugs.append(platform.platform_slug)

    LanguageSet.objects.create(lang_set_name='Zanata', lang_set_slug='zanata', lang_set_color='#0
 43%|████▎     | 43/100 [07:29<08:45,  9.21s/it]2024-12-22 02:37:39,145 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:39,924 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:39,924 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3113])
2024-12-22 02:37:40,027 - [Process 0/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:0')
2024-12-22 02:37:41,513 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:41,513 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3047])
2024-12-22 02:37:41,636 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:37:42,959 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:42,959 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2905])
2024-12-22 02:37:42,968 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       },
        requires = {
                AndroidTestCase.class,
                AndroidContextTest.class,
                ContentProviderSaver.class,
                ContentProviderReader.class,
                MainActivity.class,
                MainActivityOhmletsTest.class,
                MainActivityTest.class,

 47%|████▋     | 47/100 [07:33<08:57, 10.15s/it]2024-12-22 02:37:43,065 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:37:43,113 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:44,453 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:44,454 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2823])
2024-12-22 02:37:44,564 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:37:44,789 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:                       viewManager.printToConsole();
                        viewManager.printToConsole();
                        viewManager.printToConsole();
                        viewManager.printToConsole();
                        viewManager.printToConsole();
                        viewManager.printToConsole();
                        viewManager
 46%|████▌     | 46/100 [07:35<08:50,  9.82s/it]2024-12-22 02:37:45,005 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:46,160 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       state = stateFactory.createState(VastAdInteractionSandBoxState.class);


        assertThat(state instanceof TestVastAdSandBox, is(true));


        //vpaid state

        state = stateFactory.createState(Vpa
 47%|████▋     | 47/100 [07:36<07:29,  8.48s/it]2024-12-22 02:37:46,368 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:46,369 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4435])
2024-12-22 02:37:46,421 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:46,533 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:37:47,672 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   public boolean add(SelectorGroupNode node) {
        return defaultValue();
    }

    @Override
    public boolean add(SelectorNode node) {
        return defaultValue();
    }

    @Override
    public boolean add(SelectorSegmentNode node) {

 44%|████▍     | 44/100 [07:38<08:27,  9.07s/it]2024-12-22 02:37:47,950 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:49,840 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:49,840 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3604])
2024-12-22 02:37:49,975 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:37:50,093 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           // 设TaskData.
            // 
            // 设Task
            TaskAPI.
            // 
            // 
            // 
            TaskAPI
            // 
            // 
            // 
            Task
            // 
            // 
            //
 45%|████▌     | 45/100 [07:40<10:09, 11.09s/it]2024-12-22 02:37:50,279 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:51,181 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:51,181 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2354])
2024-12-22 02:37:51,292 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:37:51,937 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:51,937 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3595])
2024-12-22 02:37:52,080 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:37:53,113 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def parse(self, packet):
        super(SignatureV4, self).parse(packet)
        self.version = packet[0]
        del packet[0]

        self.sigtype = SignatureType(packet[0])
        del packet[0
 48%|████▊     | 48/100 [07:43<08:47, 10.15s/it]2024-12-22 02:37:53,262 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:54,284 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:	public void acceptString(final StringTag tag, final CharSequence value) throws InvalidValueException {
		if (isValid(tag)) {
			acceptString(value);
		} else {
			throw new InvalidValueException(tag, value);
		
 48%|████▊     | 48/100 [07:44<07:15,  8.38s/it]2024-12-22 02:37:54,470 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:54,817 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:54,818 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2627])
2024-12-22 02:37:54,901 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:37:55,436 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def _render_descriptor_helper(self, descriptor, indent):
        lines = []

        if isinstance(descriptor, SequenceDescriptor):
            lines.append('{}{} {}'.format(indent, descriptor, descriptor.name))
            for member in
 47%|████▋     | 47/100 [07:45<08:53, 10.07s/it]2024-12-22 02:37:55,660 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:57,810 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       await r.on_request_successful(Entry(request=Request("http://example.com"), response=StaticResponse(404, {"Content-Type": "text/html"}, "<!DOCTYPE html><html><body>404 Not Found</body></html>")))


 46%|████▌     | 46/100 [07:48<09:04, 10.08s/it]2024-12-22 02:37:57,985 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:57,985 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4935])
2024-12-22 02:37:57,994 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:37:58,185 - [Process 3/5] - DEBUG - predict_token:tensor([[29892]], device='cuda:3')
2024-12-22 02:37:59,342 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:37:59,342 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3082])
2024-12-22 02:37:59,466 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:38:01,180 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:01,180 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2895])
2024-12-22 02:38:01,298 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:38:02,010 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:
































































 45%|████▌     | 45/100 [07:52<09:45, 10.65s/it]2024-12-22 02:38:02,251 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:02,490 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   elif is_element_node(item):
        yield item



@method(nullary('..'))
def select_ancestor(self, context=None):
    if context is None:
        raise self.missing_context()

    elif isinstance(context
 49%|████▉     | 49/100 [07:52<08:25,  9.92s/it]2024-12-22 02:38:02,597 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:04,460 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   parser.set_defaults(main=Main)
    return parser


if __name__ == "__main__":
    main = get_arg_parser().parse_args()
    main.main()
    sys.exit(0)







 48%|████▊     | 48/100 [07:54<08:27,  9.75s/it]2024-12-22 02:38:04,635 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:04,916 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:04,916 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 5174])
2024-12-22 02:38:05,125 - [Process 4/5] - DEBUG - predict_token:tensor([[5453]], device='cuda:4')
2024-12-22 02:38:05,168 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:05,168 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3739])
2024-12-22 02:38:05,311 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:38:07,803 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:07,803 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2852])
2024-12-22 02:38:07,921 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:38:08,103 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:08,103 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3135])
2024-12-22 02:38:08,211 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:38:08,864 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       //用户的邮箱
        stuEmail = utils.getEmail();
        //用户的手机号
        stuPhone = utils.getPhone();
        //用户的qq
        stuQQ = utils.getQQ
 47%|████▋     | 47/100 [07:59<09:09, 10.37s/it]2024-12-22 02:38:08,955 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:08,956 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2349])
2024-12-22 02:38:09,046 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:38:09,073 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:09,441 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:
































































 49%|████▉     | 49/100 [07:59<08:50, 10.41s/it]2024-12-22 02:38:09,565 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:11,114 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       final AbstractMeter[][] meterResults = benchRes.getMeterResults();
        final int numberOfMeters = meterResults.length;
        final int numberOfDataPoints = meterResults[0].length;
        final NiceTable table = new NiceTable(numberOfColumns,
 46%|████▌     | 46/100 [08:01<09:10, 10.19s/it]2024-12-22 02:38:11,168 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self, sink_properties, *args, **kwargs):
        super(HeapBalancerSink, self).__init__(sink_properties, *args, **kwargs)
        self.heap = Heap()
        self.heap_varz =
 50%|█████     | 50/100 [08:01<07:57,  9.54s/it]2024-12-22 02:38:11,295 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:11,374 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:12,054 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   contacts = Contact.objects.filter(phone_number=self.phone_number)


I'm getting an error in the line:

    def get_contacts(self):

TypeError: string indices must be integers

Please help me resolve this issue.


 49%|████▉     | 49/100 [08:02<07:44,  9.11s/it]2024-12-22 02:38:12,264 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:13,576 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:13,576 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2402])
2024-12-22 02:38:13,666 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:38:15,844 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:15,844 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3454])
2024-12-22 02:38:15,970 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:38:16,579 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   def parse(self, line):
        ...








def parse_qdisc_line(line):
    ...






def parse_filter_line(line):
    ...





def parse_iptables
 48%|████▊     | 48/100 [08:07<08:17,  9.57s/it]2024-12-22 02:38:16,852 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:17,660 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:17,660 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3434])
2024-12-22 02:38:17,705 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:17,705 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3800])
2024-12-22 02:38:17,788 - [Process 3/5] - DEBUG - predict_token:tensor([[418]], device='cuda:3')
2024-12-22 02:38:17,821 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:38:19,273 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                       expectLastCall().andReturn(EXPECTED_RESULT);
                        mocksControl.replay();

                        // WHEN
                        Throwable thrown = catchThrowable(() -> givenSut(systemUnderTestMock)
                                .givenArgument(() ->
 50%|█████     | 50/100 [08:09<08:31, 10.24s/it]2024-12-22 02:38:19,543 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:20,809 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:20,809 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4334])
2024-12-22 02:38:20,910 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       nc.connect(('127.0.0.1', 1234), protocol=protocol)
        nc.send_message('11:hello world,'.encode('utf-8'))
        self.assertEqual(protocol.send_string.call_
 51%|█████     | 51/100 [08:11<07:50,  9.60s/it]2024-12-22 02:38:20,981 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:38:21,016 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:21,044 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:     for edge in visitor.visit(node.out_edges(node)):
        yield edge
      yield edge

      parent_loop = None
      for edge in visitor.edges:
        if edge.kind == ControlFlow.E_LOOP:
          parent_loop
 47%|████▋     | 47/100 [08:11<08:55, 10.11s/it]2024-12-22 02:38:21,236 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:23,367 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:23,368 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3484])
2024-12-22 02:38:23,492 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:38:24,579 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   // Read.apply(KVoid(String)
    // Read.of(String)
    // Read.of(String
    // Read.
    PCollection<KVoid(String
    // Read.
    // Read.
    PCollection<String
    // Read
 50%|█████     | 50/100 [08:15<08:26, 10.13s/it]2024-12-22 02:38:24,728 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:24,729 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2800])
2024-12-22 02:38:24,763 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:24,836 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:38:25,868 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:25,869 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2648])
2024-12-22 02:38:25,965 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:38:26,846 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       except AttributeError:
            inv = []
        state["inventory"] = inv

    def add_location_property(self, state: Dict[str, Any], obj: MudObject) -> None:
        try:
            loc = obj.location     # type:
 49%|████▉     | 49/100 [08:17<08:18,  9.78s/it]2024-12-22 02:38:27,084 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:27,517 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:27,517 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3453])
2024-12-22 02:38:27,644 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:38:28,032 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def __getitem__(self, num):
        return self.elementList[num]
    
    def __setitem__(self, num, value):
        self.elementList[num] = value
    
    def __delitem__(self, num):
        del self.
 51%|█████     | 51/100 [08:18<07:59,  9.79s/it]2024-12-22 02:38:28,261 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:28,766 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       StatementExecution se = (StatementExecution) qe;
        assertEquals("query", se.getQuery(), "query");
        // end::query[]
    }

    public void parameter() {
        // tag::parameter[]
        ProxyTestDataSource ds = new Proxy
 52%|█████▏    | 52/100 [08:19<07:15,  9.08s/it]2024-12-22 02:38:28,879 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:30,895 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:					case HuffmanTree.name:
					this.tree = new HuffmanTree(this.k, this.m);
					break;
			}
		}
	}

	
	
	
 48%|████▊     | 48/100 [08:21<08:41, 10.03s/it]2024-12-22 02:38:31,178 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:33,308 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:33,309 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4247])
2024-12-22 02:38:33,484 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:38:34,327 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:34,327 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3139])
2024-12-22 02:38:34,453 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:38:34,790 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:34,790 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3033])
2024-12-22 02:38:34,911 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:38:35,193 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:35,194 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4383])
2024-12-22 02:38:35,347 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 02:38:37,039 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       def test_pattern = re.find_pattern = re.find_pattern = re.sample(sample(sample(sample(self.assert_pattern = re.assert_sample(sample(sample(sample(sample(sample(sample = assert_sample(sample(sample(sample = sample(
 51%|█████     | 51/100 [08:27<08:50, 10.83s/it]2024-12-22 02:38:37,261 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:37,303 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:37,304 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3162])
2024-12-22 02:38:37,433 - [Process 3/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:3')
2024-12-22 02:38:37,766 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def move_to(self, position):
        """Move the Stream to a specific endpoint or time, or load state from a token.

        Moving to an endpoint with "trim_horizon" or "latest" and loading from a previous token are both
        very efficient.


 52%|█████▏    | 52/100 [08:28<07:49,  9.78s/it]2024-12-22 02:38:37,874 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       Assert.assertThat(ds, executions(0, failure()));
    }

    @Test
    public void testPreparedBatchExecutionSuccess() {
        PreparedBatchExecution pbe = new PreparedBatchExecution();
        pbe.setSuccess(true);

 53%|█████▎    | 53/100 [08:28<07:07,  9.09s/it]2024-12-22 02:38:38,022 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:38,030 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:38,991 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   public Query visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit(Visit
 50%|█████     | 50/100 [08:29<08:44, 10.49s/it]2024-12-22 02:38:39,244 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:40,624 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def translate_pattern(pattern: str, flags: int = 0, xsd_version: str = '1.0',
                      back_references: bool = True, lazy_quantifiers: bool = True,
                      anchors: bool = True) -> str:

 49%|████▉     | 49/100 [08:31<08:26,  9.94s/it]2024-12-22 02:38:41,001 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:41,991 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:41,991 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2530])
2024-12-22 02:38:42,091 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:38:43,656 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:43,657 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2490])
2024-12-22 02:38:43,746 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:38:44,262 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:44,262 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3425])
2024-12-22 02:38:44,388 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:38:45,034 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       ffibuilderRX.cdef(preimageRXDA.header)
        ffibuilderRX.set_source(module_name, preimageRXDA.source)

        cls.tmpdirnameRX = tempfile.TemporaryDirectory()

 52%|█████▏    | 52/100 [08:35<07:59,  9.98s/it]2024-12-22 02:38:45,225 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:45,769 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:45,769 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3990])
2024-12-22 02:38:45,923 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:38:46,706 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:			this.schemaValidator = new SchemaValidatorImpl();
		this.cleaningFolder = true;
	}

	public void setSurveyName(String surveyName) {
		this.surveyName = surveyName;
	}

	public void generate
 51%|█████     | 51/100 [08:37<07:53,  9.66s/it]2024-12-22 02:38:46,919 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:47,465 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def _createLinkADRRequest(self, device):
        """Creates a Link ADR request message.
        
        Args:
            device (Device): The device to create the request for.
        
        Returns:
            A Link ADR request message object.
       
 54%|█████▍    | 54/100 [08:37<07:04,  9.24s/it]2024-12-22 02:38:47,636 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:49,416 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def bloquear_sat(self):
        """Sobrepõe :meth:`~satcfe.base.FuncoesSAT.bloquear_sat`.

        :return: Uma resposta SAT padrão.
        :r
 53%|█████▎    | 53/100 [08:39<08:05, 10.34s/it]2024-12-22 02:38:49,669 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:49,899 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:49,899 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2369])
2024-12-22 02:38:49,994 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:38:50,219 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:50,219 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4701])
2024-12-22 02:38:50,405 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:38:53,008 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   conditions.append(Condition(
        AndCondition(
            condition_for("gt", column=c),
            condition_for("lt", column=c),
        )
    )
)


def test_conditions_for():
    conditions = conditions_for("gt
 53%|█████▎    | 53/100 [08:43<07:20,  9.38s/it]2024-12-22 02:38:53,190 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:53,346 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:53,346 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3579])
2024-12-22 02:38:53,471 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:38:54,235 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:














































state
stateRo
stateRo










 50%|█████     | 50/100 [08:44<09:12, 11.04s/it]2024-12-22 02:38:54,509 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:54,714 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:54,715 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3769])
2024-12-22 02:38:54,857 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:38:56,059 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:38:56,060 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3244])
2024-12-22 02:38:56,187 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:38:56,784 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       try {

            //load dataDir and dataLogDir from command line parameters
            String dataDir = getDataDir(dataDirDir);
            String dataLogDir = getDataLogDir(dataLogDirDir);

            //load zxid from command line parameter
            long
 52%|█████▏    | 52/100 [08:47<07:49,  9.78s/it]2024-12-22 02:38:57,072 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:58,074 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                   function = FmllrRescoreFunction(args)
                    p = KaldiProcessWorker(i, return_queue, function, error_dict, stopped)
                    procs.append(p)
                    p.start()
                while True:
                   
 55%|█████▌    | 55/100 [08:48<07:14,  9.65s/it]2024-12-22 02:38:58,241 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:38:59,452 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   with pytest.raises(TypeError):
        engine.bind(ComplexModel)



def test_bind_model_with_stream(engine):
    class StreamModel(BaseModel):
        class Meta:
            stream = {
                "include": {"new
 54%|█████▍    | 54/100 [08:49<07:51, 10.25s/it]2024-12-22 02:38:59,632 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:00,120 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:00,120 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3515])
2024-12-22 02:39:00,259 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:39:00,865 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:00,866 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3413])
2024-12-22 02:39:00,994 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:39:02,940 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:02,940 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2518])
2024-12-22 02:39:03,042 - [Process 0/5] - DEBUG - predict_token:tensor([[1753]], device='cuda:0')
2024-12-22 02:39:03,594 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:03,594 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3471])
2024-12-22 02:39:03,668 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       for i in xrange(10):
            square = get_square_idx(i)
            print square



class TestRotateMatrix(object):
    """
    Question 6.19
    """

    def test_basic_example(self):
 54%|█████▍    | 54/100 [08:54<07:29,  9.76s/it]2024-12-22 02:39:03,719 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:39:04,098 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:04,388 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:	public List<Pipeline> generatePipelines(InFormat inFormat, OutFormat outFormat, PreProcessing preProcessing, PostProcessing postProcessing) {
		List<Pipeline> pipelines = new ArrayList<>();
		
		// In2Out Generator

 51%|█████     | 51/100 [08:54<08:47, 10.78s/it]2024-12-22 02:39:04,783 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:05,861 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:def show(self, with_trans=False):
    tr, re = '', ''
    if self.is_transparent():
        # todo: eval the best convinience here
        if not with_trans:
            return ''
        tr = ' - transparent'
    else
 56%|█████▌    | 56/100 [08:56<06:40,  9.09s/it]2024-12-22 02:39:05,880 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:05,880 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3206])
2024-12-22 02:39:06,006 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:39:06,010 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:07,030 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   def get_from_queue(self):
        try:
            return self.q_work.get()
        except:
            traceback.print_exc()
            return None

    def enqueue(self, target):
        try:
            self.q_
 53%|█████▎    | 53/100 [08:57<07:46,  9.92s/it]2024-12-22 02:39:07,328 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:08,762 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:08,762 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2704])
2024-12-22 02:39:08,851 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:39:09,301 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   assert u6.dimensions == mass
    assert u6.base_value == 1.0

    # nonzero MKS conversion factor
    u7 = Unit("kg/m**3 * s**-1")
    assert u7.dimensions == volume
    assert
 55%|█████▌    | 55/100 [08:59<07:35, 10.13s/it]2024-12-22 02:39:09,490 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:11,211 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:11,212 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2846])
2024-12-22 02:39:11,319 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:39:11,796 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   private final IJavaProject project;

    private final ILocalVariable localVariable;

    private final IMethod method;

    private final IType type;

    private final Text text;

    private final Button button;

    private final GroupMethodSelectionCtrl group;
 55%|█████▌    | 55/100 [09:02<06:57,  9.27s/it]2024-12-22 02:39:11,972 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:12,946 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:12,946 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4339])
2024-12-22 02:39:13,103 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:39:14,190 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       // Get the chat from the fragment manager
        chat = getChat();
        if (chat == null) {
            Log.e(getClass().getSimpleName(), "Chat is null");
            return;
        }
        // Update the UI with the chat information
       
 57%|█████▋    | 57/100 [09:04<06:21,  8.86s/it]2024-12-22 02:39:14,296 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:15,715 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:15,715 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4245])
2024-12-22 02:39:15,886 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:39:16,628 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       # pragma.topreleves_detallees = self.GetNext(self.GetNext(self.GetNext(self.GetNext(self.GetNext(self.GetNext(self.GetNext(self.GetNext(self.GetNext(self.GetNext(
 52%|█████▏    | 52/100 [09:07<08:58, 11.21s/it]2024-12-22 02:39:16,775 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:16,775 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3626])
2024-12-22 02:39:16,848 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:16,932 - [Process 4/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:4')
2024-12-22 02:39:18,221 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:18,221 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3102])
2024-12-22 02:39:18,347 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:39:19,548 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       MovieCursor movieCursor movieCursor.close();
        DataTestUtilities.close();
        MovieCursor movieCursor.query(MovieCursor.query(MovieCursor.query(MovieCursor.close();
        MovieCursor.query(MovieCursor.close();
        MovieCursor.query(MovieCursor
 54%|█████▍    | 54/100 [09:09<08:12, 10.70s/it]2024-12-22 02:39:19,780 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:20,505 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   ['Verify instance command references --no, returns paths, explicit ns',
     ['references', 'TST_Person.name="Mike"', '--namespace', 'root/cimv2', '--no'],
     {'stdout': ['"root/cimv2:
 56%|█████▌    | 56/100 [09:10<07:39, 10.45s/it]2024-12-22 02:39:20,845 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:21,522 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:```































































 56%|█████▌    | 56/100 [09:11<06:53,  9.41s/it]2024-12-22 02:39:21,750 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:22,865 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:22,865 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4415])
2024-12-22 02:39:23,036 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:39:23,433 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:23,433 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3581])
2024-12-22 02:39:23,562 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:39:26,210 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:26,211 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2170])
2024-12-22 02:39:26,310 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:39:26,465 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:
































































 58%|█████▊    | 58/100 [09:16<06:55,  9.89s/it]2024-12-22 02:39:26,630 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:26,862 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           H += dot(crosstens(m * J_rot), H_trans) + dot(I, H_rot)
        return H

    def compute_angular_momentum_jacobian_hessian(self, p):
        """
        Comput
 53%|█████▎    | 53/100 [09:17<08:33, 10.92s/it]2024-12-22 02:39:27,067 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:27,067 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3902])
2024-12-22 02:39:27,150 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:27,211 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:39:27,498 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:27,498 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3455])
2024-12-22 02:39:27,639 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:39:29,243 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:	public void setDebugImages(boolean debugImages) {
		this.debugImages = debugImages;
	}

	public void parse(LapdfDocument document) {
		AbstractModelFactory factory = new AbstractModelFactory();
		FrequencyCounter<Integer> frequencyCounter
 57%|█████▋    | 57/100 [09:19<06:22,  8.90s/it]2024-12-22 02:39:29,490 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:30,752 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       DevisAccueilModifications(self.inscrit, self.current_cotisation[0]).GenerateDevis()

    def EvtGenerationContrat(self, _):
        ContratAccueilModifications(self.inscrit, self.current_cotisation[0
 55%|█████▌    | 55/100 [09:21<08:08, 10.85s/it]2024-12-22 02:39:30,945 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:31,171 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       // Load modules
        List<Module> modules = new ArrayList<>();
        modules.add(new LoanModule());
        modules.add(new SqlModule());
        // Load plugins
        List<Plugin> plugins = new ArrayList<>();
        plugins.add(this);
        // Initialize
 57%|█████▋    | 57/100 [09:21<07:32, 10.52s/it]2024-12-22 02:39:31,363 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:32,797 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:32,798 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3234])
2024-12-22 02:39:32,922 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:39:34,074 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:34,075 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3703])
2024-12-22 02:39:34,212 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:39:35,974 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def loadPickle(self, dir_path, file_name):
    """ Load a pickle file.

    Arguments:
        dir_path: [str] Path of the directory where the pickle file will be stored.
        file_name: [str] Name
 59%|█████▉    | 59/100 [09:26<06:40,  9.77s/it]2024-12-22 02:39:36,214 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:37,549 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       View view = getView();
        if (view != null) {
            view.setVisibility(View.VISIBLE);
        }
        name.setText(queuedProjectResult.queuedProject.name);
        author.setText(queuedProjectResult.queuedProject.
 54%|█████▍    | 54/100 [09:28<08:19, 10.85s/it]2024-12-22 02:39:37,617 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:37,617 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3424])
2024-12-22 02:39:37,743 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:39:37,889 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:38,148 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:38,148 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4319])
2024-12-22 02:39:38,318 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:39:38,997 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:38,997 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4084])
2024-12-22 02:39:39,163 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:39:41,101 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       sh_ptr = c_void_p()
        self._call_fmod("FMOD_Sound_GetTag", index, byref(sh_ptr), name)
        return TAG(sh_ptr)

    def get_length(self):
        """Ret
 58%|█████▊    | 58/100 [09:31<07:14, 10.34s/it]2024-12-22 02:39:41,350 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:41,903 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   var result = docerConf.class -> file.
    var files.add(Javadoc.class -> add(Javadoc.add(Javadoc.add(Javadoc.add(Javoc.add(Javoc.add(Javoc
 58%|█████▊    | 58/100 [09:32<07:01, 10.03s/it]2024-12-22 02:39:42,152 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:42,763 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertEqual(etree_iter_nodes(self.node).next()


        self.assertEqual(etree_iter_nodes(self.node).next()

        self.assertEqual(etree_iter_nodes(self.node).next()


 56%|█████▌    | 56/100 [09:33<08:12, 11.20s/it]2024-12-22 02:39:42,954 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:44,259 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:44,259 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4208])
2024-12-22 02:39:44,424 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:39:45,646 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:45,646 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4071])
2024-12-22 02:39:45,801 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:39:47,485 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:47,486 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2773])
2024-12-22 02:39:47,591 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:39:47,717 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:47,717 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2453])
2024-12-22 02:39:47,806 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 02:39:47,808 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:47,808 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3358])
results:       if (mPublicRoomsListListList != null) {
            mPublicRoomsList = new ArrayList<PublicRoom>();
            mPublicRoomsList = new ArrayList<PublicRoom>();
            mPublicRoomsList = new ArrayList<PublicRoom();
        }


 60%|██████    | 60/100 [09:38<06:55, 10.39s/it]2024-12-22 02:39:47,813 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:39:47,937 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:47,943 - [Process 4/5] - DEBUG - predict_token:tensor([[462]], device='cuda:4')
2024-12-22 02:39:49,357 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       });
    }

    private void initializeFiles() {
        // Initialize currency files
        currencyManager.load();
        currencyEconService = new CurrencyEconService();
        // Initialize virtual files
        virtualEconService = new VirtualDataService();
    }


 55%|█████▌    | 55/100 [09:39<08:21, 11.14s/it]2024-12-22 02:39:49,567 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:50,735 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       return RespostaSAT.analisar(retorno)

    def consultar_numero_sessao(self):
        """Sobrepõe :meth:`~satcfe.base.FuncoesSAT.consultar_numero_s
 59%|█████▉    | 59/100 [09:41<06:36,  9.67s/it]2024-12-22 02:39:50,914 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:			setContentView(R.layout.map_view_activity);
			mapView = (MapView) findViewById(R.id.map_view);
			locationManager = (LocationManager) getSystemService(Context.LOCATION_SERVICE);
			
 57%|█████▋    | 57/100 [09:41<07:22, 10.29s/it]2024-12-22 02:39:50,921 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:51,366 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:51,390 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                   @Override
                    public void startNewOrderActivity() {
                        startActivity(NewOrderActivity.newIntent(MainActivity.this));
                    }

                    @Override
                    public void startAccountEditActivity(Account account) {
                        startActivity(AccountEdit
 59%|█████▉    | 59/100 [09:41<07:03, 10.32s/it]2024-12-22 02:39:51,625 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:51,830 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:51,830 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2524])
2024-12-22 02:39:51,899 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:39:54,559 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       self.bingo_data_generator = BingoDataGenerator()
        self.bingo_card_manager = BingoCardManager()
        self.bingo_card_writer = BingoCardWriter()


    @commands.Cog.listener()
    async def on
 61%|██████    | 61/100 [09:45<06:02,  9.30s/it]2024-12-22 02:39:54,698 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:39:56,361 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:56,361 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2798])
2024-12-22 02:39:56,434 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:56,434 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3640])
2024-12-22 02:39:56,478 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:39:56,571 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:39:58,569 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:58,570 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3751])
2024-12-22 02:39:58,707 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:39:59,391 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:39:59,392 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4085])
2024-12-22 02:39:59,556 - [Process 2/5] - DEBUG - predict_token:tensor([[462]], device='cuda:2')
2024-12-22 02:39:59,762 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       registerCallAction(new SMSReplyAction(this), SMSReplyAction.SMS_REPLY_ACTION_ID);
        registerCallAction(new ToggleMicrophoneAction(this), ToggleMicrophoneAction.TOGGLE_MICROPHONE_
 60%|██████    | 60/100 [09:50<06:19,  9.48s/it]2024-12-22 02:39:59,954 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:00,005 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def _fit(self, *args, **kwargs):
        """
        Fit the model using the MCMC sampler.

        Parameters
        ----------
        *args
        **kwargs

        Returns
        -------

        """
        _log.
 56%|█████▌    | 56/100 [09:50<08:03, 10.99s/it]2024-12-22 02:40:00,185 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:01,982 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:01,982 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4050])
2024-12-22 02:40:02,121 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:40:02,224 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:	Call<ImgurResponseWrapper<List<Image>>> listAccountImages(
			@Path("username") String userName,
			@Path("page") int page );

	@GET("/3/account/{username}/images/ids/{page}")
	Call<
 60%|██████    | 60/100 [09:52<06:59, 10.48s/it]2024-12-22 02:40:02,476 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:03,155 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:                   mushroom = new MushroomBlue(world, new Vector2(Mushroom.DEF_SIZE, Mushroom.DEF_SIZE), new Vector3(position));
                    break;
                case Item.TYPE_MUSHROOM_GHOST:
                   
 58%|█████▊    | 58/100 [09:53<07:36, 10.87s/it]2024-12-22 02:40:03,362 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:05,371 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       chim_detect = ChimeraDetector(raw_bp_graphs, target_sequences,
                                     phylogeny, naming_ref)

    #running synteny backend to get synteny blocks
    for stage in run_stages:
       
 62%|██████▏   | 62/100 [09:55<06:10,  9.75s/it]2024-12-22 02:40:05,569 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:06,553 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:06,553 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3325])
2024-12-22 02:40:06,680 - [Process 3/5] - DEBUG - predict_token:tensor([[418]], device='cuda:3')
2024-12-22 02:40:06,838 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:06,838 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3487])
2024-12-22 02:40:06,974 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:40:08,598 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:08,598 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2809])
2024-12-22 02:40:08,708 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:40:10,038 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:10,038 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3964])
2024-12-22 02:40:10,115 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:     return Const.fromValue(arg)
    elif op == LOAD_GLOBAL:
      return Ref.fromValue(arg)
    else:
      raise ValueError("Unsupported load opcode: %s" % op)
  else:
    raise ValueError
 57%|█████▋    | 57/100 [10:00<07:41, 10.73s/it]2024-12-22 02:40:10,188 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:40:10,338 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:10,424 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   try:
      result = self.commands[name](gopts, argv)
    except DownloadError as e:
      result = 1
      print(e, file=sys.stderr)
      git_trace2_event_log.Write(e)
    except
 61%|██████    | 61/100 [10:00<06:23,  9.83s/it]2024-12-22 02:40:10,832 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:11,876 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           self.write_line("  Internet protocol processing enabled")
            self.write_line("  IP address 10.10.10.10/30")
            self.write_line("  IP address 10.10.10.11/
 59%|█████▉    | 59/100 [10:02<06:59, 10.23s/it]2024-12-22 02:40:12,151 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:13,091 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:13,091 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3938])
2024-12-22 02:40:13,244 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:40:13,718 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       resource = ThermometerResource(aircraft.instruments.thermometer)

        thermometer_data = resource.get()

        self.assertAlmostEqual(aircraft.instruments.thermometer.temperature, thermometer_data["temperature"], 
 61%|██████    | 61/100 [10:04<07:00, 10.78s/it]2024-12-22 02:40:13,910 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:15,652 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:15,652 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2867])
2024-12-22 02:40:15,761 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:40:16,386 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:16,386 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2897])
2024-12-22 02:40:16,503 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:40:16,524 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       plugins.registerEvents(new BlockPlaceListener(this), this);
        plugins.registerEvents(new InteractListener(this), this);
        plugins.registerEvents(new SignChangeListener(this), this);
        plugins.registerEvents(new SignLockerCommand(this), this);
 63%|██████▎   | 63/100 [10:06<06:16, 10.17s/it]2024-12-22 02:40:16,605 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:18,356 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:18,357 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3363])
2024-12-22 02:40:18,482 - [Process 2/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:2')
2024-12-22 02:40:18,960 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:		public ClientMaster(ResourceLoader res) {
			this.res = res;
		}

	public void dispose() {
		if (!disposed) {
			disposed = true;
			controllers.removeAll();
		
 58%|█████▊    | 58/100 [10:09<07:06, 10.16s/it]2024-12-22 02:40:19,254 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:19,733 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   @Named @Singleton
    ImageSaver provideImageSaver() {
        return new ImageSaver(app);
    }

    @Provides @Singleton
    FileCache provideFileCache() {
        return new FileCache(app.getExternalCacheDir(), FILE
 62%|██████▏   | 62/100 [10:10<06:07,  9.68s/it]2024-12-22 02:40:19,919 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:20,176 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:20,176 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3396])
2024-12-22 02:40:20,303 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:40:20,870 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:20,871 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2369])
2024-12-22 02:40:20,951 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:40:21,819 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results: assert isinstance(config, LowRateConfig)



def test_selectOutputConfig1():
  '''
  Output configuration selection test
  '''
  config = selectOutputConfig('normal_rate')
Next line of code:
  assert isinstance(config, NormalRateConfig
 60%|██████    | 60/100 [10:12<06:45, 10.14s/it]2024-12-22 02:40:22,082 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:23,628 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:               // 加载 MiPush 客户端
                MiPushClient.init(this, miSettings.getString("mi_app_id", ""), miSettings.getString("mi_app_key", ""));
                startMiPush();
                Log.e(MYTAG
 64%|██████▍   | 64/100 [10:14<05:33,  9.25s/it]2024-12-22 02:40:23,664 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   column = User.age
    value = 3
    expected_ref = ":v0.#n0"
    expected_action = actions.set({"N": "3"})
    expected_values = {":v0.#n0": expected_action.value}
 62%|██████▏   | 62/100 [10:14<06:40, 10.53s/it]2024-12-22 02:40:23,673 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:23,987 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:26,738 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:26,738 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3529])
2024-12-22 02:40:26,873 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:40:27,321 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:27,321 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4136])
2024-12-22 02:40:27,476 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:40:29,588 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:29,588 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3955])
2024-12-22 02:40:29,737 - [Process 2/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:2')
2024-12-22 02:40:29,803 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:29,803 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3336])
2024-12-22 02:40:29,928 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:40:30,333 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       Assert.assertThat(ds, totalQueryCount(5));


        // assertions
        assertThat(ds, totalQueryCount(5));
        assertThat(ds, totalQueryCount(is(5)));
    }


    @Test
    public void testP
 63%|██████▎   | 63/100 [10:20<06:08,  9.95s/it]2024-12-22 02:40:30,581 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:31,065 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       }
        }
        stage.show();
    }

    private void onQuickBut(ActionEvent event) {
        QuickPresenter presenter = new QuickPresenter();
        QuickView view = new QuickView();
        presenter.setQuickView(view);
 59%|█████▉    | 59/100 [10:21<07:20, 10.74s/it]2024-12-22 02:40:31,243 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:32,073 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:32,073 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4009])
2024-12-22 02:40:32,240 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:40:32,998 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       registry.put((byte) (QUERY_NAMES ^ RESPONSE_MASK), new MarshalledResponseHandler<Set<String>>(SET_STRING_ARRAY));
        registry.put((byte) (REMOVE_NOTIFICATION_LISTENER ^ RES
 65%|██████▌   | 65/100 [10:23<05:25,  9.29s/it]2024-12-22 02:40:33,167 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:33,229 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       return RespostaConsultarUltimaSessaoFiscal.analisar(retorno)

    def consultar_numero_sessao(self, numero_sessao):
        """Sobrepõe :meth:`~satcfe.
 61%|██████    | 61/100 [10:23<06:50, 10.52s/it]2024-12-22 02:40:33,424 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:35,791 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       } catch (NoSuchBuildNumberException e) {
            e.printStackTrace();
            result = new MethodResult<Integer>(-1);
        } catch (NoSuchVersionException e) {
            e.printStackTrace();
            result = new MethodResult<Integer>(-1);
 63%|██████▎   | 63/100 [10:26<06:47, 11.01s/it]2024-12-22 02:40:35,941 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:36,555 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:36,555 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2767])
2024-12-22 02:40:36,660 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:40:37,507 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:37,508 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3616])
2024-12-22 02:40:37,650 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:40:38,189 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:38,189 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2924])
2024-12-22 02:40:38,285 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:40:39,802 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:	public List<AuthorWithBooks> findAll() {
		List<AuthorWithBooks> list = new ArrayList<>();
		AtomicInteger atomicInteger = new AtomicInteger();
		jdbcTemplate.query("SELECT * FROM AUTHOR, BOOK WHERE AUTHOR.
 60%|██████    | 60/100 [10:30<06:45, 10.14s/it]2024-12-22 02:40:40,009 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:41,087 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:41,088 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4010])
2024-12-22 02:40:41,115 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   return new SvnJavaChangeLogCommand();
}

    protected SvnCommand getBranchCommand()
    {
        return new SvnJavaBranchCommand();
    }

    protected SvnCommand getCheckInCommand()
    {
        return new SvnJavaCheck
 64%|██████▍   | 64/100 [10:31<06:07, 10.20s/it]2024-12-22 02:40:41,142 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   public PersonalVulnsWindow() {
        initComponents();
        setTitle("Personal Vulns");
        setDefaultCloseOperation(3);
        setResizable(false);
        // setIconImage(ToolTipManager.getCurrentToolTip().getImage());

 66%|██████▌   | 66/100 [10:31<05:04,  8.94s/it]2024-12-22 02:40:41,241 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:40:41,261 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:41,417 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:42,828 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:42,829 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3492])
2024-12-22 02:40:42,965 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:40:44,770 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   ax.set_xlabel('Grid points')
    ax.set_ylabel(f'Predicted {output}')
    ax.set_title(f'Interaction Predicted Values')
    ax.legend(phase_legend(dbf, comps, phase_name
 62%|██████▏   | 62/100 [10:35<06:51, 10.83s/it]2024-12-22 02:40:44,993 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:45,220 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:45,221 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2759])
2024-12-22 02:40:45,319 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:40:46,368 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   form = self.form_class(request.POST, instance=post)
        else:
            form = self.form_class()
        return render(request, self.template_name, {'form' : form, 'post' : post})

    def post(self,
 64%|██████▍   | 64/100 [10:36<06:31, 10.88s/it]2024-12-22 02:40:46,497 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:47,443 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:47,443 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3202])
2024-12-22 02:40:47,528 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:47,528 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3310])
2024-12-22 02:40:47,560 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:40:47,652 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:40:48,348 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results: public SQLQuery createQuery(String name) {
    return new SQLQuery(name, schema);
  }
}





































 61%|██████    | 61/100 [10:38<06:16,  9.66s/it]2024-12-22 02:40:48,547 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:50,545 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:50,546 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2964])
2024-12-22 02:40:50,661 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:40:50,697 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   public void addGCTrace(GCTrace gcTrace) {
        // ...
    }
}

src/main/java/gchisto/gctrace/GCTrace.java
public class GCTrace {

    private static final int[][] gcActivity
 67%|██████▋   | 67/100 [10:41<05:01,  9.13s/it]2024-12-22 02:40:50,788 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       verifiers.put("LocalFilesExistVerifier", new LocalFilesExistVerifierImpl(processor));
        verifiers.put("RemoteFilesExistVerifier", new RemoteFilesExistVerifierImpl(processor));
        verifiers.put("ValidSchemaNameVerifier", new
 65%|██████▌   | 65/100 [10:41<05:51, 10.04s/it]2024-12-22 02:40:50,828 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:51,022 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:51,135 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:51,136 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2418])
2024-12-22 02:40:51,230 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:40:53,926 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   _add_fdm_resources(api, fdm, aircraft)
















































 63%|██████▎   | 63/100 [10:44<06:22, 10.33s/it]2024-12-22 02:40:54,211 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:54,307 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   Call<EmojiResponse> getEmoji();


    @GET("tags?platform=android&type=emoji")
    Call<EmojiResponse> getEmoji();

    @GET("trending_gifs")
    Call<TrendingG
 65%|██████▌   | 65/100 [10:44<05:49, 10.00s/it]2024-12-22 02:40:54,501 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:55,608 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:55,608 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2439])
2024-12-22 02:40:55,707 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:40:55,748 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:55,748 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3638])
2024-12-22 02:40:55,901 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:40:58,138 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:40:58,138 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3808])
2024-12-22 02:40:58,281 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:40:58,486 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       if (packet.getType() == PacketType.ACK) {
            AckRequest ackRequest = new AckRequest(packet, client);
            ackManager.ack(ackRequest);
        } else if (packet.getType() == PacketType.
 68%|██████▊   | 68/100 [10:48<04:39,  8.73s/it]2024-12-22 02:40:58,621 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:40:59,337 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       auto_app_test(adb, app_info)
        # download_logs(adb, download_dir)
        # dynamic_main(file_path)
        # 结束动态分析
        stop_device(adb)
       
 62%|██████▏   | 62/100 [10:49<06:22, 10.06s/it]2024-12-22 02:40:59,487 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:00,693 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:00,693 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3591])
2024-12-22 02:41:00,820 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:41:01,726 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           showProgress(false);
        }
    }

    private boolean yasmeDeviceCheck() {
        // Check if there is a device in the Database
        SharedPreferences sharedPreferences = getSharedPreferences();
        long deviceId = sharedPreferences.getLong(AbstractYasmeActivity
 66%|██████▌   | 66/100 [10:52<05:50, 10.31s/it]2024-12-22 02:41:01,906 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:02,997 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:02,997 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4263])
2024-12-22 02:41:03,172 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:41:03,938 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:03,939 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2526])
2024-12-22 02:41:04,030 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:41:04,155 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   def get_utterance_features(self, utterance: Utterance) -> None:
        """
        Calculate the features for an utterance

        Parameters
        ----------
        utterance: Utterance
            Utterance for which to calculate features


 64%|██████▍   | 64/100 [10:54<06:10, 10.30s/it]2024-12-22 02:41:04,515 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:05,607 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:05,607 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3493])
2024-12-22 02:41:05,750 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:41:06,821 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           @NonNull
            Disconnect();
            @NonNull
            Disconnect();
            @NonNull
            Disconnect();
            @NonNull
            connect();
            @NonNull
            connect();
            @NonNull
            connect();
            @NonNull
            connect
 66%|██████▌   | 66/100 [10:57<06:05, 10.75s/it]2024-12-22 02:41:06,953 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   Genotype parseGenotypes(InputStream inputStream);


    /**
     * Parse the specified input stream and return a trait.
     *
     * @param inputStream input stream, must not be null
     * @return the specified input stream parsed into a trait
     */
 63%|██████▎   | 63/100 [10:57<05:45,  9.33s/it]2024-12-22 02:41:07,096 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:07,134 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:08,212 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:08,213 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3357])
2024-12-22 02:41:08,340 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:41:08,893 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       lMS.addTrack(createVideoTrack("video", appRtcClient.videoConstraints().videoTrack()));
      }
      videoSource = new VideoSource(lMS.nativeStream);
      videoSource.setOnDispose(new Runnable() {
        public void run() {
 69%|██████▉   | 69/100 [10:59<04:46,  9.23s/it]2024-12-22 02:41:08,967 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:10,291 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:10,291 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3189])
2024-12-22 02:41:10,408 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:41:11,564 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   DrupalUser login(String username, String password) throws DrupalLoginException, DrupalFetchException;









































 67%|██████▋   | 67/100 [11:02<05:35, 10.17s/it]2024-12-22 02:41:11,770 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:13,588 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   private CameraSource mCameraSource;
    private OcrDetectorProcessor mOcrDetectorProcessor;
    private ExpenseManagerDAO mExpenseManagerDAO;
    private DisplayMetrics mDisplayMetrics;
    private RelativeLayout mRootLayout;
    private FrameLayout
 65%|██████▌   | 65/100 [11:04<05:51, 10.04s/it]2024-12-22 02:41:13,724 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:13,724 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3490])
2024-12-22 02:41:13,824 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:13,851 - [Process 3/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:3')
2024-12-22 02:41:14,297 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:14,297 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3768])
2024-12-22 02:41:14,442 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:41:15,238 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:15,238 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3259])
2024-12-22 02:41:15,364 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:41:17,110 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:               Symbol.typed(bar_type, 'bar'),
                Symbol.typed(Record.typed(IntType, [
                    {'baz': IntType},
                ]), 'baz'),
            ]),
        ]),
        {'inc': inc_type, 'bar
 64%|██████▍   | 64/100 [11:07<05:44,  9.58s/it]2024-12-22 02:41:17,308 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:17,827 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           LightSensorCollectorManager.flushDBCache(deviceID);
        }
        if(type == 6 || type == 0) {
            PressureSensorCollector.flushDBCache(deviceID);
        }
        if(type == 7 || type == 
 67%|██████▋   | 67/100 [11:08<05:57, 10.83s/it]2024-12-22 02:41:18,034 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:18,079 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:18,080 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2176])
2024-12-22 02:41:18,168 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:41:18,410 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           contentValues.put(JobStorage.COLUMN_BACKOFF_MS, 1000L);
            contentValues.put(JobStorage.COLUMN_BACKOFF_POLICY, 1);
            contentValues.put(JobStorage.COLUMN_RE
 70%|███████   | 70/100 [11:08<04:39,  9.32s/it]2024-12-22 02:41:18,666 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:18,908 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:18,908 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3725])
2024-12-22 02:41:19,049 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:41:21,016 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:	public static Util getInstance() {
		return instance;
	}
}













































 66%|██████▌   | 66/100 [11:11<05:14,  9.25s/it]2024-12-22 02:41:21,240 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:22,413 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       self.history.add(text)
        for q in quirks:
            q.apply(text)
        self.textInput.clear()
        self.textInput.setFocus()
        self.textArea.clear()
        self.textArea.insertPlainText
 68%|██████▊   | 68/100 [11:12<05:31, 10.37s/it]2024-12-22 02:41:22,666 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:24,071 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:24,071 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3145])
2024-12-22 02:41:24,174 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:41:24,778 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:24,778 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3617])
2024-12-22 02:41:24,914 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:41:27,127 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        // Set the content view
        setContentView(R.layout.settings);

        // Find preferences
        preferenceMenuBarPosition = findPreference("menu_bar_position");
 71%|███████   | 71/100 [11:17<04:24,  9.14s/it]2024-12-22 02:41:27,237 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:27,259 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:27,259 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2445])
2024-12-22 02:41:27,349 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:41:28,249 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       for (PlaySession playSession : stack.getPlaySessions()) {
            playSession.addListener(this);
        }
    }

    private void removeListenersFromStack(Stack stack) {
        stack.removeListener(this);
        for (Card card :
 68%|██████▊   | 68/100 [11:18<05:42, 10.71s/it]2024-12-22 02:41:28,546 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:28,622 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:28,622 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4015])
2024-12-22 02:41:28,765 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:41:28,917 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:28,917 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 5517])
2024-12-22 02:41:29,149 - [Process 3/5] - DEBUG - predict_token:tensor([[4351]], device='cuda:3')
2024-12-22 02:41:30,321 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           (r'/logout', LogOutHandler),
            (r'/register', RegisterHandler),
            (r'/problem', ProblemHandler),
            (r'/status', StatusHandler),
            (r'/debug', DebugHandler),
            (r'/problemlist
 69%|██████▉   | 69/100 [11:20<04:58,  9.63s/it]2024-12-22 02:41:30,552 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:32,204 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       List<ServerConfiguration> serverConfigurations = new ArrayList<>();
        File mongoDir = new File(System.getProperty("mongo.home"), "config");
        File[] files = mongoDir.listFiles(new FileFilter() {
            @Override
            public boolean accept(File
 67%|██████▋   | 67/100 [11:22<05:24,  9.83s/it]2024-12-22 02:41:32,412 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:32,465 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:32,465 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2993])
2024-12-22 02:41:32,569 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:41:33,212 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:

























































//
Љ
 65%|██████▌   | 65/100 [11:23<06:43, 11.53s/it]2024-12-22 02:41:33,557 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:35,205 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:35,205 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3317])
2024-12-22 02:41:35,342 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:41:35,491 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   # ...





























































 72%|███████▏  | 72/100 [11:25<04:09,  8.90s/it]2024-12-22 02:41:35,604 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:36,324 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:36,324 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3121])
2024-12-22 02:41:36,442 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:41:37,056 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:37,056 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2717])
2024-12-22 02:41:37,146 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:41:38,683 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self, *args, **kwargs):
        super().__init__(**kwargs)
        self.vad_path = args[0]
        self.segmentation_options = args[1]

    def run(self):
        self.log_k
 69%|██████▉   | 69/100 [11:29<05:29, 10.62s/it]2024-12-22 02:41:38,874 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:39,608 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:				public ExpansionResult visit(AlvisIRNearQueryNode nearQueryNode, Void param) {
				List<MatchExplanation> explanations = getNearQueryNodeExplanations(nearQueryNode);
				return expandAtom(ex
 70%|███████   | 70/100 [11:30<04:45,  9.53s/it]2024-12-22 02:41:39,951 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:40,210 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:		Fluent header = container.section("header");
		Fluent h1 = header.h1("TodoMVC");
		Fluent inputNext = header.inputNext();
		h1.txt("Welcome to TodoMVC!");
	
 68%|██████▊   | 68/100 [11:30<04:57,  9.29s/it]2024-12-22 02:41:40,583 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:40,668 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:40,669 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2999])
2024-12-22 02:41:40,765 - [Process 0/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:0')
2024-12-22 02:41:41,683 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:41,683 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4335])
2024-12-22 02:41:41,838 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:41:42,385 - [Process 0/5] - INFO - res.shape is :torch.Size([36])
results:   def determine_executable(self, desktop_file):
        # ...

Please provide more context or specify the exact line you want to know more about.
 73%|███████▎  | 73/100 [11:32<03:44,  8.30s/it]2024-12-22 02:41:42,538 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:43,319 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:43,319 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2480])
2024-12-22 02:41:43,411 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:41:45,358 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:																																																																
 66%|██████▌   | 66/100 [11:35<06:38, 11.72s/it]2024-12-22 02:41:45,633 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:46,348 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   private final Jobs jobs;
    private final Deployments deployments;
    private final Errands errands;
    private final SpringTasks tasks;
    private final SpringVms vms;

    public SpringDirectorClient(RestTemplate restTemplate, URI root) {
       
 70%|███████   | 70/100 [11:36<04:52,  9.74s/it]2024-12-22 02:41:46,543 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:47,386 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:47,386 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3583])
2024-12-22 02:41:47,551 - [Process 1/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:1')
2024-12-22 02:41:49,139 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:49,139 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4164])
2024-12-22 02:41:49,312 - [Process 2/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:2')
2024-12-22 02:41:50,699 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:50,699 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4285])
2024-12-22 02:41:50,863 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:41:50,986 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:               throw new ProtTestInternalException("Invalid selection criterion: " + criterion + " + " + " + " + models + " + " + " + " + " + " + " + " + " + " + " + " + " + " + " + " + " + "
 71%|███████   | 71/100 [11:41<04:52, 10.08s/it]2024-12-22 02:41:51,159 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:51,882 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:51,883 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3334])
2024-12-22 02:41:52,008 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:41:52,864 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:               //compress(imageTaken, output);
                //compress(imageTaken, output;
                //compress(imageTaken, output;
                //compress(imageTaken, output;
                //compress(imageTaken, output;
                //compress
 69%|██████▉   | 69/100 [11:43<05:19, 10.30s/it]2024-12-22 02:41:52,896 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:52,896 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3300])
2024-12-22 02:41:53,024 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:41:53,145 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:54,250 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   Iterable<Board its = new Iterable<Board>();
    its = new Iterable<Board>();
    its = new Iterable<>();
    its = new Iterable<>();
    List<Board> its = new Iterable<>();
    List<Board its = new Iterable<>();
   
 74%|███████▍  | 74/100 [11:44<04:03,  9.37s/it]2024-12-22 02:41:54,370 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:55,211 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                           return new NotFoundException(cause);
                        default:
                            return new APIIncorrectException(cause);
                    }
                } else {
                    return new APIIncorrectException(cause);
                }
            }
        };
        // Set
 67%|██████▋   | 67/100 [11:45<06:08, 11.16s/it]2024-12-22 02:41:55,389 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:56,271 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       ckresult(self._call_fmod("FMOD_Sound_GetTag", index, byref(tag), byref(name)))
        return tag

    def get_length(self):
        """Retrieve the length of the sound in seconds.

       
 71%|███████   | 71/100 [11:46<04:43,  9.79s/it]2024-12-22 02:41:56,509 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:41:57,236 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:57,236 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3250])
2024-12-22 02:41:57,354 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:41:59,243 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:59,243 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3048])
2024-12-22 02:41:59,267 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:41:59,267 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2649])
2024-12-22 02:41:59,364 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:41:59,370 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:42:00,530 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   return Pair(ty1, ty2, e1, e2)



@with_info(st_typ)
def type_of(expr):
    """Return the type of an expression.
    
    Arguments:
    - `expr`: an expression

 72%|███████▏  | 72/100 [11:50<04:37,  9.92s/it]2024-12-22 02:42:00,861 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:01,427 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:01,427 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3179])
2024-12-22 02:42:01,553 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:42:02,170 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       DatabaseManager.INSTANCE.getUserDAO().getUsers(new GetImageWithoutSavingTask() {
            @Override
            protected Boolean doInBackground(Long... params) {
                //TODO: Get image from server
                // ...
            }
        });
   
 75%|███████▌  | 75/100 [11:52<03:43,  8.94s/it]2024-12-22 02:42:02,400 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:02,541 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {
        View view = inflater.inflate(R.layout.song_list_fragment, container, false);

        mRecyclerView = (RecyclerView) view.findViewById(R.
 70%|███████   | 70/100 [11:52<05:03, 10.11s/it]2024-12-22 02:42:02,763 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:02,839 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:02,840 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3300])
2024-12-22 02:42:02,965 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:42:04,738 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:			registerRenderer(Iterable.class, new DefaultIterableRenderer());
		registerRenderer(Token.class, new DefaultTokenRenderer());
		registerRenderer(IfToken.class, new DefaultIfTokenRenderer());
		registerRenderer(SilentErrorHandler.class, new Default
 68%|██████▊   | 68/100 [11:55<05:41, 10.67s/it]2024-12-22 02:42:04,940 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:05,649 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:05,649 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2448])
2024-12-22 02:42:05,746 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:42:06,201 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   def process_section(self, bufr_message, bit_writer, section):
        """
        Encodes a section of the BUFR message.

        :param bufr_message: The BUFR message object.
        :param bit_writer: The bit writer object
 72%|███████▏  | 72/100 [11:56<04:35,  9.83s/it]2024-12-22 02:42:06,376 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:07,652 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:07,653 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2985])
2024-12-22 02:42:07,758 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:42:08,665 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   void inject(KioskModeHandler kioskModeHandler);
    void inject(EventBus eventBus);
    void inject(AnalyticsTracker analyticsTracker);
    void inject(Player player);
    void inject(ConfigurationContentProvider configurationContentProvider);
    void inject(
 73%|███████▎  | 73/100 [11:59<04:13,  9.39s/it]2024-12-22 02:42:08,913 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:09,607 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:09,607 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3502])
2024-12-22 02:42:09,744 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:42:10,690 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   if (writeTML) {
	    // generate TML
	    generateTML(model, testDataFactories, monitor);
	} else {
	    // generate Java
	    generateJava(model, testDataFactories, monitor);
	}
	return null
 76%|███████▌  | 76/100 [12:01<03:31,  8.81s/it]2024-12-22 02:42:10,808 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:12,732 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:12,732 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3405])
2024-12-22 02:42:12,860 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:42:13,036 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   with pytest.raises(InvalidSearch):
    search = Search(
        engine=engine, model=model, index=index, key=key, filter=None, projection="all", consistent=True, forward=False)
    search.mode = "query"
    search.
 71%|███████   | 71/100 [12:03<04:56, 10.23s/it]2024-12-22 02:42:13,260 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:13,844 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:13,844 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2629])
2024-12-22 02:42:13,942 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:42:14,311 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:14,311 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4640])
2024-12-22 02:42:14,503 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:42:16,120 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       form = ReportAbuseForm(request.POST)
        if form.is_valid():
            # Log moderation event
            log_moderator_event(msg_type=ModerationLogMsg.WARNING,
                              user=logged_against,

 73%|███████▎  | 73/100 [12:06<04:26,  9.86s/it]2024-12-22 02:42:16,347 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:17,030 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   public ChartCategoryFragment() {
        // Required empty public constructor
    }



    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container, Bundle savedInstanceState) {
        View view = inflater.inflate(R.layout.chart_category, container
 74%|███████▍  | 74/100 [12:07<03:56,  9.08s/it]2024-12-22 02:42:17,223 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:18,334 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:











Name
Name
Name
Name
Name
Name
Name
Name
Name
Name
Name
Name
Name
Name
Name
Name
Name





Name





Name
name
name

 69%|██████▉   | 69/100 [12:08<05:57, 11.55s/it]2024-12-22 02:42:18,613 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:19,276 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:19,276 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3039])
2024-12-22 02:42:19,400 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:42:21,885 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:21,885 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2742])
2024-12-22 02:42:22,008 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:42:22,515 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:22,515 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2958])
2024-12-22 02:42:22,582 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           GCActivitySet gcActivitySet,GCActivity gcActivity) {
        // do nothing
    }

    public void gcActivityNameAdded(
            GCTrace gcTrace,int id,String gcActivityName) {
        // do nothing
    }

 72%|███████▏  | 72/100 [12:13<04:40, 10.02s/it]2024-12-22 02:42:22,620 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:42:22,812 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:23,937 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:23,938 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 6165])
2024-12-22 02:42:24,200 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:42:24,542 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:24,543 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3049])
2024-12-22 02:42:24,660 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:42:25,145 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   @Override
    public void doOutput(final AbstractResult paramResult) {
        // do nothing
    }

    @Override
    public void doOutput(final BenchmarkResult paramResult) {
        // do nothing
    }

    @Override
    public void doOutput
 74%|███████▍  | 74/100 [12:15<04:09,  9.61s/it]2024-12-22 02:42:25,559 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:25,777 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   response = PyramidSwaggerResponse(
        Mock(
            body='{"error": "Content-Type missing"}',
            content_type='application/json',
            status_code=500
        )
    )
    with pytest.raises(ResponseValidation
 75%|███████▌  | 75/100 [12:16<03:44,  8.98s/it]2024-12-22 02:42:26,018 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:27,512 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:27,512 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2442])
2024-12-22 02:42:27,606 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:42:27,857 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   url(r'^(?P<pipeline_id>[\w\-\+]+)/(?P<job_id>[0-9a-f-]+)/(?P<release_id>[0-9a-f-]+)/detail$', PipelineDetailView.as_view
 70%|███████   | 70/100 [12:18<05:28, 10.94s/it]2024-12-22 02:42:28,200 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:28,473 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:.
.
....
























































 77%|███████▋  | 77/100 [12:18<04:24, 11.50s/it]2024-12-22 02:42:28,636 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:30,501 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   @login_required
    @wait_for(timeout=TIMEOUT)
    @inlineCallbacks
    def get(self, *args, **kwargs):
        """Method to get all applications"""
        try:
            apps = yield Application.all()
            if apps is
 73%|███████▎  | 73/100 [12:20<04:13,  9.39s/it]2024-12-22 02:42:30,738 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:30,861 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:30,861 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2427])
2024-12-22 02:42:30,966 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:42:32,612 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:32,612 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3552])
2024-12-22 02:42:32,756 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:42:33,935 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:	@Override
	public void enterAddOrDeleteEdgeCommand(AddOrDeleteEdgeCommandContext context) {
		// ...
	}
}































 76%|███████▌  | 76/100 [12:24<03:29,  8.73s/it]2024-12-22 02:42:34,158 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:35,534 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:35,534 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3843])
2024-12-22 02:42:35,659 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:42:35,844 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:35,844 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2771])
2024-12-22 02:42:35,941 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:42:36,036 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:36,036 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4184])
2024-12-22 02:42:36,130 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       // Initializing the application options
        ApplicationOptions options = factory.getApplicationOptions();
        options.setProperty(ApplicationOptions.PROTTEST_HOME, ".");
        options.setProperty(ApplicationOptions.PROTTEST_MANUAL, URL_MANUAL);
        options
 75%|███████▌  | 75/100 [12:26<04:10, 10.02s/it]2024-12-22 02:42:36,182 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:42:36,317 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:38,822 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           showResponse = true;
            textareaMessage.setSyntaxEditingStyle(SyntaxConstants.SYNTAX_STYLE_HTML);
            textareaMessage.setEditable(true);
            textareaMessage.setLineWrap(true);
            textareaMessage.set
 78%|███████▊  | 78/100 [12:29<04:05, 11.16s/it]2024-12-22 02:42:38,938 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 02:42:38,940 - [Process 0/5] - INFO - len(per_windows_prompt):2
results:       self.tree_view = tree_view
        self.tree_view.setModel(self._df_manager)
        self.tree_view.setRootIndex(self._df_manager.get_root_index())
        self.tree_view.setSelectionBehavior(self
 74%|███████▍  | 74/100 [12:29<03:56,  9.10s/it]2024-12-22 02:42:39,160 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:39,500 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:39,500 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2986])
2024-12-22 02:42:39,608 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:42:39,633 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                       startActivity(intent);
                    } catch (Throwable t) {
                        LogUtil.e(t, "Error applying wallpaper", t);
                    }
                }
            } else if (id == R.id.longClick) {
                if (s
 71%|███████   | 71/100 [12:30<05:24, 11.19s/it]2024-12-22 02:42:39,830 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:40,644 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:40,644 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2283])
2024-12-22 02:42:40,736 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:42:42,716 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def add_file(self, file: File) -> None:
        """
        Add a file to the corpus

        Parameters
        ----------
        file: :class:`~montreal_forced_aligner.corpus.classes.File`
            File
 77%|███████▋  | 77/100 [12:33<03:21,  8.75s/it]2024-12-22 02:42:42,927 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:43,604 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:			tropo.record(URL("http://example.com/record"));
		
		tropo.sendTones(true);
		tropo.mute(true);
		tropo.exitTone(true);
		

 76%|███████▌  | 76/100 [12:34<03:42,  9.26s/it]2024-12-22 02:42:43,792 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:45,893 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:45,893 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3658])
2024-12-22 02:42:45,926 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:45,926 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3627])
2024-12-22 02:42:46,029 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:42:46,063 - [Process 2/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:2')
2024-12-22 02:42:46,997 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:46,998 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3815])
2024-12-22 02:42:47,142 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:42:49,198 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   assert compare_tables(SimpleModel, description)



@pytest.mark.parametrize("table_status, gsi_status, expected_status", [
    ("ACTIVE", "ACTIVE", ready),
    ("ACTIVE", None, ready),
   
 79%|███████▉  | 79/100 [12:39<03:49, 10.92s/it]2024-12-22 02:42:49,303 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:49,400 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   if __name__ == "__main__":
        main()
    else:
        parser.parse_args()


I'm not sure what you're trying to achieve by copying the code, but it seems that you want to use the `tcconfig` module to configure traffic
 75%|███████▌  | 75/100 [12:39<03:57,  9.51s/it]2024-12-22 02:42:49,519 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:49,519 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3096])
2024-12-22 02:42:49,573 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:49,574 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3328])
2024-12-22 02:42:49,626 - [Process 4/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:4')
2024-12-22 02:42:49,647 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:49,710 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:42:50,569 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   account = account_for_vimeo_id(access_token.user_id)


I'm getting an error:

TypeError: cannot use a string as a function


I'm not sure what is happening here, but it seems like the `account_
 72%|███████▏  | 72/100 [12:41<05:11, 11.11s/it]2024-12-22 02:42:50,913 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:52,762 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:				// Verificar se a resposta está correta ou não
				if (resposta.getNota() != null && resposta.getNota() > 0) {
					// Se a resposta está correta, adiciona
 77%|███████▋  | 77/100 [12:43<03:32,  9.23s/it]2024-12-22 02:42:52,951 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:53,026 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   for sensor in sensors:
        api.add_resource(sensor.get_resource(), sensor.get_path())

    ...

    api.add_resource(PitotTubeResource, "/aircraft/sensors/pitot_tube",
                     resource
 78%|███████▊  | 78/100 [12:43<03:22,  9.22s/it]2024-12-22 02:42:53,204 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:54,048 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:54,048 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2448])
2024-12-22 02:42:54,132 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:42:55,283 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:55,283 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3116])
2024-12-22 02:42:55,409 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:42:57,016 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   private final Map<ChatChannel, ChannelStatus> channels = Maps.newHashMap();

    private final Map<ChatChannel, Channel> channelsToDraw = Maps.newHashMap();

    private int lastChannelId = 0;

    private int lastChannelX = 0
 76%|███████▌  | 76/100 [12:47<03:34,  8.94s/it]2024-12-22 02:42:57,316 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:57,529 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:57,529 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2370])
2024-12-22 02:42:57,611 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:42:58,260 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:58,260 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3552])
2024-12-22 02:42:58,415 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:42:58,428 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def save(self, filething=None, v1=1, v2_version=4, v23_sep='/', padding=None):
        """save(filething=None, v1=1, v2_version=4, v23_sep='/
 80%|████████  | 80/100 [12:48<03:28, 10.41s/it]2024-12-22 02:42:58,579 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:42:59,026 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:42:59,026 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3206])
2024-12-22 02:42:59,143 - [Process 4/5] - DEBUG - predict_token:tensor([[12]], device='cuda:4')
2024-12-22 02:43:00,585 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       airspeed_indicator = AirspeedIndicator(fdmexec)
        airspeed = airspeed_indicator.airspeed
        self.assertAlmostEqual(airspeed, gps.airspeed)


















 79%|███████▉  | 79/100 [12:51<03:03,  8.72s/it]2024-12-22 02:43:00,787 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:01,923 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:
































































 73%|███████▎  | 73/100 [12:52<05:02, 11.19s/it]2024-12-22 02:43:02,146 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:02,313 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:				public void onResult(int code, DefaultDataConnector connector) {
					if (code == 0) {
						ProgramManager programManager = new ProgramManager();
						programManager.parseTrack("file
 78%|███████▊  | 78/100 [12:52<03:25,  9.32s/it]2024-12-22 02:43:02,544 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:03,545 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:03,546 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3455])
2024-12-22 02:43:03,672 - [Process 2/5] - DEBUG - predict_token:tensor([[308]], device='cuda:2')
2024-12-22 02:43:06,931 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:        */
        void onDateClick(Date date);
     }

    public VCalendar() {
        initWidget(outer);

        // Set the style name
        setStyleName(PRIMARY_STYLE);

        // Set the first day of the week
       
 77%|███████▋  | 77/100 [12:57<03:32,  9.23s/it]2024-12-22 02:43:07,103 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:07,103 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4224])
2024-12-22 02:43:07,129 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:07,273 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:07,274 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2519])
2024-12-22 02:43:07,282 - [Process 0/5] - DEBUG - predict_token:tensor([[418]], device='cuda:0')
2024-12-22 02:43:07,375 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:43:08,921 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:08,922 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4021])
2024-12-22 02:43:09,093 - [Process 1/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:1')
2024-12-22 02:43:09,144 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:09,144 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3529])
2024-12-22 02:43:09,284 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:43:10,328 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   @Inject
    public ExoPlayerFSMTest() {
        super();
    }

    @Before
    public void setUp() {
        // Set up mocks for media models
        when(movieMedia.getVideoUrl()).thenReturn("http://example.com/
 79%|███████▉  | 79/100 [13:00<03:07,  8.93s/it]2024-12-22 02:43:10,516 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:10,701 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   public Query() {
    visit(Search
    public Query()
    visit(Search
    public Query()
   
   
    public Query
   
   
    public Query
   
   
    public Query
    public Query
    public Query
    public Query
    public
 81%|████████  | 81/100 [13:01<03:28, 10.97s/it]2024-12-22 02:43:10,842 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:12,746 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   CMD_OPTION_HELP_LINE,
    CMD_OPTION_HELP_LINE,
    CMD_HELP_LINE,
    CMD_HELP_LINE,
    CMD_HELP_LINE,
    CMD
 80%|████████  | 80/100 [13:03<03:15,  9.75s/it]2024-12-22 02:43:12,761 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   grid = _gen_grid(options['density'], options['param_ests'])


    # Define function to approximate posterior distribution
    def psyfunction(x, scale, slope, gamma, lambda_):
        # Calculate posterior distribution using numerical integration
        return _calc
 74%|███████▍  | 74/100 [13:03<04:48, 11.08s/it]2024-12-22 02:43:13,079 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:13,178 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:13,956 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:13,957 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3487])
2024-12-22 02:43:14,093 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:43:16,997 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:16,997 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3387])
2024-12-22 02:43:17,123 - [Process 0/5] - DEBUG - predict_token:tensor([[462]], device='cuda:0')
2024-12-22 02:43:17,367 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       self._db_type = db_type
        return self

    def set_client_id(self, client_id):
        self._client_id = client_id
        return self

    def set_user(self, user):
        self._user = user
 78%|███████▊  | 78/100 [13:07<03:31,  9.60s/it]2024-12-22 02:43:17,583 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:19,005 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:19,006 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3320])
2024-12-22 02:43:19,115 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:43:20,188 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:20,188 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4805])
2024-12-22 02:43:20,213 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:                   new EdgeTransformation(),
                    new EmbossTransformation(),
                    new GlowTransformation(),
                    new GradientTransformation(),
                    new HueTransformation(),
                    new InvertTransformation(),
                    new LaplacianTransformation(),
                
 82%|████████▏ | 82/100 [13:10<03:09, 10.53s/it]2024-12-22 02:43:20,314 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:20,382 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 02:43:21,881 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:21,881 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4512])
2024-12-22 02:43:22,043 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:43:22,301 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:	@RequestMapping(value = "/manage", method = RequestMethod.GET)
	public ModelAndView manage(@PathVariable Long id, ModelAndView mav) {
		mav.setViewName("manage");
		return mav;
	}
	

 81%|████████  | 81/100 [13:12<03:04,  9.69s/it]2024-12-22 02:43:22,433 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:22,659 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:22,659 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2805])
2024-12-22 02:43:22,761 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:43:24,222 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:                       
 
 




   
   _
_ _        
                  
 80%|████████  | 80/100 [13:14<03:28, 10.42s/it]2024-12-22 02:43:24,453 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:25,649 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 
    // 

 75%|███████▌  | 75/100 [13:16<04:50, 11.62s/it]2024-12-22 02:43:25,831 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:25,831 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:25,831 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2985])
2024-12-22 02:43:25,877 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           Namespace namespace = namespacesHub.get(packet.getNsp());
            namespace.onUpgrade(client);
            break;
        }

        case DISCONNECT: {
            client.getBaseClient().disconnect();
            Namespace namespace = namespaces
 79%|███████▉  | 79/100 [13:16<03:14,  9.27s/it]2024-12-22 02:43:25,948 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:43:26,142 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:28,449 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:28,449 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3216])
2024-12-22 02:43:28,566 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:43:28,914 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def _decode_field(self, field):
        try:
            value = struct.unpack(field[1], self._input_buffer[field[0]:])
            return value
        except struct.error as e:
            raise PyOrientBadMethodCallException(

 83%|████████▎ | 83/100 [13:19<02:49,  9.98s/it]2024-12-22 02:43:29,009 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:31,739 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
2024-12-22 02:43:31,740 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:31,741 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2952])
results:                           logError(title + ": " + t.getMessage());
                        } else {
                            logError(title + ": " + t.getMessage());
                        }
                    }
                }
            });
        } finally {
            setControlsEnabled(true);
           
 82%|████████▏ | 82/100 [13:22<02:53,  9.62s/it]2024-12-22 02:43:31,857 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:43:31,911 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:31,911 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3833])
2024-12-22 02:43:31,962 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:32,064 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:43:33,636 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:33,636 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4086])
2024-12-22 02:43:33,794 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:43:35,134 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   private List<StarItem> starItemList = new ArrayList<StarItem>();

    private CommitTask commitTask;
    private List<CommitItem> commitItemList = new ArrayList<CommitItem>();

    private RepoContentTask repoContentTask;
    private List<
 80%|████████  | 80/100 [13:25<03:05,  9.27s/it]2024-12-22 02:43:35,421 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:35,478 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:35,478 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3449])
2024-12-22 02:43:35,613 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:43:35,629 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   String adapterName = adapterName(type.getQualifiedName());
    String injectorName = adapterName(INJECT_ADAPTER_SUFFIX);
    String injectorPackage = getPackage(type).getQualifiedName().toString();

    JavaFile javaFile = Java
 81%|████████  | 81/100 [13:26<03:23, 10.72s/it]2024-12-22 02:43:35,842 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:36,690 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:36,691 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2630])
2024-12-22 02:43:36,781 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:43:37,308 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       paths = list(etree_iter_paths(root))


Expected output:
['/', '/a', '/a/b1', '/a/b1/c1', '/a/b1/c2', '/a/b2', '/a/b3',
 76%|███████▌  | 76/100 [13:27<04:39, 11.63s/it]2024-12-22 02:43:37,509 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:38,728 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   api = BMUNITSEARCH(args.apikey)
    if not api.get_data(**{'documenttype': 'BM Unit Search'}) is False:
        print("No data returned.")
        return None

    fmt = StdoutFormatter("1
 84%|████████▍ | 84/100 [13:29<02:38,  9.93s/it]2024-12-22 02:43:38,868 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:39,764 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       def run_e2e_flow_for_language(self, language, person_name, join_keyword):
            # Create a new contact with the given name in the given language
            contact = Contact.objects.create(name=person_name, language=language)
            #
 83%|████████▎ | 83/100 [13:30<02:35,  9.14s/it]2024-12-22 02:43:39,995 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:43,117 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:43,117 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2957])
2024-12-22 02:43:43,234 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:43:43,646 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:43,646 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4197])
2024-12-22 02:43:43,814 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:43:45,425 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:45,425 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4783])
2024-12-22 02:43:45,535 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:45,535 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3584])
2024-12-22 02:43:45,616 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:43:45,670 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:43:45,773 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:45,773 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2896])
2024-12-22 02:43:45,898 - [Process 1/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:1')
2024-12-22 02:43:46,386 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       self.runner.add(ContentSimhashSampling(), with_child=True)
        self.runner.add(ContentSignature())
        self.runner.add(SimilarPathGenerator())
        self.runner.add(RejectStatusCode())
        self.runner.add
 77%|███████▋  | 77/100 [13:36<04:09, 10.87s/it]2024-12-22 02:43:46,637 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:47,442 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:				LOGGER.info("Creating post-processors...");
			Postprocessor[] postprocessors = setPostProcessors(pipeline.getPostProcessing());
			
			LOGGER.info("End of setPostProcessors");
		
 81%|████████  | 81/100 [13:37<03:13, 10.18s/it]2024-12-22 02:43:47,646 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:48,794 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:						item = new GalleryAlbum( proxy );
				} else {
					item = new GalleryImage( proxy );
				}
				items.add( item );
			}

			
 85%|████████▌ | 85/100 [13:39<02:29,  9.97s/it]2024-12-22 02:43:48,922 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:49,057 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results: @Override
  protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    // Set the theme to match the current API level
    // Set the theme to match the current API level
    setTheme(PlacesConstants.getThemeForApiLevel(getApplication
 84%|████████▍ | 84/100 [13:39<02:26,  9.19s/it]2024-12-22 02:43:49,242 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:49,415 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:
































































 82%|████████▏ | 82/100 [13:39<03:29, 11.64s/it]2024-12-22 02:43:49,676 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:52,940 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:52,940 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2295])
2024-12-22 02:43:53,024 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:43:53,947 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:53,948 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3737])
2024-12-22 02:43:54,093 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:43:54,404 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:54,404 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3606])
2024-12-22 02:43:54,540 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:43:54,801 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:54,801 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3204])
2024-12-22 02:43:54,898 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:43:55,712 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   assert dump_key(user, user_key)



def test_extract_key(engine):
    user = User(id="foo")
    key = {"id": {"S": "foo"}}
    assert extract_key(user, key) == user


 86%|████████▌ | 86/100 [13:46<02:06,  9.06s/it]2024-12-22 02:43:55,830 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:57,320 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:43:57,320 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3949])
2024-12-22 02:43:57,474 - [Process 4/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:4')
2024-12-22 02:43:57,578 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   cfe = CFeCancelamento(
            chCanc=_opcao('--ch-canal-num'),
            # (!) ...
            # (!) ...
            # (!) ...
            # (!) ...
            # (!) ...
            # (!) ...
 78%|███████▊  | 78/100 [13:48<04:01, 10.96s/it]2024-12-22 02:43:57,730 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:57,892 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   PCollection<KV<List<String>, String>> projectKeys = pipeline.apply("Read project keys", Read.from(
        new LiveProjectSource(org)));
    // Convert project keys to GCPResource objects.
    PCollection<GCPResource> projects = projectKeys.apply
 82%|████████▏ | 82/100 [13:48<03:04, 10.26s/it]2024-12-22 02:43:58,100 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:43:58,120 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   connect(adb)


def get_identifier():
    return getADB(TOOLSDIR)

def refresh_avd(adb, avd_path, reference_name, dup_name, emulator):
    refresh_avd(adb, avd
 85%|████████▌ | 85/100 [13:48<02:17,  9.15s/it]2024-12-22 02:43:58,414 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:00,338 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:00,338 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2435])
2024-12-22 02:44:00,427 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:44:00,955 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results: assert isinstance(args.gps_sv[0].getL1CAMessage(), CNavMessage)
  assert args.gps_sv[0].getL1CAMessage().bitValue == 1



def test_parameters_encoder0():
  '''

 83%|████████▎ | 83/100 [13:51<03:17, 11.61s/it]2024-12-22 02:44:01,233 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:02,381 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:02,381 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2386])
2024-12-22 02:44:02,474 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:44:02,721 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:02,722 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2571])
2024-12-22 02:44:02,811 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:44:03,181 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       new ProgressBar(10, BAR_EQUALS),
        new Percentage(100),
        new Fraction(10),
        new Status(""),
        new TaskName("My Task"),
        new StaticString(""),
        new Spinner
 87%|████████▋ | 87/100 [13:53<01:51,  8.58s/it]2024-12-22 02:44:03,346 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:05,354 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   List<UserName> parseUserNames(InputStream inputStream);


    /**
     * Parse the specified input stream and return a carrier.
     *
     * @param inputStream input stream, must not be null
     * @return the specified input stream parsed into a carrier

 79%|███████▉  | 79/100 [13:55<03:30, 10.01s/it]2024-12-22 02:44:05,488 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:05,489 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3897])
2024-12-22 02:44:05,583 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:05,623 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:44:05,748 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   return MyModel()



def test_model_creation(session):
    # ...

def test_model_mutation(session):
    # ...

def test_model_validation(session):
    # ...

def test_model_query(session
 83%|████████▎ | 83/100 [13:56<02:42,  9.54s/it]2024-12-22 02:44:05,950 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:08,380 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:08,381 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3820])
2024-12-22 02:44:08,525 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:44:08,984 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self, *args, **kwargs):
        self.Layer = Layer()
        self.Pt = 0
        self.bit_len = 0
        self.Repr = None
        self.Dict = None
        self.Trans = None

 86%|████████▌ | 86/100 [13:59<02:15,  9.66s/it]2024-12-22 02:44:09,212 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:10,977 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:10,978 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4063])
2024-12-22 02:44:11,133 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:44:11,932 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   if is_xml_codepoint(ord(suffix)):
        return f'/{suffix}'
    else:
        return f'/{path}{suffix}'

def is_xml_codepoint(c: int) -> bool:
    return c in (
 84%|████████▍ | 84/100 [14:02<03:02, 11.42s/it]2024-12-22 02:44:12,118 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:12,474 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:12,474 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3482])
2024-12-22 02:44:12,600 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:44:13,812 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:13,813 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4318])
2024-12-22 02:44:13,965 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:44:14,455 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:	public static <T> T tag(Class<T> clazz, String name, Object... attributes) {
		return (T) tag(clazz, name, attributes);
	}





















 88%|████████▊ | 88/100 [14:04<01:52,  9.39s/it]2024-12-22 02:44:14,574 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:15,872 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   def apply(self, input_ids, input_mask, output_mask):
        # ...










































 84%|████████▍ | 84/100 [14:06<02:35,  9.71s/it]2024-12-22 02:44:16,070 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:17,458 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           EtsiResponse response = this.client.sendSignature(req(req);

            }
            }







































 80%|████████  | 80/100 [14:07<03:32, 10.64s/it]2024-12-22 02:44:17,560 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:17,561 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4252])
2024-12-22 02:44:17,729 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:44:17,752 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:18,511 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:18,511 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3247])
2024-12-22 02:44:18,640 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:44:20,791 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:20,791 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3306])
2024-12-22 02:44:20,915 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:44:21,271 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:					# Create adjacency set
									
						
					# Create adjacency set
						
					# Create adjacency set
				
 87%|████████▋ | 87/100 [14:11<02:15, 10.45s/it]2024-12-22 02:44:21,465 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:21,867 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   t = Table()
    t.add_column(k, v, dtype=np.float64, index=True, align='top')
    t.add_column(k, v, dtype=np.float64, index=True, align='top')
    t
 85%|████████▌ | 85/100 [14:12<02:44, 10.97s/it]2024-12-22 02:44:22,001 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:22,001 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3075])
2024-12-22 02:44:22,064 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:22,118 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:44:23,975 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self, data):
        self.id = data.get("id")
        self.name = data.get("name")
        self.description = data.get("description")
        self.status = data.get("status")
        self.type = data
 89%|████████▉ | 89/100 [14:14<01:43,  9.43s/it]2024-12-22 02:44:24,065 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:24,569 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:24,569 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3638])
2024-12-22 02:44:24,707 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:44:25,274 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   public ChatManager(ChatManagerCallback delegate)
    {
        _delegate = delegate;
        _lockObject.lock();
        try
        {
            // Set up SSL sockets.
            for (int i = 0; i < RX_SOCKET_
 85%|████████▌ | 85/100 [14:15<02:24,  9.62s/it]2024-12-22 02:44:25,490 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:28,030 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:
































































 81%|████████  | 81/100 [14:18<03:21, 10.62s/it]2024-12-22 02:44:28,303 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:28,467 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:28,468 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3669])
2024-12-22 02:44:28,602 - [Process 1/5] - DEBUG - predict_token:tensor([[418]], device='cuda:1')
2024-12-22 02:44:29,137 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:29,138 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3622])
2024-12-22 02:44:29,283 - [Process 4/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:4')
2024-12-22 02:44:29,836 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:29,836 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3020])
2024-12-22 02:44:29,952 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:44:30,414 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:30,414 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2631])
2024-12-22 02:44:30,513 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:44:31,928 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:     result.append(" = ");
    } else {
      result.append(" ").append(exp.getOperator());
      acceptOrVisitValue(exp.getRhsValue(), baseVariableName);
    }
    result.append(CLOSE_BRACKET);

 88%|████████▊ | 88/100 [14:22<02:06, 10.51s/it]2024-12-22 02:44:32,071 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:32,740 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               TypingMessage m = (TypingMessage)message;

                if (_isTyping != m.isTyping())
                {
                    _isTyping = m.isTyping();
                    if (_isTyping)
                    {
                        addMessage(
 86%|████████▌ | 86/100 [14:23<02:33, 10.94s/it]2024-12-22 02:44:32,901 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   Matcher<? super ParameterHolder> parameterHolderMatcher) {
        return BatchParameterHolderAssertions.batch(index, parameterHolderMatcher);
    }

    ///////////////////////////////////////////////////////////////////////////////////////////
    // CallableBatchExecutionAssertions
    //////////////////////////////////////////////////////////////////////////////////
 90%|█████████ | 90/100 [14:23<01:32,  9.28s/it]2024-12-22 02:44:32,985 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:33,119 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:33,627 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   private void onItemClick(AdapterView.OnItemClickListener listener, View view, int position, long id) {
        CustomGallery item = dataT.get(position);
        if (item != null) {
            if (item.isVideo()) {
                // Handle video item
 86%|████████▌ | 86/100 [14:24<02:09,  9.24s/it]2024-12-22 02:44:33,861 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:35,414 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:35,415 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3750])
2024-12-22 02:44:35,557 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:44:38,918 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           RotationVectorSensorCollector.flushDBCache(deviceID);
        }
        if(type == 12 || type == 0) {
            RelativeHumiditySensorCollector.flushDBCache(deviceID);
        }
        if(type ==
 82%|████████▏ | 82/100 [14:29<03:12, 10.70s/it]2024-12-22 02:44:39,102 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:40,289 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:40,289 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3707])
2024-12-22 02:44:40,305 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:40,305 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4260])
2024-12-22 02:44:40,346 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:40,346 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3806])
2024-12-22 02:44:40,441 - [Process 4/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:4')
2024-12-22 02:44:40,471 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:44:40,496 - [Process 0/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:0')
2024-12-22 02:44:41,309 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:41,310 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3904])
2024-12-22 02:44:41,458 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:44:43,730 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:           applyWallpaper();
        } else if (id == R.id.menu_cancel) {
            WallpapersAdapter.sIsClickable = false;
        }
    }

    private void applyWallpaper() {
        Wallpaper wallpaper = Preferences.
 91%|█████████ | 91/100 [14:34<01:27,  9.74s/it]2024-12-22 02:44:43,814 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:43,814 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2477])
2024-12-22 02:44:43,863 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:43,912 - [Process 3/5] - DEBUG - predict_token:tensor([[12]], device='cuda:3')
2024-12-22 02:44:43,979 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:			SobelZ  = reg_rm_ancova_two_bs_factor(data,
					EXOG,
					dmy_covariates=dmy_covariates,
					output_fvalues
 87%|████████▋ | 87/100 [14:34<02:23, 11.03s/it]2024-12-22 02:44:44,015 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       cacheService.addCacheList(listName);
        cacheService.addCache(new Cache(listName);
        cacheService.save();
      }
    }
  }
  }


  }


  @FXML
  public void updateStatus(String
 89%|████████▉ | 89/100 [14:34<02:00, 10.98s/it]2024-12-22 02:44:44,182 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:44,324 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:45,012 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   RotateInDownRight(RotateInDownRight.class),
    RotateInUpLeft(RotateInUpLeft.class),
    RotateInUpRight(RotateInUpRight.class),

    RotateOut(RotateOut.class),
    Rotate
 87%|████████▋ | 87/100 [14:35<02:08,  9.88s/it]2024-12-22 02:44:45,187 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:46,853 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:	public Team(String teamId) {
		this.teamSummary = new TeamSummary(teamId);
		this.roster = new ArrayList<>();
	}

	public TeamSummary getTeamSummary() {
		return teamSummary;
	}

	public void
 83%|████████▎ | 83/100 [14:37<02:47,  9.87s/it]2024-12-22 02:44:47,063 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:50,000 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:50,000 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3102])
2024-12-22 02:44:50,110 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:44:50,668 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:50,668 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3543])
2024-12-22 02:44:50,803 - [Process 0/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:0')
2024-12-22 02:44:51,012 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:51,013 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3309])
2024-12-22 02:44:51,151 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:44:52,492 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:52,492 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3912])
2024-12-22 02:44:52,635 - [Process 2/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:2')
2024-12-22 02:44:53,277 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       assertThat(main.getLock(), nullValue());

        // Wait for main thread to be ready
        pause(1000);

        // Check if main thread is still running
        assertThat(main.getStatus(), equalTo(ThreadStatus.RUNNABLE));
 88%|████████▊ | 88/100 [14:43<02:06, 10.51s/it]2024-12-22 02:44:53,591 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:53,664 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:53,664 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3622])
2024-12-22 02:44:53,790 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:44:53,929 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results: private static final int[][][] BOOL_TO_LUCENE = new int[10][2];
  static {
    BOOL_TO_LUCENE[0][0] = 0; // AND
    BOOL_TO_LUCENE[
 92%|█████████▏| 92/100 [14:44<01:19,  9.88s/it]2024-12-22 02:44:54,097 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:54,438 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   'CallName': 'SM-CP',
    'ReprName': 'SM-CP',
    'Pt': 0,
    'PtFunc': None,
    'Val': 0,
    'Len': 0,
    'BitLen': 0
 90%|█████████ | 90/100 [14:44<01:48, 10.82s/it]2024-12-22 02:44:54,650 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:56,091 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:           return render(request, self.template_name, {'form': form})
        else:
            raise Http404("User not found")

class CrearUser(UserBase, CreateView):
    template_name = 'relevamiento/crear_user.html'
 88%|████████▊ | 88/100 [14:46<02:02, 10.24s/it]2024-12-22 02:44:56,305 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:57,094 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       FeedbackUseCase feedbackUseCase = new FeedbackUseCase(getContext());
        feedbackUseCase.sendFeedback(userFeedback, new Function1<Unit, Unit>() {
            @Override
            public Unit invoke(Unit unit) {
                onSuccess();
               
 84%|████████▍ | 84/100 [14:47<02:39,  9.98s/it]2024-12-22 02:44:57,325 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:44:59,385 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:44:59,386 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3200])
2024-12-22 02:44:59,502 - [Process 4/5] - DEBUG - predict_token:tensor([[18884]], device='cuda:4')
2024-12-22 02:45:00,841 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:00,841 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3337])
2024-12-22 02:45:00,964 - [Process 1/5] - DEBUG - predict_token:tensor([[462]], device='cuda:1')
2024-12-22 02:45:01,988 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:01,989 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4080])
2024-12-22 02:45:02,153 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:45:02,682 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:               //  Calendar calendar = Calendar.getInstance();
                //  calendar.set(Calendar.YEAR, 2015);
                //  calendar.set(Calendar.MONTH, 11);
                //  calendar.set(Calendar.DAY_OF_MONTH, 
 89%|████████▉ | 89/100 [14:53<01:51, 10.18s/it]2024-12-22 02:45:02,953 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:03,208 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:03,208 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3612])
2024-12-22 02:45:03,350 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:45:04,153 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:                       TimeEntryOvertimeAggregation.createAggregation(timeEntriesTable, dataManager, workTimeConfigBean, timeSource))
        );

        groupsDl.setParameter("user", userSession.getUser());
        groupsDl.load();

       
 91%|█████████ | 91/100 [14:54<01:34, 10.49s/it]2024-12-22 02:45:04,392 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:04,627 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:04,627 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3734])
2024-12-22 02:45:04,772 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:45:05,507 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:
		#ARG_CHUNKER.hdf5(self.pytable.hdf5.hdf5.hdf5.hdf5.hdf5.hdf5.hdf5.hdf5.hdf5.h5.hdf5.
 93%|█████████▎| 93/100 [14:55<01:12, 10.39s/it]2024-12-22 02:45:05,617 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:06,708 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   public static Fog convert(FogComponentDescriptor descriptor) {
        Fog fog = new Fog();
        fog.setColor(descriptor.getColor());
        fog.setStart(descriptor.getStart());
        fog.setEnd(descriptor.getEnd());
 89%|████████▉ | 89/100 [14:57<01:53, 10.35s/it]2024-12-22 02:45:07,150 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:08,133 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           request[object_index[obj] = {
                "Key": key,
                "ConsistentRead": consistent,
            }

        for obj in objs:
            table_index[obj] = {
                "TableName": table_name,
                "Index":
 85%|████████▌ | 85/100 [14:58<02:34, 10.30s/it]2024-12-22 02:45:08,356 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:10,158 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:10,159 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3733])
2024-12-22 02:45:10,302 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:45:10,944 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:10,944 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3358])
2024-12-22 02:45:11,081 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:45:12,189 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:12,189 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3326])
2024-12-22 02:45:12,326 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:45:13,684 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           GyroscopeSensorCollector.flushDBCache(deviceID);
        }
        if(type == 5 || type == 0) {
            LightSensorCollector.flushDBCache(deviceID);
        }
        if(type == 6 || type == 
 90%|█████████ | 90/100 [15:04<01:44, 10.43s/it]2024-12-22 02:45:13,903 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:14,181 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:14,181 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3242])
2024-12-22 02:45:14,288 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:45:14,351 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       }.get(self.pkalg, self.pkalg)

        if self.pkalg in [PubKeyAlgorithm.RSAEncryptOrSign, PubKeyAlgorithm.RSAEncrypt, PubKeyAlgorithm.RSASign]:
            self.signature = R
 92%|█████████▏| 92/100 [15:04<01:23, 10.40s/it]2024-12-22 02:45:14,618 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:15,259 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:15,260 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4393])
2024-12-22 02:45:15,413 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:45:15,422 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       # name = filenameToDatetime(self.vid_path)
        # print(name)

        # Get the middle time of the video
        middle_time = getMiddleTimeFF(self.vid_path, self.config.fps, ret_milliseconds=True)
 94%|█████████▍| 94/100 [15:05<01:01, 10.25s/it]2024-12-22 02:45:15,564 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:17,444 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def draw_art(self, key_size, key_algo, key_fpr, color=True, longid=True):
        ...

    def maybe_push_bundle(self, bundle):
        ...

    def pull(self, full=False):

 86%|████████▌ | 86/100 [15:07<02:20, 10.00s/it]2024-12-22 02:45:17,658 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:18,946 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:		
	
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
}
 90%|█████████ | 90/100 [15:09<01:49, 10.92s/it]2024-12-22 02:45:18,962 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:18,963 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2588])
2024-12-22 02:45:19,069 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:45:19,127 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:20,742 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:20,742 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3328])
2024-12-22 02:45:20,860 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:45:22,093 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_home);
        //  init data
        initData();
        //  init view
        initView();
    }

    private
 91%|█████████ | 91/100 [15:12<01:28,  9.82s/it]2024-12-22 02:45:22,363 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:24,060 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   self.get_expansion_info()

    def get_expansion_info(self):  # -> list[ExpansionInfo]
        expansion_info = []

        for channel_type in ExpansionInfo:
            try:
                channel_info = ul.get
 93%|█████████▎| 93/100 [15:14<01:11, 10.19s/it]2024-12-22 02:45:24,225 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:24,225 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3333])
2024-12-22 02:45:24,237 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:24,363 - [Process 3/5] - DEBUG - predict_token:tensor([[6406]], device='cuda:3')
2024-12-22 02:45:24,889 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:24,889 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3148])
2024-12-22 02:45:25,006 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:45:25,328 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:25,329 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4828])
2024-12-22 02:45:25,530 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:45:27,384 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:27,385 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2625])
2024-12-22 02:45:27,485 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:45:27,614 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   admin.site.register(Target, TargetAdmin)
    admin.site.register(Indicator, IndicatorAdmin)
    admin.site.register(Component, ComponentAdmin)
    admin.site.register(Progress, ProgressAdmin)












 87%|████████▋ | 87/100 [15:18<02:10, 10.05s/it]2024-12-22 02:45:27,861 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:28,211 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   ActorInfoComponent plus(ActorInfoModule actorInfoModule);
}















































 91%|█████████ | 91/100 [15:18<01:33, 10.42s/it]2024-12-22 02:45:28,471 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:29,155 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:
































































 95%|█████████▌| 95/100 [15:19<00:56, 11.29s/it]2024-12-22 02:45:29,272 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:30,518 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   self.link.connect()


    def ntp_setup(self):
        self.ntp_sysinfo = TimeManager.TimeManager().ntp_sysinfo
        self.ntp_method = self.ntp_sysinfo["method"]
        if self.
 92%|█████████▏| 92/100 [15:20<01:15,  9.40s/it]2024-12-22 02:45:30,602 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:30,603 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3259])
2024-12-22 02:45:30,701 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:30,730 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:45:33,929 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       self.objects = {}
        for i in range(count):
            key = read_u16le(f)
            value = read_u16le(f)
            self.objects[key] = value

        s.close()

    def read_
 94%|█████████▍| 94/100 [15:24<01:00, 10.10s/it]2024-12-22 02:45:34,114 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:35,394 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:35,395 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2724])
2024-12-22 02:45:35,397 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:35,397 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3726])
2024-12-22 02:45:35,485 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:45:35,555 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:45:35,844 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:35,844 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3751])
2024-12-22 02:45:35,996 - [Process 2/5] - DEBUG - predict_token:tensor([[12]], device='cuda:2')
2024-12-22 02:45:36,819 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:36,819 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4055])
2024-12-22 02:45:36,970 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:45:38,610 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   assert isinstance(deltas.deltas[4].attributes, Thread)
    assert deltas.deltas[4].cursor == "thread_cursor"
    assert deltas.deltas[4].event == "create"
    assert deltas.
 93%|█████████▎| 93/100 [15:29<01:03,  9.01s/it]2024-12-22 02:45:38,698 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:38,699 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2397])
2024-12-22 02:45:38,789 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:45:38,855 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:38,979 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       return RespostaExtrairLogs.analisar(retorno)

    def cancelar_ultima_venda(self, chave_cfe, dados_cancelamento):
        """Sobrepõe :meth:`~satcfe.base.Fun
 88%|████████▊ | 88/100 [15:29<02:05, 10.45s/it]2024-12-22 02:45:39,176 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:39,523 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:					view.setProgress(vals);
				return true;
			} catch (final SocketViewException e) {
				throw new IllegalStateException(e);
			}
		}

	}


 92%|█████████▏| 92/100 [15:29<01:25, 10.69s/it]2024-12-22 02:45:39,747 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:40,251 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:		FoodGroupConfig.sync(player);
}
}
}
```

I'm trying to understand how the FoodTracker mod works, and I'm having trouble understanding how it interacts with other mods. Can someone explain how the FoodTracker mod interacts with other
 96%|█████████▌| 96/100 [15:30<00:44, 11.23s/it]2024-12-22 02:45:40,341 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:41,685 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:           create_inactive_user(full_name, email)




def save_skills(request, user, formset):
    """
    Save paired skills and proficiency levels for a user.
    """
    for form in formset:
        skill
 95%|█████████▌| 95/100 [15:32<00:46,  9.39s/it]2024-12-22 02:45:41,911 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:44,945 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:44,945 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2995])
2024-12-22 02:45:44,962 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:44,962 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3312])
2024-12-22 02:45:45,044 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:45:45,079 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:45:46,822 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:46,822 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3487])
2024-12-22 02:45:46,947 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:45:47,334 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:47,334 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4167])
2024-12-22 02:45:47,492 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:45:48,280 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   class DashboardView(TemplateView):
        template_name = 'dashboard/index.html'
        context_object_name = 'dashboard_context'
        slug = 'dashboard'
        page_title = 'Dashboard'
        page_description =
 93%|█████████▎| 93/100 [15:38<01:10, 10.11s/it]2024-12-22 02:45:48,419 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       if isinstance(emails, six.string_types):
            emails = [[emails]]
        elif isinstance(emails[0], list) is False:
            raise ValueError("'emails' must be a list of lists.")
        if isinstance(duration, tim
 94%|█████████▍| 94/100 [15:38<00:55,  9.25s/it]2024-12-22 02:45:48,492 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:48,663 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:49,182 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:49,182 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3860])
2024-12-22 02:45:49,320 - [Process 1/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:1')
2024-12-22 02:45:50,057 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:       comments = Comment.objects.filter(post=post).order_by('-net_votes')
        comments_count = comments.count()
        comment_form = self.form_class(request.POST or None)
        if comment_form.is_valid():
           
 97%|█████████▋| 97/100 [15:40<00:32, 10.81s/it]2024-12-22 02:45:50,224 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:51,055 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       self.assertEqual(node_document_uri(self.elem), self.base_uri)
        self.assertIsNone(node_document_uri(self.elem))
        self.assertIsNone(node_document_uri(self.elem))
        self.assertIs
 89%|████████▉ | 89/100 [15:41<02:00, 10.94s/it]2024-12-22 02:45:51,376 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:52,790 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:       await Utilities.message(ctx.channel, f"{draft.draft_content}")










































 96%|█████████▌| 96/100 [15:43<00:39,  9.91s/it]2024-12-22 02:45:52,978 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:45:58,429 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:58,429 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3585])
2024-12-22 02:45:58,555 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:58,555 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 5085])
2024-12-22 02:45:58,573 - [Process 3/5] - DEBUG - predict_token:tensor([[462]], device='cuda:3')
2024-12-22 02:45:58,708 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:58,708 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4958])
2024-12-22 02:45:58,754 - [Process 2/5] - DEBUG - predict_token:tensor([[18627]], device='cuda:2')
2024-12-22 02:45:58,908 - [Process 4/5] - DEBUG - predict_token:tensor([[579]], device='cuda:4')
2024-12-22 02:45:59,221 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:59,221 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3080])
2024-12-22 02:45:59,346 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:45:59,591 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:45:59,591 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4576])
2024-12-22 02:45:59,788 - [Process 0/5] - DEBUG - predict_token:tensor([[12]], device='cuda:0')
2024-12-22 02:46:02,018 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:                   Arrays.sort(sorted, new Comparator<CalendarItem>() {

                        @Override
                        public int compare(CalendarItem o1, CalendarItem o2) {
                            return o1.getStartDate().compareTo(o2.getStartDate());
                
 90%|█████████ | 90/100 [15:52<01:49, 10.94s/it]2024-12-22 02:46:02,285 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:02,680 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   def create_markets(exchange_names):
        markets = {}
        for name in exchange_names:
            if name == "BTC-BCH":
                base_currency, market_currency = "BTC", "BCH"
            elif name == "BCH
 97%|█████████▋| 97/100 [15:53<00:29,  9.90s/it]2024-12-22 02:46:02,717 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:TO	
	



TO_token everybody everybody everybody

_token Hinweis










 Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis
																							
 94%|█████████▍| 94/100 [15:53<01:08, 11.41s/it]2024-12-22 02:46:02,873 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results: 
  


 
 














,
,
,
,
,































 95%|█████████▌| 95/100 [15:53<00:54, 10.81s/it]2024-12-22 02:46:02,904 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:02,925 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:03,071 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:03,339 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:*/









state
state
state

state






































state



State
 98%|█████████▊| 98/100 [15:53<00:23, 11.55s/it]2024-12-22 02:46:03,444 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:08,882 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:08,883 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3216])
2024-12-22 02:46:09,000 - [Process 2/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:2')
2024-12-22 02:46:09,120 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:09,121 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3652])
2024-12-22 02:46:09,220 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:09,220 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3225])
2024-12-22 02:46:09,248 - [Process 3/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:3')
2024-12-22 02:46:09,346 - [Process 1/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:1')
2024-12-22 02:46:09,675 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:09,675 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3387])
2024-12-22 02:46:09,801 - [Process 0/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:0')
2024-12-22 02:46:10,832 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:10,832 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3866])
2024-12-22 02:46:10,985 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:46:12,239 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:       planillas_modelo = PlanillaModelo.objects.all()
        initial = {}
        if len(planillas_modelo) == 1:
            planilla_modelo = planillas_modelo[0]
            initial = {"planilla_modelo": plan
 95%|█████████▌| 95/100 [16:02<00:54, 10.84s/it]2024-12-22 02:46:12,429 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:12,686 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   def _trainer_initialization(self) -> None:
        """
        Initialize the trainer

        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        pass

    def compute_calculated_properties(
 91%|█████████ | 91/100 [16:03<01:37, 10.86s/it]2024-12-22 02:46:12,741 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   yield item


    elif self.xsd_types is None:
        for item in context.iter_self():
            if item is not None:
                yield item

    else:
        # XSD typed selection
        for item in context.iter_self():
           
 98%|█████████▊| 98/100 [16:03<00:19,  9.95s/it]2024-12-22 02:46:12,857 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   def __init__(self):
        super(RevocationKey, self).__init__()
        self.fingerprint = Fingerprint()

    def parse(self, packet):
        super(RevocationKey, self).parse(packet)
        self.
 99%|█████████▉| 99/100 [16:03<00:10, 10.94s/it]2024-12-22 02:46:12,924 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:12,990 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:12,996 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:14,464 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           non_equilibrium_thermochemical_prob = calculate_non_equilibrium_thermochemical_probability(parameters=parameters, **non_equilibrium_thermochemical_kwargs)
        else:
            non_equilibrium_ther
 96%|█████████▌| 96/100 [16:04<00:44, 11.04s/it]2024-12-22 02:46:14,747 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:17,417 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:17,417 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2808])
2024-12-22 02:46:17,516 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:46:17,576 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:17,576 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2580])
2024-12-22 02:46:17,666 - [Process 0/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:0')
2024-12-22 02:46:18,109 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:18,109 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2864])
2024-12-22 02:46:18,213 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:46:20,186 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:20,186 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3732])
2024-12-22 02:46:20,328 - [Process 1/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:1')
2024-12-22 02:46:20,443 - [Process 0/5] - INFO - res.shape is :torch.Size([64])
results:   if is_processing_instruction_node(item):
        yield item



# ...

# ...

# ...

# ...

# ...

# ...

# ...

# ...

# ...

# ...

# ...

100%|██████████| 100/100 [16:10<00:00,  9.93s/it]100%|██████████| 100/100 [16:10<00:00,  9.71s/it]
2024-12-22 02:46:20,530 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   self.tree = root.data[1:]
    self._cstr = root.data
    self.ast = Tree(root.data)
    self.live = set(self.ast.live)
    self.root = None
    self.ast = None
    self
 96%|█████████▌| 96/100 [16:10<00:40, 10.08s/it]2024-12-22 02:46:20,721 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:21,309 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   private final List<Scope> scopes;
    
    private final List<OAuth2Guild> guilds;
    
    private final List<OAuth2User> users;
    
    private final List<OAuth2Action> actions;
    
    public O
 92%|█████████▏| 92/100 [16:11<01:21, 10.19s/it]2024-12-22 02:46:21,501 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:21,649 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:21,649 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3487])
2024-12-22 02:46:21,787 - [Process 4/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:4')
2024-12-22 02:46:23,761 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:   helpTestCursor.assertCursor(solutions, AFTER_LAST);
  
  }
}

  public static void assertCursor(CursoredResult<?> result, int state) {
    Assert.assertEquals(state, result.getState());
  }
}
 99%|█████████▉| 99/100 [16:14<00:10, 10.27s/it]2024-12-22 02:46:23,962 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:25,091 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:           Str('Identity', Pt=Identity(), Len=8), \
            if with_options])
        self.set_hierarchy(0)
        self.inBlock = False
        self.Trans = True
        self.Dict = {}
        self.DictFunc =
 97%|█████████▋| 97/100 [16:15<00:32, 10.92s/it]2024-12-22 02:46:25,288 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:25,951 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:25,951 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2488])
2024-12-22 02:46:26,043 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:46:26,331 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:26,331 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2962])
2024-12-22 02:46:26,447 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:46:28,972 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   private final Jobs jobs;
    private final Vms vms;
    private final Errands errands;
    private final Tasks tasks;
    private final SpringJobs springJobs;
    private final SpringReleases springReleases;
    private final SpringVms springV
 93%|█████████▎| 93/100 [16:19<01:06,  9.43s/it]2024-12-22 02:46:29,202 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:29,567 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results: private ActionSupportHandler<StartRecordingAction> startRecordingActionSupportHandler = new ActionSupportHandler<StartRecordingAction>(StartRecordingAction.class);

  private ActionSupportHandler<TransferAction> transferActionSupportHandler = new ActionSupportHandler<TransferAction>(TransferAction
 97%|█████████▋| 97/100 [16:20<00:29,  9.76s/it]2024-12-22 02:46:29,768 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:30,541 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:30,541 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3423])
2024-12-22 02:46:30,678 - [Process 1/5] - DEBUG - predict_token:tensor([[12]], device='cuda:1')
2024-12-22 02:46:32,274 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:32,274 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3543])
2024-12-22 02:46:32,413 - [Process 4/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:4')
2024-12-22 02:46:33,956 - [Process 1/5] - INFO - res.shape is :torch.Size([64])
results:				textExpander = new ExpansionFacetLabelFactory(new File(getFileAttribute(basedir, elt, AlvisIRConstants.XML_SEARCH_EXPANDER_INDEX_DIR)));
		}
		result.setIndex(index);
100%|██████████| 100/100 [16:24<00:00, 10.25s/it]100%|██████████| 100/100 [16:24<00:00,  9.84s/it]
2024-12-22 02:46:35,458 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:35,458 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3373])
2024-12-22 02:46:35,585 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:46:35,729 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:       self.__class_parser.parse(device, run_tc_show(TcSubCommand.CLASS, device, self.__tc_command_output))


    def __parse_device(self, device):
        if typepy.is_null_string(device):

 98%|█████████▊| 98/100 [16:26<00:21, 10.83s/it]2024-12-22 02:46:35,911 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:38,815 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   public void broadcastState()
    {
        synchronized (_lockObject) {
            LocalBroadcastManager localBroadcastManager = _service.getLocalBroadcastManager();
            if (localBroadcastManager != null) {
                localBroadcastManager.sendB
 94%|█████████▍| 94/100 [16:29<00:57,  9.55s/it]2024-12-22 02:46:39,052 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:39,791 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:39,792 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 5006])
2024-12-22 02:46:39,990 - [Process 2/5] - DEBUG - predict_token:tensor([[29918]], device='cuda:2')
2024-12-22 02:46:42,198 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:42,198 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3233])
2024-12-22 02:46:42,322 - [Process 4/5] - DEBUG - predict_token:tensor([[28956]], device='cuda:4')
2024-12-22 02:46:43,853 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:


   
   
   
                              
           
 
 
 
 
 
 
 
 
    
 














	


 98%|█████████▊| 98/100 [16:34<00:22, 11.12s/it]2024-12-22 02:46:44,079 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:45,594 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   result, index = apply_fast_eq(left.values, right.values, left.index, right.index)
    return Column(result, index)


    else:
        return Column(operator.eq(left.values, right), left.index)



 99%|█████████▉| 99/100 [16:36<00:10, 10.54s/it]2024-12-22 02:46:45,775 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:45,907 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:45,907 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3581])
2024-12-22 02:46:46,048 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:46:48,725 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:48,726 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2716])
2024-12-22 02:46:48,816 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:46:49,375 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       return new SvnJavaAddCommand( getClientManager() );
    }

    protected SvnCommand getBlameCommand()
    {
        return new SvnJavaBlameCommand( getClientManager() );
    }

    protected SvnCommand getBranchCommand()

 95%|█████████▌| 95/100 [16:39<00:49,  9.86s/it]2024-12-22 02:46:49,571 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:51,770 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   # Test ECFP
    ecfp = ECFP(protein, depth=2, size=4096, count_bits=True, sparse=True)
    ecfp_sparse = sparse_to_dense(ecfp)
    assert_array
 99%|█████████▉| 99/100 [16:42<00:10, 10.16s/it]2024-12-22 02:46:52,052 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:53,324 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:53,325 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3932])
2024-12-22 02:46:53,475 - [Process 4/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:4')
2024-12-22 02:46:54,024 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:54,024 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2486])
2024-12-22 02:46:54,115 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:46:56,925 - [Process 4/5] - INFO - res.shape is :torch.Size([64])
results:   with pytest.raises(ValidationError) as excinfo:
        validate_response(response, validator_map)
    assert 'Swagger mapping error' in str(excinfo.value)

def test_should_exclude_path_raises_PathNotMatch
100%|██████████| 100/100 [16:47<00:00, 10.78s/it]100%|██████████| 100/100 [16:47<00:00, 10.07s/it]
2024-12-22 02:46:57,045 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   @Override
    public void onMouseClick(GuiMouseEvent e) {
        super.onMouseClick(e);
        if (e.getButton() == MouseEvent.LEFT) {
            if (e.getX() >= 0 && e.getX() < 5
 96%|█████████▌| 96/100 [16:47<00:36,  9.20s/it]2024-12-22 02:46:57,256 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:46:59,541 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:46:59,541 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 3876])
2024-12-22 02:46:59,692 - [Process 2/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:2')
2024-12-22 02:47:02,430 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:47:02,430 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2785])
2024-12-22 02:47:02,536 - [Process 3/5] - DEBUG - predict_token:tensor([[4706]], device='cuda:3')
2024-12-22 02:47:03,111 - [Process 2/5] - INFO - res.shape is :torch.Size([64])
results:   return result



###
# String functions
@method(function('concat', nargs=(2, 3), sequence_types=('xs:string', 'xs:string', 'xs:string')))
def evaluate_concat_function(self, context=None):

100%|██████████| 100/100 [16:53<00:00, 10.51s/it]100%|██████████| 100/100 [16:53<00:00, 10.14s/it]
2024-12-22 02:47:05,564 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:       DATA[looptest][looppc][0], EPSILON);
        double[] d3Put = FiniteDifferenceFirstOrder.differentiate(callPut[looppc] ? new blackPut() : new blackCall(), DATA[looptest][lo
 97%|█████████▋| 97/100 [16:56<00:26,  9.00s/it]2024-12-22 02:47:05,808 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:47:10,292 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:47:10,293 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2404])
2024-12-22 02:47:10,383 - [Process 3/5] - DEBUG - predict_token:tensor([[9651]], device='cuda:3')
2024-12-22 02:47:13,264 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:           VotingModule.class,
    };



    public static void loadModules() {
        // Load modules
        for (Class moduleClass : moduleClasses) {
            try {
                moduleClass.newInstance();
            } catch (InstantiationException | Illegal
 98%|█████████▊| 98/100 [17:03<00:17,  8.61s/it]2024-12-22 02:47:13,462 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:47:22,702 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:47:22,702 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 4609])
2024-12-22 02:47:22,885 - [Process 3/5] - DEBUG - predict_token:tensor([[1678]], device='cuda:3')
2024-12-22 02:47:26,519 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results:   *
    *
    *
    *
    *
    *
 *
    *
    *
    *
    *
    *
    *
    *
 * @Response
 *
 *
 *
 *
 *
 *
 *
 *
 *
 *
 99%|█████████▉| 99/100 [17:17<00:10, 10.00s/it]2024-12-22 02:47:26,708 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:47:30,979 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:47:30,980 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2317])
2024-12-22 02:47:31,069 - [Process 3/5] - DEBUG - predict_token:tensor([[29871]], device='cuda:3')
2024-12-22 02:47:33,929 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
results: public static void main(String[] args) {
    // ...
  }

  public static void main(String[] args) {
    // ...
  }
}

Please complete the code by implementing the remaining methods of the Plugin interface.

Note: The code you
100%|██████████| 100/100 [17:24<00:00,  9.22s/it]100%|██████████| 100/100 [17:24<00:00, 10.44s/it]
2024-12-22 02:47:33,990 - [Process 3/5] - DEBUG - datasets_name:repobench-p
2024-12-22 02:47:33,990 - [Process 0/5] - DEBUG - datasets_name:repobench-p
2024-12-22 02:47:33,991 - [Process 1/5] - DEBUG - datasets_name:repobench-p
2024-12-22 02:47:33,991 - [Process 2/5] - DEBUG - datasets_name:repobench-p
2024-12-22 02:47:33,991 - [Process 4/5] - DEBUG - datasets_name:repobench-p
Running evaluation for dataset: gov_report
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.11s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.46s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.46s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:49:41,713 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 02:49:41,713 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 02:49:41,714 - [Process 0/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:49:41,720 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 02:49:41,721 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 02:49:41,721 - [Process 4/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:49:41,729 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 02:49:41,729 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 02:49:41,729 - [Process 2/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:49:41,734 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 02:49:41,734 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-22 02:49:41,735 - [Process 3/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 02:49:41,739 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 02:49:41,739 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 02:49:41,740 - [Process 1/5] - INFO - output_max_len: 512
2024-12-22 02:49:41,756 - [Process 0/5] - INFO - Max Length is 40508
2024-12-22 02:49:41,757 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 02:49:41,757 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:49:41,792 - [Process 4/5] - INFO - Max Length is 40508
2024-12-22 02:49:41,792 - [Process 2/5] - INFO - Max Length is 40508
2024-12-22 02:49:41,792 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 02:49:41,792 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 02:49:41,792 - [Process 4/5] - INFO - get_predicted begin
2024-12-22 02:49:41,793 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:49:41,807 - [Process 3/5] - INFO - Max Length is 40508
2024-12-22 02:49:41,808 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 02:49:41,808 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:49:41,810 - [Process 1/5] - INFO - Max Length is 40508
2024-12-22 02:49:41,810 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 02:49:41,811 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 02:49:46,510 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:49:46,565 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:49:46,589 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:49:46,593 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:49:46,593 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:49:50,446 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:49:50,446 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1740])
2024-12-22 02:49:50,508 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:49:50,584 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:49:50,585 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2044])
2024-12-22 02:49:50,655 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:49:50,880 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:49:50,880 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1973])
2024-12-22 02:49:50,887 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:49:50,887 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 02:49:50,898 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:49:50,899 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2073])
2024-12-22 02:49:50,954 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:49:50,955 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:49:50,966 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 02:49:59,224 - [Process 0/5] - INFO - res.shape is :torch.Size([213])
results:

The report provides an overview of the use of multiyear procurement (MYP) and block buy contracting (BBC) in the Department of Defense (DOD) acquisition process. MYP and BBC are special contracting mechanisms that allow for reduced procurement costs by locking in prices for a limited number of defense acquisition programs. The report discusses the potential issues for Congress concerning the use of MYP and BBC, including whether to use them more frequently, create a permanent statute, and whether the Coast Guard should begin using them. The report also highlights the savings potential of MYP compared to annual contracting, ranging from less than 5% to over 15%. However, actual savings can be difficult to observe due to cost growth during the execution of the contract. The report concludes by stating that the use of MYP and BBC can reduce procurement costs, but it is essential to track and validate actual savings to inform future acquisition decisions.
  2%|▎         | 1/40 [00:17<11:21, 17.47s/it]2024-12-22 02:49:59,409 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:00,464 - [Process 3/5] - INFO - res.shape is :torch.Size([208])
results:The report examines the use of hosted payloads in the Department of Defense (DOD) and identifies potential benefits and challenges associated with their use. Hosted payloads are instruments or packages of equipment integrated onto a host satellite, which operates on orbit making use of the host satellite's available resources. The report highlights that DOD has identified potential benefits of using commercially hosted payloads, including cost savings, faster on-orbit capability, increased deterrence and resilience, and continual technology upgrades. However, the report also notes that limitations and fragmentation of data and knowledge within DOD can create resistance to using hosted payloads. The report recommends that DOD require programs using hosted payloads to provide cost and technical data, and lessons learned to a central office, to mitigate these challenges. The report also suggests that DOD consider centralizing agency-wide knowledge on commercially hosted payloads to enable more informed decision-making.
  2%|▎         | 1/40 [00:18<12:07, 18.66s/it]2024-12-22 02:50:00,783 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:02,451 - [Process 4/5] - INFO - res.shape is :torch.Size([249])
results:
The report discusses the history and significance of Congressional Gold Medals, which are awarded by the US government to recognize individuals and groups for their contributions to the country. The report highlights that since the late 1700s, Congress has awarded gold medals to express public gratitude for distinguished contributions, dramatize the virtues of patriotism, and perpetuate the remembrance of great events. The report also provides an overview of the process for awarding Congressional Gold Medals, including the requirements for legislation, the role of committees in considering the awards, and the lack of statutory provisions for the creation of the medals. Additionally, the report notes that while there is no limit on the number of gold medals that can be awarded, the process for considering legislation varies between the House and Senate, and many enacted gold medal bills receive no formal committee consideration. The report concludes by highlighting the importance of considering the factors that Members of Congress consider when introducing gold medal legislation, such as who should be honored, how many medals should be awarded, and whether specific design elements should be prescribed.
  2%|▎         | 1/40 [00:20<13:26, 20.67s/it]2024-12-22 02:50:02,661 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:02,899 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:02,899 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1799])
2024-12-22 02:50:02,978 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:50:04,349 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:04,349 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-22 02:50:04,421 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:50:05,432 - [Process 2/5] - INFO - res.shape is :torch.Size([338])
results:The Land and Water Conservation Fund (LWCF) was established in 1965 to preserve, develop, and ensure accessibility to outdoor recreation resources. The report highlights the fund's history, purpose, and appropriations from FY1998 to FY2019. The LWCF Act authorizes the President to use $900 million annually, but Congress has only appropriated $18.9 billion since 1998. The majority of the appropriations, $11.4 billion, went to land acquisition, while $4.8 billion was allocated to the state grant program. Other purposes, such as facility maintenance, ecosystem restoration, and the Historic Preservation Fund, received $2.7 billion. Since FY2008, the LWCF has been used for only two programs: Forest Legacy and Cooperative Endangered Species Conservation Fund. These programs have received $1.7 billion, or 63% of the total appropriations for other purposes. The report also discusses the other purposes for which LWCF appropriations have been provided, including funding for the Forest Legacy program, Cooperative Endangered Species Conservation Fund, and State and Tribal Wildlife Grants. The total appropriation for other purposes was $72.0 million in FY1998, and the amount has varied widely since then. In FY2019, the total appropriation for other purposes was $93.3 million, the highest since FY2010.
  2%|▎         | 1/40 [00:23<15:22, 23.65s/it]2024-12-22 02:50:05,651 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:06,187 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:06,187 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 02:50:06,258 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:50:09,154 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:09,154 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1944])
2024-12-22 02:50:09,227 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:50:09,303 - [Process 1/5] - INFO - res.shape is :torch.Size([404])
results:
The report provides an overview of the funding for contingency operations in the US Department of Defense (DOD) since 2001. It highlights that Congress has appropriated approximately $2 trillion in discretionary budget authority for emergencies or Overseas Contingency Operations (OCO/GWOT) to support the US government's response to the 9/11 attacks and other related international affairs activities. The report notes that beginning in FY2014, the Administration requested direct appropriations to the OCO/GWOT, rather than through the DOD base budget. It also provides information on the different types of contingency operations and activities funded through the DOD base budget, including NATO operations in the Balkans, Joint Task Force-Bravo, Operation Juniper Shield, Operation Noble Eagle, Operation Enduring Freedom-Horn of Africa, and Operation Enduring Freedom-Caribbean and Central America. Additionally, the report mentions that Congress has created various funds and programs through the OCO authorization and appropriation process, such as the Afghanistan Security Forces Fund, Counter-ISIS Train and Equip Fund, Commanders' Emergency Response Program, Joint Improvised Explosive Device Defeat Fund, Mine Resistant Ambush Protected Vehicle Fund, European Deterrence Initiative, Global Security Contingency Fund, Complex Crises Fund, Migration and Refugee Assistance Fund, and Ukraine Security Assistance Initiative. The report concludes by highlighting that the emergency funding designation predates the OCO/GWOT designation and that the definition of emergency requirements is not explicitly specified in statute, allowing for flexibility in how Congress and the President determine how much emergency or OCO/GWOT spending to appropriate each year.
  2%|▎         | 1/40 [00:27<17:52, 27.51s/it]2024-12-22 02:50:09,527 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:10,502 - [Process 0/5] - INFO - res.shape is :torch.Size([187])
results:The report examines the sustainment of Air Force and Navy fixed-wing aircraft, including aircraft availability, maintenance, and supply support. The report finds that 6 of 12 aircraft experienced decreased availability between 2011 and 2016, and only one aircraft met availability goals in every year. The report also identifies sustainment challenges such as aging, maintenance, and supply support, and notes that DOD relies on program managers to lead the development, delivery, and sustainment of individual weapon systems through their life cycles. The report highlights the importance of regular review of sustainment metrics and the implementation of a Life-cycle Sustainment Plan to manage sustainment performance. The report concludes that DOD needs to complete information on single sources of supply to proactively manage risks and improve sustainment efforts.
  5%|▌         | 2/40 [00:28<08:45, 13.83s/it]2024-12-22 02:50:10,652 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:13,056 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:13,056 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1956])
2024-12-22 02:50:13,129 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:50:14,122 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:14,123 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2108])
2024-12-22 02:50:14,190 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:50:18,315 - [Process 4/5] - INFO - res.shape is :torch.Size([269])
results:The report examines the operational availability of Cobra Dane and LRDR radar systems in supporting ballistic missile defense missions. Cobra Dane, located at Shemya Island, Alaska, has an operational availability of 88% over a two-year period, while LRDR, located at Clear Air Force Station, Alaska, has an operational availability of 99.7% over the same period. The report highlights that Cobra Dane's location provides earlier detection of missile threats, but LRDR's location provides longer detection of missile threats. The Air Force and MDA have a shared funding plan for the operation and sustainment of Cobra Dane and LRDR, with the Air Force responsible for the operation and sustainment of Cobra Dane and MDA responsible for the modernization of the radar systems. The report also notes that the Air Force and MDA have plans to use the radar systems for space surveillance and intelligence-gathering missions. The report concludes that while Cobra Dane and LRDR have different strengths and weaknesses, they are both essential for ballistic missile defense and space surveillance missions.
  5%|▌         | 2/40 [00:36<11:18, 17.84s/it]2024-12-22 02:50:18,524 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:20,966 - [Process 2/5] - INFO - res.shape is :torch.Size([255])
results:The report by the Government Accountability Office (GAO) highlights the challenges faced by the US Navy in maintaining its fleet readiness, particularly in the areas of maintenance, training, and overseas basing. The Navy has experienced four significant mishaps at sea since January 2017, resulting in damage to ships and loss of sailors. The report notes that the Navy's reliance on overseas homeporting has led to increased operational demands, higher costs, and decreased ship conditions. The Navy has also faced challenges in implementing its readiness rebuilding efforts, including delays in completing maintenance and emerging demands. The report recommends that the Navy develop a comprehensive plan to address these challenges and ensure the sustainability of its fleet. The Navy's reliance on overseas homeporting has led to increased operational demands, higher costs, and decreased ship conditions. The Navy has also faced challenges in implementing its readiness rebuilding efforts, including delays in completing maintenance and emerging demands. The report recommends that the Navy develop a comprehensive plan to address these challenges and ensure the sustainability of its fleet.
  5%|▌         | 2/40 [00:39<11:57, 18.88s/it]2024-12-22 02:50:21,286 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:22,045 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:22,046 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2057])
2024-12-22 02:50:22,114 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:50:23,047 - [Process 1/5] - INFO - res.shape is :torch.Size([223])
results:The report by the Government Accountability Office (GAO) examines the disclosure of conflict minerals in products by companies in the United States. The report finds that 92% of companies reported conducting due diligence on the source and chain of custody of conflict minerals in their products, which is similar to the percentage reported in 2016 and 2015. The remaining 8% of companies did not report conducting due diligence. The report also found that 53% of companies reported whether the conflict minerals in their products came from covered countries, which is an increase from the 49% reported in 2016. The report highlights the challenges faced by companies in determining the country of origin of conflict minerals, including lack of access to suppliers and complex supply chains. The report concludes that while progress has been made in improving data collection processes, more work needs to be done to ensure that companies are accurately reporting on their use of conflict minerals.
  5%|▌         | 2/40 [00:41<12:17, 19.41s/it]2024-12-22 02:50:23,366 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:24,801 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:24,801 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1975])
2024-12-22 02:50:24,874 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:50:26,459 - [Process 3/5] - INFO - res.shape is :torch.Size([490])
results:The report examines the employment practices of technology companies in the United States, focusing on federal contractors and subcontractors. The report is based on data from the Equal Employment Opportunity Commission (EEOC) and the Office of Federal Contract Compliance Programs (OFCCP). The report finds that while technology companies have a high representation of women and minorities in their workforce, there are still issues with discrimination and unequal opportunities. The report highlights the following key findings:

* Technology companies are required to file EEO-1 reports, which collect data on the gender, race, and ethnicity of their employees. The report finds that these reports show a high representation of women and minorities in technology companies.
* OFCCP enforces affirmative action and nondiscrimination requirements for federal contractors. The report finds that OFCCP has a limited number of cases each year to combat systemic discrimination.
* The report identifies 55 North American Industry Classification System (NAICS) codes as technology-related industries. These industries include computer systems design and related services, data processing, hosting, and related services, and telecommunications.
* The report finds that technology companies are required to comply with federal nondiscrimination and affirmative action requirements. OFCCP conducts compliance evaluations and complaint investigations to ensure compliance.
* The report highlights the importance of recordkeeping and reporting requirements for federal contractors. OFCCP requires covered contractors to maintain certain records, including hiring, promotion, layoff or termination, rates of pay, and application records.
* The report identifies the leading technology companies that have completed evaluations between fiscal year 2011 and 2016. These companies include Cisco Systems, Inc., Facebook, Inc., Google Inc., Hewlett Packard Enterprise Company, Intuit Inc., Oracle America, Inc., and others.

Overall, the report highlights the importance of ensuring equal employment opportunity and nondiscrimination in technology companies, particularly those that receive federal contracts. The report provides insights into the employment practices of technology companies and identifies areas for improvement to promote diversity and inclusion.
  5%|▌         | 2/40 [00:44<14:33, 22.98s/it]2024-12-22 02:50:26,509 - [Process 0/5] - INFO - res.shape is :torch.Size([305])
results:The report by the United Nations, the International Criminal Police Organization, and other organizations revealed that wildlife trafficking networks span the globe, with an estimated value of illegal trade ranging from $7 billion to $23 billion annually. The US Fish and Wildlife Service (FWS) and the National Oceanic and Atmospheric Administration (NOAA) also reported that they track all expenditures, including reward payments, in their financial databases but struggle to identify reward payments due to a lack of unique identifiers. Despite these challenges, FWS and NOAA officials provided sufficient information to report the rewards they paid. The report also highlighted the collaboration between FWS and NOAA with other government agencies and organizations to combat wildlife trafficking, including the use of financial rewards to incentivize individuals to provide information on wildlife trafficking violations. The report also identified 14 laws that prohibit wildlife trafficking and authorize the payment of rewards for information leading to arrests, criminal convictions, or forfeiture of property. The report found that FWS and NOAA paid rewards totaling $7 million from fiscal years 2007 to 2017 for information on wildlife trafficking violations. The report emphasized the importance of collaboration and the use of financial rewards to combat wildlife trafficking.
  8%|▊         | 3/40 [00:44<09:08, 14.82s/it]2024-12-22 02:50:26,658 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:26,724 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:26,930 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:26,930 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 02:50:27,001 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:50:30,133 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:30,134 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1984])
2024-12-22 02:50:30,206 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:50:30,302 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:30,303 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2028])
2024-12-22 02:50:30,374 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:50:32,078 - [Process 4/5] - INFO - res.shape is :torch.Size([224])
results:
The report by the Government Accountability Office (GAO) examines the disposal of unused prescription drugs in the United States. According to the report, the Drug Enforcement Administration (DEA) allows authorized collectors to use disposal bins to collect unused prescription drugs from the public. However, the report reveals that the cost of using these bins is a significant barrier for many entities, with some estimating the cost to be thousands of dollars per location per year. The report also highlights the uncertainty surrounding the regulations for disposal bins, with some stakeholders expressing confusion over how to comply with DEA's requirements. Additionally, the report notes that only a small percentage of eligible entities have registered to become authorized collectors, with pharmacies and hospitals being the most common types of entities participating. The report concludes that while DEA has taken steps to address the cost issue, more needs to be done to increase participation rates and reduce uncertainty among stakeholders.
  8%|▊         | 3/40 [00:50<09:51, 15.98s/it]2024-12-22 02:50:32,288 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:34,741 - [Process 2/5] - INFO - res.shape is :torch.Size([222])
results:
The report examines the taxation of digital economy markets, particularly in Europe, and the impact of digital services taxes (DSTs) on multinational corporations (MNCs). The report highlights that DSTs are primarily driven by politicians and commentators in Europe, and they aim to have MNCs pay their "fair share" of taxes. The report analyzes the effects of DSTs on MNCs using two-sided markets, where they provide services to individual users at no charge and sell other services to businesses. The analysis shows that DSTs reduce consumer surplus, have no effect on producer surplus, and create deadweight loss inefficiencies. The report also discusses the challenges of taxing digital economy firms in monopoly markets, where MNCs have "monopoly power." The report concludes that DSTs can have significant economic implications, and it is essential to consider the potential impact on MNCs and the digital economy as a whole.
  8%|▊         | 3/40 [00:52<10:12, 16.55s/it]2024-12-22 02:50:35,031 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:35,807 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:35,807 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1958])
2024-12-22 02:50:35,880 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:50:37,569 - [Process 0/5] - INFO - res.shape is :torch.Size([183])
results:The report discusses the importance of cross-border data flows in today's globally connected world and the challenges of balancing privacy and security concerns with the need for open data flows. The report highlights the need for a comprehensive national data privacy policy in the US to protect individual privacy while promoting economic growth and innovation. The report also notes that the EU and China are taking steps to establish their own data privacy frameworks, which could become the global norms if the US does not act. The report concludes that Congress should consider enacting comprehensive privacy legislation to ensure a balance between privacy and security concerns and promote digital trade. The report also suggests that the US should work with other countries to establish international rules and norms on data privacy and security to avoid fragmentation of global markets and the balkanization of the internet.
 10%|█         | 4/40 [00:55<08:00, 13.34s/it]2024-12-22 02:50:37,729 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:38,586 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:38,586 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 02:50:38,659 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:50:39,672 - [Process 1/5] - INFO - res.shape is :torch.Size([283])
results:The report discusses the government agency's role in screening and vetting non-immigrant visa (NIV) applicants. The agency has various roles and responsibilities related to the screening and vetting process, including the authority to issue visas, conduct background checks, and make decisions on visa applications. The report highlights several key points, including:


* The agency has the authority to issue visas to foreign nationals seeking to travel to the United States and apply for admission.
* The agency conducts background checks on U.S.-based petitioners and foreign beneficiaries as part of the visa application process.
* The agency has the discretion to refuse visas to individuals who are inadmissible under the Immigration and Nationality Act.
* The agency has implemented additional security measures to prevent exploitation of the U.S. Refugee Admissions Program.
* The report also provides information on the various NIVs, including their validity period, number of entries, and requirements for issuance.

Overall, the report highlights the government agency's role in ensuring the safety and security of the United States by thoroughly screening and vetting non-immigrant visa applicants.
  8%|▊         | 3/40 [00:57<11:11, 18.14s/it]2024-12-22 02:50:39,956 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:40,994 - [Process 3/5] - INFO - res.shape is :torch.Size([238])
results:
The report discusses the progress of the CHIP-IN Act, a government agency's initiative to construct medical facilities using non-federal funding sources. The report highlights two main factors that led to the creation of the act: a lack of a senior-level leader for the pilot and the need for strong pilot leadership going forward. The report also outlines the roles and responsibilities of various VA offices and staff members involved in the project, including the Omaha donor group, ORP officials, and the Center for Strategic Partnerships. The report notes that while the CHIP-IN Act authorizes the pilot program, VA officials have not yet documented how they will staff the working group, including how they will obtain the needed expertise within their existing resources. The report concludes by making three recommendations to VA, including ensuring that internal stakeholders agree to and document clear, measurable objectives for the CHIP-IN pilot, developing an assessment methodology and evaluation plan, and documenting the roles and responsibilities of the CHIP-IN steering committee.
  8%|▊         | 3/40 [00:59<11:47, 19.12s/it]2024-12-22 02:50:41,270 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:41,270 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1876])
2024-12-22 02:50:41,314 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:41,348 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:50:43,527 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:43,527 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 02:50:43,598 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:50:44,878 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:44,879 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2090])
2024-12-22 02:50:44,948 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:50:46,301 - [Process 4/5] - INFO - res.shape is :torch.Size([233])
results:
The report examines the use of the suspension procedure in the House of Representatives, which allows for expedited consideration of legislation. The majority of measures considered under suspension are bills, and the majority of these are sponsored by members of the majority party. The suspension procedure is used to consider complex or contentious legislation, and the majority of measures considered under suspension are approved without floor amendments or extensive debate. The report also notes that the suspension procedure is not always related to the subject matter of the measure, and that decisions to schedule bills for consideration under suspension are based on how widely supported the measures are, how long Members wish to debate them, and whether they want to propose floor amendments. The report also provides statistics on the length of consideration of suspension measures, the number of speakers, and the percentage of measures that are passed. Finally, the report notes that the suspension procedure is not guaranteed to result in passage in the Senate, and that measures that are passed in the House may not become law if the Senate does not agree to the final version of the bill.
 10%|█         | 4/40 [01:04<09:10, 15.29s/it]2024-12-22 02:50:46,531 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:50,108 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:50,109 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 02:50:50,177 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:50:51,371 - [Process 2/5] - INFO - res.shape is :torch.Size([284])
results:
The report discusses the legal framework for analyzing the constitutionality of commercial disclosure requirements under the First Amendment. The Supreme Court has established that commercial speech receives less protection under the First Amendment than other speech, and therefore, government actions regulating commercial speech are subject to less rigorous scrutiny. However, the Court has also recognized that commercial disclosure requirements may be subject to heightened scrutiny, particularly if they compel speech that is not misleading or related to illegal activity. In recent years, the Court has suggested that commercial disclosure requirements may be subject to strict scrutiny, and lower courts have upheld certain disclosure requirements under this standard. The report highlights the Supreme Court's decision in NIFLA, which held that a disclosure requirement was likely unconstitutional under Zauderer because the government had not presented sufficient evidence to justify the measure. The report also discusses the Court's recent decisions that have suggested that commercial disclosure requirements may be subject to heightened scrutiny, and notes that some members of the Court have questioned whether commercial speech should categorically receive less protection under the First Amendment. The report concludes by highlighting the importance of developing a record with more evidence demonstrating a need for the regulation when Congress and federal agencies consider adopting new commercial disclosure requirements.
 10%|█         | 4/40 [01:09<09:56, 16.58s/it]2024-12-22 02:50:51,604 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:51,793 - [Process 1/5] - INFO - res.shape is :torch.Size([186])
results:The report by the Government Accountability Office (GAO) examines the Federal Communications Commission's (FCC) approach to collecting broadband availability data, particularly in tribal lands. The report finds that FCC's approach has limitations, including overstating broadband availability on tribal lands due to providers' reporting methods. The report highlights the importance of accurate data collection to ensure that tribal communities have access to affordable and reliable broadband internet. The GAO recommends that FCC improve its data collection methods, including collecting more granular data on tribal lands and involving tribal stakeholders in the validation process. The report also notes that FCC's strategic goals and the National Broadband Plan emphasize the importance of broadband access, but the agency's current process for collecting data may not fully address these goals.
 10%|█         | 4/40 [01:09<09:27, 15.76s/it]2024-12-22 02:50:52,191 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:54,798 - [Process 3/5] - INFO - res.shape is :torch.Size([220])
results:
The report by the government agency provides an overview of the agency's efforts to address lead paint hazards in federally-assisted housing. The report highlights the agency's primary responsibility for addressing lead paint hazards in housing, as well as its collaboration with other agencies to set federal standards for lead hazards. The report also discusses the agency's grant programs for lead hazard control and its efforts to monitor and enforce compliance with lead paint regulations. Additionally, the report examines the agency's adoption of federal health guidelines and environmental standards for lead paint hazards in its lead grant and rental assistance programs. Finally, the report reviews the agency's performance goals and measures, program evaluations, and reporting to assess its effectiveness in addressing lead paint hazards. Overall, the report demonstrates the agency's commitment to ensuring that federally-assisted housing is lead-safe and free from lead paint hazards.
 10%|█         | 4/40 [01:12<10:12, 17.02s/it]2024-12-22 02:50:55,124 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:55,125 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1970])
2024-12-22 02:50:55,198 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:50:55,209 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:55,734 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:55,735 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1981])
2024-12-22 02:50:55,808 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:50:56,811 - [Process 0/5] - INFO - res.shape is :torch.Size([382])
results:
The report examines the history and current state of airport infrastructure development in the United States, focusing on the federal government's role in supporting airport development through various programs and funding sources. The report highlights key legislation and reauthorizations that have shaped the Airport Improvement Program (AIP) and the Airport and Airway Trust Fund (AATF), which provides the primary source of funding for AIP. The report also discusses the different funding sources available to airports, including the Passenger Facility Charge (PFC), tax-exempt bonds, state and local grants, and operating revenues. The report concludes by summarizing the current funding levels and authorization levels for AIP through FY2023.

The report provides a detailed overview of the history of airport infrastructure development in the United States, including the evolution of federal support for airport development and the various programs and funding sources that have been established over time. The report highlights key legislation and reauthorizations that have shaped the AIP and AATF, including the Airport and Airway Revenue Act of 1970, the Airport and Airway Improvement Act of 1982, and the FAA Modernization and Reform Act of 2012. The report also discusses the different funding sources available to airports, including PFCs, tax-exempt bonds, state and local grants, and operating revenues, and how these sources are used to finance airport projects. Finally, the report provides an overview of the current funding levels and authorization levels for AIP through FY2023, highlighting the challenges and opportunities facing the airport infrastructure in the United States.
 12%|█▎        | 5/40 [01:15<09:01, 15.47s/it]2024-12-22 02:50:56,953 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:50:58,843 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:50:58,843 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1633])
2024-12-22 02:50:58,934 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:51:00,503 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:00,503 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1845])
2024-12-22 02:51:00,583 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:51:01,518 - [Process 4/5] - INFO - res.shape is :torch.Size([254])
results:The National Cemetery Administration (NCA) manages the national cemetery system and provides burial options for veterans. The report highlights NCA's efforts to increase access to burial options for veterans, particularly in urban and rural areas. NCA has revised its strategic goal to provide reasonable access to 95% of the veteran population by the end of fiscal year 2022. The agency has also revised its cost-estimating guidance to better reflect the 12 steps in the GAO Cost Estimating and Assessment Guide. However, NCA has faced challenges in acquiring suitable land for new national cemeteries, estimating costs associated with new cemetery construction, and using all available data to inform its Grants Program. Despite these challenges, NCA has made progress in opening new cemeteries, with four new traditional national cemeteries and two rural initiative cemeteries opened as of September 2019. The report concludes that NCA should continue to revise its cost-estimating guidance and prioritize grant applications based on the number of unserved veterans in each area.
 12%|█▎        | 5/40 [01:19<08:54, 15.26s/it]2024-12-22 02:51:01,737 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:05,265 - [Process 2/5] - INFO - res.shape is :torch.Size([226])
results:
The report examines the capabilities and capacity of the Afghan National Defense and Security Forces (ANDSF) to operate and maintain US-purchased weapon systems and equipment. The report reveals that the ANDSF has improved in some areas, but still faces significant capability gaps. The US government has invested nearly $18 billion in equipment and transportation for the ANDSF since 2003, with the majority of the funds going towards weapons, vehicles, and communication equipment. However, the ANDSF still lacks proficiency in certain areas, such as intelligence, surveillance, and reconnaissance (ISR). The report highlights that the US has taken steps to address these gaps, including providing training and advisory support to the ANDSF through the Resolute Support mission. Despite these efforts, the ANDSF still relies heavily on coalition assistance to operate and maintain US-purchased equipment. The report concludes that the US should continue to provide support to the ANDSF to address the remaining capability gaps and ensure the security of Afghanistan.
 12%|█▎        | 5/40 [01:23<09:06, 15.61s/it]2024-12-22 02:51:05,294 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:05,294 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2034])
2024-12-22 02:51:05,366 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:51:05,536 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:07,542 - [Process 3/5] - INFO - res.shape is :torch.Size([191])
results:
The report by the government agency highlights the progress made in addressing performance and management challenges in the federal government. The report identifies 315 targeted actions where opportunities exist to better manage fragmentation, overlap, and duplication, and 145 of these actions have been addressed by the executive branch and Congress. The report also notes that agencies have taken steps to improve their performance management, including establishing cross-agency priority goals, strategic objectives, and goal leaders. Additionally, the report highlights the need for greater transparency in public reporting on the quality of performance information and the potential for collaboration among federal agencies to address issues of fragmentation, overlap, and duplication. Overall, the report suggests that while progress has been made in addressing performance and management challenges, there is still work to be done to improve the effectiveness and efficiency of the federal government.
 12%|█▎        | 5/40 [01:25<09:01, 15.48s/it]2024-12-22 02:51:07,780 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:09,065 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:09,066 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 02:51:09,134 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:51:11,373 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:11,373 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 02:51:11,446 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:51:12,892 - [Process 1/5] - INFO - res.shape is :torch.Size([379])
results:

The report examines the regulatory burden faced by community banks and credit unions in the United States. The study found that while no commonly accepted definition of a community bank exists, these institutions are generally smaller banks that provide banking services to local communities and have management and board members who reside in those communities. The report used a non-random sample of 23 community banks and 23 credit unions for focus groups to gather information on the regulatory burden faced by these institutions. The study found that small banks tend to differ from larger banks in their relationships with customers, with small banks engaging in relationship banking and providing personalized services to their customers. The report also found that community banks and credit unions face significant regulatory burdens, including compliance with the Dodd-Frank Act, Home Mortgage Disclosure Act, Bank Secrecy Act, and other federal regulations. The study identified three sets of regulations that were considered burdensome by community banks and credit unions: mortgage fee and other disclosures to consumers, reporting of mortgage borrower and loan characteristics, and anti-money laundering activities. The report also found that federal regulators have taken steps to reduce the regulatory burden on community banks and credit unions, including providing relief from certain regulations and implementing the Economic Growth and Regulatory Paperwork Reduction Act. The study concluded that while the Dodd-Frank Act exempts small institutions from certain provisions, it also contains provisions that impose additional restrictions and compliance costs on community banks and credit unions. Overall, the report highlights the regulatory burden faced by community banks and credit unions and the steps taken by federal regulators to reduce this burden.
 12%|█▎        | 5/40 [01:31<10:19, 17.69s/it]2024-12-22 02:51:13,260 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:14,499 - [Process 4/5] - INFO - res.shape is :torch.Size([201])
results:The report examines the administration of federal elections in the United States, focusing on the distribution of election administration duties between state and local governments. It highlights that state and local governments are primarily responsible for conducting federal elections, with state legislatures holding the primary decision-making role in election administration. The report also notes that federal laws and regulations can affect the way state and local systems administer elections, and that Congress has the authority to require or encourage changes to these systems. The report highlights several examples of how federal actions have affected state and local election systems, including the use of voter registration lists, election technology, and funding. Additionally, the report notes that the structure of state and local election systems can affect the implementation of federal election administration laws, and that there are ongoing debates about the role of federal government in election administration. Overall, the report emphasizes the complex relationship between federal and state governments in the administration of federal elections.
 15%|█▌        | 6/40 [01:32<08:12, 14.49s/it]2024-12-22 02:51:14,744 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:17,002 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:17,002 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2115])
2024-12-22 02:51:17,071 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:51:18,301 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:18,302 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 02:51:18,373 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:51:19,687 - [Process 2/5] - INFO - res.shape is :torch.Size([234])
results:
The report by the Government Accountability Office (GAO) discusses the challenges states face in implementing the provisions of the Child Abuse Prevention and Treatment Act (CAPTA) related to substance-affected infants. The report highlights that states are struggling to understand the requirements for health care providers to notify Child Protective Services (CPS) of substance-affected infants and the development of plans of safe care for these infants. The report also notes that states are experiencing confusion about the definition of "plans of safe care" and the population of infants and families covered by the provision. The GAO recommends that the Secretary of Health and Human Services (HHS) provide additional guidance and technical assistance to states to address these challenges. The report also notes that HHS has provided some guidance on the issues, but states still need more help. The GAO believes that providing additional guidance and technical assistance would help states better understand their responsibilities under CAPTA and improve the safety and well-being of substance-affected infants.
 15%|█▌        | 6/40 [01:37<08:37, 15.21s/it]2024-12-22 02:51:19,891 - [Process 3/5] - INFO - res.shape is :torch.Size([184])
results:The Colorado River Basin is a vital water source for 40 million people and 5.5 million acres of land in the western United States. The federal government plays a significant role in managing the river's water through the Bureau of Reclamation, which has authorized two agreements, the Upper Basin DCP and the Lower Basin DCP, to protect the river's flow and stored water. The agreements aim to reduce water use in the Lower Basin and protect against critically low elevations in the Upper Basin. The report highlights the importance of the Colorado River Basin, the role of the federal government in its management, and the impact of climate change on the river's flow. It also discusses the congressional role in managing the basin's water and the potential for future agreements and developments in the basin.
 15%|█▌        | 6/40 [01:38<08:10, 14.41s/it]2024-12-22 02:51:19,976 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:19,989 - [Process 0/5] - INFO - res.shape is :torch.Size([478])
results:The report provides an overview of the Military Construction (MILCON) appropriations for the active component of the US Department of Defense (DOD) from fiscal year 2005 to 2016. The report highlights the following key points:

* MILCON appropriations are used to fund military construction projects, including the acquisition, construction, installation, and equipping of temporary or permanent public works, military installations, facilities, and real property needed to support US military forces.
* The report shows the combined MILCON appropriations, obligations, and unexpended funds for each of the 18 programmatic MILCON appropriations accounts from fiscal year 2005 to 2016.
* The report also provides details on unexpended and unobligated balances by appropriation year and military department for the active component's MILCON appropriations.
* The report explains that MILCON appropriations are generally available for obligation for 5 fiscal years, after which any remaining unexpended amounts are canceled and returned to the US Treasury.
* The report highlights the flexibility of DOD to reprogram MILCON funds without congressional approval, subject to certain limits.
* The report provides information on the budget execution for each military department's MILCON appropriation for "congressionally directed" and "other" MILCON projects from fiscal year 2010 to 2016.
* The report also discusses the two levels of military construction guidance used by DOD: the Unified Facilities Criteria and component-level guidance.
* The report concludes by summarizing the estimated initial costs and contract award amounts of projects initiated and completed during fiscal year 2010 to 2016, and provides information on the active component's MILCON guidance and procedures.

Overall, the report provides a comprehensive overview of the MILCON appropriations for the active component of DOD from fiscal year 2005 to 2016, highlighting the key trends, challenges, and opportunities in the management of these funds.
 15%|█▌        | 6/40 [01:38<10:15, 18.09s/it]2024-12-22 02:51:20,113 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:20,155 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:23,527 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:23,528 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2041])
2024-12-22 02:51:23,599 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:51:23,723 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:23,724 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1956])
2024-12-22 02:51:23,777 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:23,777 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1922])
2024-12-22 02:51:23,798 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:51:23,856 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:51:28,686 - [Process 4/5] - INFO - res.shape is :torch.Size([224])
results:The report by the Government Accountability Office (GAO) examines the Federal Railroad Administration's (FRA) grade-crossing safety research, state selection and implementation of grade-crossing projects, and challenges states face in implementing and assessing projects. The report also looks at FHWA's role in assessing the Section 130 Program's effectiveness. According to the report, while grade-crossing safety has improved since 1975, the number of crashes and fatalities has plateaued since 2009. FRA conducts research to understand the causes of grade-crossing crashes and identifies potential improvements. States select projects based on FRA's requirements, which include the installation of protective devices at grade crossings. However, states face challenges in implementing and assessing projects, and FHWA does not adequately assess the program's effectiveness. The report recommends that FRA and FHWA improve their oversight and assessment of the program to ensure its continued effectiveness.
 18%|█▊        | 7/40 [01:46<07:54, 14.39s/it]2024-12-22 02:51:28,915 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:32,525 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:32,525 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1901])
2024-12-22 02:51:32,605 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:51:32,674 - [Process 0/5] - INFO - res.shape is :torch.Size([219])
results:
The report by the Government Accountability Office (GAO) examines the process of certifying postsecondary schools to participate in federal student aid programs. The report finds that the US Department of Education's (ED) Federal Student Aid (FSA) has implemented various measures to ensure that schools meet eligibility requirements, including conducting compliance audits and program reviews. However, the report identifies several issues that can affect the quality of compliance audits, such as inadequate training of auditors and schools hiring less experienced auditors to save money. The report also notes that FSA has taken steps to address audit quality, including providing training for auditors and enhancing the OIG's quality reviews. The report highlights the importance of ensuring that compliance audits provide accurate and reliable information on school administrative capability for Education's recertification decisions. The report concludes that FSA's efforts to address audit quality could help improve the accuracy and reliability of compliance audits.
 18%|█▊        | 7/40 [01:50<08:58, 16.32s/it]2024-12-22 02:51:32,864 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:34,315 - [Process 1/5] - INFO - res.shape is :torch.Size([383])
results:
The Federal Emergency Management Agency (FEMA) is responsible for managing grants for various programs, including disaster relief and emergency assistance. The agency has identified several areas for improvement in its grants management processes, including the need for a more comprehensive and integrated grants management system. The current system is complex and decentralized, with multiple stakeholders and IT systems, which can lead to inefficiencies and errors. FEMA has identified 10 primary legacy IT systems that support its grants management activities, but these systems are not interoperable and can lead to poor information sharing and reporting capabilities. The agency has initiated a modernization effort, known as the Grants Management Modernization (GMM) program, to improve its grants management processes. The GMM program aims to provide a single grants processing solution and improve the agency's ability to manage grants more efficiently and effectively. The program has identified five key practices for grants management, including categorizing the system based on security risk, selecting and implementing security controls, assessing security controls, obtaining an authorization to operate the system, and monitoring security controls on an ongoing basis. FEMA has also identified the need for better cybersecurity practices, including the implementation of security controls and the authorization to operate the system. The agency has identified 45 grant programs and has developed a standard framework for grants management, which includes five sequential phases: pre-award, award, post-award, closeout, and post-closeout. The agency has also identified the need for better data quality and accuracy in its grants management processes. Overall, the report highlights the need for FEMA to improve its grants management processes and systems to better manage its grants and ensure effective and efficient use of funds.
 15%|█▌        | 6/40 [01:52<10:44, 18.96s/it]2024-12-22 02:51:34,722 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:35,959 - [Process 2/5] - INFO - res.shape is :torch.Size([268])
results:The report examines the challenges faced by non-profit organizations (NPOs) in accessing banking services in high-risk countries, particularly those providing humanitarian assistance. The report finds that NPOs face challenges in transferring funds from the US to recipient countries due to banking access challenges, including the decline in correspondent banking relationships. The report highlights that the number of correspondent banking relationships has declined over the past several years, especially for banks located in higher-risk jurisdictions. The study also notes that the decline in correspondent banking relationships could affect the ability to send and receive international payments and may drive some payment flows underground, with potential consequences on growth, financial inclusion, and the stability and integrity of the financial system. The report recommends that U.S. agencies take actions to help prevent financial crimes, such as developing a reasonable methodology to conduct surveys and examinations, and implementing regulations that require banks to establish and maintain a BSA compliance program. Additionally, the report suggests that NPOs should be aware of the risks associated with banking access challenges and take steps to mitigate them, such as diversifying their banking relationships and using alternative payment methods.
 18%|█▊        | 7/40 [01:54<08:33, 15.56s/it]2024-12-22 02:51:36,206 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:36,353 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:36,353 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1974])
2024-12-22 02:51:36,426 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:51:37,808 - [Process 3/5] - INFO - res.shape is :torch.Size([306])
results:
The report examines the extent to which government-wide data on collections of fees, fines, and penalties are publicly available and useful for congressional oversight. The report finds that while some data on collections of fees, fines, and penalties are publicly available, there are challenges to government-wide reporting of these data. The report identifies three types of collections: offsetting collections, offsetting receipts, and governmental receipts. The type of collection determines how the data is reported and how useful it is for congressional oversight. The report recommends that OMB and Treasury provide more detailed information on collections of fees, fines, and penalties and improve the transparency and accountability of federal spending data. The report also identifies sources of data on fees, fines, and penalties, including the Budget of the U.S. Government, the Financial Report of the U.S. Government, and USAspending.gov. However, the report notes that there is no source that lists all specific fees, fines, and penalties at a government-wide or agency level. The report concludes that while there are some challenges to government-wide reporting of fees, fines, and penalties, providing more detailed information on these collections can improve transparency and accountability in federal spending.
 18%|█▊        | 7/40 [01:56<08:33, 15.56s/it]2024-12-22 02:51:38,132 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:38,338 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:38,338 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1873])
2024-12-22 02:51:38,417 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:51:39,764 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:39,764 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2008])
2024-12-22 02:51:39,836 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:51:40,522 - [Process 4/5] - INFO - res.shape is :torch.Size([179])
results:The report discusses the history and eligibility criteria for the Purple Heart, a military award given to members of the US Armed Forces who have been wounded or killed in combat. The report highlights how the eligibility criteria have evolved over time, with recent changes including the expansion of eligibility to include those wounded or killed in terrorist attacks and international terrorist attacks outside of the United States. The report also notes that there is ongoing debate over whether mental injuries such as PTSD should be eligible for the Purple Heart, with some arguing that it would dishonor those who have received the award for physical injuries. The report concludes by summarizing current legislation related to the Purple Heart and noting that the Department of Defense does not maintain a record of the number of Purple Heart recipients.
 20%|██        | 8/40 [01:58<07:14, 13.58s/it]2024-12-22 02:51:40,761 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:41,727 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:41,728 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 02:51:41,799 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:51:44,329 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:44,329 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2005])
2024-12-22 02:51:44,401 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:51:48,803 - [Process 2/5] - INFO - res.shape is :torch.Size([199])
results:The report examines the Exceptional Family Member Program (EFMP) of the US Department of Defense (DOD), which provides support to families with special needs at their current and proposed locations. The report highlights the program's objectives, components, and challenges faced by families with special needs. The report also discusses the issues faced by EFMP participants, including overall satisfaction, school liaison officers, family support personnel, stigma, assignment coordination, special education services, surveys, warm hand-off, outreach, and joint base family support services. The report concludes that while the EFMP has made significant progress in providing support to families with special needs, there are still challenges that need to be addressed, such as inconsistent service delivery and lack of timely documentation. The report recommends that DOD should continue to work towards improving the EFMP's effectiveness and efficiency in providing support to families with special needs.
 20%|██        | 8/40 [02:07<07:50, 14.69s/it]2024-12-22 02:51:49,043 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:51,082 - [Process 0/5] - INFO - res.shape is :torch.Size([362])
results:
The report by the Government Accountability Office (GAO) assesses the federal government's approach and strategy to securing agency information systems. The report finds that while agencies have reported implementing various security measures, there are still significant challenges in securing information systems. The report highlights several areas of concern, including:

* Lack of a comprehensive cybersecurity strategy: The report notes that while there are various laws and policies in place, there is no overarching cybersecurity strategy that articulates priority actions, assigns responsibilities, and sets time frames for completion.
* Limited implementation of the NIST cybersecurity framework: The report finds that while agencies have reported implementing some of the 23 categories and 108 subcategories of the NIST cybersecurity framework, there is still a lack of implementation across all categories.
* Insufficient training and guidance: The report notes that agencies need more training and guidance from the Department of Homeland Security (DHS) to effectively implement the cybersecurity framework.
* Limited progress in meeting the cybersecurity-focused cross-agency priority goal: The report finds that agencies have made limited progress in meeting the 10 performance indicators set by the President's Management Agenda to improve cybersecurity.

Overall, the report concludes that while there have been some efforts to improve cybersecurity, there is still a long way to go in securing agency information systems. The report recommends that the federal government develop a comprehensive cybersecurity strategy, improve training and guidance, and increase progress in meeting the cybersecurity-focused cross-agency priority goal.
 20%|██        | 8/40 [02:09<09:03, 16.99s/it]2024-12-22 02:51:51,204 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:52,602 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:52,602 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 02:51:52,674 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:51:53,312 - [Process 1/5] - INFO - res.shape is :torch.Size([331])
results:
The report provides an overview of the funding for wastewater and drinking infrastructure programs since 1972. The Clean Water Act (CWA) authorizes the federal government to provide funding for these programs, and the report highlights the changes in funding levels and program structures over the years. The report shows that the federal government has provided $41 billion in funding for wastewater treatment assistance and $863 million for drinking water SRF capitalization grants since 1972. In 1987, the Water Infrastructure Financing and Investment Act was passed, which provided $18 billion for sewage treatment plant construction and established the Clean Water State Revolving Fund (CWSRF) program. The program was phased in beginning in FY1989 and entirely replaced the previous Title II program in FY1991. In 1996, the Safe Drinking Water Act (SDWA) was amended to establish a parallel program to help communities finance projects needed to comply with federal drinking water regulations. The report also highlights the funding levels and program structures for wastewater and drinking infrastructure programs in recent years, including the authorization of $6.15 billion for EPA in FY2019. The report concludes by summarizing the history of appropriations for both wastewater and drinking water infrastructure programs and providing a detailed chronology of congressional activity regarding wastewater and drinking water infrastructure funding.
 18%|█▊        | 7/40 [02:11<10:26, 18.97s/it]2024-12-22 02:51:53,591 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:54,683 - [Process 4/5] - INFO - res.shape is :torch.Size([225])
results:The report by the USDA's Food and Nutrition Service (FNS) provides an overview of the agency's efforts to prevent, detect, and respond to retailer trafficking in the Supplemental Nutrition Assistance Program (SNAP). The report highlights the agency's progress in implementing new regulations and policies to prevent trafficking, including increasing food stock requirements for retailers, focusing on high-risk retailers, and providing additional funding for state SNAP E&T programs. The report also discusses the agency's efforts to improve the accuracy of SNAP payments and address improper payments, which totaled $2.6 billion in 2014. Additionally, the report outlines the agency's approach to preventing and detecting fraud by recipients, including the use of data analytics and state agency oversight. The report concludes by highlighting the importance of continued efforts to prevent trafficking and improve program integrity.
 22%|██▎       | 9/40 [02:12<07:06, 13.76s/it]2024-12-22 02:51:54,694 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:54,694 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2062])
2024-12-22 02:51:54,763 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:51:54,932 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:55,537 - [Process 3/5] - INFO - res.shape is :torch.Size([305])
results:The Small Business Administration (SBA) administers several loan guaranty programs to support small businesses, including the 7(a) loan program, which is the agency's flagship program. In FY2018, the SBA approved 60,353 7(a) loans totaling nearly $25.4 billion. The program's borrower and lender eligibility standards and program requirements are outlined in the report, as well as program statistics, including loan volume, loss rates, use of the proceeds, borrower satisfaction, and borrower demographics. The report also examines issues raised concerning the SBA's administration of the 7(a) program, including the oversight of 7(a) lenders and the program's lack of outcome-based performance measures. Additionally, the report summarizes congressional and presidential actions taken in recent years to help small businesses gain greater access to capital, such as the American Recovery and Reinvestment Act of 2009 and the Small Business Jobs Act of 2010. Finally, the report highlights the SBA's Community Advantage loan initiative, which provides the same loan terms, guaranty fees, and guaranty as the 7(a) program on loan amounts up to $250,000.
 20%|██        | 8/40 [02:13<08:40, 16.25s/it]2024-12-22 02:51:55,843 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:51:57,203 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:57,203 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1734])
2024-12-22 02:51:57,285 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:51:58,555 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:58,556 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2138])
2024-12-22 02:51:58,621 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:51:59,468 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:51:59,469 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2078])
2024-12-22 02:51:59,538 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:52:03,328 - [Process 2/5] - INFO - res.shape is :torch.Size([239])
results:The James Webb Space Telescope (JWST) is a large deployable space telescope designed to observe infrared sources and study planets in other solar systems. The report highlights the challenges faced by the JWST program, including schedule delays and cost increases, and the measures taken to address these issues. The program has implemented various oversight augmentations, including award fees to incentivize the contractor to improve performance, and has reserved schedule and cost reserves to manage risks. The program has also conducted audits to minimize the risk of failures during the remaining phases of integration and test. The report notes that the program has a history of significant schedule delays and cost growth, which led to a replan in 2011 and a rebaseline in 2018. The program is currently awaiting authorization from Congress to continue due to cost increases above the $8 billion cost cap. The report concludes that the JWST program faces numerous risks and unforeseen technical challenges, and it is essential to manage these risks to ensure the program's success.
 22%|██▎       | 9/40 [02:21<07:33, 14.64s/it]2024-12-22 02:52:03,562 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:04,850 - [Process 0/5] - INFO - res.shape is :torch.Size([250])
results:The report examines the status of Economic Support Fund (ESF) assistance to the West Bank and Gaza for fiscal years 2015 and 2016, including payments to Palestinian Authority (PA) creditors. The report finds that USAID obligated $544.1 million and expended $350.6 million of the allocated $544.5 million for the West Bank and Gaza in fiscal years 2015 and 2016. The majority of the funds went towards the Economic Growth and Infrastructure development objective, which accounted for 60% of the total obligations. The report also found that USAID conducted required vetting of PA creditors to ensure that assistance would not support entities or individuals associated with terrorism. Additionally, the report identified weaknesses in USAID's financial systems, including inadequate accountability for USAID funds. Overall, the report concludes that while USAID's management of ESF assistance to the West Bank and Gaza was generally effective, there were areas for improvement in the vetting process and financial systems.
 22%|██▎       | 9/40 [02:23<08:15, 15.98s/it]2024-12-22 02:52:05,027 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:06,555 - [Process 1/5] - INFO - res.shape is :torch.Size([204])
results:
The report examines the differences in prices of products targeted towards men and women in the consumer market. The study found that women's products are often priced higher than men's products, even when controlling for observable factors such as brand, size, and promotional expenses. The report analyzed data from 10 product categories and found that in 5 out of 10 categories, women's versions of the product sold at a higher average price per ounce or count than men's. The study also found that men's versions of products sold at a higher average price in 2 out of 10 categories. The report concludes that gender-based price differences may be attributed to gender bias, but other factors such as differences in costs and consumer preferences may also play a role. The report highlights the need for further research to fully understand the reasons behind these price differences and to address any potential gender bias in the pricing of consumer products.
 20%|██        | 8/40 [02:24<09:08, 17.15s/it]2024-12-22 02:52:06,829 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:07,119 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:07,119 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 02:52:07,190 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:52:07,500 - [Process 4/5] - INFO - res.shape is :torch.Size([194])
results:The report discusses the potential risks and benefits of LNG as a maritime fuel, particularly in light of the International Maritime Organization's (IMO) upcoming sulfur emissions standards. The IMO's standards limiting the maximum sulfur content in shipping fuels could lead to increased demand for LNG as a bunker fuel, which could create new market opportunities for U.S. natural gas producers, shipbuilders, and infrastructure developers. However, the report also highlights potential safety and security concerns associated with LNG bunkering, including the risk of explosions or fires within 500 meters of the spill and the potential for terrorist attacks on LNG tankers or bunkering vessels. The report concludes that while LNG bunkering could present economic opportunities for the United States, it is important to address the safety and security concerns associated with its use.
 25%|██▌       | 10/40 [02:25<06:44, 13.47s/it]2024-12-22 02:52:07,821 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:08,573 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:08,574 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 02:52:08,642 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:52:09,980 - [Process 3/5] - INFO - res.shape is :torch.Size([234])
results:
The report examines the Summer Food Service Program (SFSP) for low-income children in the United States. The program provides free meals to children in areas where at least half of the children are eligible for free or reduced-price school meals. The report provides an overview of the program's administration, data collection, and meal service availability. It also discusses the challenges faced by the program, including funding, sponsor turnover, and outreach efforts. The report highlights that the number of SFSP meals served nationwide increased by 32% from fiscal year 2007 to 2016, with the majority of meals served during the summer months. The report also notes that supper and breakfast meals had the largest percentage increases over the 10-year period. The report concludes by stating that the program faces challenges in providing summer meals to children, including funding and sponsor turnover, and that alternative meal delivery models and nonfederally funded programs are being employed by SFSP sponsors.
 22%|██▎       | 9/40 [02:28<08:06, 15.69s/it]2024-12-22 02:52:10,223 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:10,404 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:10,404 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 02:52:10,476 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:52:11,349 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:11,349 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1938])
2024-12-22 02:52:11,423 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:52:13,863 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:13,864 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1867])
2024-12-22 02:52:13,943 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:52:16,171 - [Process 0/5] - INFO - res.shape is :torch.Size([187])
results:The report examines the Small Business Lending Fund (SBLF) established by the Small Business Jobs Act of 2010. The SBLF aimed to increase small business lending by providing capital investments to eligible community banks. The report discusses the program's structure, eligibility criteria, and the arguments for and against its enactment. The program's implementation and bills introduced in recent Congresses to amend the SBLF are also discussed. The report highlights that the SBLF's advocates argued that it was an important part of a larger effort to enhance the supply of small business loans, while opponents claimed that the program could lose money and lack sufficient oversight provisions. The report concludes by examining the SBLF's implementation and bills introduced in recent Congresses to amend the program.
 25%|██▌       | 10/40 [02:34<07:16, 14.54s/it]2024-12-22 02:52:16,368 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:17,568 - [Process 2/5] - INFO - res.shape is :torch.Size([233])
results:The report discusses the current state of child nutrition programs in the United States, including the challenges and issues faced by schools in providing healthy meals to children. The report highlights the need for national standards for meal charges and the provision of alternate meals, as well as the impact of the Healthy, Hunger-Free Kids Act of 2010 on these programs. It also covers the implementation of the Act's requirements, including the provision of free, reduced-price, and full-price meals in participating schools, and the use of nonfederal funding sources to cover costs. The report also discusses the issues related to lunch shaming and unpaid meal costs, including the use of visual indicators to identify children with unpaid meal debt and the impact of the 2018 farm bill on these issues. Finally, the report provides an overview of the current issues in child nutrition programs, including the need for local control of meal charge policies and the use of nonfederal funding sources to cover costs.
 25%|██▌       | 10/40 [02:35<07:15, 14.52s/it]2024-12-22 02:52:17,935 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:19,935 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:19,935 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2171])
2024-12-22 02:52:20,000 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:52:21,495 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:21,496 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 02:52:21,567 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:52:23,458 - [Process 4/5] - INFO - res.shape is :torch.Size([261])
results:The report provides information on the testing and remediation of lead in school drinking water in the United States. According to the report, about 36% of charter school districts tested for lead in school drinking water, and 20 schools had lead levels above the action level. The report also provides information on the 3Ts guidance for reducing lead in drinking water, which includes telling students, parents, staff, and the larger community about monitoring programs, potential risks, the results of testing, and remediation actions. The report also highlights the importance of clear guidance on lead levels in school drinking water and the need for schools to test their water regularly. Additionally, the report notes that lead can enter drinking water through corrosion of lead-containing plumbing materials and fixtures, and that the best way to know if a school's water is contaminated with lead is to test the water after it has gone through the school's pipes, faucets, and other fixtures. The report concludes by stating that EPA should strengthen its oversight of the Lead and Copper Rule and provide additional data and statistical analysis to enhance its monitoring of lead in drinking water.
 28%|██▊       | 11/40 [02:41<06:52, 14.23s/it]2024-12-22 02:52:23,731 - [Process 3/5] - INFO - res.shape is :torch.Size([220])
results:The report by the Government Accountability Office (GAO) discusses the efforts of the Centers for Medicare and Medicaid Services (CMS) to prevent and detect fraud in the Medicare program. The report highlights that CMS has taken steps to develop an antifraud strategy, including creating a dedicated entity to lead antifraud efforts and conducting fraud risk assessments. However, the report notes that CMS has not conducted a fraud risk assessment or developed a risk-based antifraud strategy for Medicare as defined in the Fraud Risk Framework. The report also notes that CMS has not fully aligned its antifraud efforts with the Fraud Risk Framework, and that the agency could expand its antifraud training to include all employees. The report recommends that CMS take actions to continue prior authorization efforts to reduce spending, fully align its antifraud efforts with the Fraud Risk Framework, and establish monitoring and evaluation mechanisms for its program-integrity control activities.
 25%|██▌       | 10/40 [02:41<07:32, 15.09s/it]2024-12-22 02:52:23,811 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:23,947 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:25,039 - [Process 1/5] - INFO - res.shape is :torch.Size([320])
results:

The report by the Government Accountability Office (GAO) examines how the Federal Energy Regulatory Commission (FERC) collects and analyzes information on dam safety, evaluates engineering studies of dam performance, and reviews dam safety information during relicensing. The report finds that FERC primarily relies on engineering judgment to assess dam safety and that its approach to ensuring compliance with safety regulations is flexible due to the variation in hazards, consequences, and dams. The report also notes that FERC's use of enforcement tools is limited, with only one civil penalty issued for a safety-related hydropower violation since 2013. The report highlights the importance of dam safety inspections, engineering studies, and relicensing processes, and identifies areas for improvement in FERC's approach to dam safety oversight. The report concludes that FERC's multi-layered oversight approach, which includes independent and coordinated actions with dam owners and independent consultants, is effective in ensuring dam safety. However, the report notes that FERC's approach to collecting, recording, and using safety information could be improved, and that the agency's reliance on engineering judgment may limit its ability to fully assess dam safety. Overall, the report provides valuable insights into FERC's dam safety oversight and identifies areas for improvement to ensure the safety of dams and the communities they serve.
 22%|██▎       | 9/40 [02:43<09:04, 17.56s/it]2024-12-22 02:52:25,296 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:27,210 - [Process 0/5] - INFO - res.shape is :torch.Size([179])
results:
The report discusses the Navy's force structure and shipbuilding plans, including the current and planned size and composition of the Navy, the rate of Navy ship procurement, and the affordability of the Navy's shipbuilding plans. The report highlights the tension between the Navy's goal of minimizing procurement cost growth and the potential risk of leaving money on the table if the Navy sets costs too low. The report also discusses the potential impact of the Navy's force structure goals on the shipbuilding industry and the importance of considering both the goal of minimizing procurement cost growth and the need to maintain a capable and effective Navy. The report concludes by emphasizing the importance of taking a holistic approach to Navy shipbuilding and considering the broader strategic and budgetary context in which the Navy's plans are being developed.
 28%|██▊       | 11/40 [02:45<06:30, 13.47s/it]2024-12-22 02:52:27,361 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:27,383 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:27,383 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 02:52:27,454 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:52:27,562 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:27,562 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2053])
2024-12-22 02:52:27,634 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:52:28,873 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:28,873 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 02:52:28,944 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:52:30,903 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:30,903 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1966])
2024-12-22 02:52:30,978 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:52:32,272 - [Process 2/5] - INFO - res.shape is :torch.Size([240])
results:The Central States Pension Fund (CSPF) is a defined benefit multiemployer pension plan that provides retirement benefits to International Brotherhood of Teamsters union members in the trucking industry. The plan is governed by a consent decree between the court, labor unions, and employers, which outlines roles and responsibilities for each party. The plan has been underfunded in the past but has shown improvement since 2009. However, some plans within the multiemployer system face financial difficulties. CSPF has a board of trustees that sets benefit levels and operates under an assumed-return approach for investment purposes. The plan has a funded percentage of around 38%, and the independent special counsel has the power to remove named fiduciaries. The plan's assets are managed by named fiduciaries, who have exclusive authority to manage and control assets. The report highlights the importance of the consent decree in governing the plan and the need for ongoing monitoring and oversight to ensure compliance with ERISA and the plan's funding status.
 28%|██▊       | 11/40 [02:50<07:02, 14.57s/it]2024-12-22 02:52:32,505 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:35,024 - [Process 4/5] - INFO - res.shape is :torch.Size([168])
results:
The report examines the use of Other Transaction Authorities (OTs) by the Department of Defense (DOD) to carry out projects for research, prototyping, and production. OTs are legally binding contracts that are exempt from federal procurement laws and regulations, allowing DOD to quickly and efficiently conduct research and development projects without unnecessary delay. The report discusses the history of OTs, their potential benefits and risks, and how they are used by DOD. It also provides data on the use of OTs by DOD and other agencies, and how they can be used to promote the defense technology and industrial base. The report concludes by highlighting the importance of OTs in promoting innovation and collaboration in the defense sector.
 30%|███       | 12/40 [02:53<06:15, 13.42s/it]2024-12-22 02:52:35,425 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:36,113 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:36,114 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2165])
2024-12-22 02:52:36,179 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:52:36,954 - [Process 1/5] - INFO - res.shape is :torch.Size([178])
results:The report provides an overview of the federal child nutrition programs in the United States, including the National School Lunch Program, School Breakfast Program, Child and Adult Care Food Program, Summer Food Service Program, and Special Milk Program. The report highlights the eligibility rules, participation, and funding of each program, as well as the history of the programs and their authorization and reauthorization. The report also discusses the Team Nutrition program, farm-to-school programs, and administrative review process. The majority of funding for the programs is provided through annual appropriations acts, with some funding mandated by law. The report concludes by listing the acronyms used in the report and providing a list of per-meal or per-snack reimbursement rates for child nutrition programs.
 25%|██▌       | 10/40 [02:55<07:54, 15.82s/it]2024-12-22 02:52:37,185 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:39,003 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:39,004 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 02:52:39,075 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:52:39,596 - [Process 3/5] - INFO - res.shape is :torch.Size([267])
results:The report by the Government Accountability Office (GAO) highlights the challenges faced by the Social Security Administration (SSA) in managing its IT systems and operations. The report notes that SSA has made progress in implementing two of the three recommendations made by GAO to improve its IT management, but it has not yet implemented the third recommendation to issue guidance on identifying IT acquisitions. The report also notes that SSA's IT systems are critical to its operations, but the agency has faced challenges in managing its software licenses, IT acquisitions, and IT workforce. The report recommends that SSA address the weaknesses in its policies regarding the role of the CIO in key management areas to improve its IT management. Additionally, the report notes that SSA has taken steps to improve its IT management by addressing 14 of the 15 recommendations made by GAO regarding data center consolidation, incremental development, IT acquisitions, and software licenses. However, the report concludes that SSA would be better positioned to effectively address longstanding IT management challenges by ensuring that it has policies in place that fully address the role and responsibilities of its CIO in the five key management areas.
 28%|██▊       | 11/40 [02:57<07:24, 15.33s/it]2024-12-22 02:52:39,819 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:40,046 - [Process 0/5] - INFO - res.shape is :torch.Size([225])
results:
The report examines the Small Business Administration's (SBA) 7(a) loan program, which provides financial assistance to small businesses. The program guarantees loans made by commercial lenders to small businesses for working capital and other general business purposes. The report finds that in 2016, the total amount of approved 7(a) loans decreased, but the number of loans approved increased. The top eight industry sectors by proportion of total 7(a) loans approved remained unchanged from 2007 to 2016. The report also analyzes creditworthiness among 7(a) program borrowers, finding that creditworthiness varied widely among borrowers. The report notes that some studies have suggested disparities in credit access among small businesses based on the borrower's race/ethnicity and other factors. The report highlights the need for SBA to continue monitoring lenders' compliance with the credit elsewhere requirement and to address any disparities in credit access among small businesses.
 30%|███       | 12/40 [02:58<06:11, 13.28s/it]2024-12-22 02:52:40,201 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:40,770 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:40,770 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 02:52:40,842 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:52:43,430 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:43,431 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 02:52:43,503 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:52:43,757 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:43,758 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1884])
2024-12-22 02:52:43,837 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:52:49,422 - [Process 4/5] - INFO - res.shape is :torch.Size([229])
results:The report examines the legal landscape of data protection in the United States, focusing on the regulations and laws that govern the collection, use, and disclosure of personal data. The report highlights the challenges in protecting personal data due to the lack of a comprehensive federal data protection law, leading to a patchwork of state and foreign laws that may create uneven regulations. The report also discusses the Supreme Court's decision in Sorrell v. IMS Health, Inc. and its implications for future data protection legislation. The report concludes that any legislative efforts to address data protection will need to consider the First Amendment and the potential impact on free speech. The report also notes that the current legal landscape provides some remedies and protections through tort and contract law, but these are limited and may not be uniform across all states. Finally, the report suggests that any future data protection legislation will need to balance the need to protect personal data with the need to protect free speech and ensure that any regulations are tailored to address the specific challenges in the digital age.
 32%|███▎      | 13/40 [03:07<06:10, 13.72s/it]2024-12-22 02:52:49,510 - [Process 2/5] - INFO - res.shape is :torch.Size([297])
results:The report by the Government Accountability Office (GAO) examines the challenges faced by community providers when contacting the third-party administrators (TPAs) for claims payment issues under the Veterans Access, Choice, and Accountability Act of 2014. The report reveals that despite the VA's efforts to improve the timeliness of TPA claim payments to providers, VA is not collecting data or monitoring compliance with two Choice Program requirements, which could affect the timeliness of community providers' payment. The report highlights three key factors affecting the timeliness of claim payments to community providers under the Choice Program, including VA's untimely payments of TPA invoices, Choice Program contractual requirements related to provider reimbursement, and inadequate provider education on filing Choice Program claims. The report recommends that VA monitor data on SAR approval decision time frames to ensure VA medical facilities are in adherence with VA policy, collect data and monitor compliance with the Choice Program contractual requirements for community providers, and take corrective actions as necessary. The report also notes that VA has included customer service requirements in its RFP for the Veterans Community Care Program contracts and plans to establish a call center to address inquiries from community providers.
 30%|███       | 12/40 [03:07<07:10, 15.39s/it]2024-12-22 02:52:49,672 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:49,819 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:52,526 - [Process 0/5] - INFO - res.shape is :torch.Size([216])
results:
The report examines the use of performance information in the federal government and identifies areas for improvement. The report is based on a survey of federal managers and finds that while some progress has been made in using performance information, there are still significant challenges to be addressed. The survey shows that managers' use of performance information to make decisions has not changed much since 2007, and there is a lack of consistency in using performance information across agencies. The report highlights five leading practices that can promote the use of performance information for policy and program decisions, and identifies areas where agencies can improve their use of performance information. The report also notes that the use of data-driven reviews has been effective in improving data-driven decision making, and that there is a need for more consistent use of performance information across agencies. The report concludes that while progress has been made in using performance information, more work needs to be done to ensure that performance information is used consistently and effectively across the federal government.
 32%|███▎      | 13/40 [03:10<05:51, 13.04s/it]2024-12-22 02:52:52,671 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:52,929 - [Process 1/5] - INFO - res.shape is :torch.Size([269])
results:The report discusses the Black Lung Disability Trust Fund, which is funded by an excise tax on coal produced and sold domestically, and how the fund is facing financial strain due to declining coal production, increased costs of black lung disease, and borrowing from the general fund to meet obligations. The report highlights the need to consider policy options to improve the fund's financial outlook, such as reducing benefits or increasing the ability of the federal government to ensure responsible coal operators pay benefits. The report also notes that the eligibility of miners and survivors for Part C benefits could be restricted to reduce expenditures from responsible operators and the trust fund. Additionally, the report mentions that the federal government pays benefits to coal miners affected by black lung disease and other lung diseases linked to coal mining, and how the Black Lung Disability Trust Fund is used to finance these benefits when responsible coal operators cannot pay. The report concludes by stating that Congress may need to examine the issue of coal operator bankruptcies and the interaction of bankruptcy law and the Black Lung Benefits Act's lien provisions to strengthen the federal government's ability to ensure responsible operators pay benefits and reduce benefit expenditures of the trust fund.
 28%|██▊       | 11/40 [03:11<07:40, 15.87s/it]2024-12-22 02:52:53,161 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:53,249 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:53,250 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2045])
2024-12-22 02:52:53,321 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:52:53,354 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:53,355 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2109])
2024-12-22 02:52:53,423 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:52:55,110 - [Process 3/5] - INFO - res.shape is :torch.Size([259])
results:The federal government collects fees and charges from businesses and households for various services and activities, such as court filings, national park entrance fees, and securities filing fees. These fees are classified as user fees, which are different from taxes. The report discusses the advantages and disadvantages of using user fees as a means of financing public activities, including that they are voluntary, connect the burden of financing activities to those who benefit from them, and can help decentralize decision-making. However, the report also notes that user fees may not be compulsory and may not cover the full cost of providing services, which can lead to fluctuations in economic conditions and instability in financing government operations. The report highlights the need for careful consideration and evaluation of user fees to ensure that they are used effectively and efficiently. Additionally, the report notes that privatization of government services may also be a means of financing public activities, but it can lead to policy or operational failure. The report concludes that user fees are a complex issue that requires careful consideration of their advantages and disadvantages, as well as their impact on the federal budget and government operations.
 30%|███       | 12/40 [03:13<07:10, 15.38s/it]2024-12-22 02:52:55,346 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:52:56,189 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:56,189 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1993])
2024-12-22 02:52:56,261 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:52:56,800 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:56,800 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2166])
2024-12-22 02:52:56,866 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:52:58,959 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:52:58,960 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1959])
2024-12-22 02:52:59,036 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:53:03,941 - [Process 4/5] - INFO - res.shape is :torch.Size([238])
results:The report by the Government Accountability Office (GAO) evaluates the reliability of the US Census Bureau's life-cycle cost estimate for the 2020 Census. The Bureau's estimate was found to be partially reliable, as it met two of the four characteristics of a high-quality estimate (comprehensive and accurate) but minimally met the other two (well-documented and credible). The report highlights that the Bureau's cost estimation and analysis guidance did not fully reflect best practices for developing reliable cost estimates, and the estimate was not fully documented, accurate, or credible. The Bureau took steps to address these issues, but more work is needed to ensure the reliability of the estimate. The report also found that the Bureau's cost estimation process was not fully integrated, and there were limitations in the data used to develop the estimate. The Bureau's estimate was compared to two independent cost estimates, and there were discrepancies in the totals overall and by WBS category. The report concludes that the Bureau needs to improve its cost estimation and analysis processes to ensure the reliability of its estimates.
 35%|███▌      | 14/40 [03:22<06:02, 13.96s/it]2024-12-22 02:53:04,025 - [Process 2/5] - INFO - res.shape is :torch.Size([238])
results:
The report examines the capital spending of the Washington Metropolitan Area Transit Authority (WMATA) from 2011 to 2017. The report was conducted by the Government Accountability Office (GAO) to assess WMATA's capital spending and compare it to leading practices in the transit industry. The report found that WMATA received $5.8 billion in capital funding from the federal government and state and local jurisdictions during the period, with the majority going towards replacing, rehabilitating, and maintaining its revenue vehicles (railcars, buses, and vans). The report also identified several areas for improvement, including the need for a more comprehensive and reliable data collection system and the implementation of a new capital planning process that incorporates industry-wide best practices. Additionally, the report found that WMATA's track preventive maintenance program was inadequate and needed improvement to meet industry standards. Overall, the report provides valuable insights into WMATA's capital spending and highlights areas for improvement to enhance the efficiency and effectiveness of its operations.
 32%|███▎      | 13/40 [03:22<06:48, 15.12s/it]2024-12-22 02:53:04,169 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:04,476 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:07,752 - [Process 3/5] - INFO - res.shape is :torch.Size([196])
results:The report discusses the history and current state of the BRAC (Base Realignment and Closure) process in the US, specifically focusing on the legal framework, recent legislation, and the 2005 BRAC round. The report highlights the complexities and challenges of the BRAC process, including the involvement of multiple stakeholders, the need for careful planning and analysis, and the potential for cost savings. The report also provides an overview of the 2005 BRAC round, including the criteria used to evaluate military installations and the timeline for implementation. Finally, the report notes that the authority for future BRAC rounds has not been proposed in the 116th Congress, and that the Department of Defense has not intended to use the new BRAC-like authorities authorized by Section 2702 of the FY2019 NDAA.
 32%|███▎      | 13/40 [03:25<06:32, 14.55s/it]2024-12-22 02:53:07,789 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:07,789 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1836])
2024-12-22 02:53:07,871 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:53:08,000 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:08,010 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:08,010 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 02:53:08,078 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:53:10,767 - [Process 0/5] - INFO - res.shape is :torch.Size([359])
results:

The report by the Government Accountability Office (GAO) examines how civilian Chief Financial Officer (CFO) Act agencies use telework in office space planning. The report finds that while 45% of federal workers are eligible to telework, only 29% participate, with an increase to 46% between 2012 and 2015. The report also reveals that agencies face challenges in using telework in space planning, including a lack of guidance and resources. The report highlights the importance of telework in reducing space needs and increasing efficiency, citing the National Strategy for the Efficient Use of Real Property and the Reduce the Footprint policy. The report also notes that GSA defines mobility as an overarching term that includes telework, desk-sharing, site work, and travel, and that agencies can strategically use telework combined with desk-sharing and hoteling to reduce office space needs. The report finds that 23 civilian CFO Act agencies reported using telework in their space-planning policies and procedures, with 15 agencies incorporating telework as a strategic space-planning tool. The report also identifies challenges faced by agencies in using telework, including a lack of guidance and resources, and notes that GSA has a role in promulgating rules and developing guidance promoting the efficient use of real property. Finally, the report finds that agencies have made progress in reducing space through telework, with examples of successful implementation at the Department of the Treasury, the Department of Health and Human Services, and the Department of Justice.
 35%|███▌      | 14/40 [03:29<06:19, 14.61s/it]2024-12-22 02:53:10,983 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:11,604 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:11,604 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 02:53:11,676 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:53:12,101 - [Process 1/5] - INFO - res.shape is :torch.Size([339])
results:The report examines the economic situation in Puerto Rico and identifies the factors that contributed to the territory's financial crisis. The report also provides recommendations for addressing these factors and improving Puerto Rico's economic situation. The key findings include:

1. Puerto Rico's economy has been in a prolonged period of contraction, with a decline in real output and population.
2. The territory's government has operated with a deficit since 2002, and its deficits have grown over time.
3. Puerto Rico's debt has grown significantly, with a total public debt of 66% of GDP and 99% of GNP in 2014.
4. The report identifies several factors that contributed to Puerto Rico's financial crisis, including a decline in the manufacturing sector, a high debt-to-GDP ratio, and a lack of economic diversification.
5. The report recommends several actions to address these factors and improve Puerto Rico's economic situation, including:
* Implementing a new public healthcare model that shifts the burden of healthcare costs to patients.
* Collaborating with the private sector for future infrastructure and service projects.
* Improving tax incentives to attract businesses and stimulate economic growth.
* Developing a long-term economic development plan that addresses the territory's economic challenges.

Overall, the report highlights the need for urgent action to address Puerto Rico's financial crisis and improve its economic situation.
 30%|███       | 12/40 [03:30<07:52, 16.87s/it]2024-12-22 02:53:12,410 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:14,517 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:14,518 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2038])
2024-12-22 02:53:14,589 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:53:15,356 - [Process 4/5] - INFO - res.shape is :torch.Size([168])
results:The report discusses the history of Congressional pay adjustments since 1789, including the various methods by which pay can be adjusted, such as legislation, the President's recommendations, and automatic adjustments based on changes in private sector wages. The report also provides historical tables on Member pay since 1789 and details on recent pay adjustments. Additionally, the report summarizes legislation introduced in the 112th Congress to freeze or reduce Member pay, as well as provisions in appropriations bills to prevent pay adjustments. The report concludes by noting that while the maximum potential January 2020 Member pay adjustment is 2.6%, the actual adjustment will depend on the Employment Cost Index and any legislative action.
 38%|███▊      | 15/40 [03:33<05:29, 13.19s/it]2024-12-22 02:53:15,626 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:15,957 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:15,957 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-22 02:53:16,026 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:53:19,250 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:19,250 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1850])
2024-12-22 02:53:19,331 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:53:19,803 - [Process 3/5] - INFO - res.shape is :torch.Size([177])
results:The report provides an overview of the requirements for institutions of higher education (IHEs) to participate in Title IV student aid programs. The report highlights the eligibility criteria for IHEs, including the 90/10 rule, program integrity triad, and campus safety policies. It also discusses the Clery Act and its requirements for IHEs to report campus crime statistics and establish safety policies. Additionally, the report covers distance education and correspondence courses, including the definition of distance education and the 50% rules. The report concludes with a discussion on the eligibility of foreign institutions for Title IV participation. The report provides a detailed overview of the requirements for IHEs to participate in Title IV student aid programs, ensuring that students have access to federal financial aid to pursue their higher education goals.
 35%|███▌      | 14/40 [03:38<05:58, 13.80s/it]2024-12-22 02:53:20,117 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:21,711 - [Process 2/5] - INFO - res.shape is :torch.Size([306])
results:The report discusses the current state of the World Trade Organization (WTO) and its role in international trade. The WTO was established in 1995 as an international organization that administers trade rules and agreements negotiated by its 164 members. The United States played a major role in the establishment of the WTO and has been a key player in shaping its rules and agreements. However, the WTO has faced challenges in recent years, including the inability to address certain Chinese policies and the growing use of protectionist trade policies by developed and developing countries. The report highlights the concerns of the United States and other WTO members regarding the role and procedures of the WTO's Appellate Body, as well as the use of developing country status by countries to avoid implementing future liberalization commitments. The report also discusses the Administration's recent skepticism towards the WTO and its potential withdrawal from the organization. The report concludes by stating that the future outlook of the WTO is uncertain and that there is a growing consensus that the status quo is no longer sustainable. The report provides background history of the WTO, its organization, and current status of negotiations, as well as key policy issues for Congress, including the value of U.S. membership and leadership in the WTO, and the potential impact of U.S. unilateral tariff actions on future trade negotiations.
 35%|███▌      | 14/40 [03:39<06:53, 15.90s/it]2024-12-22 02:53:21,919 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:22,959 - [Process 0/5] - INFO - res.shape is :torch.Size([208])
results:The report highlights the risks associated with the implementation of IT systems in federal agencies and critical infrastructures. The report notes that these systems are complex, diverse, and geographically dispersed, making it difficult to identify and manage risks. The report also notes that cyber threats pose an increasing risk to national security, economic well-being, and public health and safety. The report recommends that agencies take steps to address these risks, including implementing solutions to address Automatic Dependent Surveillance-Broadcast Out-related security risks, enhancing cybersecurity in the maritime environment, and measuring cybersecurity implementation progress. The report also notes that advancements in technology have made it easier to collect and process large amounts of personal information, which poses challenges to ensuring the privacy of such information. The report concludes that top-level attention and action are needed to address these risks and ensure the privacy of personal information.
 38%|███▊      | 15/40 [03:41<05:46, 13.88s/it]2024-12-22 02:53:23,098 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:23,868 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:23,868 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1924])
2024-12-22 02:53:23,949 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:53:25,480 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:25,480 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 02:53:25,551 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:53:26,608 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:26,608 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 02:53:26,676 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:53:30,345 - [Process 4/5] - INFO - res.shape is :torch.Size([238])
results:The report by the Government Accountability Office (GAO) examines the efforts of the US Department of Defense (DOD) to address unwanted sexual behaviors, including sexual assault, domestic violence, and sexual harassment, within the military. The report finds that while DOD has taken steps to collect and maintain data on these incidents, there are still gaps in the data and inconsistencies in the way data is collected and reported across the military services. The report also identifies areas where DOD can improve its prevention and response efforts, including developing a more comprehensive prevention strategy and improving coordination among different offices and agencies within DOD. The report highlights the need for greater leadership commitment and oversight to ensure that sexual assault prevention and response efforts are effective and that victims receive the support and resources they need. Overall, the report concludes that while progress has been made in addressing unwanted sexual behaviors in the military, more work is needed to ensure that all servicemembers are safe and that the military culture is changed to prevent these behaviors from occurring in the first place.
 40%|████      | 16/40 [03:48<05:29, 13.73s/it]2024-12-22 02:53:30,703 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:31,327 - [Process 1/5] - INFO - res.shape is :torch.Size([339])
results:
The report by the Government Accountability Office (GAO) discusses the Centers for Medicare and Medicaid Services' (CMS) efforts to measure or reduce improper payments in its four principal programs: Medicare, Medicaid, Children's Health Insurance Program (CHIP), and the health insurance marketplaces. CMS uses return-on-investment and savings estimates to measure the effectiveness of its Medicare program-integrity activities and the Federal Protection and Affordable Care Act (PPACA) of 2010. CMS has also conducted individual evaluations of its program-integrity activities, such as the prior-authorization demonstration for power mobility devices. However, the report notes that CMS could benefit from more fully aligning its efforts with the four components of the Fraud Risk Framework, which includes developing an antifraud strategy based on fraud risk assessments for Medicare and Medicaid. The report highlights challenges associated with measuring fraud rates and identifying improper payments, and notes that CMS is well-positioned to leverage its fraud risk management efforts to provide additional antifraud training and develop an antifraud strategy based on fraud risk assessments for Medicare and Medicaid. The report also recommends that CMS provide fraud-awareness training to all employees, conduct fraud risk assessments for Medicare and Medicaid, and create an antifraud strategy that is aligned with and responsive to regularly assessed fraud risks.
 32%|███▎      | 13/40 [03:49<07:54, 17.59s/it]2024-12-22 02:53:31,637 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:34,261 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:34,262 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2094])
2024-12-22 02:53:34,331 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:53:35,221 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:35,222 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 02:53:35,293 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:53:35,566 - [Process 3/5] - INFO - res.shape is :torch.Size([255])
results:
The report by the USDA provides an overview of Foot-and-Mouth Disease (FMD) and its potential impact on the United States. The agency has identified 11 challenges related to FMD, including vaccination, and has taken various measures to address these challenges. The report highlights the importance of surveillance, epidemiologic tracing, diagnostic testing, quarantine, and biosecurity measures in controlling FMD. The agency has also developed a database of corrective actions to mitigate challenges related to FMD. The report concludes that USDA's evaluations of foreign animal health systems could benefit from better guidance and greater transparency, and that a plan is needed to evaluate the agency's efforts to address FMD. The report also notes that USDA has taken actions to reduce risks associated with avian influenza, but a plan is needed to evaluate these efforts. The report recommends that USDA develop a comprehensive strategy to strengthen disease surveillance in livestock and poultry, and that the agency work with federal agencies, states, tribes, and industry partners to address FMD challenges.
 38%|███▊      | 15/40 [03:53<05:59, 14.39s/it]2024-12-22 02:53:35,809 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:36,118 - [Process 2/5] - INFO - res.shape is :torch.Size([230])
results:
The report by the Government Accountability Office (GAO) examines the improper payment estimation methods used by executive branch agencies to identify and address improper payments in their programs. The report finds that while agencies have made progress in developing improper payment estimates, there are still areas of improvement. Specifically, the report notes that agencies are not consistently including all improper payments in their estimates, and some agencies are using alternative approaches that may not be appropriate. The report recommends that the Office of Management and Budget (OMB) provide guidance on how agencies should test to identify improper payments and clarify the treatment of nonresponse cases during improper payment testing. Additionally, the report suggests that agencies should update their estimation methodologies to include recovered overpayments in their estimates. The report also finds that agencies are using different sampling approaches and data populations to estimate improper payments, which can lead to varying results. Overall, the report highlights the need for consistency and accuracy in improper payment estimation to effectively address the problem of improper payments in federal programs.
 38%|███▊      | 15/40 [03:54<06:26, 15.45s/it]2024-12-22 02:53:36,340 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:38,310 - [Process 0/5] - INFO - res.shape is :torch.Size([288])
results:The report by the Government Accountability Office (GAO) discusses the challenges faced by the Department of Homeland Security's (DHS) Policy, Strategy, and Plans (PLCY) office in leading, coordinating, and developing policies and strategies across the department. The report highlights that PLCY lacks clear roles and responsibilities, and there is a lack of communication and collaboration among DHS components, which hinders the office's ability to carry out its functions effectively. The report recommends that the Secretary of Homeland Security should finalize a delegation of authority or similar document that clearly defines PLCY's mission, roles, and responsibilities, and create processes and procedures to implement it. Additionally, the report suggests that PLCY should use the DHS Workforce Planning Guide to identify and analyze gaps in its workforce, design strategies to address them, and communicate this information to DHS leadership. The report also recommends that PLCY enhance its collaboration and communication mechanisms with DHS stakeholders at the staff level to better identify and address emerging policy and strategy needs. Overall, the report emphasizes the importance of clear roles and responsibilities, effective communication, and collaboration among DHS components to ensure the department's policies and strategies are well-coordinated and effective.
 40%|████      | 16/40 [03:56<05:43, 14.32s/it]2024-12-22 02:53:38,439 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:39,570 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:39,570 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2113])
2024-12-22 02:53:39,639 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:53:39,943 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:39,943 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1913])
2024-12-22 02:53:40,023 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:53:41,973 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:41,973 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2047])
2024-12-22 02:53:42,045 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:53:44,932 - [Process 1/5] - INFO - res.shape is :torch.Size([210])
results:The Rental Assistance Demonstration (RAD) program was established in 2011 to help preserve the affordability of federally assisted rental properties and improve their physical and financial conditions. The program allows public housing agencies (PHAs) to convert public housing units to project-based vouchers or project-based rental assistance contracts. The report by the Government Accountability Office (GAO) provides an overview of the RAD program, including its goals, data collection, and financing sources. The report also examines the challenges faced by PHAs in implementing the program and the safeguards in place to protect residents. The GAO found that the RAD program has been successful in preserving affordability and improving the physical and financial conditions of the properties, but there are still challenges in accessing financing and managing the properties. The report highlights the need for continued support and oversight from HUD to ensure the success of the program.
 35%|███▌      | 14/40 [04:03<07:05, 16.38s/it]2024-12-22 02:53:45,182 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:48,829 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:48,830 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2142])
2024-12-22 02:53:48,896 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:53:50,538 - [Process 4/5] - INFO - res.shape is :torch.Size([360])
results:The Secret Service's Information Integration and Technology Transformation (IITT) investment includes two programs and one project with capabilities in planning or development and modernization. The Secret Service has not fully implemented all three selected practices within the workforce area, but has substantially implemented two practices. The Secret Service's IT workforce includes 190 staff, with 166 located in the headquarters and 24 in domestic field offices. The Secret Service has not implemented any of the three selected practices within the workforce area. The Secret Service's acquisition decision authority has not reviewed the program at predetermined checkpoints or milestones, and the Secret Service has not measured and monitored agile projects on agile metrics. The Secret Service's acquisition management framework is consistent with DHS's acquisition management policy, but the Secret Service has not tailored its framework to its non-major acquisition programs. The Secret Service's cost, schedule, and agile-related data are reliable, and the Secret Service has implemented agile as the preferred development approach for its IT programs and projects. The Secret Service's acquisition review process is effective, and the component-level CIOs have various responsibilities related to IT oversight and management. The Secret Service has planned to spend approximately $104.8 million on IT in fiscal year 2018, with $34.6 million for the development and modernization of its IT infrastructure and services, and $70.2 million for operations and maintenance. The Secret Service has one major IT investment and seven non-major IT investments, and has faced challenges in managing its IT infrastructure.
 42%|████▎     | 17/40 [04:08<06:00, 15.68s/it]2024-12-22 02:53:50,821 - [Process 3/5] - INFO - res.shape is :torch.Size([243])
results:The report by the Government Accountability Office (GAO) discusses the importance of evaluating the effectiveness of the Department of Health and Human Services' (HHS) efforts to address the opioid epidemic. The report highlights the need for HHS to establish performance measures with targets and timeframes for its evaluation of the expansion of access to medication-assisted treatment (MAT) for opioid use disorders. The report also notes that HHS has not yet finalized its evaluation approach or established related timeframes, which increases the risk of not completing the planned evaluation by September 2018. The report recommends that HHS establish performance measures with targets related to expanding access to MAT and establish timeframes in its evaluation approach. The report also highlights the importance of diversion control plans in preventing the misuse of prescription medication and notes that HHS has implemented five key efforts to expand access to MAT for opioid use disorders. The report concludes that while HHS has taken steps to address the opioid epidemic, more needs to be done to evaluate the effectiveness of its efforts.
 40%|████      | 16/40 [04:09<05:51, 14.65s/it]2024-12-22 02:53:50,865 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:50,972 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:52,289 - [Process 2/5] - INFO - res.shape is :torch.Size([268])
results:
The report examines the reliability of the Transition Assistance Program (TAP) data on participation levels and the factors affecting Coast Guard servicemembers' participation. The report found that the Coast Guard lacks policies and procedures governing reliable data collection, resulting in incomplete and unreliable data on TAP participation. The report also found that the Coast Guard does not have a Commandant Instruction that specifies when to record TAP participation data, leading to inconsistent and incomplete data updates. Additionally, the report found that the Coast Guard's data collection system is not sufficient to ensure reliable data, as TAP staff enter data into a shared spreadsheet that can be edited by multiple users. The report recommends that the Coast Guard adopt a new data system to more reliably track TAP participation and ensure that data are complete and accurate. The report also notes that the Coast Guard plans to issue a new Commandant Instruction in May 2018, which will provide guidance on TAP implementation and data collection. Overall, the report highlights the need for the Coast Guard to improve its data collection and reporting on TAP participation to ensure that it can effectively monitor and evaluate the program's effectiveness in supporting Coast Guard servicemembers' transition to civilian life.
 40%|████      | 16/40 [04:10<06:15, 15.66s/it]2024-12-22 02:53:52,441 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:53,646 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:53,647 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1516])
2024-12-22 02:53:53,698 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:53:54,396 - [Process 0/5] - INFO - res.shape is :torch.Size([306])
results:The report discusses the challenges faced by the Federal Emergency Management Agency (FEMA) in evaluating Individual Assistance (IA) declaration requests from states and tribes. The report highlights the difficulties in gathering information on the six factors used to evaluate IA requests, including state fiscal capacity and resource availability, uninsured home and personal property losses, disaster-impacted population profile, impact to community infrastructure, casualties, and disaster-related unemployment. The report also notes that FEMA has not consistently obtained and documented information on all elements of the current IA regulatory factors, which could lead to incorrect conclusions about the need for IA. The report recommends that FEMA evaluate why regions are not completing the Regional Administrator’s Validation and Recommendations for each element of the current IA regulatory factors and take corrective steps if necessary. FEMA has obligated over $8.6 billion in IA from calendar years 2008 through 2016, highlighting the importance of evaluating states' and tribes' IA declaration requests. The report also provides information on the number of IA declarations requested, declared, and denied, by region, state, and tribe, and the types of disasters for which IA declaration requests were received. Finally, the report includes information on FEMA's actual obligations for IA by state and type of disaster.
 42%|████▎     | 17/40 [04:12<05:41, 14.85s/it]2024-12-22 02:53:54,419 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:54,419 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 02:53:54,493 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:53:54,555 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:55,155 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:55,156 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1589])
2024-12-22 02:53:55,210 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:53:57,152 - [Process 1/5] - INFO - res.shape is :torch.Size([187])
results:The report examines how federal agencies acquire and manage heavy equipment. The General Accounting Office (GAO) reviewed data from 20 agencies and found that most of them acquire heavy equipment through leases rather than purchases. The agencies use various factors to decide whether to lease or purchase equipment, including cost and maintenance requirements. The report also found that agencies have different policies and procedures for managing heavy equipment utilization, and some agencies have developed reports to track and manage equipment utilization. The GAO interviewed officials from the selected agencies and reviewed data on heavy equipment acquisitions and utilization. The report concludes that agencies need to improve their lease-versus-purchase analyses and better document their decisions. The GAO recommends that agencies develop standardized policies and procedures for managing heavy equipment utilization.
 38%|███▊      | 15/40 [04:15<06:18, 15.13s/it]2024-12-22 02:53:57,377 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:53:58,096 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:53:58,096 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 02:53:58,167 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:54:01,010 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:01,011 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1838])
2024-12-22 02:54:01,091 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:54:03,044 - [Process 3/5] - INFO - res.shape is :torch.Size([212])
results:
The Holman rule is a provision in the House of Representatives' rules that allows for the consideration of legislation reducing expenditures in appropriations bills. The rule was established in 1835 and has been modified several times since then. The current version of the rule allows for retrenchments resulting from a reduction of the number and salary of officers of the United States or the reduction of the compensation of any person paid out of the Treasury of the United States. The rule does not apply to funds other than those appropriated in the pending general appropriations bill, and it does not allow for legislative provisions that would expand the scope of the bill. The House Parliamentarian is the sole definitive authority on questions relating to the chamber's precedents and procedures, and any specific parliamentary question should be consulted for a formal opinion. In the 115 th Congress, one amendment was considered in order based on the Holman rule, but it failed to pass.
 42%|████▎     | 17/40 [04:21<05:20, 13.92s/it]2024-12-22 02:54:03,316 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:05,119 - [Process 0/5] - INFO - res.shape is :torch.Size([173])
results:The Forest Service's internal control over its budget execution processes is inadequate, according to a recent report by the Government Accountability Office (GAO). The report found that the agency did not have an adequate process and related control activities for reasonably assuring that appropriated funds are used for their intended purposes. Specifically, the Forest Service did not have a process for allotting its budgetary resources, and its control activities for reimbursable agreements and unliquidated obligations were not properly designed. The agency also did not have a process for reviewing unobligated, no-year funds from prior years for continuing need. The GAO report recommends that the Forest Service improve its internal control over its budget execution processes to ensure that appropriated funds are used for their intended purposes.
 45%|████▌     | 18/40 [04:23<04:59, 13.61s/it]2024-12-22 02:54:05,249 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:05,338 - [Process 2/5] - INFO - res.shape is :torch.Size([230])
results:The report examines the funding provided by seven federal agencies for research on the health effects of low-dose radiation exposure, including the National Cancer Institute, the National Institute of Environmental Health Sciences, the Department of Energy, the National Institute of Biomedical Imaging and Bioengineering, the National Institute of Allergy and Infectious Diseases, the National Institute of Environmental Health Sciences, and the National Toxicology Program. The report finds that while the agencies have provided funding for epidemiological studies on low-dose radiation, there is a lack of coordination and consistency in their funding priorities and approaches. The report recommends that the Department of Energy lead the development of a mechanism for interagency collaboration to determine roles and responsibilities for addressing research priorities related to low-dose radiation. The report also notes that while federal agencies have established dose limits for radiation exposure, there is uncertainty about the effectiveness of these limits and the need for further research to reduce uncertainty about the health effects of low-dose radiation.
 42%|████▎     | 17/40 [04:23<05:42, 14.88s/it]2024-12-22 02:54:05,576 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:06,765 - [Process 4/5] - INFO - res.shape is :torch.Size([274])
results:
The report by the government agency highlights the issue of disproportionate discipline in K-12 public schools, particularly against Black students, boys, and students with disabilities. The report is based on data from the Department of Education's Civil Rights Data Collection (CRDC) for school year 2013-14. The analysis shows that Black students were disproportionately disciplined in all six categories of discipline, including out-of-school suspensions, in-school suspensions, referrals to law enforcement, expulsions, corporal punishment, and school-related arrests. The report also found that boys were disproportionately disciplined in all categories except for expulsions. Students with disabilities were also disproportionately disciplined in all categories. The report concludes that the findings suggest that implicit bias, including stereotypes or unconscious associations about students, may contribute to the disproportionate discipline. The report highlights the need for schools to address these biases and implement strategies to improve school climate and reduce discipline disparities. The report also notes that the findings are consistent with previous research on the issue and suggests that the issue of disproportionate discipline is a persistent problem in schools.
 45%|████▌     | 18/40 [04:24<05:48, 15.84s/it]2024-12-22 02:54:06,937 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:06,938 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1962])
2024-12-22 02:54:06,994 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:07,014 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:54:08,825 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:08,826 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1918])
2024-12-22 02:54:08,906 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:54:09,098 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:09,098 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1940])
2024-12-22 02:54:09,171 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:54:10,548 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:10,548 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1943])
2024-12-22 02:54:10,622 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:54:14,621 - [Process 1/5] - INFO - res.shape is :torch.Size([302])
results:
The report by the Government Accountability Office (GAO) examines the Department of Defense's (DOD) management of its workforce, specifically the size and composition of its civilian and contractor workforces. The report finds that DOD's methodology for identifying labor costs associated with federal civilian and service contractors is partially addressed, as it only includes direct labor costs and excludes non-labor costs. The report also notes that DOD did not account for the fully-burdened cost of federal civilians and service contractors, as directed by Congress, by excluding non-labor costs in its cost comparisons. Additionally, the report finds that DOD's methodology for determining the functions performed by contractor personnel is similar to that used in previous reports, but it did not make adjustments for differences in the number of government and contractor personnel. The report recommends that DOD assess the advantages and disadvantages of allowing the continued use of different cost-estimation tools across the department or directing department-wide application of one tool, and revise its guidance in accordance with the findings of its assessment. Overall, the report concludes that DOD needs to improve its management of its workforce by accounting for the fully-burdened cost of federal civilians and service contractors and making adjustments for differences in the number of government and contractor personnel.
 40%|████      | 16/40 [04:32<06:20, 15.83s/it]2024-12-22 02:54:15,203 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:16,173 - [Process 3/5] - INFO - res.shape is :torch.Size([203])
results:The report provides an overview of the challenges faced by youth who run away from home or are homeless, highlighting the vulnerabilities they face, including untreated mental health disorders, drug use, and sexual exploitation. It also discusses the various programs and services provided by the government to address these issues, including the Runaway Youth Act, the Education for Homeless Children and Youth program, and the Higher Education Act. The report notes that while there is no single federal definition of homeless youth, the Runaway and Homeless Youth program defines homeless youth as individuals under 18 who lack safe living arrangements and are unable to live with their families. Additionally, the report highlights the challenges in counting the number of homeless youth, as they often evade detection by shelter workers and census takers due to their mobile lifestyle. The report concludes by discussing the need for continued support and funding for programs that serve homeless youth.
 45%|████▌     | 18/40 [04:34<05:01, 13.68s/it]2024-12-22 02:54:16,391 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:16,705 - [Process 0/5] - INFO - res.shape is :torch.Size([194])
results:The report discusses the process of confirmation of presidential nominations in the US Senate. It highlights the various steps involved in the process, including the receipt of pre-hearing information, distribution of hearing transcripts, and committee vote on the nomination. The report also explains the reasons why the Senate confirms a high percentage of nominations, such as the President's prerogative to select executive branch officials and the difficulty of invoking cloture to overcome a filibuster. Additionally, the report discusses the recent changes to the confirmation process, including the reduction of the time allowed for consideration of nominations after cloture is invoked, and the impact of these changes on the nominations process. Finally, the report notes that the Constitution grants the President the authority to fill temporarily vacant positions during the recess of the Senate, and that recess appointments do not require Senate confirmation.
 48%|████▊     | 19/40 [04:34<04:33, 13.00s/it]2024-12-22 02:54:16,821 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:18,785 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:18,785 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1790])
2024-12-22 02:54:18,866 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:54:19,394 - [Process 2/5] - INFO - res.shape is :torch.Size([227])
results:The Community Reinvestment Act (CRA) was established in 1977 to encourage banks to provide credit and other financial services to low- and moderate-income (LMI) communities. The CRA requires federal banking regulators to conduct examinations to assess whether a bank is meeting local credit needs and provides incentives for banks to invest in LMI areas. The report discusses the different types of investments that banks can make to meet CRA requirements, including investments in Community Development Financial Institutions (CDFIs), Small Business Investment Companies (SBICs), and New Markets Tax Credits (NMTCs). The report also discusses the challenges of attributing bank lending decisions to CRA incentives and the recent policy discussions regarding modernization of the CRA. Overall, the report highlights the importance of the CRA in promoting economic development in LMI areas and the need for banks to continue to invest in these areas to meet the credit needs of the communities they serve.
 45%|████▌     | 18/40 [04:37<05:21, 14.63s/it]2024-12-22 02:54:19,810 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:20,052 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:20,052 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2140])
2024-12-22 02:54:20,118 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:54:20,358 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:20,358 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 02:54:20,430 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:54:21,767 - [Process 4/5] - INFO - res.shape is :torch.Size([249])
results:The federal government owns over 640 million acres of land, mostly in the western states, and has various authorities to acquire and dispose of these lands. The Bureau of Land Management (BLM) has the broadest authority for both acquisitions and disposals, while the National Park Service (NPS) has limited authority to acquire land for new park units. The Forest Service (FS) and Fish and Wildlife Service (FWS) have more limited authorities. The BLM can sell or exchange land for agricultural use, community development, mineral extraction, or educational purposes. The FS can sell land for agricultural use, community development, or mineral extraction, but not for educational purposes. The FWS can acquire land for species conservation and recreation, but not for agricultural use or community development. The Land and Water Conservation Fund Act provides funding for land acquisition, but this authority has expired. The BLM also has geographically limited land sale authorities, such as the Southern Nevada Public Land Management Act. The report provides an overview of the various authorities and their limitations, as well as the sources of funding for land acquisition.
 48%|████▊     | 19/40 [04:39<05:27, 15.59s/it]2024-12-22 02:54:22,011 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:23,344 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:23,345 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2085])
2024-12-22 02:54:23,413 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:54:25,569 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:25,570 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1953])
2024-12-22 02:54:25,644 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:54:26,254 - [Process 1/5] - INFO - res.shape is :torch.Size([168])
results:
The report by the government agency provides an overview of the policies and processes in place for managing the developmental testing, acquisition management, and resource allocation for major acquisition programs at the Department of Homeland Security (DHS). The report highlights the importance of having a reliable and effective acquisition management system to ensure that the department's missions are executed and that the systems are delivered on time, within budget, and meet the required performance parameters. The report also identifies areas where improvements can be made, such as better defining oversight roles and improving program reporting to Congress. The report concludes that while DHS has made progress in improving its acquisition management, there are still areas that need attention to ensure that the department's major acquisition programs are managed effectively and efficiently.
 42%|████▎     | 17/40 [04:44<05:35, 14.57s/it]2024-12-22 02:54:26,494 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:30,124 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:30,124 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1636])
2024-12-22 02:54:30,214 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:54:32,157 - [Process 3/5] - INFO - res.shape is :torch.Size([268])
results:
The report provides an overview of the FY2019 budget request and appropriations for the International Trade Administration (ITA), the U.S. International Trade Commission (USITC), and the Office of the United States Trade Representative (USTR). The report highlights that the Administration requested $590.8 million for the three trade-related agencies, which represents an 8.9% decrease from the FY2018 appropriated amount. The report also notes that the House and Senate Appropriations Committees largely declined the budget cuts requested by the Administration for these three trade agencies. The report provides a detailed breakdown of the requested and enacted funding for each agency, including $440.1 million in direct funding for ITA, $87.6 million for USITC, and $63.0 million for USTR. Additionally, the report notes that the Consolidated Appropriations Act, 2019 ( P.L. 116-6 ) provided a total of $647.0 million in funding for the three trade-related agencies, which is a 0.2% decrease from FY2018 appropriations.
 48%|████▊     | 19/40 [04:50<05:01, 14.37s/it]2024-12-22 02:54:32,432 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:33,787 - [Process 2/5] - INFO - res.shape is :torch.Size([228])
results:
The report by the government agency provides an overview of the various ways in which misconduct is addressed within the agency. The report highlights the different offices and departments responsible for handling misconduct allegations, including the Office of Human Resource Management, Office of Accountability Review, Office of Inspector General, Office of Accountability and Whistleblower Protection, Corporate Senior Executive Management Office, Client Services Response Team, National Cemetery Administration, Office of the Medical Inspector, Office of Resolution Management, and Office of Security and Law Enforcement. The report also outlines the process for addressing employee misconduct, which involves receiving allegations, reviewing and referring them to the appropriate office, providing notice to employees, taking disciplinary action, and reporting to the Office of Personnel Management. The report also acknowledges the contributions of various staff members who played a key role in the investigation. Overall, the report demonstrates the agency's commitment to addressing misconduct and ensuring that employees are held accountable for any wrongdoing.
 48%|████▊     | 19/40 [04:52<05:05, 14.56s/it]2024-12-22 02:54:33,986 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:35,038 - [Process 0/5] - INFO - res.shape is :torch.Size([361])
results:The Longshore and Harbor Workers' Compensation Act (LHWCA) provides workers' compensation benefits to private-sector workers engaged in longshore, harbor, or other maritime occupations on or adjacent to navigable waters of the United States. The program is administered by the Department of Labor (DOL) and covers medical and disability benefits, vocational rehabilitation services, and survivors benefits. The LHWCA excludes certain workers from coverage, including those covered by state workers' compensation laws, employees of clubs, camps, recreational operations, museums, or retail outlets, marina employees not engaged in construction, and aquaculture workers. The program also provides cash benefits to surviving spouses and minor children of workers killed on the job. The LHWCA has a schedule of benefits for permanent partial disability and provides additional benefits for workers who become ill as a result of their employment. The program has a no-fault system, meaning workers are entitled to benefits without having to prove fault or negligence on the part of their employer. The LHWCA has a limited role in workers' compensation and administers benefits only for federal employees and certain private-sector workers. The program is funded through private insurers or self-insured firms and has an annual adjustment for inflation. The LHWCA has been amended to provide additional benefits for survivors of workers killed on the job and to exclude certain workers from coverage. The program has a formal claims process for reporting injuries, illnesses, or deaths and provides for hearings and appeals if disputes arise.
 50%|█████     | 20/40 [04:53<04:52, 14.60s/it]2024-12-22 02:54:35,179 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:35,649 - [Process 4/5] - INFO - res.shape is :torch.Size([225])
results:The report provides an overview of the legislative process in the House of Representatives, including the rules and procedures that govern floor debates and votes. The report highlights the two main reasons why gaining a fundamental understanding of the House's legislative procedures is not as difficult as it may seem: predictability of the ways in which the House applies its rules and the underlying premise that a majority of Members should ultimately be able to work their will on the floor. The report also discusses the restrictions on House procedures imposed by the Constitution, such as the requirement for a quorum and the ability of the House to amend its rules unilaterally. Additionally, the report covers the various modes of consideration that the House uses when considering legislation, including suspension of the rules, and the procedures for debating and voting on amendments. Finally, the report provides information on the parliamentary reference sources available to Members and staff, including the House manual, Jefferson's Manual, and Deschler's Precedents of the U.S. House of Representatives.
 50%|█████     | 20/40 [04:53<05:01, 15.08s/it]2024-12-22 02:54:35,864 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:36,069 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:36,070 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1867])
2024-12-22 02:54:36,150 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:54:37,514 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:37,514 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1948])
2024-12-22 02:54:37,588 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:54:38,767 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:38,767 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2122])
2024-12-22 02:54:38,833 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:54:39,160 - [Process 1/5] - INFO - res.shape is :torch.Size([201])
results:The report by the Government Accountability Office (GAO) highlights the challenges faced by the US Navy in completing maintenance on time and the impact it has on the readiness of its ships. The report states that the Navy has struggled to complete maintenance on time, resulting in delays and cost overruns. The Navy has also faced challenges in managing its personnel and resources, leading to a shortage of skilled workers and inadequate training. The report recommends that the Navy develop a comprehensive plan to address these challenges and improve its readiness. The Navy has taken steps to address these issues, including implementing new training programs and increasing the number of dedicated training periods. However, more work needs to be done to ensure that the Navy's ships are fully operational and ready for deployment. The report also highlights the need for the Navy to develop a comprehensive plan to address the challenges it faces in managing its personnel and resources.
 45%|████▌     | 18/40 [04:57<05:09, 14.07s/it]2024-12-22 02:54:39,374 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:39,415 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:39,415 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1948])
2024-12-22 02:54:39,489 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:54:42,927 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:42,928 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1950])
2024-12-22 02:54:43,001 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:54:48,250 - [Process 3/5] - INFO - res.shape is :torch.Size([265])
results:
The report by the government agency provides an analysis of the state and local government sector's fiscal outlook over the next 50 years. The sector is expected to face a persistent difference between revenue and spending, with expenditures growing faster than revenues. The report highlights several factors contributing to this trend, including healthcare expenditures, which are projected to increase significantly, and pension costs. The report also notes that federal investment grants and other grants unrelated to Medicaid are likely to decline as a share of GDP, while state and local governments' tax revenues are expected to increase as a percentage of GDP. To address these fiscal challenges, the report suggests that state and local governments will need to make policy changes to avoid fiscal imbalances and ensure that revenues are sufficient to cover expenditures. The report also provides sensitivity analysis results, which show that changes in certain variables, such as population growth and pension costs, can significantly impact the sector's fiscal outlook. Overall, the report highlights the need for state and local governments to take proactive steps to manage their finances and ensure long-term fiscal sustainability.
 50%|█████     | 20/40 [05:06<04:57, 14.89s/it]2024-12-22 02:54:48,475 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:50,094 - [Process 4/5] - INFO - res.shape is :torch.Size([238])
results:
The report discusses the overtime provisions in the Fair Labor Standards Act (FLSA) for executive, administrative, and professional employees (EAP). The FLSA requires that employees be paid at least one and a half times their regular rate of pay for any hours worked over 40 in a workweek, unless they are exempt under the EAP provisions. The report explains that the EAP exemptions were established in 1938 and have been updated several times since then. The current threshold for the EAP exemptions is $679 per week, and it is expected to increase to $147,414 per year in 2020. The report also discusses the application of the EAP exemptions to nonprofits, institutions of higher education, and public sector employees. Additionally, the report highlights the impact of the proposed rule on workers, including the estimated 4.9 million workers who would be affected by the new rule. The report concludes by stating that the proposed rule would expand overtime coverage to EAP employees and provide strengthened overtime protections for workers.
 52%|█████▎    | 21/40 [05:08<04:42, 14.89s/it]2024-12-22 02:54:50,363 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:50,977 - [Process 0/5] - INFO - res.shape is :torch.Size([301])
results:
The report discusses the budget authority for the Department of Homeland Security (DHS) for FY2019. The report explains that the budget authority is divided into discretionary and mandatory spending, with discretionary spending being appropriated by Congress each year and mandatory spending being funded through permanent appropriations. The report highlights that the Trump Administration requested $47.43 billion in adjusted net discretionary budget authority for DHS for FY2019, which is a $0.29 billion decrease from the FY2018 enacted level. The report also notes that the Senate Appropriations Committee recommended $48.33 billion in adjusted net discretionary budget authority for DHS, while the House Appropriations Committee recommended $51.44 billion. The report also discusses the budget control act, which sets discretionary spending limits for FY2018 and FY2019, and the adjustments that can be made to these limits for emergency spending, overseas contingency operations, and disaster relief. Finally, the report notes that the continuing resolution for FY2019 continued funding for DHS at a rate of operations equal to that of the Department of Homeland Security Appropriations Act, 2018, with some exceptions.
 52%|█████▎    | 21/40 [05:09<04:45, 15.00s/it]2024-12-22 02:54:51,165 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:52,047 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:52,047 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1970])
2024-12-22 02:54:52,121 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:54:54,006 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:54,006 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2130])
2024-12-22 02:54:54,072 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:54:54,702 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:54,703 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 02:54:54,721 - [Process 2/5] - INFO - res.shape is :torch.Size([377])
results:The Federal Communications Commission (FCC) is an independent agency established by the Communications Act of 1934 to regulate interstate and international communications by radio, television, wire, satellite, and cable. The FCC's mission is to ensure that all Americans have access to rapid, efficient, and affordable communication services without discrimination. The agency is led by five commissioners appointed by the President and confirmed by the Senate, with one serving as chairman. The FCC publishes four periodic reports for Congress, including the Strategic Plan, Agency Financial Report, Annual Performance Report, and a report on one-time hearings. The FCC's current strategic goals are focused on promoting competition, reducing regulatory burdens, and ensuring that the agency's actions reflect the realities of the current marketplace. The agency has also made changes to its structure and bureaus to better reflect the industries it regulates. The FCC has faced criticism for its handling of net neutrality, with some arguing that more proscriptive regulations are necessary to protect the marketplace, while others believe existing laws and the current approach are sufficient. The agency has also faced changes in leadership and priorities, with the current chairman, Ajit Pai, focusing on promoting market incentives and efficiency. The FCC's budget for the 2020 fiscal year was approved by the House Committee on Appropriations Subcommittee on Financial Services and General Government in April 2019. The agency operates under a public interest mandate and has wide latitude and flexibility to interpret this mandate, but its priorities and structure have changed over time due to changes in leadership and political priorities.
 50%|█████     | 20/40 [05:12<05:29, 16.48s/it]2024-12-22 02:54:54,774 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:54:54,959 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:55,140 - [Process 1/5] - INFO - res.shape is :torch.Size([271])
results:
The Federal Housing Administration (FHA) is a government agency that provides mortgage insurance to protect lenders against borrower default. The agency was created in 1934 to stimulate the housing market and provide affordable mortgages to low-income households. FHA's market share has fluctuated over the years, with a peak of 30% in 2009 during the housing market turmoil. In 2017, FHA insured 19.5% of new home purchase mortgages and about 16.7% of new mortgages overall. FHA's eligibility and underwriting criteria require mortgages to meet certain standards, such as a minimum down payment of 3.5%, and a maximum debt-to-income ratio of 31%. The agency also provides data on the number and dollar volume of mortgages insured, as well as the number of mortgages insured by FHA in each year since 1996. Overall, FHA plays a crucial role in providing affordable mortgages to low-income households and helping to stabilize the housing market during economic downturns.
 48%|████▊     | 19/40 [05:13<05:07, 14.64s/it]2024-12-22 02:54:55,728 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:54:58,517 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:58,517 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:54:58,590 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:54:59,327 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:54:59,327 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2278])
2024-12-22 02:54:59,390 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:55:03,870 - [Process 0/5] - INFO - res.shape is :torch.Size([226])
results:The report discusses the role of race in higher education, specifically in relation to affirmative action policies in colleges and universities. The report focuses on two areas of affirmative action: the mandatory use of race-conscious measures to eliminate vestiges of de jure segregation in public universities and the voluntary use of race-conscious measures to increase diversity in private universities. The report highlights the legal framework for affirmative action in higher education, including the Equal Protection Clause of the Fourteenth Amendment and Title VI of the Civil Rights Act of 1964. The report also discusses the role of the federal courts in enforcing affirmative action policies, including the Supreme Court's jurisprudence on the matter. Additionally, the report touches on the issue of disparate impact liability and private right of action under Title VI. Finally, the report concludes by discussing the role of Congress in ensuring equal protection in higher education through amendments to Title VI and other legislative actions.
 55%|█████▌    | 22/40 [05:22<04:18, 14.37s/it]2024-12-22 02:55:04,034 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:04,991 - [Process 3/5] - INFO - res.shape is :torch.Size([287])
results:The report discusses the development and future plans of the US Marine Corps' Amphibious Combat Vehicle (ACV) program. The ACV is intended to replace the aging Amphibious Assault Vehicle (AAV) and provide a more capable and sustainable platform for amphibious operations. The program has faced cost growth and performance issues, leading to the cancellation of the Expeditionary Fighting Vehicle (EFV) program. However, the ACV program is moving forward with the selection of BAE Systems as the contractor and the development of the Marine Personnel Carrier (MPC) to provide a survivable and mobile platform for Marines ashore. The program is expected to deliver 700 vehicles, and the Marines are exploring the possibility of a future ACV 2.0 variant with enhanced capabilities. The report also discusses the potential implications of adopting the Navy's Expeditionary Advance Base Operations (EABO) operational concept, which could require changes to the ACV design to enhance survivability and lethality. Overall, the report highlights the ongoing challenges and opportunities in the ACV program and the importance of ensuring that the platform can meet the Marines' operational requirements for amphibious operations.
 52%|█████▎    | 21/40 [05:23<04:53, 15.45s/it]2024-12-22 02:55:05,203 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:05,826 - [Process 2/5] - INFO - res.shape is :torch.Size([159])
results:The report discusses the history and development of emergency powers in the United States, specifically focusing on the National Emergencies Act of 1976. The report highlights the various ways in which the President can exercise emergency powers, including through statutory grants of authority, standby laws, and the Constitution. It also examines the role of Congress in regulating emergency powers and the limitations placed on the President's authority. The report notes that the National Emergencies Act has been used by various Presidents to declare national emergencies, and some of these declarations have been revoked, while others remain in effect. The report concludes by suggesting that further improvements and reforms in this policy area may be pursued and perfected.
 52%|█████▎    | 21/40 [05:24<04:42, 14.86s/it]2024-12-22 02:55:06,040 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:07,531 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:07,531 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1972])
2024-12-22 02:55:07,604 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:55:08,145 - [Process 4/5] - INFO - res.shape is :torch.Size([314])
results:The report provides an overview of the FY2019 budget request for the Department of State, Foreign Operations, and Related Programs. The request includes $41.86 billion in base funding, with $13.26 billion designated for State operations and $28.60 billion for foreign operations. The report highlights that the request represents a 23.3% decrease in SFOPS funding compared to FY2018 levels, with a 18.7% decrease in State operations and a 25.2% decrease in foreign operations. The report also notes that the Administration proposed to merge the International Affairs Agency (IAA) and the United States Agency for International Development (USAID) and requested only small amounts of funding to close out their independent activities. The report provides a detailed breakdown of the funding levels for each account, including the Overseas Contingency Operations (OCO) account, which would have received $8.0 billion, or 15% of the total SFOPS funding. The report also compares the Administration's request with the FY2018 actual funding and the FY2019 enacted appropriation, highlighting the differences in funding levels for each account. Additionally, the report provides information on the House and Senate SFOPS legislation, continuing resolutions, and the enacted appropriation for FY2019.
 55%|█████▌    | 22/40 [05:26<04:45, 15.84s/it]2024-12-22 02:55:08,380 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:08,807 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:08,807 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 02:55:08,880 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:55:09,573 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:09,573 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 02:55:09,595 - [Process 1/5] - INFO - res.shape is :torch.Size([225])
results:The report provides an overview of the current state of the Gerald R. Ford (CVN-78) class aircraft carrier program, including the Navy's proposed budget for the program, the potential oversight issues, and the strategic and budgetary context. The report also discusses the Navy's current aircraft carrier force, the requirement for a minimum of 11 operational aircraft carriers, and the potential impact of the CVN-78 class on the Navy's force structure. Additionally, the report highlights the issue of the shock trial for the CVN-78 class, which has been a matter of disagreement between the Navy and the Office of the Secretary of Defense (OSD). The Navy has proposed to conduct the shock trial on the second ship in the class, while OSD has argued that the risks of delaying the trial are not acceptable. The report concludes by summarizing the Navy's current plan for the CVN-78 class and the potential impact of the program on the Navy's force structure.
 50%|█████     | 20/40 [05:27<04:51, 14.59s/it]2024-12-22 02:55:09,641 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:55:09,830 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:11,962 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:11,963 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 02:55:12,035 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:55:13,412 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:13,412 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 02:55:13,483 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:55:15,847 - [Process 3/5] - INFO - res.shape is :torch.Size([153])
results:The report discusses the current state of relations between the United States and Israel, focusing on the military and political aspects of their relationship. It highlights the strengths that Israel has in terms of military and homeland security capabilities, as well as its efforts to address potential threats from Iran and Hezbollah. The report also covers the ongoing tensions between Israel and Iran, particularly in Syria, and the potential for conflict in the region. Additionally, it touches on the issue of the Palestinian-Israeli conflict and the impact of U.S. aid to the Palestinians. The report concludes by mentioning the upcoming Israeli elections and their potential impact on the country's leadership and policies.
 55%|█████▌    | 22/40 [05:34<04:13, 14.07s/it]2024-12-22 02:55:16,141 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:16,346 - [Process 0/5] - INFO - res.shape is :torch.Size([217])
results:
The report by the Government Accountability Office (GAO) examines the implementation of pre-employment transition services for students with disabilities under the Workforce Innovation and Opportunity Act (WIOA). The report finds that while states have made progress in providing these services, there are challenges in implementing them, particularly in coordinating with state educational agencies. The report highlights the importance of interagency agreements between VR and educational agencies to ensure the provision of pre-employment transition services. The GAO also notes that Education has taken steps to address these challenges through its guidance and technical assistance documents. The report concludes that Education's efforts to address state VR agencies' challenges in providing pre-employment transition services are adequate, but more needs to be done to ensure effective coordination with state educational agencies. The report recommends that Education continue to work with state VR and educational agencies to improve the provision of pre-employment transition services for students with disabilities.
 57%|█████▊    | 23/40 [05:34<03:54, 13.80s/it]2024-12-22 02:55:16,538 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:19,744 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:19,744 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2048])
2024-12-22 02:55:19,798 - [Process 2/5] - INFO - res.shape is :torch.Size([227])
results:
The report by the USPS Office of Inspector General (OIG) examines the accuracy of scanning for competitive products at the United States Postal Service (USPS). The report finds that while USPS has made progress in improving scanning accuracy, there are still issues with missed and inaccurate scans, particularly for competitive products. The report highlights the need for USPS to improve communication of standard operating procedures for scanning and to provide managers with detailed information on how to use reports to investigate and resolve scanning issues. The report also recommends that USPS identify and adopt a set of internal control standards for operational internal control activities, improve communication of scanning procedures, and create standard operating procedures for managers on how to address inaccurate scans. USPS agreed with the recommendations and committed to completing corrective actions by November 2018. The report concludes that while USPS has a high scanning rate, some missed and inaccurate scans do occur, which could potentially affect millions of competitive products.
 55%|█████▌    | 22/40 [05:38<04:22, 14.59s/it]2024-12-22 02:55:19,816 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:55:20,015 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:20,033 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:20,033 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1945])
2024-12-22 02:55:20,107 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:55:21,860 - [Process 4/5] - INFO - res.shape is :torch.Size([217])
results:
The report by the Government Accountability Office (GAO) examines the vacancies in the Department of State's (State) Foreign Service staffing at overseas posts, the effects of these vacancies on diplomatic readiness, and State's efforts to address these vacancies. The report finds that there are persistent vacancies in both generalist and specialist positions at overseas posts, with specialist positions remaining vacant at a higher rate. The report also notes that State's ability to hire Foreign Service employees to fill persistent vacancies has been affected by factors such as reduced appropriations. The data in State's Global Employment Management System (GEMS) show that the majority of Foreign Service employees work in positions at overseas posts, but there are still vacancies in both generalist and specialist positions. The report recommends that State develop an integrated action plan to address the root causes of persistent Foreign Service vacancies at overseas posts and provide steps necessary to implement solutions.
 57%|█████▊    | 23/40 [05:40<04:18, 15.20s/it]2024-12-22 02:55:22,124 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:22,728 - [Process 1/5] - INFO - res.shape is :torch.Size([203])
results:The report by the Government Accountability Office (GAO) examines how selected federal agencies' assessment methodologies align with the Interagency Security Committee's (ISC) risk management standard for identifying necessary countermeasures in federal facilities. The report finds that while some agencies have implemented risk assessment methodologies that consider all undesirable events, others have gaps in their assessments. The report highlights the challenges faced by the agencies in conducting physical security assessments and monitoring results, including lack of documentation of methodologies and inconsistent application of risk management principles. The report also notes that some agencies have not fully aligned their methodologies with the ISC Standard, and some have not updated their policies and procedures to reflect the latest revisions to the standard. The report recommends that agencies should prioritize updating their policies and procedures to align with the ISC Standard and ensure that their risk assessment methodologies consider all undesirable events.
 52%|█████▎    | 21/40 [05:40<04:28, 14.15s/it]2024-12-22 02:55:22,937 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:23,587 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:23,587 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 02:55:23,658 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:55:25,730 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:25,731 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2076])
2024-12-22 02:55:25,800 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:55:26,513 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:26,513 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1995])
2024-12-22 02:55:26,584 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:55:28,935 - [Process 0/5] - INFO - res.shape is :torch.Size([219])
results:The report provides an overview of Medicare, a federal program that provides health insurance to individuals 65 and older, and individuals with disabilities under the age of 65. The program is funded through payroll taxes, premiums, and general revenues. In 2019, Medicare will cover approximately 61 million persons, with total spending estimated to reach $772 billion. The program is expected to place increasing financial demands on the federal budget and on beneficiaries. The report highlights the different parts of Medicare, including Part A (hospital insurance), Part B (supplementary medical insurance), Part C (Medicare Advantage), and Part D (prescription drug coverage). It also discusses the payment structures for health care providers and the impact of sequestration on Medicare spending. The report concludes by noting that Medicare is a high-priority issue in Congress and that reforms to the program are likely to be considered in the future.
 60%|██████    | 24/40 [05:47<03:35, 13.44s/it]2024-12-22 02:55:29,104 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:32,673 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:32,673 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1829])
2024-12-22 02:55:32,753 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:55:33,139 - [Process 2/5] - INFO - res.shape is :torch.Size([206])
results:The report discusses NASA's JWST project and its challenges with cost overruns and schedule delays. The project experienced significant cost increases and schedule delays, leading to a launch delay of 52 months. The report highlights the importance of regularly updating project cost and schedule estimates to reflect changes in the project's scope, risks, and uncertainties. NASA's portfolio of major space telescopes includes three projects - WFIRST, TESS, and JWST - that vary in cost, complexity, and phase of the acquisition life cycle. The report emphasizes the need for NASA to implement lessons learned from JWST, such as developing a joint cost and schedule confidence level, regularly updating project cost and schedule estimates, and enhancing oversight of contractors. The report also highlights the importance of NASA's ongoing work to address challenges in executing these projects, including communication and oversight of contractors.
 57%|█████▊    | 23/40 [05:51<04:01, 14.22s/it]2024-12-22 02:55:33,411 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:34,204 - [Process 3/5] - INFO - res.shape is :torch.Size([321])
results:The report examines how the Department of Defense (DOD) establishes foreign currency budget rates for overseas expenditures and manages foreign currency fluctuations. The report finds that DOD uses various methodologies to establish foreign currency budget rates, including a centered weighted average model that combines both a 5-year average of exchange rates and an average of the most recently observed 12 months of exchange rates. The report also finds that DOD's analysis showed that using a 6-month average of exchange rates resulted in more realistic estimates than the historical approach. Additionally, the report notes that DOD's preferred payment method for foreign currency transactions is the Department of Treasury's (Treasury) comprehensive international payment and collection system, which offers various rates, including advanced rates available up to 5 days in advance of disbursement, and the spot rate. The report also examines how DOD manages foreign currency fluctuations in its Foreign Currency Fluctuation, Defense (FCFD) account to cover losses and maintain quality information to manage these funds. The report finds that DOD has taken steps to reduce costs in selecting foreign currency rates at which to make disbursements, and that the FCFD account is maintained at various budgetary levels within the service components. Finally, the report notes that the data used in addressing the objectives of the audit was found to be sufficiently reliable, with the exception of the financial reporting on foreign currency gains and losses.
 57%|█████▊    | 23/40 [05:52<04:21, 15.36s/it]2024-12-22 02:55:34,369 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:35,231 - [Process 4/5] - INFO - res.shape is :torch.Size([207])
results:The report discusses the current state of the Strategic Petroleum Reserve (SPR) in the United States and its potential future changes. The SPR is a stockpile of crude oil and petroleum products that can be released to the market in case of supply disruptions. The report highlights several issues with the current SPR system, including aging infrastructure, inadequate maintenance, and a lack of flexibility in responding to supply disruptions. The report also discusses the potential impact of increasing domestic crude oil production on the SPR and the need for a long-term target for the size and configuration of the SPR. Additionally, the report touches on the role of the SPR in meeting international energy program obligations and the potential for regional petroleum product reserves. The report concludes by highlighting the need for DOE to conduct periodic reviews of the SPR and to consider a full range of options for handling potentially excess assets.
 60%|██████    | 24/40 [05:53<03:54, 14.65s/it]2024-12-22 02:55:35,437 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:36,941 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:36,942 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 02:55:37,010 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:55:37,278 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:37,278 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1716])
2024-12-22 02:55:37,337 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:55:38,519 - [Process 1/5] - INFO - res.shape is :torch.Size([265])
results:
The report by the Government Accountability Office (GAO) highlights the challenges in measuring program integrity risks in Medicaid managed care. The report notes that the Centers for Medicare & Medicaid Services' (CMS) Payment Error Rate Measurement (PERM) methodology underestimates program integrity risks in Medicaid managed care. The report recommends that CMS consider and take steps to mitigate the program risks that are not measured in the PERM, such as overpayments and unallowable costs. The report also notes that CMS has taken steps to improve its oversight of Medicaid managed care, but these efforts fall short of ensuring that the agency and states will be able to identify and address overpayments to providers and unallowable MCO costs. The report highlights the limitations of the PERM methodology and the need for a more comprehensive approach to measuring program integrity risks in Medicaid managed care. The report also notes that HHS concurred with the recommendation and will review regulatory authority and audit resources to determine the best way to account for Medicaid program risks that are not accounted for in the PERM.
 55%|█████▌    | 22/40 [05:56<04:23, 14.64s/it]2024-12-22 02:55:38,757 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:39,022 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:39,023 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2001])
2024-12-22 02:55:39,094 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:55:39,682 - [Process 0/5] - INFO - res.shape is :torch.Size([172])
results:The report examines the history and evolution of U.S. foreign aid, including its objectives, costs, and organization. It highlights how U.S. foreign aid policy has transformed since the 9/11 attacks, with a shift towards supporting global development as a third pillar of U.S. national security. The report also discusses how U.S. aid is integrated into other areas of foreign policy, such as defense and diplomacy. Additionally, it provides data on the amount of aid disbursed by the U.S. government in recent years and compares it to other donor countries. The report concludes by noting that while the U.S. leads in dollar amounts of aid flows to developing countries, it often ranks low when aid is calculated as a percentage of gross national income.
 62%|██████▎   | 25/40 [05:57<03:09, 12.63s/it]2024-12-22 02:55:39,852 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:42,340 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:42,340 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2040])
2024-12-22 02:55:42,412 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:55:43,384 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:43,385 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 02:55:43,456 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:55:47,332 - [Process 3/5] - INFO - res.shape is :torch.Size([220])
results:
The report by the Government Accountability Office (GAO) provides an update on the progress made by the Department of Homeland Security (DHS) in implementing the Chemical Facility Anti-Terrorism Standards (CFATS) program. The report highlights several areas where DHS has made progress, including reducing the time it takes to review facility security plans, improving data sharing among agencies, and implementing an Expedited Approval Program (EAP) for facilities. However, the report also identifies areas where DHS still needs to improve, such as ensuring the accuracy of data used to identify high-risk facilities and implementing a more effective compliance inspection process. The report notes that DHS has taken steps to address these issues, including updating its risk assessment approach and developing a new tiering methodology. The report concludes that while DHS has made progress in implementing the CFATS program, there are still areas where improvement is needed to ensure the program's effectiveness in protecting chemical facilities from terrorist attacks.
 60%|██████    | 24/40 [06:05<03:54, 14.69s/it]2024-12-22 02:55:47,575 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:49,227 - [Process 2/5] - INFO - res.shape is :torch.Size([270])
results:The Small Business Administration (SBA) Surety Bond Guarantee Program was established in 1971 to help small businesses access surety bonds, which are necessary for contracting with the federal government. The program guarantees up to 90% of losses for bonds issued to small businesses that cannot obtain them through regular commercial channels. The program has been operational since 1971 and has guaranteed over $6.5 billion in bonds since then. The program has faced challenges in recent years, including a decline in the number of preferred sureties participating in the program. The program's eligibility standards and requirements have remained relatively unchanged since its inception and have been influenced by congressional testimony and GAO examinations. The program has been successful in helping small businesses access surety bonds, particularly minority-owned and women-owned businesses. However, the program's size in comparison to the private sector is not well determined, and there is a need for measures or metrics to determine the appropriate size of the program. The program's sponsors have argued for more participation in the program, and the SBA has focused on strengthening relationships with individual surety companies and bond agents to reach more small businesses.
 60%|██████    | 24/40 [06:07<03:56, 14.78s/it]2024-12-22 02:55:49,455 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:49,483 - [Process 0/5] - INFO - res.shape is :torch.Size([150])
results:
The report discusses the National Popular Vote (NPV) initiative, which aims to elect the President and Vice President through a national popular vote rather than the current electoral college system. The report highlights the history of the electoral college system, the arguments for and against the NPV initiative, and the legal challenges it may face. The report also examines the potential impact of the NPV initiative on the presidential election process and the role of the states in selecting electors. The report concludes that while the NPV initiative has gained some momentum in recent years, it remains to be seen whether it will ultimately succeed in changing the way the President and Vice President are elected.
 65%|██████▌   | 26/40 [06:07<02:44, 11.78s/it]2024-12-22 02:55:49,621 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:55:51,176 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:51,177 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 02:55:51,249 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:55:53,158 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:53,158 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1923])
2024-12-22 02:55:53,158 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:55:53,158 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 02:55:53,230 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:55:53,238 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:55:59,921 - [Process 4/5] - INFO - res.shape is :torch.Size([458])
results:The report by the Government Accountability Office (GAO) examines the progress of the Department of Veterans Affairs (VA) in implementing its Medical Supply Chain Management Program (MSPV-NG). The program aims to standardize medical supplies across VA medical centers to achieve cost avoidance and improve buying power. The report finds that VA's approach to implementing MSPV-NG has been hindered by several factors, including:

1. Lack of a clear program strategy and leadership stability, which has resulted in ineffective contracting processes and limited clinical involvement in requirements development.
2. Insufficient staffing and schedule constraints within the program office, which has delayed the development of the formulary and prioritization of supply categories for future phases of requirement development and contracting.
3. Limited clinical involvement in requirements development, which has resulted in a formulary that does not meet the needs of medical centers and has yet to achieve utilization and cost avoidance goals.
4. Challenges in replacing the existing Phase 1 non-competitive agreements with competitive awards based on the Phase 2 requirements generated by the integrated product teams (IPTs), which may not be able to keep up with expiring agreements due to an unrealistic schedule.
5. Unrealistic timelines and insufficient staffing for the IPTs, which have limited the extent of clinician involvement and resulted in a rushed and incomplete review of items for the formulary.
6. Cancellation of all outstanding Phase 2 solicitations due to low response rates, protests from service-disabled veteran-owned small businesses, and changes in overall MSPV-NG strategy.
To address these challenges, GAO recommends that VA develop a clear program strategy, prioritize clinical involvement in requirements development, and establish a plan to mitigate the potential risk of gaps in contract coverage while SAC is still working to make competitive Phase 2 awards. VA agrees with these recommendations and is taking steps to address them.
 62%|██████▎   | 25/40 [06:18<04:24, 17.67s/it]2024-12-22 02:56:00,175 - [Process 3/5] - INFO - res.shape is :torch.Size([198])
results:The report examines the regulations and oversight of coal mining in the United States, specifically focusing on the use of self-bonding as a financial assurance for reclamation. The report finds that self-bonding poses a financial risk to the government and states, as operators may go bankrupt or fail to provide adequate financial assurance for reclamation. The report highlights the need for amending the Surface Mining Control and Reclamation Act (SMCRA) to eliminate the use of self-bonding for coal mining. The report also discusses the financial assurance requirements for coal mining, including the use of surety bonds, trust funds, and other financial mechanisms. The report concludes that the federal government and states should consider amending SMCRA to eliminate the use of self-bonding to ensure that reclamation is completed and to protect the environment.
 62%|██████▎   | 25/40 [06:18<03:31, 14.13s/it]2024-12-22 02:56:00,210 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:00,358 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:00,449 - [Process 1/5] - INFO - res.shape is :torch.Size([392])
results:The Federal Reserve's Responsibilities and Monetary Policy

The Federal Reserve, the central bank of the United States, has four main responsibilities: monetary policy, provision of emergency liquidity, supervision of banks, and provision of payment system services. The Fed's monetary policy is aimed at maximizing employment, stable prices, and moderate long-term interest rates. The Fed has defined stable prices as a 2% inflation rate, and it uses the federal funds rate as its primary tool for monetary policy. The Fed has recently ended its large-scale asset purchases and is now focused on normalizing monetary policy by gradually reducing its balance sheet. The Fed intends to maintain a larger balance sheet than before the crisis, as it believes that it would not be appropriate to specify a fixed goal for employment. The Fed's regulatory responsibilities include bank supervision and payment system regulation. The Fed has also been granted the authority to pay interest on reserves, which helps to keep excess liquidity in check. The Fed uses reverse repurchase agreements to drain liquidity from the system and influence short-term market rates. The Fed's policy path poses risks, as overheating the economy could lead to high inflation and financial instability. The Fed's intended policy path also raises concerns about the potential risks of becoming a dominant participant in the market and expanding its counterparties. The Fed's regulatory responsibilities include bank supervision, payment system regulation, and the supervision of nonbank systemically important financial institutions. The Fed's actions have been focused on mitigating systemic risk since the financial crisis, and it has restructured its internal operations to facilitate a macroprudential approach to supervision and regulation.
 57%|█████▊    | 23/40 [06:18<04:46, 16.83s/it]2024-12-22 02:56:00,723 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:00,957 - [Process 0/5] - INFO - res.shape is :torch.Size([192])
results:The report highlights the need for the federal government to improve its cybersecurity measures to protect against failed IT projects, security threats, and data breaches. The government agency has identified several areas of improvement, including implementing risk-based security programs, improving cyber incident detection and response capabilities, expanding the cyber workforce, and strengthening critical infrastructure security. The report also notes that many agencies have not fully implemented key information security program requirements, and there are still significant weaknesses in IT security. To address these issues, the government has implemented the Federal Information Technology Acquisition Reform Act (FITARA) and the National Institute of Standards and Technology's (NIST) Framework for Improving Critical Infrastructure Cybersecurity. The report concludes by stating that the government must continue to monitor agencies' efforts to implement these recommendations and take steps to improve cybersecurity.
 68%|██████▊   | 27/40 [06:19<02:31, 11.69s/it]2024-12-22 02:56:01,114 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:03,743 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:03,743 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1737])
2024-12-22 02:56:03,801 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:03,801 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 02:56:03,815 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:56:03,875 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:56:04,311 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:04,312 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1988])
2024-12-22 02:56:04,385 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:56:04,656 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:04,656 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 02:56:04,728 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:56:05,114 - [Process 2/5] - INFO - res.shape is :torch.Size([264])
results:
The report examines the compliance of lobbyists with the Lobbying Disclosure Act (LDA) of 1995. The LDA requires lobbyists to register with the Secretary of the Senate and the Clerk of the House and file quarterly reports disclosing their lobbying activities. The report found that while most lobbyists complied with the LDA, there were some instances of confusion regarding rounding errors and inconsistencies in reporting. The report also found that lobbyists generally provided documentation to support their reported income and expenses, but some did not round their income or expenses according to the guidance provided. The report recommends that the Secretary of the Senate and the Clerk of the House continue to guide and assist lobbyists with the registration and reporting requirements and consider updating the guidance to address any remaining issues. Additionally, the report suggests that the USAO should continue to enforce the LDA and refer non-compliant lobbyists to the USAO for further action. Overall, the report concludes that while there are some areas for improvement in lobbyist compliance with the LDA, the majority of lobbyists are in compliance with the law.
 62%|██████▎   | 25/40 [06:23<03:46, 15.11s/it]2024-12-22 02:56:05,351 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:09,058 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:09,058 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1926])
2024-12-22 02:56:09,138 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:56:11,205 - [Process 3/5] - INFO - res.shape is :torch.Size([170])
results:
The report examines the history of funding gaps in the federal government, specifically from 1977 to 2019. It highlights that most funding gaps occur due to a lack of appropriations, and agencies may continue to operate during these gaps, but with limited activities. The report notes that the Antideficiency Act prohibits the obligation or expenditure of funds without appropriations, and agencies may not employ personnel during a funding gap unless authorized by law. The report also discusses the Civiletti opinions, which provide guidance on the limits of federal government activities during a funding gap. The report concludes that funding gaps can range in duration from one to 34 full days and may not result in a government shutdown.
 65%|██████▌   | 26/40 [06:29<03:04, 13.20s/it]2024-12-22 02:56:11,482 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:12,815 - [Process 0/5] - INFO - res.shape is :torch.Size([201])
results:The report discusses the current state of Bosnia and Herzegovina (Bosnia) and its challenges, particularly in the aftermath of the 1992-1995 war. The report highlights the country's political fragmentation, dysfunction, and the impact of external actors on its stability. It also discusses the role of the United States in Bosnia's affairs, including its involvement in the Dayton Peace Agreement and its ongoing engagement in the country's security and political issues. The report notes that while the U.S. has played a significant role in shaping Bosnia's political system, its engagement has decreased under recent administrations. The report concludes by highlighting the need for the U.S. to continue to support Bosnia's stability and territorial integrity, particularly in the face of external threats such as corruption and the potential for conflict with neighboring countries.
 70%|███████   | 28/40 [06:31<02:20, 11.74s/it]2024-12-22 02:56:12,970 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:13,797 - [Process 4/5] - INFO - res.shape is :torch.Size([217])
results:
The report discusses the naming conventions for US Navy ships and the current policies and practices for naming new Navy ships. The Secretary of the Navy has the authority to name new Navy ships, but there is no set time for assigning a name, and the name is typically chosen based on a variety of factors, including the type of ship, the location of the ship's homeport, and the contributions of the ship's sponsor. The report also discusses the history of ship naming in the Navy and how it has evolved over time, with some exceptions to the naming rules made for specific ships. The report concludes that current ship naming policies and practices are consistent with historical policies and practices and that fixed policies for naming one or more classes of vessels would be inadvisable. The report also mentions that the Navy has not stated what the naming rule for the 12 planned next-generation ballistic missile submarines will be, leaving possibilities for the naming rule including cities, capital cities, or states and federal districts and territories.
 65%|██████▌   | 26/40 [06:32<03:51, 16.53s/it]2024-12-22 02:56:14,072 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:15,085 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:15,085 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 02:56:15,157 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:56:16,504 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:16,505 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 02:56:16,576 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:56:17,635 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:17,635 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 02:56:17,704 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:56:19,650 - [Process 1/5] - INFO - res.shape is :torch.Size([338])
results:
The report examines the federal government's procurement of foreign products under the Buy American Act, which establishes a preference for domestic end products. The report analyzes data from the Federal Procurement Data System-Next Generation (FPDS-NG) for fiscal year 2017 to determine the extent of obligations for foreign end products and the use of exceptions and waivers. The report finds that while the Buy American Act restrictions apply to approximately 60 countries, the Act's requirements are waived for certain countries under international trade agreements or other criteria. The report identifies four agencies—the Departments of Defense (DOD), Health and Human Services (HHS), Homeland Security (DHS), and Veterans Affairs (VA)—that had the highest fiscal year 2017 obligations in the product codes for manufactured products, which are potentially subject to the Buy American Act restrictions. The report also highlights the overlap between the Buy American Act requirements and trade agreements, and notes that the Act's restrictions are waived for eligible products from countries covered by the World Trade Organization's Government Procurement Agreement, Free Trade Agreements, and the Israeli Trade Act. The report concludes that while the Buy American Act restrictions apply to a significant portion of federal procurement, the Act's requirements are waived for certain countries under international trade agreements or other criteria. The report also identifies areas where training and guidance could be improved to ensure compliance with the Buy American Act requirements.
 60%|██████    | 24/40 [06:37<04:40, 17.54s/it]2024-12-22 02:56:19,910 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:23,494 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:23,495 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2023])
2024-12-22 02:56:23,567 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:56:26,149 - [Process 2/5] - INFO - res.shape is :torch.Size([377])
results:
The Small Business Administration (SBA) has received various appropriations from Congress since FY2000 to support its programs, including loan guaranty and venture capital programs, contracting programs, direct loan programs, and small business management and technical assistance training programs. The SBA's appropriations have varied significantly from year to year, with some years showing increases and others showing decreases. For example, in FY2018, the SBA received $2.359 billion in appropriations, while in FY2007, it received $571.8 million. The variation in appropriations is largely due to changing economic conditions and the impact of major hurricanes on disaster assistance. The SBA's Entrepreneurial Development Initiative (EDI) has received recommended appropriations from Congress since FY2010, with a total of $50 million in FY2019. However, the Trump Administration recommended in its FY2018 and FY2019 budget requests that the EDI receive no appropriations. The SBA's Growth Accelerator Initiative received its first recommended appropriation from Congress in FY2014 and has since received $2 million in FY2019. The report also provides total available funding for the SBA's programs, which includes carryover from the prior fiscal year, carryover into the next fiscal year, account transfers, rescissions, and sequestration. Overall, the SBA's appropriations have not kept pace with inflation, and the variation in appropriations has resulted in fluctuations in the SBA's expenditures for its various programs.
 65%|██████▌   | 26/40 [06:44<03:56, 16.89s/it]2024-12-22 02:56:26,391 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:26,803 - [Process 4/5] - INFO - res.shape is :torch.Size([205])
results:
The report examines the potential impact of a carbon tax on the US economy and GHG emissions. The US government agency conducted a study on the effects of a carbon tax on various sectors of the economy, including fossil fuel industries, consumers, and the overall economy. The study found that a carbon tax would lead to increased energy prices, particularly for coal, and a reduction in GHG emissions. The report also discusses the potential distribution of carbon tax revenues and the challenges associated with designing a carbon tax program. The study highlights the importance of considering the potential impacts on emission-intensive, trade-exposed industries and the need for transition assistance for affected communities. The report concludes that a carbon tax would be an effective way to reduce GHG emissions, but policymakers must carefully consider the design of the tax to minimize economy-wide costs and ensure that the benefits of the tax are shared fairly among different groups.
 68%|██████▊   | 27/40 [06:45<03:21, 15.47s/it]2024-12-22 02:56:27,036 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:29,380 - [Process 0/5] - INFO - res.shape is :torch.Size([317])
results:The report examines the role of the Department of Justice (DOJ) and the judiciary in requesting and collecting restitution for victims of federal crimes. The report finds that while DOJ and the judiciary have taken steps to improve the restitution process, there are still issues that need to be addressed. Specifically, the report identifies three main areas of concern: 1) the lack of coordination between DOJ's Asset Forfeiture Program and the judiciary's restitution process, 2) the incomplete USSC data on the reasons restitution was not ordered, and 3) the limited use of forfeited assets to compensate victims. The report recommends that DOJ develop and implement performance measures and goals for each USAO related to the collection of restitution, gather information on cases where victims have been identified but restitution is not ordered, and provide training to probation officers on the proper collection of restitution. The report also recommends that the judiciary determine why USSC data on the reasons restitution was not ordered are incomplete and take action to ensure that the data includes all required information. Finally, the report suggests that DOJ and the judiciary should work together to increase the use of forfeited assets to compensate eligible victims. Overall, the report highlights the need for improved coordination and data collection between DOJ and the judiciary to ensure that victims receive the restitution they are entitled to under federal law.
 72%|███████▎  | 29/40 [06:47<02:25, 13.19s/it]2024-12-22 02:56:29,580 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:29,978 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:29,979 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2074])
2024-12-22 02:56:30,047 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:56:30,676 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:30,676 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2126])
2024-12-22 02:56:30,742 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:56:32,110 - [Process 3/5] - INFO - res.shape is :torch.Size([374])
results:The report examines the document services provided by the Department of Defense (DOD) and identifies opportunities for improvement. The report finds that DOD has made progress in consolidating its document services, but there are still opportunities for further efficiency gains. The report highlights the following key findings:


* DOD has consolidated its printing and reproduction services, but there are still opportunities for further consolidation.
* DOD has not fully implemented electronic content management, and there are opportunities to improve the accuracy and completeness of financial information reported to key stakeholders.
* DLA Document Services has a transformation plan in place to increase efficiency, but there are still opportunities to improve the management of fragmentation in printing and reproduction services and reduce overlap in procuring print devices.
* DOD has taken steps to reduce the cost and number of office print devices, but there is still room for improvement.
* DLA Document Services has a plan to consolidate facilities, but there are still delays in implementing this plan.

The report makes several recommendations to improve DOD's document services, including:

* Developing a comprehensive plan to consolidate all document services.
* Implementing electronic content management to improve the accuracy and completeness of financial information reported to key stakeholders.
* Reducing fragmentation in printing and reproduction services and overlap in procuring print devices.
* Continuing to reduce the cost and number of office print devices.
* Completing the consolidation of DLA Document Services facilities.

Overall, the report highlights the need for DOD to continue its efforts to improve the efficiency and effectiveness of its document services.
 68%|██████▊   | 27/40 [06:50<03:21, 15.52s/it]2024-12-22 02:56:32,379 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:33,001 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:33,001 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1675])
2024-12-22 02:56:33,084 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:56:34,102 - [Process 1/5] - INFO - res.shape is :torch.Size([237])
results:The report provides an overview of the current state of NG9111 implementation across the United States, highlighting the challenges and federal actions taken to address them. The report notes that while 911 services have evolved from basic 911 to Enhanced 911 and NG911, which provides a secure environment for emergency communications, there are limitations to how the data can be used. The report also highlights the need for states and localities to have a secure environment for emergency communications, acquire and integrate additional data for routing and answering calls, and process all types of emergency calls, including multimedia messages. The report also notes that federal agencies have taken various actions to address state and local implementation challenges, including issuing guides on state and legislative planning, exploring NG911 governance implementation issues, and providing technical assistance. The report concludes that while there are no federally mandated time frames for implementing NG911, federal agencies will need to take steps to ensure state NG911 networks are interoperable and connected.
 62%|██████▎   | 25/40 [06:52<04:09, 16.61s/it]2024-12-22 02:56:34,315 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:35,978 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:35,978 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-22 02:56:36,050 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:56:37,899 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:37,900 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2016])
2024-12-22 02:56:37,971 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:56:41,336 - [Process 2/5] - INFO - res.shape is :torch.Size([253])
results:The Black Lung Disability Trust Fund provides financial assistance to coal miners who are disabled due to their work. The report examines the fund's finances and how they may change in the future. The fund has a balance of $6.6 billion in fiscal year 2019, and the scheduled tax rate decrease in 2019 may result in a deficit of about $15.4 billion by 2050. The fund's debt and the amount of debt forgiveness needed to balance the fund are also discussed. The report also explores various options for balancing the fund, including adjusting coal tax rates, forgiving debt interest, and forgiving debt principal and interest. The options are simulated to demonstrate how potential financial adjustments could affect future Trust Fund borrowing from Treasury's general fund through fiscal year 2050. The report concludes that the fund's finances have been challenged due to expenditures exceeding revenue, and legislative actions have not completely addressed the debt issue. The report highlights the need for a comprehensive approach to address the fund's financial challenges.
 68%|██████▊   | 27/40 [06:59<03:32, 16.38s/it]2024-12-22 02:56:41,585 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:43,687 - [Process 0/5] - INFO - res.shape is :torch.Size([263])
results:The report provides an overview of the actions taken by the Department of Defense (DOD), the Obama Administration, and Congress to address the issue of sexual assault in the U.S. military. The report covers the period from June 2013 to May 2016, including the release of the 2012 Workplace and Gender Relations Survey of Reserve Component Members, the announcement of the creation of the Response Systems to Adult Sexual Assault Crimes Panel, and the establishment of the Sexual Assault Prevention and Response Office (SAPRO). The report also highlights the efforts of the DOD to combat sexual assault, including the implementation of the 2013 Sexual Assault Prevention and Response Strategy, the development of methods to hold all military commanders accountable for establishing command climates of dignity and respect, and the establishment of the National Defense Authorization Act for Fiscal Year 2016. Additionally, the report notes the comments made by senior military leaders, including Secretary of Defense Chuck Hagel and Chairman of the Joint Chiefs of Staff Army Gen. Martin Dempsey, on the issue of sexual assault in the military.
 75%|███████▌  | 30/40 [07:01<02:15, 13.52s/it]2024-12-22 02:56:43,780 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:45,152 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:45,152 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 02:56:45,223 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:56:46,502 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:46,503 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1649])
2024-12-22 02:56:46,557 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:56:46,652 - [Process 4/5] - INFO - res.shape is :torch.Size([353])
results:The report by the Government Accountability Office (GAO) examines the financial challenges facing the US Postal Service (USPS) and its retiree health benefits program. The report highlights that USPS is facing an unsustainable financial condition, with a projected depletion of the Retiree Health Benefits Fund (RHB Fund) in 12 years. The fund is currently underfunded by approximately $38 billion, and USPS has missed payments totaling nearly $4.3 billion in fiscal year 2017 and $4.5 billion in fiscal year 2018. The report identifies eight potential policy approaches for addressing the financial challenges facing USPS and its retiree health benefits program, including increasing Medicare participation, prefunding at a lower level, and investing assets outside the US Treasury. However, the report notes that USPS has inadequate resources to cover its required payments to the RHB Fund and appears unlikely to make partial payments. The report also highlights that if the RHB Fund is depleted, USPS would be required to resume "pay-as-you-go" payments for its share of retiree health premiums, which could be challenging given USPS's current financial situation. The report concludes that USPS's financial condition continues to deteriorate, and it is uncertain how funding will be provided if the RHB Fund is depleted. The report recommends that Congress consider passing legislation to put postal retiree health benefits on a more sustainable financial footing.
 70%|███████   | 28/40 [07:04<03:21, 16.79s/it]2024-12-22 02:56:46,951 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:47,328 - [Process 3/5] - INFO - res.shape is :torch.Size([252])
results:The report examines the federal government's real property management and its efforts to reduce the size of its real property portfolio. The report highlights the challenges faced by the CFO Act agencies in achieving their RTF targets, including space reduction costs, mission delivery, employee organization concerns, and workload growth. The report also identifies five approaches used by agencies to reduce space, including consolidation, co-location, disposition of unneeded space, better utilization of existing space, and teleworking and hoteling. The report found that all 24 CFO Act agencies described strategies for reducing office and warehouse space in their 2016 and 2017 Plans, with consolidation, co-location, and disposal being the primary means to reduce space. The report also found that GSA has two key leadership responsibilities related to real property management, including acquiring, managing, and disposing of federally owned real property and leasing commercial buildings on behalf of agencies. The report concludes that while agencies have made progress in reducing their real property portfolio, more work needs to be done to achieve their RTF targets.
 70%|███████   | 28/40 [07:05<03:05, 15.42s/it]2024-12-22 02:56:47,614 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:50,428 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:50,428 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1719])
2024-12-22 02:56:50,511 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:56:51,220 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:51,221 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 02:56:51,293 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:56:52,445 - [Process 1/5] - INFO - res.shape is :torch.Size([322])
results:The Railroad Retirement Board (RRB) administers retirement, survivor, disability, unemployment, and sickness insurance for railroad workers and their families under the Railroad Retirement Act (RRA) and the Railroad Unemployment Insurance Act (RUIA). The report provides an overview of the programs, including how each program is financed, the eligibility rules, and the types of benefits available. The RRA authorizes retirement, survivor, and disability benefits for railroad workers and their families, while the RUIA provides unemployment and sickness benefits. The report also discusses how railroad retirement relates to the Social Security system. As of June 2018, there were 526,100 RRA beneficiaries, with an average monthly benefit of $1,986. The program is primarily financed by payroll taxes, with the remaining funding coming from federal income taxes, interest on investments, and general appropriations. The report also provides data on the number of beneficiaries and the monthly benefit amounts for each program. Additionally, it notes that the RUIA provides for employers to pay a surcharge if the Railroad Unemployment Insurance Account falls below an indexed threshold amount. The summary concludes by highlighting the key points of the report, including the number of beneficiaries, the average benefit amounts, and the funding sources for the programs.
 65%|██████▌   | 26/40 [07:10<03:59, 17.13s/it]2024-12-22 02:56:52,694 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:54,214 - [Process 2/5] - INFO - res.shape is :torch.Size([203])
results:The report examines how federal agencies manage and dispose of personal property, including office equipment, furniture, and specialized items such as scientific devices, fire control equipment, and heavy machinery. The report finds that agencies have varying policies and processes for maintaining inventory controls and accountability systems for personal property, and many do not track or inventory low-value items. The report also identifies that agencies do not have policies for continually surveying property for excess, and there is no centralized system for tracking personal property disposals. The report highlights that GSA is responsible for assisting agencies in disposing of excess property and has established a government-wide personal-property disposal process. The report recommends that agencies should have policies for identifying and assessing property for continued need, and there should be a systematic process for doing so. The report also suggests that agencies should consider implementing a centralized system for tracking personal property disposals.
 70%|███████   | 28/40 [07:12<03:03, 15.33s/it]2024-12-22 02:56:54,445 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:56,280 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:56,281 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2018])
2024-12-22 02:56:56,353 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:56:56,615 - [Process 0/5] - INFO - res.shape is :torch.Size([264])
results:The report discusses the Federal Emergency Management Agency's (FEMA) Fire Management Assistance Grant (FMAG) program, which provides federal assistance for fire suppression activities. The program is authorized by the Robert T. Stafford Disaster Relief and Emergency Assistance Act and is available to states and tribes when a fire is burning out of control and threatens to become a major disaster. FMAGs can be requested by a state when the governor determines that a fire is burning out of control and threatens to become a major disaster. The request must include cost estimates and information about the fire, and FEMA has 45 days to approve or deny the application. The program has two types of fire cost thresholds used to determine eligibility for assistance, and FMAGs are funded through FEMA's Disaster Relief Fund. The report also discusses the criteria used to evaluate wildfires and make a determination whether to issue an FMAG, and the process for appealing a denial of a request for assistance. Additionally, the report notes that most fires that begin on federal land are the responsibility of the federal agency that owns or manages the land, but there are some exceptions.
 78%|███████▊  | 31/40 [07:14<02:00, 13.34s/it]2024-12-22 02:56:56,760 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:56:58,003 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:56:58,003 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 02:56:58,074 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:56:59,296 - [Process 4/5] - INFO - res.shape is :torch.Size([198])
results:The report examines the treatment of neonatal abstinence syndrome (NAS) in infants born to mothers who used opioids during pregnancy. According to the report, NAS is a withdrawal condition that can occur in infants when they are exposed to opioids in the womb. The report highlights that the treatment of NAS varies depending on the severity of the symptoms and the hospital's capacity to provide care. Most infants with NAS are treated in special care nurseries or neonatal intensive care units (NICUs), while some may receive treatment in well newborn nurseries. The report also notes that there is currently no national standard of care for screening or treating NAS, and that more research is needed to determine the most effective treatment approaches. The report concludes by highlighting the need for better coordination and planning among federal agencies and states to address the issue of NAS.
 72%|███████▎  | 29/40 [07:17<02:50, 15.54s/it]2024-12-22 02:56:59,594 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:00,291 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:00,291 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2009])
2024-12-22 02:57:00,363 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:57:02,981 - [Process 1/5] - INFO - res.shape is :torch.Size([151])
results:The report by the Government Accountability Office (GAO) provides an overview of the Department of Veterans Affairs' (VA) IT systems and their effectiveness in delivering benefits and services to veterans. The report highlights several areas of concern, including VA's failure to meet its data center consolidation and optimization targets, lack of interoperability between systems, and inadequate software license management. The report also notes that VA has made progress in some areas, such as implementing a comprehensive software licensing policy and modernizing its electronic health record system. The GAO recommends that VA take steps to address these areas of concern and improve its IT systems to better serve veterans.
 68%|██████▊   | 27/40 [07:21<03:16, 15.15s/it]2024-12-22 02:57:03,183 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:03,183 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-22 02:57:03,256 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:57:03,308 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:06,885 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:06,885 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1749])
2024-12-22 02:57:06,967 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:57:08,785 - [Process 3/5] - INFO - res.shape is :torch.Size([387])
results:The report examines the role of cash in the payment system and the potential implications of a reduced role of cash payments in the economy. The report highlights that cash remains an important method of payment in the US, with almost half of the unbanked households citing economic obstacles as the reason for not accessing banking services. The report also notes that newer digital currencies, such as cryptocurrencies, offer alternative payment methods that are less expensive and more accessible to individuals and small businesses. However, the report raises concerns about the potential benefits, costs, and risks of a reduced role of cash payments, including the impact on marginalized communities, cybersecurity threats, and privacy concerns. The report concludes that policymakers may face decisions about whether to impede or hasten the decline of cash and consider the implications of doing so. The report also provides case studies of Sweden and Kenya, where cash usage has significantly declined, and examines the reasons behind this trend. The report finds that the growth in demand for cash in Sweden is due to consumer preference, while in Kenya, the lack of access to traditional banking services and the proliferation of mobile phones have contributed to the popularity of mobile money services. The report concludes that the success of mobile money services in Kenya has caused financial institutions to reevaluate their business models and shift their focus to offering services to lower-income groups. However, the report notes that the use of cash still remains prevalent in both countries, with 80% of transactions in Kenya still involving cash. Overall, the report highlights the complexities and challenges of transitioning to a cashless society and the need for careful consideration of the potential implications of such a shift.
 72%|███████▎  | 29/40 [07:26<03:09, 17.24s/it]2024-12-22 02:57:09,003 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:09,005 - [Process 2/5] - INFO - res.shape is :torch.Size([238])
results:The report examines how well the US government's agencies oversee the use of funds provided to four inter-American organizations: the Organization of American States (OAS), the Pan American Health Organization (PAHO), the Inter-American Institute for Cooperation on Agriculture (IICA), and the Pan-American Institute on Geography and History (PAIGH). The report finds that these organizations have established mechanisms for overseeing their use of funds, including internal auditors and oversight committees. The US agencies also conduct monitoring activities to ensure that the organizations use their funds effectively. The report identifies key monitoring provisions in the assistance agreements between the US agencies and the inter-American organizations, which include financial and progress reports, site visits, and other forms of oversight. The report concludes that the US agencies have confidence in the organizations' management of their funds and that the organizations have established mechanisms for overseeing their use of funds. The report also identifies areas for improvement, such as the need for USDA to improve its oversight of the use of funds by IICA.
 72%|███████▎  | 29/40 [07:27<02:46, 15.17s/it]2024-12-22 02:57:09,168 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:09,709 - [Process 0/5] - INFO - res.shape is :torch.Size([232])
results:The Indian Health Service (IHS) faces challenges in recruiting and retaining healthcare providers, particularly in rural areas. The IHS lacks information on the costs and number of temporary providers used at its facilities, which hinders its ability to make informed decisions about resource allocation and provider staffing. The IHS relies on contract providers to fill staffing gaps, but there is a shortage of providers in many areas. The IHS has a centralized reporting mechanism for contract providers, but it is not fully implemented agency-wide. The IHS has a vacancy rate for providers, particularly for nurse practitioners, dentists, and pharmacists. The IHS uses strategies such as offering housing units and implementing accredited residency programs to recruit and retain providers. However, these strategies are not always effective due to the challenges of rural locations and geographic isolation. The IHS needs to improve its data collection and analysis to better understand the provider workforce and make informed decisions about staffing and resource allocation.
 80%|████████  | 32/40 [07:27<01:46, 13.27s/it]2024-12-22 02:57:09,832 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:12,180 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:12,180 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1760])
2024-12-22 02:57:12,239 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:57:12,615 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:12,615 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2050])
2024-12-22 02:57:12,687 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:57:12,984 - [Process 4/5] - INFO - res.shape is :torch.Size([219])
results:

The report examines the current state of Internet privacy regulations in the United States and identifies areas where legislation may be necessary to protect consumers' privacy. The Federal Trade Commission (FTC) is the primary agency responsible for enforcing privacy laws, but its authority is limited. The report highlights several areas where FTC could use its existing authority to better protect consumers' privacy, such as implementing APA rulemaking and civil penalty authority for privacy. Additionally, the report recommends that Congress consider comprehensive legislation to address Internet privacy, including specific standards and enforcement tools. The report also notes that the lack of a comprehensive federal privacy law governing the collection, use, and sale or other disclosure of personal information by private-sector companies in the United States is a significant gap in the federal privacy framework. The report concludes that it is an appropriate time for Congress to consider comprehensive Internet privacy legislation to enhance consumer protections and provide more certainty in the marketplace.
 75%|███████▌  | 30/40 [07:31<02:29, 14.99s/it]2024-12-22 02:57:13,155 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:13,510 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:13,510 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1923])
2024-12-22 02:57:13,591 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:57:16,037 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:16,038 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1698])
2024-12-22 02:57:16,096 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:57:18,043 - [Process 1/5] - INFO - res.shape is :torch.Size([247])
results:
The report highlights the potential risks of climate change on national security, particularly in the areas of migration and displacement. According to the report, climate change may lead to population migration within and across international borders, which can contribute to instability and national security challenges. The report notes that while there is debate about the role of climate change in migration, it is clear that the effects of climate change will impact internal migration more than international migration. The report also highlights the need for better incorporating adaptation into planning and collaboration at overseas installations, as well as strengthening monitoring and assessment of climate-related risks. Additionally, the report mentions that the U.S. government agencies, such as State, USAID, and DOD, have a role in responding to issues related to climate change, including as a driver of migration. However, the report notes that climate change was not a focus of the policy documents reviewed for either the current or previous administrations. The report concludes by stating that a national system for climate information could help federal, state, local, and private sector decision makers use climate information to enhance national resilience for future disasters.
 70%|███████   | 28/40 [07:36<03:01, 15.13s/it]2024-12-22 02:57:18,400 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:21,990 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:21,990 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 02:57:22,062 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:57:23,065 - [Process 3/5] - INFO - res.shape is :torch.Size([227])
results:The report discusses the operations and activities of the Southwestern Power Administration (SWA), one of the four regional power marketing administrations (PMAs) operated by the US Department of Energy. SWA is responsible for marketing power from a small reservoir system and selling it to customers, while also balancing power needs with flood control and other required uses. The report highlights SWA's efforts to address changing electric power needs, including the development of a strategic plan to guide its actions for the next 10 years. It also discusses SWA's role in relieving transmission congestion within its marketing area and its participation in the Regional Transmission Organization (RTO). Additionally, the report notes that SWA has been working to address environmental concerns, including the impact of dams on salmon populations and the potential for divestment of its transmission assets. Overall, the report provides an overview of SWA's operations and activities, highlighting its efforts to balance power needs with environmental concerns and its role in the changing electric power landscape.
 75%|███████▌  | 30/40 [07:41<02:43, 16.35s/it]2024-12-22 02:57:23,293 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:25,257 - [Process 0/5] - INFO - res.shape is :torch.Size([289])
results:The report by the Government Accountability Office (GAO) examines the current state of attack submarines in the US Navy. According to the report, the Navy has 51 attack submarines, with 33 Los Angeles class, 3 Seawolf class, and 15 Virginia class submarines. However, maintenance delays and idle time are increasing, with the Navy incurring $266 million in operating and support costs in fiscal year 2018 constant dollars for idle submarines from fiscal year maintenance delays and cannibalization rates. The report also found that the Navy has been unable to begin or complete the vast majority of its attack submarine maintenance periods on time, resulting in significant maintenance delays and operating and support cost expenditures. The Navy's SUBSAFE program, which was established in 1963, has been successful in preventing any SUBSAFE-certified submarine from being lost, but the program has not addressed the root causes of the maintenance delays. The report recommends that the Navy address workforce shortages and facilities needs at public shipyards, develop a comprehensive plan for shipyard capital investment, and develop a sustainable plan and comprehensive assessment to mitigate long-term risks to ships assigned to overseas homeports.
 82%|████████▎ | 33/40 [07:43<01:37, 13.95s/it]2024-12-22 02:57:25,400 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:26,716 - [Process 2/5] - INFO - res.shape is :torch.Size([328])
results:

The report examines the challenges posed by the rise of e-commerce to the U.S. government agencies, particularly the Customs and Border Protection (CBP) and Immigration and Customs Enforcement (ICE), in enforcing intellectual property rights (IPR). The report highlights the increasing sophistication of counterfeit goods, which makes it difficult for law enforcement officers to distinguish between legitimate and counterfeit goods. The shift towards primary markets, including e-commerce websites, corporate and government supply chains, and traditional retail stores, poses challenges to CBP and ICE as they have to deal with a wider variety of goods to screen. The report also notes that counterfeiters may exploit third-party online marketplaces to gain an appearance of legitimacy and access to consumers. Additionally, the growth of e-commerce has accelerated the pace at which counterfeiters can gain access to consumers or reinvent themselves if shut down. The report recommends that CBP take steps to evaluate the effectiveness of its IPR enforcement efforts and shares information with private sector entities to enhance IPR enforcement. The report also notes that CBP and ICE interagency collaboration on IPR enforcement is generally consistent with selected key practices for effective interagency collaboration. Finally, the report highlights the limitations of information sharing between CBP and private sector entities, which can limit the ability of rights holders and e-commerce websites to protect IPR.
 75%|███████▌  | 30/40 [07:44<02:39, 15.93s/it]2024-12-22 02:57:26,868 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:26,868 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 02:57:26,911 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:26,937 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:57:28,217 - [Process 4/5] - INFO - res.shape is :torch.Size([278])
results:

The report from the Government Accountability Office (GAO) discusses the challenges faced by the Federal Bureau of Investigation (FBI) in managing its headquarters facilities. The FBI has been operating in annexes due to the inadequacy of the Hoover Building, which has resulted in security concerns. The building's condition is deteriorating, and the FBI has faced operational and logistical challenges due to space constraints. The GSA has been exploring alternative funding mechanisms for real property projects, including swap exchanges, which can help agencies meet their real property needs while leveraging other authorized resources. However, GSA has limited experience in successfully completing swap exchange transactions, and there are obstacles in using this authority. The report highlights the need for GSA to evaluate its strategy to minimize major repair and recapitalization investments and address any facility condition issues that could put FBI operations at risk. The FBI has canceled the procurement for the new FBI headquarters consolidation project due to a lack of funding, and GSA and the FBI will continue to work together to address the space requirements of the FBI. The report concludes that changes to the budgetary structure itself may also help agencies meet their real property needs.
 78%|███████▊  | 31/40 [07:46<02:15, 15.06s/it]2024-12-22 02:57:28,432 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:29,086 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:29,086 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2113])
2024-12-22 02:57:29,154 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:57:30,441 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:30,441 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2103])
2024-12-22 02:57:30,509 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:57:31,073 - [Process 1/5] - INFO - res.shape is :torch.Size([202])
results:The report examines the use of executive discretion in liberal democracies, specifically in the United States, regarding the declaration of national emergencies and the invocation of the International Emergency Economic Powers Act (IEEPA). The report highlights that the US Constitution does not explicitly define what constitutes a national emergency, leaving it to Congress and the President to interpret the term. The report notes that Congress has not explicitly disapproved of the President's use of IEEPA, despite some criticism, and that the statute has been used to grant the President extensive powers to regulate economic transactions during a state of emergency. The report also discusses the limitations of IEEPA, including the lack of a sunset provision and the potential for abuse of power. Finally, the report mentions recent presidential actions that have drawn attention to the use of IEEPA and the need for Congress to reconsider its role in limiting executive authority.
 72%|███████▎  | 29/40 [07:49<02:39, 14.50s/it]2024-12-22 02:57:31,415 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:32,055 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:32,055 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1913])
2024-12-22 02:57:32,135 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:57:34,968 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:34,968 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2065])
2024-12-22 02:57:35,037 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:57:38,540 - [Process 0/5] - INFO - res.shape is :torch.Size([233])
results:The report discusses the Aegis BMD program, which is carried out by the Missile Defense Agency (MDA) and the Navy, and provides a capability for conducting BMD operations. The program has undergone various tests and evaluations, and the results show that the Aegis BMD system can intercept non-separating, simple-separating, and complex-separating ballistic missiles in the midcourse phase. However, there are still issues with the system, such as technical risk and test and evaluation issues, which need to be addressed. The report also mentions that the Navy plans to modify the DDG-51s to make them BMD-capable, and that the MDA is collaborating with DOT&E and the Under Secretary of Defense (Research and Engineering) to establish an affordable ground testing approach to support assessments of reliability. Additionally, the report highlights the potential for the Aegis BMD system to intercept intercontinental ballistic missiles (ICBMs) with the new SM-3 Block IIA interceptor.
 85%|████████▌ | 34/40 [07:56<01:22, 13.75s/it]2024-12-22 02:57:38,672 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:38,861 - [Process 3/5] - INFO - res.shape is :torch.Size([266])
results:The report discusses the role and responsibilities of the minority leader in the US House of Representatives. The minority leader is the head of the "loyal opposition" and is elected every two years by their party caucus or conference. The position has evolved over time, and the minority leader now has various responsibilities, including serving as the leader and spokesperson for the minority party, participating in institutional prerogatives, and attending meetings of the Intelligence Committee. The minority leader faces challenges in promoting and publicizing the party's priorities, serving the interests of rank-and-file members, managing intraparty conflict, and forging party unity. Despite these challenges, the ultimate goal of the minority leader is to lead the party into majority status. The report highlights the limitations of the minority leader's position, including inferior numbers and the concentration of agenda control and institutional resources in the majority leadership. However, the minority leader can overcome these limitations with the right strategic approach. The report concludes by noting that the minority leader's choices in how to respond to their limited influence in the House can have significant implications for the party's future success.
 78%|███████▊  | 31/40 [07:57<02:25, 16.18s/it]2024-12-22 02:57:39,216 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:41,330 - [Process 4/5] - INFO - res.shape is :torch.Size([206])
results:The report provides an overview of the Unemployment Insurance (UI) system in the United States, including its two primary objectives, the various programs that provide benefits for eligible unemployed workers, and the funding sources for these programs. The report highlights recent legislation and proposals related to UI, including the Extended Benefit (EB) program, the Economic Ladders to End Volatility and Advance Training and Employment Act of 2019 (ELEVATE Act), and the Violence Against Women Reauthorization Act of 2019. The report also discusses the eligibility requirements for UI benefits, including the requirement that workers have lost their jobs through no fault of their own, and the different funding sources for UI programs. Finally, the report notes that states have the flexibility to determine their own UI policies and procedures, but are required to follow federal guidelines and regulations.
 80%|████████  | 32/40 [07:59<01:55, 14.48s/it]2024-12-22 02:57:41,579 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:42,200 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:42,200 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 02:57:42,272 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:57:42,794 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:42,794 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2067])
2024-12-22 02:57:42,863 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:57:44,083 - [Process 1/5] - INFO - res.shape is :torch.Size([202])
results:The report highlights the importance of pipeline security and the need for a comprehensive approach to address vulnerabilities in the national pipeline system. The report identifies several areas of concern, including the inability to reliably measure progress in addressing vulnerability gaps, the lack of a feedback mechanism to inform risk assessment, and the need to consider cybersecurity threats. The report also notes that pipeline operators are not adequately addressing the potential impact of natural disasters and aging infrastructure on pipeline security. Additionally, the report emphasizes the need for a cross-sector approach to address interdependencies between pipelines and other critical infrastructure. The report recommends the development of a feedback mechanism to inform risk assessments and the integration of cybersecurity threats into the risk assessment process. Overall, the report underscores the need for a comprehensive and coordinated approach to pipeline security to ensure the safety and resilience of the national pipeline system.
 75%|███████▌  | 30/40 [08:02<02:20, 14.05s/it]2024-12-22 02:57:44,299 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:45,176 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:45,176 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2051])
2024-12-22 02:57:45,248 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:57:46,069 - [Process 2/5] - INFO - res.shape is :torch.Size([344])
results:The Temporary Assistance for Needy Families (TANF) block grant provides financial assistance to low-income families with children. The report provides an overview of TANF's work requirements, funding, and participation standards. In 2019, the President signed legislation that allows states to receive their quarterly TANF grants for the second and third quarters of FY2019. The report highlights that TANF programs are funded through a combination of federal and state funds, and the minimum MOE amount is $10.3 billion per year for the 50 states, the District of Columbia, and the territories. The report also shows that TANF has two federal grants to states, and the bulk of the TANF funding is in a basic block grant to the states. Additionally, the report states that states are required to expend a minimum amount of their own funds for TANF and TANF-related activities under the MOE requirement. The report also provides information on the uses of federal TANF grants to states and state MOE funds in FY2017, including the all-families work participation rate, which was 50% in FY2017. The report also shows that TANF is a major contributor of child care funding and helps low-wage parents by helping to finance state refundable tax credits. Finally, the report highlights that TANF and MOE funds are also used for short-term and emergency benefits and a wide range of other social services.
 78%|███████▊  | 31/40 [08:04<02:32, 16.96s/it]2024-12-22 02:57:46,318 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:47,907 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:47,907 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 02:57:47,976 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:57:49,875 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:49,876 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 02:57:49,947 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:57:49,952 - [Process 0/5] - INFO - res.shape is :torch.Size([191])
results:

The report examines the guidance provided by US government agencies to implement funding restrictions on promoting tobacco exports overseas. The US Department of State, US Department of Commerce, and US Department of Agriculture have issued guidance to their officials to comply with the restrictions. However, there is a lack of clarity in the guidance provided, and some agencies have more discretion than others in how they address the restrictions. The report finds that State officials have more discretion in how they address the restrictions than Commerce and USDA officials. The report also finds that there are instances where US government agencies have provided support to US tobacco companies despite the funding restrictions. The report recommends that the agencies provide clearer guidance and ensure that all officials understand the restrictions. Additionally, the report suggests that the agencies should consider the impact of their actions on US tobacco control efforts.
 88%|████████▊ | 35/40 [08:08<01:05, 13.05s/it]2024-12-22 02:57:50,023 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:52,038 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:52,038 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1225])
2024-12-22 02:57:52,080 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:57:53,390 - [Process 3/5] - INFO - res.shape is :torch.Size([236])
results:The report discusses the Title IX of the Education Amendments of 1972, which provides legal protection for victims of sexual abuse and harassment in educational institutions. The report highlights that the statute does not explicitly mention sexual abuse or harassment, but federal courts have interpreted it to provide a private right of action for damages against educational institutions for sexual harassment or abuse committed by teachers or other students. The report explains that the Supreme Court has established a deliberate indifference standard for determining liability under Title IX, which requires schools to respond adequately to allegations of sexual harassment or abuse. The report also discusses how the proposed regulations by the Department of Education would significantly change the way schools respond to sexual harassment allegations, including requiring schools to provide supportive measures to complainants and to conduct an individualized risk and safety analysis before removing an accused student from an educational program or activity. The report concludes by noting that the proposed regulations reflect a shift towards a more formal and structured approach to addressing sexual harassment in educational institutions.
 80%|████████  | 32/40 [08:11<02:05, 15.69s/it]2024-12-22 02:57:53,631 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:57:57,170 - [Process 4/5] - INFO - res.shape is :torch.Size([265])
results:
The report by the Government Accountability Office (GAO) provides an overview of the Bureau of Prisons' (BOP) use of retention incentives to attract and retain employees. The report finds that BOP has used its authority to pay retention incentives to employees in various occupations, including medical professionals, correctional officers, and other occupations. The report also identifies the internal controls in place to monitor the application, approval, and funds disbursement processes of BOP's retention incentive program. According to the report, BOP's retention incentive expenditures generally increased from $10.7 million in fiscal year 2012 to $14.0 million in fiscal year 2016, with a total of 3,382 employees receiving retention incentive payments. The report also highlights the challenges that BOP faces in retaining medical professionals and other staff, particularly at institutions in California. The report concludes that BOP has taken steps to address these challenges, including providing group retention incentives to staff at certain institutions and using other compensation-based human capital flexibilities to attract and retain employees.
 82%|████████▎ | 33/40 [08:15<01:44, 14.89s/it]2024-12-22 02:57:57,201 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:57:57,202 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2072])
2024-12-22 02:57:57,271 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:57:57,438 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:00,448 - [Process 1/5] - INFO - res.shape is :torch.Size([277])
results:The report by the Government Accountability Office (GAO) discusses the U.S. Department of Defense's (DOD) global defense posture and its implications for Europe. The report highlights that DOD's posture in Europe has decreased significantly since the end of the Cold War, with two heavy combat brigades deactivated by the end of fiscal year 2014. The report also notes that DOD has requested funding for enhancements to its posture in Europe using its Overseas Contingency Operations (OCO) budget, but this approach has raised concerns about the long-term sustainability of these initiatives. The report recommends that DOD prioritize its posture initiatives under OCO relative to those funded in its base budget, develop comprehensive cost estimates for posture initiatives, and provide Congress with estimates of future costs for posture initiatives. The report also highlights that DOD has identified a need for additional funding over the next several years for additional posture enhancements in Europe, but the details of these plans are classified. The report concludes that DOD needs to improve its planning for posture initiatives funded under OCO and provide more information to Congress on the costs and implications of these initiatives.
 78%|███████▊  | 31/40 [08:18<02:12, 14.75s/it]2024-12-22 02:58:00,678 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:01,028 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:01,029 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 02:58:01,101 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:58:01,617 - [Process 2/5] - INFO - res.shape is :torch.Size([261])
results:
The report examines the payment rates for laboratory tests under the Centers for Medicare & Medicaid Services (CMS) and the challenges the agency faces in setting accurate Medicare payment rates. The report highlights the changes to payment rates for individual laboratory tests and panel tests under the Protecting Access to Medicare Act (PAMA) of 2014. PAMA replaced the previous fee schedule with a single national fee schedule based on private-payer rates. The report also discusses the impact of unbundling payment rates for panel tests and the potential increase in Medicare expenditures. The agency faces challenges in collecting complete data from laboratories, which could affect Medicare payment rates. The report recommends that CMS take steps to collect all data from laboratories required to report, phase in payment-rate reductions based on actual payment rates prior to 2018, and use bundled rates for panel tests. The report also notes that private-payer rates for laboratory tests vary by type and other characteristics, and CMS should consider this when setting payment rates. Finally, the report highlights the importance of reducing the potential of paying more than necessary for laboratory tests.
 80%|████████  | 32/40 [08:19<02:12, 16.53s/it]2024-12-22 02:58:01,896 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:02,432 - [Process 0/5] - INFO - res.shape is :torch.Size([287])
results:
The report by the Government Accountability Office (GAO) reveals that the Department of Veterans Affairs (VA) medical centers failed to report providers who were under investigation for substandard care to the National Practitioner Data Bank (NPDB) and state licensing boards as required by policy. The report found that out of 148 providers who raised concerns, only 1 was reported to the NPDB, and 21 were never reported. The lack of documentation and timely reporting of providers' clinical care raises concerns for the safety of veterans. The report also found that VA medical centers did not always conduct reviews of providers' clinical care in a timely manner, and VHA policy does not require VA medical centers to document all types of reviews. The GAO recommended that VHA require its networks to oversee VA medical centers to ensure they are reporting providers to the NPDB and state licensing boards and to ensure timely reporting. VA concurred with the recommendation and plans to revise existing policy and update the standardized audit tool used by the networks. Additionally, the report found that only one of nine providers required to be reported to the NPDB under VHA policy was reported, and none were reported to state licensing boards.
 90%|█████████ | 36/40 [08:20<00:51, 12.88s/it]2024-12-22 02:58:02,616 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:04,261 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:04,261 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 02:58:04,333 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:58:05,470 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:05,471 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1961])
2024-12-22 02:58:05,546 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:58:06,119 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:06,119 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1951])
2024-12-22 02:58:06,193 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:58:07,029 - [Process 3/5] - INFO - res.shape is :torch.Size([219])
results:
The report by the Government Accountability Office (GAO) discusses the National Science Foundation's (NSF) use of two programs to bring in rotators, IPA and VSEE, to support its scientific mission and programmatic goals. The report highlights that NSF relies on these programs to bring in fresh perspectives and new ideas, and to supplement its permanent staff. However, the report also notes that NSF has not fully evaluated the results of its rotator programs, which limits its ability to demonstrate the programs' benefits to external stakeholders and to adjust the programs as needed. The report recommends that NSF develop an agency-wide workforce strategy for balancing its use of IPA and VSEE rotators with permanent staff, and evaluate the contributions of the rotator programs to NSF's human capital goals and programmatic results. NSF concurred with the recommendations and stated that their implementation will enhance efforts to fulfill the agency's mission and strengthen its workforce.
 82%|████████▎ | 33/40 [08:25<01:45, 15.07s/it]2024-12-22 02:58:07,298 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:10,936 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:10,936 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1890])
2024-12-22 02:58:11,016 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:58:11,981 - [Process 4/5] - INFO - res.shape is :torch.Size([243])
results:The National Highway Traffic Safety Administration (NHTSA) has been conducting safety defect vehicle recalls since 2011, and the number of recalls has increased significantly since then. The report examines the usability of NHTSA's website for consumers to access information about recalls, and it found several areas of improvement. The website's design and content, navigation, clarity, identity and purpose, and accessibility need to be improved. The report also found that consumers' experiences with the website's usability are mixed, with some finding it easy to use and others finding it vague and difficult to understand. The report recommends that NHTSA modify its requirements for defect notification letters to include additional information to obtain readers' attention and improve the letter's clarity. Additionally, the report suggests that NHTSA should address the inconvenience of the recall by providing better estimates of how long repairs might take and including options for scheduling needed repairs. Overall, the report highlights the need for NHTSA to improve its website's usability and communication with consumers to encourage them to complete recall repairs.
 85%|████████▌ | 34/40 [08:30<01:29, 14.86s/it]2024-12-22 02:58:12,196 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:15,108 - [Process 1/5] - INFO - res.shape is :torch.Size([240])
results:The report by the Government Accountability Office (GAO) highlights the importance of effective oversight and monitoring of state Medicaid programs, particularly in the provision of long-term care services. The report notes that while federal and state Medicaid spending on long-term care services continues to increase, there are gaps in state reporting to the Centers for Medicare and Medicaid Services (CMS) on deficiencies that may affect beneficiaries' health and welfare. The report recommends that CMS provide guidance and clarify requirements regarding monitoring and reporting of deficiencies, establish standard Medicaid reporting requirements for all states, and ensure that all states submit annual reports on time. The report also notes that there is limited information on the oversight and quality of care in assisted living facilities, and that states establish their own licensing and oversight requirements for these facilities. The report concludes that CMS should take steps to improve oversight and state reporting to ensure that states are meeting their commitments to protect the health and welfare of Medicaid beneficiaries receiving assisted living services.
 80%|████████  | 32/40 [08:33<01:57, 14.72s/it]2024-12-22 02:58:15,361 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:15,628 - [Process 2/5] - INFO - res.shape is :torch.Size([227])
results:The report discusses the issue of banking preemption in the United States, specifically the extent to which federal law preempts state law in the banking industry. The report provides an overview of the history of banking preemption, including the Supreme Court's decision in Barnett Bank of Marion County, N.A. v. Nelson, which held that federal law preempts state laws that significantly interfere with the powers of national banks. The report also discusses the OCC's recent decision to accept applications for special purpose national bank charters from FinTech companies, and the ongoing debate over the extent to which non-banks can benefit from federal preemption of state usury laws. Additionally, the report touches on current issues in banking preemption, including proposals to provide legal protections to banks serving marijuana businesses that comply with state law. The report concludes by noting that banking preemption is a complex and contentious issue that continues to be the subject of ongoing debate and litigation.
 82%|████████▎ | 33/40 [08:33<01:50, 15.78s/it]2024-12-22 02:58:15,840 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:15,932 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:15,932 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2116])
2024-12-22 02:58:16,001 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:58:18,918 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:18,918 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-22 02:58:18,987 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:58:19,460 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:19,460 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2123])
2024-12-22 02:58:19,526 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:58:20,422 - [Process 0/5] - INFO - res.shape is :torch.Size([352])
results:
The report examines the application of the Congressional Review Act (CRA) to five agency actions: (1) a bulletin issued by the Bureau of Consumer Financial Protection (Bureau) that implemented the Fair Housing Act; (2) two agency actions implementing President Trump's policy on global health assistance funding; (3) a statement by the Social Security Administration (SSA) describing how SSA will process and adjudicate claims for disability benefits; (4) a revenue procedure issued by the Internal Revenue Service (IRS) modifying the information certain tax-exempt entities are required to report to IRS on their annual returns; and (5) a memorandum issued by the Attorney General directing federal prosecutors to adopt a zero-tolerance policy for illegal border crossings at the southwestern border of the United States. GAO concluded that four of the five agency actions were not rules under the CRA because they did not meet the definition of "rule" in the statute. The fifth agency action, the memorandum issued by the Attorney General, was also not a rule under the CRA because it was a rule of agency organization, procedure, or practice that did not substantially affect the rights or obligations of non-agency parties. GAO's opinions were based on the text of the CRA and relevant court decisions interpreting the statute. The report provides a detailed analysis of the statutory definition of "rule" and the exceptions to that definition, as well as the procedures for considering legislation to overturn agency rules under the CRA.
 92%|█████████▎| 37/40 [08:38<00:43, 14.41s/it]2024-12-22 02:58:20,533 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:23,810 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:23,811 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1937])
2024-12-22 02:58:23,875 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:58:25,781 - [Process 4/5] - INFO - res.shape is :torch.Size([219])
results:The report discusses the use of cluster munitions by various countries and the potential risks associated with their use. Cluster munitions are weapons that release smaller submunitions over a wide area, and they have been used in various conflicts, including World War II and more recent conflicts in the Middle East. However, the use of cluster munitions has been criticized due to their potential to cause civilian casualties and the difficulty in removing unexploded submunitions after use. The report highlights the need for countries to consider the humanitarian impact of cluster munitions and to work towards reducing their use. The Convention on Cluster Munitions, signed in 2008, aims to prohibit the use of cluster munitions that do not meet certain safety standards, and 105 countries have signed the treaty as of 2019. The report concludes by stating that while cluster munitions can be effective in some situations, their use should be carefully considered due to the potential risks involved.
 88%|████████▊ | 35/40 [08:43<01:12, 14.54s/it]2024-12-22 02:58:26,103 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:27,148 - [Process 3/5] - INFO - res.shape is :torch.Size([358])
results:The report discusses the housing-related issues that may be of interest to the 116th Congress. The report highlights the following issues: 

* Low levels of housing inventory contribute to higher home prices in some areas, while surplus housing inventory and higher levels of vacant homes are prevalent in other areas.
* The exclusion for canceled mortgage debt and the ability to deduct mortgage insurance premiums may be extended beyond 2017.
* Rising home prices, low interest rates, and rising rental costs have been prominent features of the housing and mortgage markets in recent years.
* The affordability of owner-occupied housing may hold steady or improve, which could impact housing market activity.
* The number and types of homes on the market affect home sales and home prices.
* Traditionally, homeowners have been able to deduct the interest paid on their mortgage, as well as property taxes they pay, as long as they itemize their tax deductions.
* The deduction for mortgage insurance premiums has been extended several times, and it is currently available through the end of 2019.
* Some factors that affect housing affordability may not be captured by traditional metrics of housing affordability, such as the ability to qualify for a mortgage, the availability of homes on the market, and regional differences in house prices and income.

Overall, the report provides an overview of the housing-related issues that may be of interest to the 116th Congress and highlights the need for ongoing monitoring and analysis of these issues.
 85%|████████▌ | 34/40 [08:45<01:39, 16.59s/it]2024-12-22 02:58:27,434 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:29,720 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:29,721 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2075])
2024-12-22 02:58:29,789 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:58:29,939 - [Process 2/5] - INFO - res.shape is :torch.Size([231])
results:
The report discusses the John Lewis (TAO-205) class oiler shipbuilding program, which aims to build 20 new oilers for the Navy. The program has been ongoing since 2015, and the Navy has requested funding for the fifth and sixth ships in the program for FY2020. The report highlights several issues related to the program, including the number of oilers the Navy will require in the future, the level of ship self-defense equipment to be equipped on the TAO-205s, and the impact of the program on the U.S. shipbuilding industrial base. The report also provides information on the design and construction of the TAO-205 class, including the use of commercial vessel designs and the incorporation of existing technologies. The program office has stated that it will revise its acquisition baseline to reflect the updated total quantities for the program, and has fully funded the third and fourth ships and advanced procurement for the fifth ship in FY2019.
 85%|████████▌ | 34/40 [08:48<01:32, 15.34s/it]2024-12-22 02:58:30,121 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:31,053 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:31,053 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1967])
2024-12-22 02:58:31,129 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:58:32,342 - [Process 1/5] - INFO - res.shape is :torch.Size([298])
results:
The report by the Government Accountability Office (GAO) discusses the safety oversight of rail transportation in the United States, specifically focusing on the Federal Railroad Administration (FRA) and the Federal Transit Administration (FTA). The report highlights that both FRA and FTA regulate rail transportation safety, with FRA overseeing safety of freight railroads and FTA overseeing safety of rail transit systems. The report notes that while FRA has a more centralized approach to enforcement, FTA's approach is more decentralized and relies on state safety agencies to monitor rail transit systems' safety operations. The report also mentions that FRA has not developed a method to evaluate the effectiveness of state safety agencies' enforcement, which could lead to deficiencies remaining unaddressed for long periods of time. Additionally, the report notes that FTA has not assumed direct safety oversight of any rail transit operators outside of Washington Metropolitan Area Transit Authority (WMATA), and that the agency is in the process of implementing new authorities and responsibilities provided in federal law. The report concludes by making two recommendations to FTA: developing a plan for developing guidance for state safety agencies on how to develop and implement a risk-based inspection program and developing a method for monitoring the effectiveness of state safety agencies' enforcement.
 82%|████████▎ | 33/40 [08:50<01:48, 15.47s/it]2024-12-22 02:58:32,633 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:33,409 - [Process 0/5] - INFO - res.shape is :torch.Size([241])
results:The report examines the challenges faced by the Department of Defense (DOD) in aligning its facilities and services with the needs of veterans. The report highlights several issues, including:

* Limited stakeholder involvement in the decision-making process
* Inadequate data quality and availability
* Outdated facilities and infrastructure
* Limited resources for modernization and renovation
* Lack of coordination among different stakeholders
* Complexity of interdependent recommendations
* Limited visibility into the implementation process

The report recommends that DOD address these challenges by improving stakeholder communication, developing a common analytical framework, establishing a robust oversight mechanism, involving the audit community, and assessing the value of the facility master plans. The report also highlights the importance of considering seven elements when evaluating proposed legislation for disposing of or realigning VA property. These elements include establishing goals for the process, developing criteria for evaluating closures and realignments, estimating costs and savings, and involving stakeholders in the decision-making process.
 95%|█████████▌| 38/40 [08:51<00:27, 13.98s/it]2024-12-22 02:58:33,488 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:33,489 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1876])
2024-12-22 02:58:33,560 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:58:33,580 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:36,220 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:36,221 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-22 02:58:36,292 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:58:37,110 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:37,110 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 02:58:37,182 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:58:39,131 - [Process 3/5] - INFO - res.shape is :torch.Size([179])
results:
The report discusses the current state of corporate inversions in the US and the efforts of the government to prevent them. Inversions are when a US-based company moves its headquarters to another country to avoid paying US taxes. The report highlights the various tax provisions and regulatory changes that have been implemented to discourage inversions, including the American Jobs Creation Act of 2004, the Treasury regulations, and the Tax Cuts and Jobs Act of 2017. The report also mentions the proposed mergers and acquisitions that have been announced, but ultimately canceled, due to the regulatory changes. The report concludes by stating that while the government's efforts have been successful in reducing the number of inversions, some corporations continue to find ways to avoid the taxes.
 88%|████████▊ | 35/40 [08:57<01:16, 15.20s/it]2024-12-22 02:58:39,361 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:42,087 - [Process 2/5] - INFO - res.shape is :torch.Size([192])
results:The report discusses the three service classifications in the federal government: the competitive service, excepted service, and Senior Executive Service (SES). The competitive service is the largest service classification, consisting of over two million employees. Excepted service employees are not in the competitive service and are generally paid according to different hiring and removal standards. SES employees are a cadre of high-level government administrators who manage major programs and projects within most federal agencies. The SES pay structure is distinct from the rest of the civil service, and employees in this service have different notice and appeal rights for adverse personnel actions. The report also discusses the conditions and procedures for removing, suspending, or taking other adverse actions against SES members, as well as the different types of SES appointments. Finally, the report highlights the removal requirements for SES members employed by the Department of Veterans Affairs.
 88%|████████▊ | 35/40 [09:00<01:11, 14.38s/it]2024-12-22 02:58:42,733 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:42,976 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:42,976 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2054])
2024-12-22 02:58:43,048 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:58:44,831 - [Process 4/5] - INFO - res.shape is :torch.Size([334])
results:
The report by the Government Accountability Office (GAO) discusses the Federal Emergency Management Agency's (FEMA) Public Assistance (PA) program, which provides funding to state, territorial, and local governments, as well as certain private nonprofit organizations, to repair or replace disaster-damaged infrastructure. The report highlights the program's successes and challenges, including the management of risk, workforce, information sharing, and hazard mitigation.

Key findings include:

* FEMA has established a risk management plan to identify and assess potential risks to the PA program, but challenges remain in workforce management, information sharing, and hazard mitigation.
* The program has experienced delays in processing change requests and has limited capacity to address hazard mitigation measures.
* FEMA has implemented a testing and evaluation plan for the system, but challenges persist in sharing information among federal, state, and local participants in the PA process.
* The program has identified system requirements and constraints but lacks a comprehensive approach to track system enhancements.
* FEMA has developed a corrective action plan to address challenges in hazard mitigation, but more work is needed to fully address these issues.

Overall, the report suggests that while the PA program has made progress in managing risk and improving its processes, there are still significant challenges that need to be addressed to ensure the program's effectiveness and efficiency in providing critical support to disaster-affected communities.
 90%|█████████ | 36/40 [09:03<01:03, 15.90s/it]2024-12-22 02:58:44,899 - [Process 0/5] - INFO - res.shape is :torch.Size([192])
results:
The report examines the Department of Homeland Security's (DHS) implementation of the Financial Management Systems Modernization (FMSM) project, which aims to replace legacy financial management systems with a modernized solution. The report finds that DHS did not fully adhere to best practices in conducting an analysis of alternatives (AOA) process, which could impact the reliability of the selected solution. Specifically, the report identifies that Coast Guard, TSA, and DNDO's AAs did not fully meet all four characteristics of a reliable, high-quality AOA process identified by GAO. The report also notes that DHS's guidance did not fully incorporate five of GAO's identified best practices for conducting an AOA process. The report recommends that DHS improve its AOA process to ensure that the selected solution best meets mission needs.
 98%|█████████▊| 39/40 [09:03<00:13, 13.24s/it]2024-12-22 02:58:45,091 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:45,116 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:46,274 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:46,274 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1751])
2024-12-22 02:58:46,354 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:58:48,629 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:48,629 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2024])
2024-12-22 02:58:48,700 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 02:58:48,729 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:48,729 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2079])
2024-12-22 02:58:48,798 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:58:51,002 - [Process 1/5] - INFO - res.shape is :torch.Size([327])
results:The Commercial Crew Program is a multi-phased effort by NASA to determine its risk tolerance level with respect to loss of crew before the agency certification review. The program has engaged several companies using Space Act agreements, which are not subject to the Federal Acquisition Regulation (FAR). NASA has documented its risk tolerance level with respect to loss of crew in its May 2011 safety memo. The program has narrowed down the number of participants and uses FAR-based contracts for the final two phases. The contractors must provide technical support and meet certain requirements, including the ISS program requirements and Commercial Crew Program requirements. NASA has taken action to address concerns about confusion regarding the risk tolerance level. The program has defined key events, such as uncrewed and crewed flight tests, design certification reviews, and certification milestones. NASA has also analyzed the contractors' schedule data and made changes to the contracts. The program has identified potential effects of certification delays on NASA's access to the ISS and reviewed safety policies, program plans, and contract documents. The program has also updated the Orbital Debris Engineering Model and assessed differences between the former and updated models. The program has made progress in closing requirements, including safety-specific requirements, and has a standing review board and an Aerospace Safety Advisory Panel to provide independent assessments. The program faces challenges in maintaining visibility into the contractors' crew transportation systems and addressing safety concerns.
 85%|████████▌ | 34/40 [09:09<01:38, 16.43s/it]2024-12-22 02:58:51,212 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:54,838 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:58:54,838 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1913])
2024-12-22 02:58:54,918 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:58:57,917 - [Process 0/5] - INFO - res.shape is :torch.Size([229])
results:The report discusses the implementation of the Foreign Account Tax Compliance Act (FATCA) and its impact on the Internal Revenue Service (IRS) in identifying and preventing tax evasion by U.S. persons holding investments in offshore accounts. The report highlights several issues with the consistency and completeness of Form 8938 and relevant parent tax return data stored across various databases, including inconsistent and incomplete data on addresses linked to Form 8938 filers, and incomplete data on assets reported on Forms 8938. The report also notes that IRS lacks the ability to link account information collected from FFIs to the account's owner due to incorrect or missing TINs. The report concludes that without valid TINs on Forms 8966 submitted by FFIs, IRS faces significant hurdles in matching accounts reported by FFIs to those reported by individual tax filers on their Forms 8938, and recommends that IRS initiate compliance efforts to address these issues.
100%|██████████| 40/40 [09:16<00:00, 13.17s/it]100%|██████████| 40/40 [09:16<00:00, 13.90s/it]
2024-12-22 02:58:57,931 - [Process 3/5] - INFO - res.shape is :torch.Size([329])
results:The report provides an overview of the United States' contribution to UN peacekeeping operations, specifically in the Central African Republic (CAR). The US government has contributed $2.1 billion to UN peacekeeping operations from 2014 to 2017, with $700 million going towards MINUSCA, the UN operation in CAR. The report highlights the differences in costs between the UN and US operations, with the US operation estimated to cost nearly $5.7 billion, more than twice the UN cost for MINUSCA. The report also identifies factors contributing to these differences, including sourcing and transporting equipment and supplies, staffing and compensating military and police personnel, and maintaining facilities and communications. The report concludes that the US costs for a hypothetical, comparable operation would be almost eight times greater than its contribution to MINUSCA. The report also compares the costs of a US operation to the UN operation, highlighting differences in staffing, deployment, and equipment. The report finds that the US operation would require more personnel, including military and police, and would have higher costs for equipment and supplies. The report also notes that the US operation would have higher costs for staffing and compensating military and police personnel, as well as for maintaining facilities and communications. The report concludes that the differences in costs between the US and UN operations are due to operational, structural, and doctrinal differences in the way the US likely would undertake a hypothetical, comparable operation.
 90%|█████████ | 36/40 [09:16<01:05, 16.28s/it]2024-12-22 02:58:58,258 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:59,503 - [Process 4/5] - INFO - res.shape is :torch.Size([240])
results:The report provides an overview of the Energy and Water Development appropriations bill for FY2020, which includes funding for various government agencies such as the Department of Energy, the Army Corps of Engineers, and the Bureau of Reclamation. The report highlights the key issues and controversies surrounding the budget request, including the reduction in funding for various programs, elimination of certain programs, and the proposal to transfer the Formerly Utilized Sites Remedial Action Program from the Army Corps of Engineers to the Department of Energy. The report also provides information on the budget history of the agencies, including the FY2019 appropriations and the Bipartisan Budget Act of 2018. Additionally, the report notes that the budget request includes three mandatory proposals related to the Power Marketing Administrations, which sell power generated by the dams operated by the Bureau of Reclamation and the Army Corps of Engineers. The report concludes by highlighting the issues that may generate controversy during congressional consideration of the Energy and Water Development appropriations bill for FY2020.
 92%|█████████▎| 37/40 [09:17<00:46, 15.53s/it]2024-12-22 02:58:59,711 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:58:59,797 - [Process 2/5] - INFO - res.shape is :torch.Size([294])
results:
The report provides an overview of China's actions in the South China Sea (SCS) and East China Sea (ECS), and their implications for U.S. strategic and policy interests. The report highlights China's island-building and base-construction activities in the Spratly Islands, which could challenge the principle of freedom of the seas and have broad implications for the region. The report also notes that China's actions could lead to a crisis or conflict between China and its neighbors, and that the United States could be drawn into such a conflict as a result of its security treaties with Japan and the Philippines. The report suggests that the United States should develop a strategy to counter China's "salami-slicing" strategy or gray zone operations, and to impose costs on China for its actions in the SCS and ECS. The report also recommends that the United States should strengthen its military presence in the region, including through the use of freedom of navigation operations, and should work with its allies and partners to develop a unified response to China's actions. Finally, the report notes that China's actions in the SCS and ECS could have implications for the broader international order, and that the United States should work with its allies and partners to defend the principle of freedom of the seas and to maintain stability in the region.
 90%|█████████ | 36/40 [09:18<01:01, 15.38s/it]2024-12-22 02:59:00,090 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:00,857 - [Process 1/5] - INFO - res.shape is :torch.Size([132])
results:The report discusses the appointment process for various leaders of legislative branch agencies and entities, including the Comptroller General, the Librarian of Congress, the Director of the Congressional Budget Office, and the Register of Copyrights. The report highlights the different appointment procedures for each position, including the role of Congress in the process and the various reforms that have been proposed over the years. It also provides information on the current leaders of each agency and their terms of office. The report concludes by noting that the appointment process for these positions is complex and has been the subject of numerous proposals for reform.
 88%|████████▊ | 35/40 [09:19<01:12, 14.46s/it]2024-12-22 02:59:01,162 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:01,834 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:01,835 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2110])
2024-12-22 02:59:01,904 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:59:03,295 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:03,295 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-22 02:59:03,368 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:59:03,649 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:03,650 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2046])
2024-12-22 02:59:03,721 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:59:04,720 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:04,720 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2095])
2024-12-22 02:59:04,789 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:59:13,321 - [Process 2/5] - INFO - res.shape is :torch.Size([208])
results:The report provides an overview of the defense laboratory enterprise of the United States government, which consists of 63 labs, warfare centers, and engineering centers across the Departments of the Army, Navy, and Air Force. The report highlights the funding models used by these labs, including appropriated funding, mission funding, and revolving funds. It also discusses the authorities granted to defense lab directors by Congress to address hiring, infrastructure, and technology transition challenges. The report focuses on four key authorities: laboratory-initiated research, direct hire, laboratory enhancement pilot program, and micro-purchase. The report finds that while most defense labs have used these authorities since 2008, their use has been limited due to DOD legal and policy restrictions and stakeholder concerns. The report concludes that the use of these authorities can help support innovation within DOD labs and improve their effectiveness.
 92%|█████████▎| 37/40 [09:31<00:44, 14.82s/it]2024-12-22 02:59:13,353 - [Process 4/5] - INFO - res.shape is :torch.Size([220])
results:The Public Safety Officers' Benefits (PSOB) program provides death and disability benefits to federal, state, and local law enforcement officers, firefighters, emergency management employees, and emergency medical services personnel who are killed or permanently disabled in the line of duty. The program is administered by the Department of Justice (DOJ) and funded by Congress through the annual Departments of Commerce and Justice Appropriations Act. To be eligible for benefits, a public safety officer must have served in a designated capacity and suffered an injury or death while performing official duties. The program covers a wide range of injuries and deaths, including those resulting from terrorist attacks, and provides a lump-sum death benefit of $359,316 and monthly disability benefits. The PSOEA program provides financial assistance for higher education to the spouses and children of eligible public safety officers. The program has an appeals process for denied claims and is funded through annual appropriations.
 95%|█████████▌| 38/40 [09:31<00:30, 15.02s/it]2024-12-22 02:59:13,559 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:13,654 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:14,019 - [Process 1/5] - INFO - res.shape is :torch.Size([202])
results:The report discusses the current state of the banking industry and the regulatory framework that governs it. The report highlights the importance of banking in the economy and the potential risks associated with it, such as banking panics and failures. The report also discusses the evolution of banking regulation since the 2007-2009 financial crisis, including the Dodd-Frank Act and the Economic Growth, Regulatory Relief, and Consumer Protection Act (EGRRCP Act). The report notes that while regulatory changes have been made to address risks, there are still concerns about the potential costs and benefits of these changes. The report also discusses issues related to safety and soundness regulation, consumer protection, cybersecurity, and interest rate risk. The report concludes by highlighting the importance of managing interest rate risk and the challenges banks may face in a rising interest rate environment.
 90%|█████████ | 36/40 [09:32<00:56, 14.07s/it]2024-12-22 02:59:14,387 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:16,178 - [Process 3/5] - INFO - res.shape is :torch.Size([317])
results:The Small Business Administration (SBA) has been given the task of defining small businesses in the United States. The SBA has been using industry-specific size standards to determine program eligibility for small businesses. However, there have been challenges in defining small businesses, as there is no consensus on the economic factors that should be used in defining small businesses. The SBA has been conducting economic analyses of each industry's overall competitiveness and the competitiveness of firms within the industry to determine its size standards. The SBA has also been using alternative size standards based on the applicant's maximum tangible net worth and average net income after federal taxes. Recently, there have been changes in the law that have given the SBA more flexibility in defining small businesses, such as the ability to use different size standards for agricultural enterprises and the ability to adjust size standards for inflation. There are also options for Congress to consider in relation to the SBA's ongoing review of its size standards, such as waiting for the SBA to issue its proposed rule before providing input or establishing a dialogue with the agency. Additionally, there is a recommendation to target the SBA's resources to small businesses in industries that are struggling to remain competitive. Overall, the report highlights the challenges in defining small businesses and the need for the SBA to continue to review and adjust its size standards to ensure that they are accurate and effective.
 92%|█████████▎| 37/40 [09:34<00:50, 16.87s/it]2024-12-22 02:59:16,448 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:17,166 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:17,167 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1905])
2024-12-22 02:59:17,244 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:17,244 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2016])
2024-12-22 02:59:17,247 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:59:17,317 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:59:18,034 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:18,034 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2122])
2024-12-22 02:59:18,100 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:59:20,087 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:20,087 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1887])
2024-12-22 02:59:20,166 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:59:25,492 - [Process 1/5] - INFO - res.shape is :torch.Size([161])
results:The report discusses the current state of Colombia's drug policy and its impact on the country. The government has made progress in reducing kidnappings and homicides, but the country still faces challenges in combating drug trafficking and addressing the root causes of violence. The report highlights the need for a more comprehensive approach to drug policy, including economic and education initiatives to address the social and economic factors that drive drug production and consumption. The report also notes the importance of addressing the issue of human rights violations and the need for greater international cooperation in addressing the problem. Overall, the report suggests that a more holistic approach to drug policy is needed to effectively address the complex issues surrounding drug use and trafficking in Colombia.
 92%|█████████▎| 37/40 [09:43<00:39, 13.29s/it]2024-12-22 02:59:25,608 - [Process 2/5] - INFO - res.shape is :torch.Size([187])
results:The report provides an overview of the various federal assistance programs available to individuals, states, territories, local governments, and businesses affected by disasters. The Federal Emergency Management Agency (FEMA) is the primary agency responsible for providing disaster relief, including the Individuals and Households Program (IHP), which provides financial and direct assistance to eligible individuals and households. Other agencies, such as the Department of Labor, also offer assistance programs, including Disaster Unemployment Assistance and Dislocated Worker Program. The report also highlights the Disaster Assistance.gov website, which provides information on how to apply for assistance and track the status of applications. Additionally, the report provides information on the Assistance Listings website, which provides detailed descriptions of over 2,200 federal assistance programs, including disaster and recovery grants and loans.
 95%|█████████▌| 38/40 [09:43<00:28, 14.06s/it]2024-12-22 02:59:25,708 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:25,849 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:26,846 - [Process 3/5] - INFO - res.shape is :torch.Size([147])
results:The report examines the trend of employers shifting from defined benefit (DB) retirement plans to defined contribution (DC) plans, which transfers risk from employers to employees. The report highlights that early withdrawals from retirement savings can result in permanent loss of assets and reduce the amount accumulated before retirement. The report also discusses potential strategies to reduce early withdrawals, including portability of DC plans, and provides data on the incidence of early withdrawals from 401(k) plans. The report concludes that while there are potential strategies to reduce early withdrawals, more research is needed to understand the impact of these strategies on retirement savings.
 95%|█████████▌| 38/40 [09:45<00:30, 15.01s/it]2024-12-22 02:59:27,070 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:29,056 - [Process 4/5] - INFO - res.shape is :torch.Size([262])
results:
The report by the Centers for Disease Control and Prevention (CDC) provides an overview of the agency's information security program and its implementation of the National Institute of Standards and Technology (NIST) cybersecurity framework. The report highlights the agency's reliance on partnerships and collaboration to achieve its mission and goals, as well as its extensive use of information technology to support its operations. The report also identifies deficiencies in the agency's security controls and recommendations for improvement. The CDC was found to have implemented numerous controls over 24 systems, but had not always effectively implemented controls to protect the confidentiality, integrity, and availability of these systems and the information maintained on them. The report concludes that the agency had not fully addressed previous security program and technical control deficiencies and provides recommendations for improvement. The CDC was also found to have categorized systems based on the potential impact of a compromise, but did not appropriately categorize a key general support system. The report highlights the importance of implementing the NIST cybersecurity framework and the need for the agency to address the identified deficiencies to ensure the security of its information and systems.
 98%|█████████▊| 39/40 [09:47<00:15, 15.23s/it]2024-12-22 02:59:29,271 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:29,294 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:29,295 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2026])
2024-12-22 02:59:29,366 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:59:29,429 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:29,429 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1963])
2024-12-22 02:59:29,505 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:59:30,684 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:30,684 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
2024-12-22 02:59:30,756 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:59:32,832 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:32,832 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-22 02:59:32,901 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 02:59:39,364 - [Process 2/5] - INFO - res.shape is :torch.Size([216])
results:
The report by the Government Accountability Office (GAO) examines the processes used by the Centers for Medicare and Medicaid Services (CMS) to review state-reported expenditures for Medicaid programs. The report finds that while CMS has implemented various processes to review state-reported expenditures, there are weaknesses in how CMS targets its oversight resources, which could result in hundreds of millions of federal dollars in errors going undetected. The report highlights three recommendations for CMS to improve its oversight processes, including conducting a comprehensive risk assessment, clarifying internal guidance on when to conduct variance analyses, and revising the sampling methodology for reviewing expenditures for the Medicaid expansion population. CMS concurred with all three recommendations and noted that it takes seriously its responsibilities to protect taxpayer funds by conducting thorough oversight of states’ claims for federal Medicaid expenditures.
 98%|█████████▊| 39/40 [09:57<00:13, 13.97s/it]2024-12-22 02:59:39,637 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:40,248 - [Process 3/5] - INFO - res.shape is :torch.Size([207])
results:The report discusses the history of U.S. aid to Georgia, focusing on the period since the 2008 war between Georgia and Russia. The U.S. provided significant aid to Georgia after the war, including military and non-military aid, to support the country's stability and security. The report highlights the different types of aid provided by the U.S., including humanitarian aid, emergency relief, and military assistance. It also notes the increase in U.S. military aid to Georgia since the 2008 war, including funding for training and equipment for the Georgian military. The report also discusses the political and social changes in Georgia since the 2008 war, including the election of a new president and the deepening of defense cooperation between Georgia and the U.S. The report concludes by highlighting the ongoing U.S. aid to Georgia and the potential for future cooperation between the two countries.
 98%|█████████▊| 39/40 [09:58<00:14, 14.53s/it]2024-12-22 02:59:40,503 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:42,519 - [Process 4/5] - INFO - res.shape is :torch.Size([208])
results:The report discusses the U.S. government's involvement in the Saudi-led coalition's military operations in Yemen, specifically in 2015. The U.S. provided logistical and intelligence support to the coalition without taking direct military action. The report highlights the controversy surrounding the coalition's airstrikes, which have resulted in civilian casualties, and the U.S. government's response to these incidents. The report also mentions the U.S. Congress's involvement in debating and overseeing U.S. policy in Yemen, including legislative proposals to reduce civilian casualties and end restrictions on the flow of goods and humanitarian aid. The report concludes by stating that the conflict in Yemen has led to a growing concern among lawmakers regarding the U.S. government's involvement in the conflict and the need for greater oversight and accountability.
100%|██████████| 40/40 [10:00<00:00, 14.70s/it]100%|██████████| 40/40 [10:00<00:00, 15.02s/it]
2024-12-22 02:59:43,196 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:43,197 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2033])
2024-12-22 02:59:43,268 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 02:59:43,963 - [Process 1/5] - INFO - res.shape is :torch.Size([325])
results:The Federal Aviation Administration (FAA) has been tasked with ensuring a balance of risk between the federal government and launch companies for third-party damages resulting from commercial space launch accidents. The Commercial Space Launch Act Amendments of 1988 requires FAA to calculate an insurance policy limit (MPL) for each launch, which reflects the greatest dollar amount of loss to third parties and the federal government. However, FAA's current methodology has not been updated since the 1990s, and the probability thresholds used to divide the risk of loss between launch companies and the federal government have not been reevaluated. In 2016, FAA issued five active licenses with an average third-party MPL of about $51 million, and the agency has not fully addressed the requirements of the Commercial Space Launch Act Amendments of 1988. FAA has revised two components of its MPL methodology since 2012, but the agency has not evaluated the cost impact of implementing an updated MPL methodology on the direct costs of launch companies and the federal government. Additionally, FAA has not consulted with launch providers and insurance providers in evaluating its MPL methodology. The report recommends that FAA fulfill the CSLCA mandate to evaluate its MPL methodology, reexamine the current probability thresholds, and consult with the commercial space sector and insurance providers.
 95%|█████████▌| 38/40 [10:02<00:29, 14.85s/it]2024-12-22 02:59:44,110 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:44,110 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1989])
2024-12-22 02:59:44,136 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:44,184 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 02:59:47,304 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:47,304 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1756])
2024-12-22 02:59:47,368 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 02:59:52,588 - [Process 2/5] - INFO - res.shape is :torch.Size([204])
results:The report provides an overview of the history of U.S. foreign aid to Egypt since 1952, focusing on the role of the military in shaping the country's political landscape. The report highlights how the U.S. government has provided billions of dollars in aid to Egypt since the 1979 peace treaty between Egypt and Israel, with the majority of aid going towards military assistance. The report also discusses how the Egyptian military has traditionally held significant power in the country, and how the U.S. government has supported this power through aid. Additionally, the report touches on the current state of Egypt's economy and how the country's economic aid has been reduced significantly over the past two decades. Finally, the report mentions how Egypt has been active in the international community, hosting forums and summits, and how President Sisi has been working to consolidate his power through political alliances and personnel moves.
100%|██████████| 40/40 [10:10<00:00, 13.75s/it]100%|██████████| 40/40 [10:10<00:00, 15.27s/it]
2024-12-22 02:59:53,902 - [Process 3/5] - INFO - res.shape is :torch.Size([213])
results:

The report examines the payment policies for long-term care hospitals (LTCHs) under the Medicare program. The report focuses on two qualifying hospitals, Shepherd Center and Craig Hospital, and their eligibility for the temporary exception to site-neutral payments. The report finds that both hospitals meet the criteria for the exception and will continue to receive payments at the standard rate. The report also highlights the growth in the number of LTCHs and the concerns regarding the payment policies. The Medicare Payment Advisory Commission (MedPAC) has recommended changes to the payment policies to ensure that they are fair and sustainable. The report concludes that the current payment policies are complex and may understate or overstate the amount that the qualifying hospitals would have been paid in the baseline years based on future payment policies. The report emphasizes the need for further analysis and review of the payment policies to ensure that they are effective and efficient.
100%|██████████| 40/40 [10:12<00:00, 14.27s/it]100%|██████████| 40/40 [10:12<00:00, 15.30s/it]
2024-12-22 02:59:55,598 - [Process 1/5] - INFO - res.shape is :torch.Size([200])
results:
The report by the Government Accountability Office (GAO) discusses the Coast Guard's performance in various areas, including marine safety, capital planning, and performance goal data. The report finds that the Coast Guard's data on marine safety is limited due to changes in industry trends, making it difficult to accurately determine actual injury rates and program performance. The report also notes that the Coast Guard's performance goal data is not reliably reported, and that the agency's capital planning transparency is limited due to incomplete data and unrealistic asset performance data. The GAO recommends that the Coast Guard improve its data transparency and incorporate field unit input to inform more realistic asset allocation decisions. The Coast Guard concurred with these recommendations and is taking steps to address them. Overall, the report highlights the need for the Coast Guard to improve its data quality and transparency to effectively carry out its missions.
 98%|█████████▊| 39/40 [10:13<00:13, 13.88s/it]2024-12-22 02:59:55,868 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 02:59:59,446 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 02:59:59,447 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 02:59:59,518 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:00:07,937 - [Process 1/5] - INFO - res.shape is :torch.Size([199])
results:The Small Business Administration (SBA) was established in 1953 to promote the interests of small businesses in the federal contracting process. The SBA's commercial market representatives conduct periodic compliance reviews of contractors to ensure they are meeting the requirements of the Small Business Act. The report highlights the various federal programs that support small businesses, including loan guaranty and venture capital programs, contracting programs, direct loan programs, and small business management and technical assistance training programs. The report also discusses the requirements and authorities in promoting contracting and subcontracting with small businesses, including the requirement to reserve contracts for small businesses, set aside contracts for specific types of small businesses, and make accelerated payments to small businesses. The report concludes with a brief discussion of the strong bipartisan support for small business contracting programs and the issues raised concerning the impact and operations of specific programs.
100%|██████████| 40/40 [10:26<00:00, 13.42s/it]100%|██████████| 40/40 [10:26<00:00, 15.65s/it]
2024-12-22 03:00:07,983 - [Process 2/5] - DEBUG - datasets_name:gov_report
2024-12-22 03:00:07,983 - [Process 3/5] - DEBUG - datasets_name:gov_report
2024-12-22 03:00:07,983 - [Process 1/5] - DEBUG - datasets_name:gov_report
2024-12-22 03:00:07,983 - [Process 0/5] - DEBUG - datasets_name:gov_report
2024-12-22 03:00:07,983 - [Process 4/5] - DEBUG - datasets_name:gov_report
Running evaluation for dataset: multi_news
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]n_windows:[2]

model_name:/mnt/Data/xiongjing/llama2chatmodel_name:/mnt/Data/xiongjing/llama2chat

multi_gpus:Truemulti_gpus:True

torch.cuda.device_count():5torch.cuda.device_count():5

n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.63s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.47s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 03:02:05,575 - [Process 0/5] - INFO - loading datasets finished
2024-12-22 03:02:05,575 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-22 03:02:05,576 - [Process 0/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 03:02:05,578 - [Process 4/5] - INFO - loading datasets finished
2024-12-22 03:02:05,578 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-22 03:02:05,578 - [Process 4/5] - INFO - output_max_len: 512
2024-12-22 03:02:05,587 - [Process 0/5] - INFO - Max Length is 10933
2024-12-22 03:02:05,587 - [Process 0/5] - INFO - Finish loading dataset
2024-12-22 03:02:05,587 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 03:02:05,589 - [Process 3/5] - INFO - loading datasets finished
2024-12-22 03:02:05,589 - [Process 3/5] - INFO - model_max_len: 3950
!!!!!!!!!!!!!!!!!!!!!!!! 这里2024-12-22 03:02:05,589 - [Process 3/5] - INFO - output_max_len: 512

2024-12-22 03:02:05,590 - [Process 2/5] - INFO - loading datasets finished
2024-12-22 03:02:05,590 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-22 03:02:05,591 - [Process 2/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-22 03:02:05,597 - [Process 1/5] - INFO - loading datasets finished
2024-12-22 03:02:05,597 - [Process 4/5] - INFO - Max Length is 10933
2024-12-22 03:02:05,597 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-22 03:02:05,597 - [Process 1/5] - INFO - output_max_len: 512
2024-12-22 03:02:05,597 - [Process 4/5] - INFO - Finish loading dataset
2024-12-22 03:02:05,598 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 03:02:05,608 - [Process 3/5] - INFO - Max Length is 10933
2024-12-22 03:02:05,608 - [Process 3/5] - INFO - Finish loading dataset
2024-12-22 03:02:05,609 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 03:02:05,611 - [Process 2/5] - INFO - Max Length is 10933
2024-12-22 03:02:05,611 - [Process 2/5] - INFO - Finish loading dataset
2024-12-22 03:02:05,612 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 03:02:05,616 - [Process 1/5] - INFO - Max Length is 10933
2024-12-22 03:02:05,617 - [Process 1/5] - INFO - Finish loading dataset
2024-12-22 03:02:05,617 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-22 03:02:10,335 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:10,371 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:10,381 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:10,401 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:10,416 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:13,178 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:13,179 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1213])
2024-12-22 03:02:13,220 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 03:02:13,531 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:13,532 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1372])
2024-12-22 03:02:13,580 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 03:02:13,597 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:13,597 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1650])
2024-12-22 03:02:13,651 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 03:02:13,944 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:13,944 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1564])
2024-12-22 03:02:14,002 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 03:02:14,326 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:14,327 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1747])
2024-12-22 03:02:14,391 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-22 03:02:21,083 - [Process 4/5] - INFO - res.shape is :torch.Size([175])
results:The Mega Millions jackpot has reached an estimated $636 million, with two people matching all the winning numbers. It is one of the largest jackpots in U.S. history, and the winner will have the option to take a lump-sum cash prize or an annuity. The lottery has seen an increase in ticket sales, with some people buying tickets at convenience stores and gas stations. The odds of winning are low, but the prize is large enough to make it worth trying. Some people have already started planning their spending, including buying a new house or car. The lottery has also seen an increase in the number of people seeking professional help with managing their winnings. The jackpot is expected to keep growing until it is won, and the next drawing is on Friday.
  2%|▎         | 1/40 [00:15<10:04, 15.49s/it]2024-12-22 03:02:21,222 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:22,156 - [Process 2/5] - INFO - res.shape is :torch.Size([193])
results:

On Wednesday night, a couple was shot and robbed while trying to buy their teenage son's first vehicle in southeast Kansas City. The couple, aged 53 and 55, were shot four times each and their vehicle was stolen by two men who approached them after they had finished test driving the car. The incident occurred within two miles of another robbery that occurred earlier in the day, where a man was robbed of his Xbox after meeting someone to purchase it on Craigslist. The police have identified three suspects in the incident and are offering a reward of up to $7,000 for information leading to their arrest. The incident has raised concerns about safety when meeting strangers for online transactions and the police have advised people to use caution and trust their instincts. A local business has donated $5,000 towards the reward fund.
  2%|▎         | 1/40 [00:16<10:45, 16.55s/it]2024-12-22 03:02:22,262 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:23,074 - [Process 3/5] - INFO - res.shape is :torch.Size([204])
results:
The news articles discuss the settlement reached between the Environmental Protection Agency (EPA) and Magnolia Homes, a renovation firm that appears on the HGTV show "Fixer Upper." Magnolia Homes agreed to pay a $40,000 fine and implement a compliance management program to ensure its own renovation operations comply with lead-based paint regulations. The firm will also produce an educational video and spend $160,000 on a lead-abatement project in its local community. The settlement comes after EPA found evidence of noncompliance with the Toxic Substances Control Act's (TSCA) Lead Renovation, Repair and Painting Rule (RRP Rule) during a review of the show. The articles highlight the importance of lead-based paint safety and the need for renovation firms to follow proper protocols to minimize lead exposure, especially in older homes.
  2%|▎         | 1/40 [00:17<11:21, 17.47s/it]2024-12-22 03:02:23,274 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:23,595 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:23,595 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1433])
2024-12-22 03:02:23,640 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:02:23,846 - [Process 1/5] - INFO - res.shape is :torch.Size([212])
results:The Pacific Crest Trail (PCT) has seen a significant increase in the number of hikers due to the popularity of the book and movie "Wild". The PCT Association has implemented a quota system to limit the number of hikers to 50 per day to mitigate environmental damage and prevent overcrowding. The trail is expected to be busier than usual this year, with over 4,000 long-distance hiking permits issued so far. The PCT starts near Campo, California, and stretches 2,650 miles through California, Oregon, and Washington. The trail is a challenging and rewarding experience that can have a profound impact on hikers' lives, but it is important to be prepared and respect the trail and its environment. The PCTA is working to promote safety and "leave no trace" practices, and there are concerns about the impact of increased hiker numbers on the trail and its environment.
  2%|▎         | 1/40 [00:18<11:51, 18.24s/it]2024-12-22 03:02:23,907 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:24,068 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:24,068 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1044])
2024-12-22 03:02:24,102 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:02:24,940 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:24,940 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 635])
2024-12-22 03:02:24,960 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:02:26,813 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:26,814 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2105])
2024-12-22 03:02:26,882 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:02:27,143 - [Process 0/5] - INFO - res.shape is :torch.Size([353])
results:The Occupy Los Angeles protesters at City Hall remained in place after the midnight deadline set by Mayor Nutter, with police avoiding an eviction. The police did not disperse the protesters, but instead allowed them to stay until at least daybreak. The Occupy Los Angeles encampment is one of the largest on the West Coast and has been tolerated by city officials for weeks despite other cities clearing out similar camps. The mayor had given protesters until just after midnight to remove their tents and leave or face a forcible removal. The police commander said it was unlikely the camp would be forced to shut down on Monday. In Philadelphia, a 5 p.m. Sunday deadline for Occupy protesters to move from a similar encampment came and went without incident, with dozens of people heeding the order but many tents and other structures staying put. The overall number of protesters had grown to at least 2,000 by late Sunday, with some wearing gas masks and a call for an 11th-hour show of support. In Los Angeles, police began to make their presence known after the mayor's eviction deadline passed, with some debris thrown by protesters and four people arrested. The LAPD declared an unlawful assembly on the streets surrounding City Hall and ordered protesters to disperse or face arrest. The mayor had complimented the protesters on Sunday for staying peaceful but said it was time for them to move from focusing on holding a particular patch of park land to spreading the message of economic justice and restoration of balance to American society.
  2%|▎         | 1/40 [00:21<14:01, 21.56s/it]2024-12-22 03:02:27,205 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:28,835 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:28,835 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1014])
2024-12-22 03:02:28,868 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:02:31,259 - [Process 1/5] - INFO - res.shape is :torch.Size([140])
results:Pete Davidson, a cast member of Saturday Night Live, has been open about his use of medical marijuana for Crohn's disease. He believes the New York State medical marijuana program is inadequate and difficult to navigate. He also revealed that he has been sober for the first time in eight years and credits Kid Cudi's music for saving his life. Davidson has been open about his struggles with drug addiction and depression, and has used marijuana to help him cope with his condition. He believes that cannabis should be legal beyond just medical purposes and is a proud and productive pot person.
  5%|▌         | 2/40 [00:25<07:31, 11.87s/it]2024-12-22 03:02:31,495 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:31,943 - [Process 4/5] - INFO - res.shape is :torch.Size([193])
results:

Ten volunteer firefighters from Fairmont, North Carolina have been arrested and charged with arson and conspiracy after an investigation revealed they intentionally set fires to abandoned homes and woods in Robeson County over a two-year period. The firefighters, aged between 17 and 42, were charged with various counts of felony arson, conspiracy, and setting fire to grass, brush, or woods. The investigation, which began a year ago, involved several law enforcement agencies, including the Robeson County Sheriff's Office, State Bureau of Investigation, Alcohol, Tobacco and Firearms, and the NC Forestry Service. The sheriff believes there may be more arrests forthcoming as the investigation continues. The fires set by the firefighters have cost taxpayers thousands of dollars in response and damage costs.
  5%|▌         | 2/40 [00:26<08:05, 12.77s/it]2024-12-22 03:02:32,088 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:34,493 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:34,493 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1422])
2024-12-22 03:02:34,540 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:02:35,041 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:35,042 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 03:02:35,113 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:02:35,383 - [Process 0/5] - INFO - res.shape is :torch.Size([188])
results:

The news passages highlight the life and legacy of Mikhail Kalashnikov, the inventor of the AK-47 assault rifle. Kalashnikov died on December 23, 2013, at the age of 94, and his letter to the Russian Orthodox Church Patriarch Kirill was published after his death. In the letter, Kalashnikov expressed his spiritual pain and guilt over the many deaths caused by his weapon, which he designed to defend his country but was later used by criminals and terrorists. Kalashnikov's daughter, Elena, said that her father was not religious and did not attend church services regularly, but he became interested in religion later in life and was baptized before his death. The letter has sparked a debate about the role of religion in the lives of public figures, particularly in Russia.
  5%|▌         | 2/40 [00:29<08:41, 13.72s/it]2024-12-22 03:02:35,441 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:35,758 - [Process 2/5] - INFO - res.shape is :torch.Size([276])
results:The news passages highlight the issue of data brokers collecting and selling highly personal and sensitive information about individuals without their consent. The World Privacy Forum uncovered several lists of sensitive information, including those of rape victims, seniors with dementia, and people suffering from HIV/AIDS, which were being sold by data brokers. These lists were obtained through investigations into the $156 billion data brokerage industry. The investigation found that some data brokers were forthcoming with their practices, while others refused to provide information or cited confidentiality clauses in their contracts. The report emphasizes the need for increased government regulations to protect individual privacy and prevent the misuse of sensitive information. The issue of data brokers collecting and selling personal information has been a concern for several years, with some data brokers offering lists of consumers based on their interests, such as sports enthusiasts or avid travelers. However, these lists can also include highly sensitive information that could put individuals at risk, such as those suffering from medical conditions or being a victim of a crime. The investigation highlights the need for greater transparency and accountability in the data brokerage industry and the importance of protecting individual privacy in the digital age.
  5%|▌         | 2/40 [00:30<09:23, 14.82s/it]2024-12-22 03:02:35,987 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:37,072 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:37,072 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1008])
2024-12-22 03:02:37,105 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:02:39,518 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:39,518 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1998])
2024-12-22 03:02:39,588 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:02:40,388 - [Process 3/5] - INFO - res.shape is :torch.Size([298])
results:
In the news, there are three passages that discuss recent discoveries in the field of archaeology. The first passage discusses how people from one group can disperse and take shelter with other groups during times of crisis. The second passage reveals that the beginning of the Middle Stone Age in Kenya was preceded by a long and tumultuous phase in the region, with repeated cycles of droughts and high rainfall. The third passage discusses how ancient toolmaking approaches varied greatly from one part of Africa to another, with hominids employing diverse mixes of old-school chopping tools and newer, sharp points. The discoveries in the Olorgesailie Basin in Kenya suggest that one or more hominin groups were doing seemingly "modern" things at a critical time period, including the use of obsidian tools and pigment. The researchers propose that these findings challenge the traditional view of a "human revolution" that occurred outside of Africa and suggest that components of this revolution occurred in the African Middle Stone Age. The team also found that the next technological revolution, marking the beginning of the Middle Stone Age, happened tens of thousands of years earlier than previously thought. Finally, the researchers suggest that long periods of stress may have pushed early humans to partner up with neighboring communities to cope with these challenges, leading to the development of more complex behaviors.
  5%|▌         | 2/40 [00:34<11:00, 17.38s/it]2024-12-22 03:02:40,564 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:42,857 - [Process 0/5] - INFO - res.shape is :torch.Size([166])
results:On November 1, 2013, a gunman opened fire at Los Angeles International Airport, killing one TSA officer and wounding three others. The gunman, Paul Ciancia, was shot by airport police and taken to the hospital, where he remains in fair condition. The incident has raised questions about the coordination between police and fire departments in responding to active shooter situations. According to an audit by the Los Angeles Police Commission Inspector General, the LAPD had a zero percent compliance rate in receiving first aid and CPR training. The TSA officers at the airport have also expressed concerns about the lack of coordination between entities at the airport. The incident has also highlighted the need for improved communication and training in active shooter situations.
  8%|▊         | 3/40 [00:37<06:42, 10.87s/it]2024-12-22 03:02:42,918 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:43,438 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:43,438 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1689])
2024-12-22 03:02:43,496 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:02:43,982 - [Process 1/5] - INFO - res.shape is :torch.Size([200])
results:
Several new allegations of sexual abuse have been made against Hollywood executives, including director Bryan Singer and producer Gary Goddard. Singer is accused of sexually assaulting a teenage boy at a party, while Goddard is accused of sexually abusing a teenager in the 1990s. The alleged victims are seeking financial damages and anonymity. Jeff Herman, a lawyer representing the victims, has filed lawsuits against the accused and plans to file more in the future. Herman has also revealed that he has uncovered another sex ring involving agents for child actors and directors in Hollywood. The allegations have sparked a wave of pushback from the Hollywood establishment, with some accusing Herman of being a publicity-seeker. However, Herman remains committed to giving voice to victims of sexual abuse and bringing perpetrators to justice.
  8%|▊         | 3/40 [00:38<07:33, 12.26s/it]2024-12-22 03:02:44,080 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:44,214 - [Process 4/5] - INFO - res.shape is :torch.Size([221])
results:
Nestle's attempt to trademark the shape of Kit Kat has been rejected by the European Court of Justice. The court ruled that the shape of the four-fingered chocolate bar is not distinctive enough to be trademarked across the entire European Union. This decision comes after a long legal battle between Nestle and Mondelez, the maker of Cadbury and other chocolate brands. The case has been ongoing since 2002 and has involved multiple appeals and court hearings. The ruling could have implications for other brands operating across the EU single market, where there are varying histories in national markets. The case has also been mirrored in the British courts, where Nestle objected to a trademark for the purple color used by Cadbury to wrap its Dairy Milk chocolate bars. The outcome of the case could bring an end to Kit Kat's protected European status and open the door to own-brand imitations at local supermarkets.
  8%|▊         | 3/40 [00:38<07:44, 12.54s/it]2024-12-22 03:02:44,432 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:44,888 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:44,889 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1100])
2024-12-22 03:02:44,928 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:02:45,735 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:45,735 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 923])
2024-12-22 03:02:45,769 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:02:48,027 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:48,027 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2127])
2024-12-22 03:02:48,092 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:02:49,712 - [Process 2/5] - INFO - res.shape is :torch.Size([223])
results:

The news articles discuss the discovery of a potential ninth planet in our solar system, which was discovered by two Caltech researchers. The planet, called Planet 9, is believed to be a gas giant and is thought to be orbiting billions of miles beyond Neptune's path. The discovery was made through mathematical and computer modeling, and the researchers anticipate its discovery via telescope within five years. The articles also mention that Mike Brown, one of the researchers, is known as the "Pluto killer" due to his role in the demotion of Pluto from a planet to a dwarf planet in 2006. Brown argues that Pluto is not a planet and that the term "planet" should be defined based on its size and orbit, rather than its location in the solar system. The articles also mention that Pluto fans continue to argue that Pluto should be reinstated as a planet, but Brown and his co-author believe that their findings provide solid evidence for a true ninth planet.
  8%|▊         | 3/40 [00:44<08:53, 14.42s/it]2024-12-22 03:02:49,806 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:51,400 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:51,400 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 875])
2024-12-22 03:02:51,432 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:02:53,351 - [Process 1/5] - INFO - res.shape is :torch.Size([175])
results:

Two teenage girls, Mary Kristene Chapa and Mollie Olgin, were shot in the head in a park in Portland, Texas on Saturday. Chapa is recovering from her injuries, but Olgin died from her injuries. The police are investigating the incident as a possible hate crime, but they have not found any evidence to confirm this. The families of the girls have set up a donation page to help pay for Chapa's medical bills. The community has come together to support the families and a candlelight vigil will be held in memory of both girls. The father of Olgin, Mario Olgin, said that his family is undergoing a difficult time but they are confident that justice will be served. He also expressed gratitude for the support they have received from the gay community.
 10%|█         | 4/40 [00:47<06:40, 11.12s/it]2024-12-22 03:02:53,619 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:02:57,161 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:02:57,162 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2093])
2024-12-22 03:02:57,230 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:02:57,934 - [Process 3/5] - INFO - res.shape is :torch.Size([327])
results:The Lowell Thomas Travel Journalism Competition awarded the best travel journalists in North America for their work in 2010. Afar, Washington Post, and San Francisco Chronicle were the top winners in their respective categories. The competition drew 1,161 entries from travel journalists across North America. The judges praised the quality of information, layout, design, and navigation of the entries, as well as the community aspect of the sites. The winners received a total of $20,600 in prize money, with the Grand Award winners receiving $500 each. The Travel Journalist of the Year award went to Andrew McCarthy, who won the gold for his freelance writing in some of North America’s best travel magazines. The judges praised McCarthy's excellent writing and poignant stories, which kept the readers engaged. The competition also recognized the best travel packages/projects, with the gold going to Jill Schensul for her in-depth coverage of Namibia. The best travel book award went to Rick Steves for his book "Travel as a Political Act," which shared his experiences of more than 30 years exploring the world. The best travel blog award went to Ben Mutzabaugh for his blog "Today in the Sky" on USAToday.com. The judges praised the entries for their focus on budget-conscious family ideas and their ability to shatter the traditional fare of glitz and faraway places.
  8%|▊         | 3/40 [00:52<10:45, 17.46s/it]2024-12-22 03:02:58,093 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:00,833 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:00,833 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1482])
2024-12-22 03:03:00,887 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:03:00,954 - [Process 4/5] - INFO - res.shape is :torch.Size([283])
results:
A new study published in the journal Evolution reveals that dinosaurs may have made sounds similar to those of birds, but not in the way we expect. The study found that closed-mouth vocalizations, which are low-pitched and throaty, were likely used by large-bodied dinosaurs, including those that are alive today. This challenges the popular depiction of dinosaurs as roaring ferociously, and suggests that they may have made more subtle sounds. The study also found that the ability to make closed-mouth vocalizations evolved at least 16 times in archosaurs, a group that includes birds and dinosaurs. The researchers used a statistical approach to analyze the distribution of this vocal ability among birds and other reptilian groups, and found that it is more common in larger animals. This suggests that closed-mouth vocalization may have emerged in dinosaurs as well, and could have been used in mating displays. The study also suggests that the ancestors of modern birds may have had a similar vocal organ to that of dinosaurs, and that closed-mouth vocalizations may have evolved in dinosaurs as well. The study's findings challenge our understanding of how dinosaurs vocalized, and suggest that they may have made more subtle sounds than previously thought.
 10%|█         | 4/40 [00:55<08:31, 14.20s/it]2024-12-22 03:03:01,023 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:02,051 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:02,051 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 624])
2024-12-22 03:03:02,065 - [Process 0/5] - INFO - res.shape is :torch.Size([483])
2024-12-22 03:03:02,071 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
results:

The European Union's foreign policy chief, Catherine Ashton, met with deposed Egyptian President Mohamed Morsi in an undisclosed location on Monday night, marking the first time he has had contact with an independent official since his ouster on July 3. Ashton declined to provide details about their conversation but said Morsi was in good condition and had access to newspapers and television. The meeting came after a weekend of violence left at least 80 dead and as Morsi's supporters rallied in Cairo, calling for his reinstatement. Ashton also met with Egypt's interim vice president, Mohamed ElBaradei, who said Morsi had "failed" during his year in power but that his Muslim Brotherhood allies should be part of the new political "road map" going forward. European Union foreign policy chief Catherine Ashton met with deposed Egyptian president Mohamed Morsi on Monday night, the first contact he has had with an independent official since he was taken into military custody almost a month ago. The meeting came after a weekend of violence left at least 80 dead and as Morsi's supporters rallied in Cairo, calling for his reinstatement. Ashton also met with Egypt's interim vice president, Mohamed ElBaradei, who said Morsi had "failed" during his year in power but that his Muslim Brotherhood allies should be part of the new political "road map" going forward. European Union foreign policy chief Catherine Ashton met with deposed Egyptian president Mohamed Morsi on Monday night, the first contact he has had with an independent official since he was taken into military custody almost a month ago. The meeting came after a weekend of violence left at least 80 dead and as Morsi's supporters rallied in Cairo, calling for his reinstatement. Ashton also met with Egypt's interim vice president, Mohamed ElBaradei, who said Morsi had "failed" during his year in power but that his Muslim Brotherhood allies should be part of the new political "road map" going forward.
 10%|█         | 4/40 [00:56<08:29, 14.16s/it]2024-12-22 03:03:02,096 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:02,727 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:02,727 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 355])
2024-12-22 03:03:02,739 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:03:03,095 - [Process 2/5] - INFO - res.shape is :torch.Size([272])
results:

The news passages describe the situation in Libya, where thousands of people have disappeared during the ongoing conflict between rebels and government forces loyal to longtime leader Moammar Gadhafi. According to a rebel fighter, Faraj, many of the missing people were taken by Gadhafi's forces and are believed to be held in underground prisons or mass graves. The rebels have opened Gadhafi's prisons and freed thousands of prisoners, but only about 10,000 have been accounted for, leaving families and friends worried about the fate of the missing. A family in Tripoli recently reunited with their son, Abdul Rauf, who was held for 48 days in a notorious prison facility and endured abuse and deprivation. The rebels have made progress in their fight against Gadhafi, but the situation remains dire, with many civilians caught in the crossfire. The news passages also highlight the human cost of the conflict, with dozens of people killed in a recent massacre at a prison and many more feared dead in mass graves. The international community is calling for an end to the violence and a peaceful resolution to the conflict.
 10%|█         | 4/40 [00:57<08:24, 14.01s/it]2024-12-22 03:03:03,196 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:04,790 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:04,791 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 938])
2024-12-22 03:03:04,822 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:03:07,574 - [Process 0/5] - INFO - res.shape is :torch.Size([143])
results:Russia has postponed a scheduled port call in Naples, Italy, for a destroyer to remain in striking distance of Syria during the crisis. Pentagon officials say this decision does not reflect any specific orders from Washington. Meanwhile, Russia has called on Syria to allow UN investigators into areas east of Damascus where an attack occurred, but American and foreign diplomats say Russia's move does not reflect any shift in its backing of President Assad or its resistance to punitive measures in the Security Council. Additionally, American intelligence agencies have detected activity at locations known to be chemical weapons sites before Wednesday's attack, which may have been preparations for the assault.
 12%|█▎        | 5/40 [01:01<06:26, 11.04s/it]2024-12-22 03:03:07,623 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:08,790 - [Process 4/5] - INFO - res.shape is :torch.Size([147])
results:
A teacher in Fort Worth, Texas, Cary Joseph Heath, 35, has been arrested on a capital murder charge after two of his neighbors were found fatally shot early Sunday morning. The victims have not been identified, and police believe Heath used an assault rifle to kill them. Heath is a teacher at Permenter Middle School and has been placed on administrative leave. The incident occurred in the 800 block of Buffalo Springs Drive, and police are still investigating. A neighbor, Jordan Milner, said he heard gunshots and believes Heath had an assault rifle. Parents are concerned about the safety of their children after the incident.
 12%|█▎        | 5/40 [01:03<06:56, 11.90s/it]2024-12-22 03:03:08,943 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:09,151 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:09,152 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 851])
2024-12-22 03:03:09,182 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:03:09,805 - [Process 1/5] - INFO - res.shape is :torch.Size([277])
results:
In a recent discovery, scientists have found a new species of dinosaur with iridescent feathers in northeastern China. The dinosaur, named Caihong juji, lived 161 million years ago during the Jurassic Period and had feathers that may have glittered with the colors of the rainbow. The discovery suggests that feathers originally evolved for display, and not just for flight, as previously thought. The team found that Caihong's feathers had pancake-shaped melanosomes, which are responsible for pigmentation, and its plumage covered its head, neck, and chest. The dinosaur also had a crested head and tail feathers, which may have been used for display or mating. The discovery of Caihong provides new insights into the evolution of feathers and their role in dinosaur behavior. Additionally, the team found that Caihong's skull and forelimbs were similar to those of modern birds, suggesting that it may have been able to fly. The discovery of Caihong juji is a significant find for paleontologists and bird enthusiasts alike, providing a glimpse into the lives of these fascinating creatures.
 12%|█▎        | 5/40 [01:04<07:36, 13.04s/it]2024-12-22 03:03:10,019 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:11,499 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:11,500 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1466])
2024-12-22 03:03:11,554 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:03:13,583 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:13,584 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 03:03:13,655 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:03:15,755 - [Process 3/5] - INFO - res.shape is :torch.Size([338])
results:

On October 1, a gunman killed nine people at an Oregon community college, and Donald Trump, the Republican presidential candidate, gave a speech in Franklin, Tennessee, where he discussed his views on gun control. Trump stated that he has a concealed carry permit in New York and that any attacker will be "shocked" if he tries to assault him. He also criticized "gun-free zones" and argued that it doesn't make sense to limit access to firearms. Trump also mentioned that he has evolved in his views on gun control over the years and now touts the National Rifle Association line. In an interview with NBC, Trump said that the gun laws have nothing to do with the mass shooting in Oregon and that it's mental illness. He also mentioned that if teachers were armed at Umpqua Community College, the shooting would not have been as tragic. Trump also defended his newly introduced tax plan and claimed that he would bring in better roads at a fraction of the cost. In response to the shooting, President Barack Obama delivered a statement, saying that the US has become numb to mass shootings. Jeb Bush also commented on the shooting, saying that "stuff happens." Trump also pushed back against those who have said he would save a significant amount of money under his tax plan, and he claimed that his major tax cut would hurt him financially. Trump also hit back when NBC's Todd pointed to polls that have him losing to both Vice President Joe Biden and Vermont Senator Bernie Sanders in a general election matchup.
 10%|█         | 4/40 [01:10<10:33, 17.60s/it]2024-12-22 03:03:15,968 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:17,947 - [Process 0/5] - INFO - res.shape is :torch.Size([257])
results:Kleiner Perkins Caufield & Byers, a venture capital firm, is seeking to recover $972,815 in legal costs from Ellen Pao, a former partner who lost a high-profile gender discrimination case against the firm. The firm offered to settle the case for $964,502 before it went to trial, but Pao's lawyers never responded. Now, Kleiner is asking Pao to pay the legal costs or drop the appeal. Meanwhile, a jury rejected all of Pao's claims in the case, and the judge made no obvious rulings in favor of the defense. The case has sparked a wider discussion about gender diversity in the tech industry. Kleiner has offered to waive its legal costs if Pao chooses not to appeal, but her legal team is considering the proposal. The firm's offer to settle the case before trial suggests that it may have been concerned about the potential costs of the case. The case has also highlighted the issue of gender discrimination in the tech industry, with many experts arguing that the case could have far-reaching implications for women in the industry.
 15%|█▌        | 6/40 [01:12<06:07, 10.81s/it]2024-12-22 03:03:18,041 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:18,768 - [Process 2/5] - INFO - res.shape is :torch.Size([324])
results:


A new study has found that people with coronary artery disease who experience large fluctuations in weight are at a higher risk of heart attack, stroke, and death. The study, conducted by Dr. Sripal Bangalore and colleagues, analyzed the data of 9,509 men and women with CHD aged between 35 and 75 years. The participants were monitored for changes in body weight and found that those with the greatest changes in body weight experienced a 124% higher risk of death, 117% higher risk of heart attack, 136% higher risk of stroke, and 124% higher risk of new onset diabetes compared to those with the smallest body weight changes. The study suggests that weight fluctuation may be a significant predictor of poor heart health outcomes in individuals with pre-existing heart disease. Additionally, the researchers found a link between changes in body weight and increased risk of new-onset diabetes. These findings raise concerns about the potential health consequences of yo-yo dieting in people with coronary heart disease. Previous studies have also associated yo-yo dieting with increased risk of poor heart health, and the current study suggests that this association may be even more severe in individuals with pre-existing heart disease. The study's lead author, Dr. Bangalore, notes that their findings warrant further investigation into how to help Americans keep weight off and avoid weight fluctuation.
 12%|█▎        | 5/40 [01:13<08:31, 14.61s/it]2024-12-22 03:03:18,849 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:19,012 - [Process 4/5] - INFO - res.shape is :torch.Size([173])
results:

Vanderbilt University has decided to rename Confederate Memorial Hall to Memorial Hall after a 14-year effort. The university will pay $1.2 million to the United Daughters of the Confederacy to remove the name of the building. The name was originally given in 1933 and was intended to honor Confederate soldiers. However, the university has decided to change the name due to its offensive nature to many people. This decision comes after a series of changes made to address diversity on campus, including the addition of a chief diversity officer. Other universities in Tennessee, such as Middle Tennessee State University, are also considering changes to their buildings' names. The Tennessee Historical Commission has made it more difficult to remove Confederate symbols and statues from public places with a new law passed earlier this year.
 15%|█▌        | 6/40 [01:13<06:25, 11.33s/it]2024-12-22 03:03:19,097 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:19,550 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:19,550 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2035])
2024-12-22 03:03:19,622 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:03:20,145 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:20,146 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 724])
2024-12-22 03:03:20,171 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:03:20,697 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:20,697 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 919])
2024-12-22 03:03:20,728 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:03:20,855 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:20,855 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1696])
2024-12-22 03:03:20,913 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:03:23,539 - [Process 1/5] - INFO - res.shape is :torch.Size([218])
results:

Protests and violence continue to spread across the Middle East and North Africa as governments crack down on dissent. In Libya, at least 15 people were killed when security forces fired on mourners in Benghazi, while in Yemen, police opened fire on anti-government protesters in the capital, Sanaa, killing one person and injuring five. In Bahrain, security forces withdrew from the streets after two days of violence, but protesters remain defiant and are planning to hold a funeral for the dead. In Algeria, police broke up a pro-democracy rally, and in Egypt, the military government is taking steps towards a handover of power. In Tunisia, a moderate Islamist group has been recognized as a political party, and President Barack Obama has called for reform in Bahrain. The wave of protests has been fueled by economic grievances, political repression, and calls for greater democracy and accountability in the region.
 15%|█▌        | 6/40 [01:17<07:31, 13.28s/it]2024-12-22 03:03:23,596 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:24,531 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:24,532 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 521])
2024-12-22 03:03:24,548 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:03:27,660 - [Process 3/5] - INFO - res.shape is :torch.Size([177])
results:Kanye West, a famous rapper and fashion icon, has been struggling with debt and has started a GoFundMe campaign to raise $53 million to pay off his debts. Fans have been generous and have raised over $1,0000 so far. A fan in Minnesota has also created a GoFundMe page for West, which has raised $50 so far. Meanwhile, West has been reaching out to tech giants like Mark Zuckerberg and Larry Page for help. Another fan, Jeremy Piatt, created a GoFundMe page to help West and raise awareness for supporting the arts. However, West's financial woes may be a publicity stunt as he has been known to make bizarre tweets and has a history of exaggerating his financial situation.
 12%|█▎        | 5/40 [01:22<09:04, 15.54s/it]2024-12-22 03:03:27,741 - [Process 0/5] - INFO - res.shape is :torch.Size([178])
results:

A man named Jack McCullough was convicted of the 1957 murder of a 7-year-old girl named Maria Ridulph in Sycamore, Illinois. He was released from prison after a judge found that the evidence used to convict him was unreliable. The case was reopened in 2008 after McCullough's half-sister came forward with information that her dying mother had whispered to her that McCullough was the killer. A prosecutor reviewed the case and found that there was not enough evidence to support the conviction. The judge who ordered McCullough's release said that the evidence presented at trial was "clearly and convincingly" unreliable. The case has been reopened and a new investigation is underway.
 18%|█▊        | 7/40 [01:22<05:45, 10.48s/it]2024-12-22 03:03:27,765 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:27,877 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:28,457 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:28,457 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 409])
2024-12-22 03:03:28,469 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:03:29,790 - [Process 4/5] - INFO - res.shape is :torch.Size([206])
results:

A Florida police officer, Laurie Graber, went above and beyond the call of duty to help an elderly woman, Betty Wagner, whose engagement ring was stolen from her finger at a hospital. The ring, which was purchased in 1946 for $400, had been on Betty's finger for 67 years until it was taken last Saturday. Officer Graber purchased a new heart-shaped diamond ring and left it with the nursing staff at the hospital, along with a signed note. The theft of the ring has left Betty's husband, Arthur Wagner, feeling helpless, but Officer Graber's act of kindness has brought some comfort to the couple. The police are still investigating the theft and are asking for any information that may lead to the recovery of the stolen ring. In unrelated news, an explorer believes he may have found the Griffin shipwreck off the coast of Florida.
 18%|█▊        | 7/40 [01:24<06:07, 11.15s/it]2024-12-22 03:03:29,867 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:30,560 - [Process 2/5] - INFO - res.shape is :torch.Size([234])
results:
A woman in Odessa, Texas stole a wedding ring from a dead woman's finger at a funeral home. The theft occurred on Friday, April 8th, at Sunset Memorial Gardens & Funeral Home, where the family of the deceased had gathered for a visitation service. The thief, who is described as a heavy-set white female, entered the funeral home asking where the restroom was but went into the room where the casket was instead. The ring, which was placed on the woman's finger to symbolize her love, was taken off and the thief fled the scene in a red or maroon 4-door Saturn. The family of the deceased is devastated by the theft and has reported it to the Odessa Police Department. The police have released a video surveillance of the thief and are asking for anyone with information to come forward. This incident is particularly disturbing as it happened during a time of grief and mourning for the family, and it highlights the lack of respect for the deceased and their loved ones.
 15%|█▌        | 6/40 [01:24<07:44, 13.65s/it]2024-12-22 03:03:30,624 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:31,206 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:31,206 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 815])
2024-12-22 03:03:31,231 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:03:31,463 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:31,464 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2029])
2024-12-22 03:03:31,535 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:03:31,614 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:31,614 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 569])
2024-12-22 03:03:31,632 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:03:34,590 - [Process 0/5] - INFO - res.shape is :torch.Size([181])
results:Saturday Night Live's opening sketch on March 3rd featured a comment from Rush Limbaugh's radio show, which led to several advertisers pulling their ads from his show. The advertisers included carmakers, insurance companies, and restaurants. According to a memo circulated to radio station traffic managers, the advertisers asked to avoid sponsoring Limbaugh's show due to his controversial comments, including calling Georgetown student Sandra Fluke a "slut." The advertisers have also asked to schedule their commercials in dayparts or programs free of content that they deem offensive or controversial. This comes after several advertisers have dropped their ads from Limbaugh's show in the wake of his misogynistic attacks on Sandra Fluke.
 20%|██        | 8/40 [01:29<04:58,  9.32s/it]2024-12-22 03:03:34,652 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:36,630 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:36,630 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1089])
2024-12-22 03:03:36,637 - [Process 1/5] - INFO - res.shape is :torch.Size([272])
results:

In a recent discovery, rare photographs from Nirvana's first concert in March 1987 have been unearthed and shared by Maggie Poukkula, the daughter of a member of Seattle band Laytem. The photos feature the band performing covers of Led Zeppelin's "Heartbreaker" and "How Many More Times," as well as nascent versions of other songs. A recording of the band's "Heartbreaker" jam appeared on the 2004 box set, With the Lights Out, while a bootleg version of "If You Must" is available on YouTube. Additionally, a documentary about Kurt Cobain and Nirvana, Montage of Heck, has been making waves after a successful festival circuit run and limited theatrical release. The documentary features unprecedented access to Cobain's archives, including notebooks and audio cassette tapes featuring eerie Beatles covers and a noise collage that gave the film its title. To further preserve the web's history, the Internet Archive is working to archive pages as they are created and link to archived versions of pages that have been changed or removed. The project aims to fix all broken links on the web.
 18%|█▊        | 7/40 [01:31<07:16, 13.22s/it]2024-12-22 03:03:36,670 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:03:36,845 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:39,269 - [Process 2/5] - INFO - res.shape is :torch.Size([179])
results:A 3-year-old Hawaii girl, Finley Boyle, died after suffering a heart attack during a dental procedure at Dr. Lilly Geyer's office in Kailua. An autopsy report revealed that the drugs given to sedate her were likely the cause of death, and new state rules have been implemented to tighten oversight of dental sedation in Hawaii. The girl's parents have filed a negligence lawsuit against Geyer, and an investigation is ongoing. In a separate incident, a 3-year-old Kailua girl died after a dental procedure, and an autopsy report found that the combination of sedatives and anesthetic administered during the procedure played a role in her death. No criminal charges have been brought against the dentist.
 18%|█▊        | 7/40 [01:33<06:37, 12.03s/it]2024-12-22 03:03:39,288 - [Process 3/5] - INFO - res.shape is :torch.Size([169])
results:

The search for MH370 continues with new leads emerging. A Chinese ship detected a signal in the southern Indian Ocean, which could be related to the missing plane's black box. The signal was detected twice, once on Friday and again on Saturday, and is consistent with the frequency of the flight recorders' pingers. However, it's not yet confirmed if the signal is linked to the missing plane. Other ships and planes are joining the search, and the area where the signal was detected is being searched. White objects were also spotted in the area, but it's unclear if they are related to the plane. The batteries powering the black boxes' locator pingers are expected to die soon, and the search is racing against time to find any evidence before they do.
 15%|█▌        | 6/40 [01:33<08:03, 14.21s/it]2024-12-22 03:03:39,362 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:39,449 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:40,269 - [Process 4/5] - INFO - res.shape is :torch.Size([202])
results:
The Boston Marathon bomber, Dzhokhar Tsarnaev, has been sentenced to death and will join the federal death row in Terre Haute, Indiana. His lawyers have appealed the conviction and death sentence, citing the publicity surrounding the case and the unconstitutionality of capital punishment. The appeal claims that the continuous and unrelenting publicity about the bombings and Tsarnaev made it impossible for him to get a fair trial in Boston. The defense also argues that the recent Supreme Court ruling on the vagueness of the definition of a "crime of violence" throws many convictions into question. Despite the appeal, it may take years for Tsarnaev's fate to be decided as the appeals process can take a long time. Only three prisoners have been executed on federal death row in the past 50 years, highlighting the slow pace of capital punishment in the US.
 20%|██        | 8/40 [01:34<05:49, 10.94s/it]2024-12-22 03:03:40,346 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:40,414 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:40,415 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2025])
2024-12-22 03:03:40,486 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:03:40,591 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:40,591 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 702])
2024-12-22 03:03:40,615 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:03:41,676 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:41,676 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 768])
2024-12-22 03:03:41,703 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:03:42,846 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:42,846 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1919])
2024-12-22 03:03:42,919 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:03:43,695 - [Process 0/5] - INFO - res.shape is :torch.Size([200])
results:

The news passages highlight several recent events and issues in the political sphere. In Montana, Greg Gianforte, the congressman-elect, pleaded guilty to misdemeanor assault after body-slamming a reporter during an election campaign. President Trump has been vocal about the incident, calling it "personal" and praising Gianforte for his actions. The president has also made controversial comments about immigration, falsely claiming that Democrats are supporting a caravan of immigrants traveling up from Central America. Trump has also been criticized for making light of Gianforte's assault on a reporter, with some calling it an attack on the First Amendment. In other news, Twitter has announced that it will allow users to add location information to their tweets, and the White House Correspondents' Association has condemned President Trump's praise for a violent assault on a reporter.
 22%|██▎       | 9/40 [01:38<04:46,  9.26s/it]2024-12-22 03:03:43,759 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:45,784 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:45,785 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1133])
2024-12-22 03:03:45,827 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:03:50,949 - [Process 2/5] - INFO - res.shape is :torch.Size([175])
results:O.J. Simpson was granted parole after serving nine years in prison for armed robbery and kidnapping. He was acquitted of murder charges in 1995 but was found liable for the deaths of his ex-wife Nicole Brown Simpson and her friend Ron Goldman in a civil trial. Simpson's parole hearing was held in July 2017, and he was granted parole unanimously by the parole board. Simpson expressed remorse for his actions and stated that he had lived a conflict-free life since his acquittal. The case has been widely covered in the media, with many outlets exploring the themes of criminal justice and race that remain relevant today. Simpson's release from prison is expected in October 2017.
 20%|██        | 8/40 [01:45<06:21, 11.92s/it]2024-12-22 03:03:50,973 - [Process 1/5] - INFO - res.shape is :torch.Size([233])
results:Clarissa Dickson Wright, a renowned British chef and television personality, passed away on Saturday at the age of 666. She was best known for her role as the co-star and co-chef of the popular BBC cooking show "Two Fat Ladies" alongside Jennifer Paterson. Dickson Wright was a brilliant cook, erudite food historian, and champion of hunting and shooting. She was also a vocal advocate for rural life and a keen supporter of the Countryside Alliance. Throughout her life, she fought for what she believed in, even if it meant going against the mainstream. Dickson Wright was born into an affluent family and became a lawyer at 21, but later turned to cooking after discovering her passion for it in her 40s. She was also a successful author and had a forthcoming birthday on June 24, which would have marked her 27th year of abstinence. Her agents said that she will be greatly missed, and her fun, laughter, and intelligence will be remembered.
 20%|██        | 8/40 [01:45<07:14, 13.57s/it]2024-12-22 03:03:51,007 - [Process 4/5] - INFO - res.shape is :torch.Size([204])
results:The global market for derivatives has eclipsed that for stocks, with the New York Stock Exchange (NYSE) being left to conduct soul-searching after its deal with Deutsche Börse collapsed. NYSE Euronext is in talks to be acquired by IntercontinentalExchange (ICE), which would link it to one of the industry's fastest-growing exchanges and reap benefits from consolidation. The potential merger would expand ICE, which has some of the highest profit margins in the business, and could pose fewer problems as it focuses on commodities like oil, natural gas, and cotton, while NYSE Euronext plies mainly in stock and stock options and derivatives. The planned merger of NYSE Euronext and Deutsche Börse fell apart due to European antitrust regulators' opposition, but the newest merger might be approved if regulators and shareholders approve.
 22%|██▎       | 9/40 [01:45<05:37, 10.88s/it]2024-12-22 03:03:51,054 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:51,079 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:51,228 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:51,829 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:51,829 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 421])
2024-12-22 03:03:51,843 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:03:52,349 - [Process 3/5] - INFO - res.shape is :torch.Size([265])
results:
President Trump has been under fire for his controversial tweets, with some Republicans expressing concerns about his habit of using social media to attack his critics. Trump defended his use of social media, calling it "modern day presidential" and saying that he won the 2016 election through interviews, speeches, and social media. However, his recent tweets have sparked criticism, including one where he body-slammed and punched a person signifying CNN, which some have interpreted as a threat. Homeland Security Adviser Tom Bossert defended the tweet, saying it was not a threat, but rather a response to unfair coverage by cable news shows. However, some critics, including Nebraska Sen. Ben Sasse and CNN contributor Ana Navarro, have expressed disappointment and concern about the tweet, with Navarro calling it an incitement to violence. Meanwhile, Trump has also been using his Twitter account to attack "Morning Joe" co-hosts Joe Scarborough and Mika Brzezinski, leading some to question whether his tweets are beneath the dignity of the presidency. Overall, Trump's use of social media continues to be a source of controversy and concern for many.
 18%|█▊        | 7/40 [01:46<07:36, 13.84s/it]2024-12-22 03:03:52,513 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:53,000 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:53,000 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1029])
2024-12-22 03:03:53,038 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:03:54,828 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:54,828 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2130])
2024-12-22 03:03:54,894 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:03:55,301 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:55,301 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1627])
2024-12-22 03:03:55,358 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:03:56,352 - [Process 0/5] - INFO - res.shape is :torch.Size([296])
results:

A recent study published in the journal Infection and Immunity has found that cigarette smoke exposure makes the superbug MRSA (Methicillin-resistant Staphylococcus aureus) more resistant to antibiotic treatment. The study, led by Dr. Laura Crotty Alexander of UC San Diego and the Veterans Affairs San Diego Healthcare System, found that MRSA exposed to cigarette smoke extract were four times more resistant to macrophage killing than non-exposed MRSA. The study also found that the increased resistance continued for 24 hours after exposure, suggesting it could be inherited. The researchers also discovered that cigarette smoke altered the cell walls of MRSA, making it more resistant to reactive oxygen species and antimicrobial peptides. Another study conducted by researchers at University of California, San Diego School of Medicine a year earlier suggested that e-cigarette smoke can also alter the structure of MRSA’s cell wall to make it more resistant to bacteria. The study suggests that cigarette smoke functions much like an alarm to MRSA, warning it to activate its defenses. The findings of these studies suggest that cigarette smoke exposure may be an additional contributing factor to the growing problem of antibiotic-resistant bacteria.
 25%|██▌       | 10/40 [01:50<05:09, 10.31s/it]2024-12-22 03:03:56,451 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:03:59,562 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:03:59,562 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1746])
2024-12-22 03:03:59,625 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:04:00,280 - [Process 4/5] - INFO - res.shape is :torch.Size([196])
results:The Academy of Motion Pictures Arts and Science has invited 683 new members to join their organization, including actors, directors, and other industry professionals. This is part of an effort to increase diversity in the organization, following criticism for lacking it in the past. The list of invitees includes both established and up-and-coming talent, with a focus on women and people of color. This move comes after the #OscarsSoWhite hashtag was used to highlight the lack of diversity in the Academy's nominees. The organization has committed to doubling the number of women and people of color in its membership by 2020. The list of invitees includes many well-known directors such as Spike Lee, Park Chan-wook, and Lynne Ramsay, as well as actors such as Idris Elba, Tessa Thompson, and Eva Mendes.
 25%|██▌       | 10/40 [01:54<05:11, 10.38s/it]2024-12-22 03:04:00,379 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:01,619 - [Process 1/5] - INFO - res.shape is :torch.Size([197])
results:
A woman fell to her death while climbing Tryfan mountain in Snowdonia, Wales, after using a smartphone app to find a safe route down from the peak. The woman, Jane Wilson, 53, fell 30ft down a vertical cliff and tumbled 150m down into an area known as Notch arrete. Her husband Gary, who was with her, raised the alarm and was led to safety by members of the Ogwen Valley Mountain Rescue Team. The team's leader, Detective Constable Tim Bird, said that the route the couple took was not straightforward and that Tryfan is responsible for about 30% of the incidents the team has to attend. The coroner recorded a conclusion of accidental death. The incident highlights the risks of relying solely on electronic navigation aids when climbing, and the importance of using a map and compass as well.
 22%|██▎       | 9/40 [01:56<06:32, 12.66s/it]2024-12-22 03:04:01,721 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:02,023 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:02,023 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 962])
2024-12-22 03:04:02,054 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:04:03,406 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:03,406 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 998])
2024-12-22 03:04:03,441 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:04:04,452 - [Process 3/5] - INFO - res.shape is :torch.Size([204])
results:Apple's latest iPhone 7 event was met with criticism and ridicule after the company announced that it would no longer include a headphone jack in the device. The decision was met with outrage from consumers and tech critics, who argue that the move is unnecessary and will force users to rely on proprietary Lightning cables or expensive dongles. The lack of a headphone jack was seen as a bold move by Apple, but many see it as a mistake that will alienate customers. The company's marketing chief, Phil Schiller, was mocked on Twitter for his explanation of the decision, which was seen as arrogant and lacking in courage. The event also saw the introduction of AirPods, a new wireless earbud product that has been met with skepticism and criticism. Overall, the event was seen as a step backward for Apple, which has traditionally been known for its innovation and user-friendly products.
 20%|██        | 8/40 [01:58<07:05, 13.28s/it]2024-12-22 03:04:04,667 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:08,300 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:08,301 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1886])
2024-12-22 03:04:08,380 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:04:09,610 - [Process 2/5] - INFO - res.shape is :torch.Size([323])
results:
During his first solo news conference as President, Donald Trump was unhinged and wild, armed with grievances and primed to unload on his favorite targets. He bullied reporters, dismissed facts, and cracked a few caustic jokes. Trump used the event to try to claw his young administration back from the brink after a defeat in court and the forced resignation of his top national security adviser. He taunted reporters, waved away their attempts to fact-check him, and bragged that his White House is a "fine-tuned machine." Trump also blamed the media for his woes, rebuffing suggestions that he was undermining confidence in the press or threatening the First Amendment. He claimed that the press has become so dishonest that if he doesn't talk about it, he's doing a tremendous disservice to the American people. Trump also talked about how the press would misrepresent the press conference he was giving, and he made a point of complimenting a softball inquiry about the first lady. He also displayed a rare moment of introspection when he discussed his love for kids amid his "very, very hard" decision whether to potentially deport young immigrants who came to the United States illegally as children. Overall, Trump's performance was vintage Trump, a throwback to the messy, zinger-filled news conferences he held during the early stages of his campaign.
 22%|██▎       | 9/40 [02:04<07:14, 14.03s/it]2024-12-22 03:04:09,744 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:10,182 - [Process 0/5] - INFO - res.shape is :torch.Size([271])
results:Fabio Sementilli, a renowned hairstylist and beauty company executive, was found dead in his Woodland Hills home on Monday, Jan. 23, 2017. He was 49 years old. The cause of death was determined to be multiple stab wounds. Police are investigating the murder and have arrested two suspects. Sementilli was a mentor to many in the hairstyling industry and was Vice President of Education for Wella, a major German hair-care company. He was also a decorated competition hair stylist and had established himself as a "mentor and positive role model to so many." The industry is in shock and mourning his death. A local news crew reported from the scene, and friends and family members were seen leaving the home, too shocked to speak. Eden Sassoon, the daughter of another industry icon Vidal Sassoon, called Sementilli a mentor and friend. The company Wella released a statement expressing their sadness and offering their condolences to the family. The death of Sementilli is unnerving, and it is not the first time there has been an increase in crime in the neighborhood. A local Facebook group has been created to discuss crime in the area.
 28%|██▊       | 11/40 [02:04<05:30, 11.38s/it]2024-12-22 03:04:10,234 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:11,762 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:11,762 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 850])
2024-12-22 03:04:11,774 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:11,774 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1237])
2024-12-22 03:04:11,794 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:04:11,815 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:04:15,517 - [Process 1/5] - INFO - res.shape is :torch.Size([277])
results:

The latest news passages highlight several key political developments in the US. In the first passage, New York Mayor Michael Bloomberg praises President Obama's candidacy, despite earlier criticism, and criticizes some Republican candidates for their anti-science positions. He also expresses concern over the government shutdown possibility due to Congress' inability to agree on a short-term spending bill. In the second passage, Senator Mitch Daniels defends Governor Rick Perry's debate performances, urging fellow Republicans to give him time to improve. Meanwhile, David Plouffe, Obama's senior adviser, dismisses the possibility of significant staff changes in the White House ahead of the 2012 election. In the third passage, Republican National Committee Chairman Reince Priebus mocks President Obama's 2008 election slogan, suggesting it won't resonate with voters in 2012. Priebus also expresses concern over the US' continued aid to Pakistan if it can't do more to impede anti-American terrorism. Finally, in the last passage, Senator Lindsey Graham calls for leaving "all options on the table" in dealing with Pakistan after US military officials accused the country's intelligence officials of assisting terrorist operations.
 25%|██▌       | 10/40 [02:09<06:31, 13.04s/it]2024-12-22 03:04:15,729 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:16,694 - [Process 3/5] - INFO - res.shape is :torch.Size([181])
results:United Airlines has updated its boarding process to reduce congestion at gates and improve the overall passenger experience. The new process involves using two lanes for boarding, with elite passengers and other priority groups loading through lane 1, and other passengers loading through lane 2. The airline has also launched push notifications for passengers to inform them when boarding has begun. Additionally, United has reached an agreement with Copa and Avianca to form a joint business agreement, which will provide customers with more choices and benefits when traveling between the US and Latin America. The agreement will allow the three carriers to serve customers as if they were a single airline, providing a more streamlined travel experience and better customer service. The new boarding process and joint business agreement are expected to drive competition and growth in the airline industry, providing customers with more choices and benefits.
 22%|██▎       | 9/40 [02:11<06:41, 12.96s/it]2024-12-22 03:04:16,915 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:19,300 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:19,301 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2031])
2024-12-22 03:04:19,372 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:04:19,943 - [Process 0/5] - INFO - res.shape is :torch.Size([239])
results:
The latest study by University of Toulouse researcher Young-Ho Eom and his team analyzed 24 language editions of Wikipedia to determine the most influential historical figures in the world. The study used two methods to measure importance: PageRank and CheiRank. The results showed that Carl Linnaeus, a Swedish naturalist, is the most influential person in the world, beating out other notable figures such as Michael Jackson, Pope Pius XII, and Frank Sinatra. The study also found that the method used, PageRank, was thrown off by a quirk of Wikipedia, as many pages on the site contain scientific names that led to Linnaeus. To get around this issue, the researchers used CheiRank, which measures the number of outgoing links instead of incoming links. The study's findings suggest that Linnaeus' influence is not just due to his work in taxonomy, but also due to the sheer number of pages on Wikipedia that link to him. This study highlights the importance of understanding the connections between historical figures and how they are linked to each other through various sources.
 30%|███       | 12/40 [02:14<05:04, 10.89s/it]2024-12-22 03:04:19,961 - [Process 2/5] - INFO - res.shape is :torch.Size([194])
results:

NASA has announced that an emergency spacewalk may be necessary to fix a malfunctioning cooling system on the International Space Station. The problem was discovered on Wednesday when a pump on one of the station's two external cooling loops shut down after hitting a temperature limit. Teams are working to get the cooling loop back up and running, but experts suspect a malfunctioning valve may be the cause. The space station's life support system is still up and running, but the problem may require a spacewalk to fix. The current mission of the space station is scheduled to go until March 2014 and the six-person crew consists of NASA astronauts Mike Hopkins and Rick Mastracchio of the United States; Russians Mikhail Tyurin, Sergey Ryazanskiy and Oleg Kotov; and Koichi Wakata of Japan.
 25%|██▌       | 10/40 [02:14<06:26, 12.89s/it]2024-12-22 03:04:19,975 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:20,036 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:20,376 - [Process 4/5] - INFO - res.shape is :torch.Size([430])
results:

The three American hikers, Shane Bauer, Josh Fattal, and Sarah Shourd, who were arrested in Iraq and detained in Iran for over two years, had their trial delayed due to the absence of the third suspect, Sarah Shourd. The Iranian authorities claimed that the hikers were agitating along the border with Iran, but Ms. Shourd denied this, stating that they had no idea they were near the border and had not been warned about anything. The State Department spokesman, Philip J. Crowley, confirmed that the United States does not know how the hikers were arrested. The hikers were on a relaxed overnight camping trip in the Kurdish mountains when they were arrested. Ms. Shourd had been teaching English in Damascus and Mr. Bauer was working as a freelance journalist while both studied Arabic. The hikers had no idea they were near the border and encountered Kurdish pesh merga soldiers who greeted them warmly. The next day, they were approached by a guard standing on a ridge above them and were taken to a second, larger structure where they were ignored their pleas to return to Iraq. The hikers have been in Evin prison for over two years, and their lawyer said he was not aware the trial had been postponed. The Iranian authorities have been trying to prod the United States to return the hikers, and Ahmadinejad suggested in February that they might be released as part of a prisoner swap for Iranians jailed in the United States. One of the Iranians, a woman who was jailed for trafficking defense hardware to Iran, gave a telephone interview to Iran's state-run English language television channel, in which she said she had been tortured. The Foreign Ministry spokesman, Ramin Mehmanparast, hoped publicity about the case would help secure the release of Iranian prisoners in U.S. jails.
 28%|██▊       | 11/40 [02:14<06:27, 13.36s/it]2024-12-22 03:04:20,484 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:20,484 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1947])
2024-12-22 03:04:20,547 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:20,559 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:04:20,905 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:20,905 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 519])
2024-12-22 03:04:20,922 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:04:21,278 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:21,278 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 672])
2024-12-22 03:04:21,303 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:04:23,547 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:23,548 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1737])
2024-12-22 03:04:23,606 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:04:26,644 - [Process 0/5] - INFO - res.shape is :torch.Size([170])
results:The ratings for the new season of Celebrity Apprentice, hosted by Arnold Schwarzenegger, were lower than expected, down 22% from the debut episode of Trump's most recent season. Despite the network's efforts to promote the show, it faced stiff competition from other TV shows, including The Bachelor and college football. The show averaged 4.9 million viewers in its premiere, which is lower than the ratings of the previous season. The ratings for the show were also lower than expected, with some early reviews not being particularly kind. However, the show still managed to provide a strong lead-in for new NBC game show The Wall, which retained most of the Apprentice's viewers in the 10 p.m. time-slot.
 32%|███▎      | 13/40 [02:21<04:19,  9.62s/it]2024-12-22 03:04:26,740 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:29,491 - [Process 3/5] - INFO - res.shape is :torch.Size([200])
results:Hurricane Irene is approaching the East Coast of the United States and is expected to make landfall this weekend. The storm is forecasted to be a Category 3 hurricane and will bring heavy rain, strong winds, and storm surges to the coastal areas. Several states have declared states of emergency and have issued mandatory evacuations for coastal areas. The National Weather Service has upgraded the Tropical Storm Watch to a Hurricane Warning for parts of North Carolina, Virginia, and Maryland. The storm is expected to cause widespread damage and power outages, and residents are advised to prepare for the worst. In New York City, officials have ordered low-lying hospitals and nursing homes to evacuate, and mass transit systems may be shut down on Saturday. The storm is also expected to affect the Mid-Atlantic region, including Washington D.C., and the Northeast.
 25%|██▌       | 10/40 [02:23<06:27, 12.91s/it]2024-12-22 03:04:29,671 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:29,712 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:29,712 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1704])
2024-12-22 03:04:29,776 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:04:32,522 - [Process 1/5] - INFO - res.shape is :torch.Size([289])
results:Seattle has implemented a new tax on sugary drinks, which has led to a 1.75 cent per ounce tax on sweetened beverages. This tax aims to reduce sugar consumption, raise revenue for important projects, and subsidize purchases of healthy foods by low-income families. However, the tax has sparked controversy and criticism, with some arguing that it will drive businesses out of the city and hurt low-income families. The tax has also led to a 64% increase in the cost of a case of Gatorade in Seattle, and customers are now shopping at suburban stores to avoid the tax. In response, some businesses are changing their pricing strategies to reflect the tax, while others are considering legal action. In related news, a similar tax in Cook County, Illinois was repealed after a tremendous public outcry, with many arguing that the tax was too burdensome and did not lead to desired health improvements. The repeal of the tax is seen as a victory for the beverage industry, but retailers and restaurants in Cook County are still feeling the impact of the tax. Overall, the implementation of sugary drink taxes has proven to be a complex and controversial issue, with no clear consensus on their effectiveness in improving public health.
 28%|██▊       | 11/40 [02:26<06:53, 14.26s/it]2024-12-22 03:04:32,594 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:32,871 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:32,871 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1846])
2024-12-22 03:04:32,934 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:04:33,639 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:33,640 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 624])
2024-12-22 03:04:33,660 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:04:34,544 - [Process 2/5] - INFO - res.shape is :torch.Size([297])
results:A powerful hurricane with unprecedented strength is predicted to hit the area, causing widespread destruction and human suffering. The National Weather Service issued an extraordinary bulletin with bold words, warning of the severity of the situation. The forecast coordinator, Robert Ricks, came up with the message after finding a way to convey the urgency and seriousness of the situation. The document predicted that most of the area would be uninhabitable for weeks, with at least half of well-constructed homes having roof and wall failure, and all gabled roofs failing. The forecast also predicted water shortages, which would cause incredible human suffering. Ricks' document was later put in the Smithsonian Museum of American History, along with a rosary from his grandmother. The local expertise confirmed the severity of the storm, and Ricks' statement was officially encouraged by the National Weather Service. During Hurricane Sandy, another meteorologist, Gary Szatkowski, issued a personal plea that earned him hero status. Ricks, who grew up in the hard-hit Lower Ninth Ward, is now back at work alongside co-workers who have no homes and are wearing the clothes they wore that day. The response to the storm did break Ricks' heart, as they always prepare for the big one but didn't think it would come so soon.
 28%|██▊       | 11/40 [02:28<06:28, 13.41s/it]2024-12-22 03:04:34,627 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:35,197 - [Process 4/5] - INFO - res.shape is :torch.Size([261])
results:


Mutinous soldiers in Mali have taken control of the country after ousting President Amadou Toumani Toure in a coup. The soldiers are angry with the government's handling of an ethnic Tuareg rebellion in the north of the country. The mutiny began on Wednesday when soldiers stormed the presidential palace and announced that they had suspended the constitution and dissolved the government. The soldiers have imposed a nationwide curfew and have taken control of the state television and radio stations. The West African regional bloc ECOWAS has condemned the coup, calling it "reprehensible" and "misguided". The whereabouts of the President Toure are unknown, and several ministers have been arrested. The coup has caused chaos in the country, with gunfire ringing out in the capital city of Bamako and troops searching for the President. The situation is still unfolding, with the US and France urging the soldiers and government to resolve their dispute peacefully. The Tuareg rebels have forced the army out of several northern towns in recent months, and a presidential election was due to take place in the country in just under a month.
 30%|███       | 12/40 [02:29<06:26, 13.80s/it]2024-12-22 03:04:35,269 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:35,915 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:35,915 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 744])
2024-12-22 03:04:35,940 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:04:36,389 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:36,390 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 634])
2024-12-22 03:04:36,412 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:04:39,196 - [Process 0/5] - INFO - res.shape is :torch.Size([243])
results:
Samsung's new Smart TVs have a feature that allows the company and its partners to listen to users' conversations through their TV's camera. This feature is enabled by default, but users can disable it through the settings menu. The feature uses facial recognition technology to authenticate users' Samsung accounts and provide personalized recommendations. Microsoft's new Xbox One console is always listening to users, even when it's turned off, to provide quick and natural responses to their commands. The new Kinect peripheral that comes with the Xbox One is always listening and can recognize users' voices and heartbeats. Microsoft collects information about users' voice commands and may share it with third-party service providers. Users can opt-out of this feature by disabling personalized recommendations in the settings menu. Microsoft has faced privacy concerns over its use of Kinect technology in the past, and the company has clarified that it does not use information for targeted advertising. The new Kinect will run in an extremely low-power state for listening mode, and users can disable voice recognition data collection through the settings menu.
 35%|███▌      | 14/40 [02:33<04:33, 10.51s/it]2024-12-22 03:04:39,232 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:39,451 - [Process 1/5] - INFO - res.shape is :torch.Size([133])
results:A 34-year-old man named Jonathan Merkley was celebrating his birthday at a hotel in San Diego when he tried to stop a friend from leaving the party by lying down in front of her car. However, the friend did not stop and ran over Merkley, killing him. The driver has not been arrested, and police are still investigating the incident. In a separate incident, a woman in San Diego was involved in a hit-and-run accident after she ran over a man who was trying to stop her from leaving a party. The woman has not been arrested, and police are still investigating the incident.
 30%|███       | 12/40 [02:33<05:36, 12.02s/it]2024-12-22 03:04:39,674 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:40,238 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:40,238 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 609])
2024-12-22 03:04:40,258 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:04:42,911 - [Process 2/5] - INFO - res.shape is :torch.Size([167])
results:Twitter has faced criticism for not doing enough to protect its users from online abuse, particularly after a user's daughter was bullied off the platform. Twitter has promised to improve and expand its policies after the incident. The company allows users to block accounts that target them, but persistent attackers often create new accounts to continue harassment. A third-party software called "Block Together" has been recommended to automatically block new accounts. Twitter's fierce commitment to free expression may be clouding its judgment on this issue, according to some. The company has been criticized for not doing enough to protect the character of its community. Alexa Internet has been donating its crawl data to the Internet Archive since 1996, which is added to the Wayback Machine after an embargo period.
 30%|███       | 12/40 [02:37<05:32, 11.88s/it]2024-12-22 03:04:42,934 - [Process 3/5] - INFO - res.shape is :torch.Size([222])
results:
The Miami Gardens Police Chief, Stephen Johnson, was arrested and fired after being caught soliciting a prostitute in Dania Beach. He was taken into custody by undercover detectives who had placed an ad on Backpage.com for a "two-girl special." Johnson had called the number and arranged to pay $100 for 30 minutes with two prostitutes. When he arrived at the hotel room, he was arrested and found to have two condoms in his pocket. This incident comes just days after protesters marched into City Hall to express their anger over the fatal police shooting of a mentally ill man. The city manager, Cameron Benson, stated that the city and police department will move on from this incident and continue to provide excellent service to the community. Johnson's arrest has brought attention to the police department's troubled history, including allegations of racial profiling and the shooting death of a mentally ill man by a police officer. Johnson is also a pastor at a local church in Hallandale Beach.
 28%|██▊       | 11/40 [02:37<06:19, 13.07s/it]2024-12-22 03:04:43,079 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:43,106 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:43,315 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:43,315 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2126])
2024-12-22 03:04:43,381 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:04:44,425 - [Process 4/5] - INFO - res.shape is :torch.Size([181])
results:In the latest news from the world of Game of Thrones, it has been revealed that Dean-Charles Chapman, who plays King Tommen Baratheon, also played another character in the show, Martyn Lannister. This comes as a surprise to many fans, as Martyn only appeared in two episodes in the third season before being killed off. It has also been revealed that Chapman has played multiple characters in the show, including a young lordling named Martyn Lannister, who is a son of Kevan Lannister, Tywin's younger brother. This information has sparked a new theory that Tommen may be a faceless man, as the Lannister family is known for their incestuous relationships. Fans are also discussing the similarity between Tommen and Martyn, with some noting that they look almost identical.
 32%|███▎      | 13/40 [02:38<05:35, 12.41s/it]2024-12-22 03:04:44,539 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:45,534 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:45,534 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1440])
2024-12-22 03:04:45,582 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:04:46,596 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:46,597 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1229])
2024-12-22 03:04:46,638 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:04:46,654 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:46,655 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
2024-12-22 03:04:46,726 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:04:48,595 - [Process 0/5] - INFO - res.shape is :torch.Size([247])
results:The New York Jets cheerleaders have reached a settlement of $325,0000 with the team after filing a class-action lawsuit over their pay. The cheerleaders claimed they were paid only $150 per game and $100 for special events, and that they did not receive compensation for practice time or rehearsals. The settlement will give each of the 52 cheerleaders $2,500 for each season they worked. The Jets had denied the claims and argued that the cheerleaders were independent contractors, but the court approved the settlement. This is not the first time a team has faced a lawsuit over cheerleader pay, as the Buffalo Bills, Tampa Bay Buccaneers, Oakland Raiders, and Cincinnati Bengals have also been sued. The Buccaneers and Raiders settled for $825,000 and $1.25 million respectively, while the Bengals have reached a tentative agreement with their cheerleaders. The NFL has been criticized for not doing enough to ensure fair pay and treatment of cheerleaders.
 38%|███▊      | 15/40 [02:43<04:14, 10.17s/it]2024-12-22 03:04:48,721 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:52,210 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:52,210 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1958])
2024-12-22 03:04:52,283 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:04:52,606 - [Process 1/5] - INFO - res.shape is :torch.Size([207])
results:
Four news passages are provided, all related to missing persons in Humboldt County, California. In Passage 1, a woman reported missing in 2017 was found on the reality TV show "The Bachelor." Passage 2 is a tweet from the missing person's sister, expressing frustration with the lack of attention to her brother's case. Passage 3 discusses a recent cover story in the North Coast Journal examining the high rate of missing persons in Humboldt County and featuring one of the missing persons, Bekah Martinez. Passage 4 provides an update on the status of some of the missing persons in Humboldt County, including one who has been found alive and another who has been living in Eureka for the past year. The article highlights the challenges of searching for missing persons in a remote area like Humboldt County and the need for more attention and resources to solve these cases.
 32%|███▎      | 13/40 [02:46<05:33, 12.37s/it]2024-12-22 03:04:52,893 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:54,083 - [Process 3/5] - INFO - res.shape is :torch.Size([190])
results:

Detroit's water department is facing criticism for its aggressive shut-off of water service to non-paying customers, including commercial properties. Activists have been protesting the shut-offs, calling them a violation of human rights, and have been arrested for their actions. The water department has been contracting with a company to help carry out the shut-offs, but the company is only equipped to handle residential properties, leading to delays in shutting off commercial accounts. The department has announced plans to use $1 million from its Detroit Residential Water Assistance Program to help low-income customers avoid shut-offs. Meanwhile, the department is intensifying efforts to collect unpaid debts from commercial customers, with plans to shut off service to delinquent commercial accounts. The water department is responsible for about $6 billion of Detroit's $18 billion debt.
 30%|███       | 12/40 [02:48<05:49, 12.49s/it]2024-12-22 03:04:54,269 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:56,435 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:56,435 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2096])
2024-12-22 03:04:56,504 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:04:56,790 - [Process 4/5] - INFO - res.shape is :torch.Size([232])
results:

The House Intelligence Committee has voted to release a classified memo written by Republicans that alleges misconduct by senior FBI officials involved in the Russia probe. The memo, which was shared with FBI Director Christopher Wray, addresses a dossier compiled by a former British spy that was used to obtain surveillance warrants. Democrats on the committee have expressed concerns that the memo could jeopardize national security and have called for their own competing memo to be released. The White House has not decided whether to authorize the release of the memo, but President Trump has expressed a desire for "full transparency." Meanwhile, Trey Gowdy has urged Devin Nunes to share the memo with the FBI director, and three Republican senators have expressed support for keeping Trump's silence on the investigation. The New York Times has also reported that the memo reveals that Deputy Attorney General Rod Rosenstein approved an application to extend the surveillance of former Trump campaign aide Carter Page in early 2017.
 35%|███▌      | 14/40 [02:51<05:22, 12.40s/it]2024-12-22 03:04:56,893 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:04:57,492 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:57,492 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1881])
2024-12-22 03:04:57,557 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:04:58,828 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:04:58,828 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1054])
2024-12-22 03:04:58,866 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:05:00,893 - [Process 0/5] - INFO - res.shape is :torch.Size([214])
results:
A mysterious disease called acute flaccid myelitis (AFM) has been affecting children in the United States, with 32 cases confirmed by the Centers for Disease Control and Prevention (CDC) so far this year. The disease, which is similar to polio, causes weakness in the arms and legs and has been linked to enterovirus D68. The CDC is investigating the cause of the disease and has seen a rise in cases compared to last year. Doctors are concerned about the increase in cases and are urging parents to be aware of the symptoms of AFM, which include sudden weakness in the arms or legs, difficulty moving the face, and difficulty swallowing. The CDC is also encouraging doctors to report any cases they suspect may be AFM. The disease can be treated with physical therapy, but there is no known cure. The CDC is working to determine the cause of the disease and to develop a vaccine.
 40%|████      | 16/40 [02:55<04:19, 10.81s/it]2024-12-22 03:05:00,986 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:01,629 - [Process 2/5] - INFO - res.shape is :torch.Size([330])
results:

A prank call made to a London hospital by two Australian radio hosts, Mel Greig and Michael Christian, resulted in the death of a nurse, Jacintha Saldanha. The call was made to the hospital where the Duchess of Cambridge was being treated for acute morning sickness. The DJs had posed as Queen Elizabeth II and Prince Charles and were put through to a colleague at the hospital who divulged details of Kate's recovery. Saldanha, 46, was found dead in her living quarters in central London. The hospital where she worked, King Edward VII's Hospital, has expressed their deepest sympathies to the family and friends of Saldanha. The Australian radio station, 2Day FM, has apologized and stopped all hoax calls by its broadcasters. The incident has sparked outrage and demands for tougher regulation of the electronic media. The Australian Communications and Media Authority is considering whether to initiate an inquiry beyond its usual process of giving broadcasters 60 days to respond to complaints. The Prime Minister of Australia, David Cameron, has also expressed his sadness over the incident. The two DJs, Greig and Christian, have spoken publicly for the first time since the incident and expressed their deepest sympathies to the family of Saldanha. They have also stated that they did not expect the call to be put through to the Duchess of Cambridge's room and that they were not aware of the hospital's policies regarding prank calls.
 32%|███▎      | 13/40 [02:56<06:16, 13.95s/it]2024-12-22 03:05:01,785 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:03,804 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:03,804 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1670])
2024-12-22 03:05:03,862 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:05:04,379 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:04,379 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1411])
2024-12-22 03:05:04,433 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:05:09,449 - [Process 3/5] - INFO - res.shape is :torch.Size([263])
results:


The news passages highlight the death of Vladimir Katriuk, a former member of the Ukrainian battalion of the Waffen SS, who was one of the most wanted Nazi war criminals. Katriuk, who lived in Quebec for over 60 years, died at the age of 93 after a long illness. The Russian Embassy in Ottawa had requested Canada to extradite Katriuk to face trial for war crimes committed during World War II, but Canada refused, citing lack of evidence. In 1999, the Federal Court ruled that Katriuk obtained Canadian citizenship under false pretenses, but found no evidence of war crimes committed. In 2007, the Harper cabinet decided not to revoke his citizenship. Recently, a study revealed that Katriuk was a key participant in a massacre in Khatyn during the Second World War. Russia has charged Katriuk with genocide in connection with the 1943 killing of civilians in Khatyn. The Ukrainian Canadian Congress has called on Canada to ignore Russia's demands and leave Katriuk alone, as the Federal Court found no evidence of war crimes committed.
 32%|███▎      | 13/40 [03:03<06:00, 13.36s/it]2024-12-22 03:05:09,521 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:09,719 - [Process 1/5] - INFO - res.shape is :torch.Size([292])
results:
In the latest news, Nicole "Snooki" Polizzi, a reality star from "Jersey Shore," has announced that she is pregnant with her first child. However, her pregnancy has raised concerns as she has a history of heavy drinking, which can lead to Fetal Alcohol Spectrum Disorder (FASD). FASD is a group of conditions that can occur in a person whose mother drank alcohol during pregnancy, causing physical, behavioral, and cognitive problems. Snooki has been open about her drinking habits and has even brokered a deal to announce her pregnancy on the cover of US Weekly. Meanwhile, a new study has found that alcohol is a teratogen, meaning it can cause birth defects, and that FASD is the most frequent known cause of intellectual disability. The study also found that Snooki's boyfriend, Jionni LaValle, is speculated to be the father of her child. In other news, a reality star named Snooki has talked about her drinking habits on "Ellen," admitting that she sometimes wakes up in a garbage can after a night of partying. This has raised concerns about the dangers of excessive drinking during pregnancy and the potential risks of FASD.
 35%|███▌      | 14/40 [03:04<05:58, 13.80s/it]2024-12-22 03:05:09,867 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:10,533 - [Process 4/5] - INFO - res.shape is :torch.Size([259])
results:The trade tensions between the US and China continue to escalate as the US is planning to increase tariffs on $200 billion of Chinese goods to 25% from 10%. This move comes after the US imposed tariffs on $34 billion of Chinese goods and China responded with the same amount of tariffs on US goods. The US is also considering a deal with ZTE Corp, a Chinese telecom equipment maker, after the Trump administration finished a deal that allowed the company to stay in business. Meanwhile, the US and China are trying to restart talks aimed at averting a full-blown trade war, but negotiations have been stalled for weeks with both sides refusing to budge. The Chinese government has vowed to respond with the same amount of tariffs on US goods if the US imposes new tariffs. The US Treasury spokesman did not respond to requests for comment, and the Ministry of Commerce did not respond to questions by phone or fax about the status of possible negotiations. The Chinese Politburo has signaled that policy makers will focus more on supporting economic growth amid risks from a campaign to reduce debt and the dispute with Trump.
 38%|███▊      | 15/40 [03:04<05:20, 12.80s/it]2024-12-22 03:05:10,648 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:10,650 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:10,651 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 633])
2024-12-22 03:05:10,675 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:05:11,989 - [Process 2/5] - INFO - res.shape is :torch.Size([174])
results:
A tragic accident occurred at Fleming Park in Augusta, Georgia, where a 12-year-old boy named Melquan Robinson died after being electrocuted by an underground electrical wire while trying to jump over a chain-link fence. Three other boys were also injured while trying to save their friend. The incident happened during football practice, and the park is owned by the city. An investigation is underway, and the city will look into how the live electrical wire happened. The family is grieving, and a vigil will be held Thursday at 6:00 pm at Bernie Ward Community Center. The community is coming together to support the family and the friends of Melquan. The incident is a reminder of the dangers of playing sports and the importance of safety measures.
 35%|███▌      | 14/40 [03:06<05:34, 12.86s/it]2024-12-22 03:05:12,022 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:12,271 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:12,271 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1394])
2024-12-22 03:05:12,321 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:05:12,609 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:12,610 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 308])
2024-12-22 03:05:12,621 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:05:12,718 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:12,718 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1176])
2024-12-22 03:05:12,760 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:05:13,599 - [Process 0/5] - INFO - res.shape is :torch.Size([254])
results:
The Shroud of Turin, a centuries-old linen cloth believed by some to be the burial cloth of Jesus, has been subject to a new forensic investigation that suggests it may be a fake. Researchers used bloodstain pattern analysis to study the cloth and found inconsistencies in the bloodstains that suggest it was not created by a body hanging on a cross. The study, published in the Journal of Forensic Sciences, found that the bloodstains on the cloth are inconsistent with any one pose, suggesting that a standing model was used to imprint the patterns. The researchers conducted seven different bloodstain tests on different body parts depicted on the fabric and found that the angles at which gravity pulled the blood dripping from a body corresponded to different body parts. The study's findings add to evidence that the shroud is a medieval fraud, and the inconsistencies in the bloodstains suggest that the image on the cloth was not created by a crucified person. The Vatican regards the shroud as an icon, rather than a religious relic, and has never officially rejected it, but the new findings may raise questions about its authenticity.
 42%|████▎     | 17/40 [03:08<04:21, 11.38s/it]2024-12-22 03:05:13,631 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:14,510 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:14,510 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 486])
2024-12-22 03:05:14,528 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:05:16,847 - [Process 3/5] - INFO - res.shape is :torch.Size([144])
results:Google CEO Sundar Pichai has been awarded $199 million worth of shares, bringing his total holdings in Alphabet to $650 million. This is one of the largest equity awards given by a US company, with Pichai being the highest-paid CEO in the US. The award vests in quarterly increments until 2019. Additionally, other Google executives such as CFO Ruth Porat and cloud computing head Diane Greene were also awarded shares worth $38 million and $42.8 million, respectively. This comes after Google agreed to pay $187 million in back taxes to the UK government.
 35%|███▌      | 14/40 [03:11<05:00, 11.56s/it]2024-12-22 03:05:16,974 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:19,062 - [Process 0/5] - INFO - res.shape is :torch.Size([135])
results:President Trump met with North Korean leader Kim Jong Un in Singapore and discussed denuclearization of the Korean Peninsula. Trump showed Kim a video simulation of possible projects that could take place in North Korea, including the possibility of building a luxury hotel on the coastline. Trump also mentioned that North Korea could have the "best hotels in the world" if they open up their economy. The U.S. delegation showed Kim a video simulation of possible projects that could take place in North Korea, including the possibility of building a luxury hotel on the coastline. Trump expressed optimism that Kim was interested in the ideas presented to him.
 45%|████▌     | 18/40 [03:13<03:31,  9.60s/it]2024-12-22 03:05:19,113 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:19,197 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:19,197 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1259])
2024-12-22 03:05:19,245 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:05:19,637 - [Process 2/5] - INFO - res.shape is :torch.Size([155])
results:In Fairfax County, Virginia, a woman is suspected of impersonating a Target employee and stealing over $40,000 worth of iPhones from a store. Surveillance footage shows the woman entering the store, making her way to the stockroom, and placing the iPhones in a box before leaving. Detectives are searching for the woman and have released images of her from the footage. In a separate incident, a woman in Virginia is accused of stealing over $40,000 worth of iPhones from a Target store by disguising herself as an employee. The suspect is believed to be familiar with store procedures and location of the iPhones in the stockroom.
 38%|███▊      | 15/40 [03:14<04:42, 11.29s/it]2024-12-22 03:05:19,653 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:19,942 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:19,943 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 135])
2024-12-22 03:05:19,947 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:05:20,740 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:20,740 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 988])
2024-12-22 03:05:20,772 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:05:22,072 - [Process 4/5] - INFO - res.shape is :torch.Size([213])
results:Sony unveiled the PlayStation 4 (PS4) at an event today, showcasing a next-generation gaming console that promises to shift the focus from the living room to the gamer. The PS4 features a new x86-based personal computer, a redesigned DualShock 4 controller with a touchpad and share button, and a new video-processing chip for streaming gameplay. The console will also have native PlayStation Cloud, allowing for streaming access to old titles without discs, and a dedicated social gaming interface with UStream integration. The PS4 will also have personalization features, including predicting the next titles users will want and downloading them ahead of time. Remote play will also be available, allowing users to continue playing on another screen. The PS4 will have a similar design to the PS3, but with improved graphics and faster performance. The console is expected to be released "Holiday 2013" but the price is still unknown.
 40%|████      | 16/40 [03:16<04:58, 12.42s/it]2024-12-22 03:05:22,160 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:23,571 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:23,572 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 811])
2024-12-22 03:05:23,599 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:05:25,796 - [Process 2/5] - INFO - res.shape is :torch.Size([129])
results:In the latest news, the creators of the popular TV show "Family Guy" have been embroiled in a heated controversy after the network announced that they will be showing an image of the prophet Muhammad. In response, the creators of the show have vowed to take drastic measures to prevent the episode from airing. Meanwhile, two boys, Cartman and Kyle, have embarked on a cross-country chase to determine the fate of the show. The outcome of this high-stakes game of cat and mouse will have far-reaching consequences for the future of "Family Guy."
 40%|████      | 16/40 [03:20<03:53,  9.75s/it]2024-12-22 03:05:25,882 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:26,639 - [Process 0/5] - INFO - res.shape is :torch.Size([171])
results:The passage highlights the contributions of Japanese-American soldiers during World War II, particularly the 442nd Regimental Combat Team and the Military Intelligence Service. These soldiers, despite facing discrimination and racism, served with distinction and bravery, earning the 442nd the title of most decorated Army unit of its size and length of service in US history. The Congressional Gold Medal will be awarded to these veterans in recognition of their service and sacrifice. Additionally, a new law will recognize over 6,000 Japanese-Americans born of immigrant parents who served in the war and fought in battles in Europe and Asia. These soldiers faced internment during the war, but their bravery and service helped pave the way towards a more tolerant and just nation.
 48%|████▊     | 19/40 [03:21<03:08,  8.99s/it]2024-12-22 03:05:26,760 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:27,388 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:27,389 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 837])
2024-12-22 03:05:27,419 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:05:28,840 - [Process 1/5] - INFO - res.shape is :torch.Size([371])
results:

Myrna Arias, a former sales executive at Intermex Wire Transfer LLC, has filed a lawsuit against the company for invasion of privacy, retaliation, unfair business practices, and other allegations. Arias claims that she was fired after refusing to install an app that tracked her location even when she was off the clock. The app, called Xora, was required by Intermex and had a GPS function that allowed managers to monitor employees' locations. Arias objected to the app's monitoring of her location during non-work hours and was fired shortly after expressing her concerns. The lawsuit seeks damages in excess of $500,000 and alleges that Intermex violated California and federal laws against invasion of privacy. Intermex did not respond to comment requests.

In related news, a new web-based management application called StreetSmart has been developed to enable office staff to instantly see where their employees are and where they have been, as well as to track their driving routes and times at and between stops to optimize performance. The app also allows employees to record their daily work and job specific hours, receive daily job assignments on their phone, and capture customer or job information on their phone. StreetSmart is offered on a monthly basis for roughly $1 a day per user and is available on over 140 different types of phones.

Finally, a recent article highlights the importance of maintaining strong relationships with customers in hypercompetitive service industries, particularly in delivering great service to keep and grow the customer base. The article notes that while businesses know they need to deliver great service, they often struggle to do so in practice.
 38%|███▊      | 15/40 [03:23<06:25, 15.41s/it]2024-12-22 03:05:28,915 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:29,960 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:29,960 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 623])
2024-12-22 03:05:29,980 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:05:30,248 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:30,248 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1930])
2024-12-22 03:05:30,321 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:05:33,419 - [Process 3/5] - INFO - res.shape is :torch.Size([317])
results:


A 7-year-old boy named Josh Hardy is in critical condition at a hospital in Memphis, Tennessee, after contracting an adenovirus that has spread throughout his body. His parents have been pleading with the drug company Chimerix to provide him with a potentially lifesaving drug called brincidofovir, but the company had initially denied their requests. However, after a public outcry and media attention, Chimerix has now agreed to provide the drug to Josh through a clinical trial. The company had been in talks with the FDA to expedite additional trials of the drug, and the FDA has committed to work expeditiously with Chimerix on the design of a pivotal Phase 3 study. The drug will be released to 20 patients in an open-label study, beginning with Josh, and the hospital where he is being treated expects to receive the medication within 48 hours. The story of Josh's fight for the drug has attracted national attention, with his family appearing on several television shows and a social media campaign gaining widespread support. Local organizations have also joined to support the Hardy family, with one restaurant offering "Joshburgers" in exchange for signed letters to Chimerix's CEO. The news has sparked a larger discussion about the need for additional clinical development to assess the drug's potential, and the role of social media in bringing attention to medical crises.
 38%|███▊      | 15/40 [03:27<05:26, 13.07s/it]2024-12-22 03:05:33,618 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:33,935 - [Process 4/5] - INFO - res.shape is :torch.Size([228])
results:

Researchers from UC Berkeley have made a groundbreaking discovery in the field of neuroscience, successfully recreating YouTube videos from viewers' brain activity. The study, published in Current Biology, demonstrates that the brain's visual cortex processes dynamic information in a way that can be decoded and reconstructed using functional MRI (fMRI) technology. The team, led by Jack Gallant, used MRI to record brain activity while subjects watched YouTube videos, and then used that data to create a computer model that could predict the visual content of the videos based on the brain activity. The result is a haunting, dream-like version of the original video, which could potentially be used to broadcast imagery into people's minds. The technology could also be used to read people's memories and dreams, offering a glimpse into their internal imagery. While the potential applications of this technology are vast, the team emphasizes that it is still in its early stages and much more research is needed to fully understand how the brain processes visual information.
 42%|████▎     | 17/40 [03:28<04:41, 12.25s/it]2024-12-22 03:05:34,033 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:35,685 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:35,686 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1016])
2024-12-22 03:05:35,697 - [Process 0/5] - INFO - res.shape is :torch.Size([134])
results:
Donald Trump's Commerce Secretary, Wilbur Ross, has been accused of lying to Forbes magazine about his net worth. According to Forbes, Ross' net worth is only $2.5 billion, not the $3.7 billion he claimed. Ross has a history of exaggerating his wealth and has been accused of misleading colleagues and investors. Trump himself has also been accused of lying about various topics, including his popularity and his views on foreign leaders. In addition, Trump's administration is filled with wealthy individuals, including Ross, who have been criticized for their wealth and lack of concern for the poor.
 50%|█████     | 20/40 [03:30<03:00,  9.01s/it]2024-12-22 03:05:35,719 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:05:35,727 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:36,709 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:36,710 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 539])
2024-12-22 03:05:36,728 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:05:36,730 - [Process 1/5] - INFO - res.shape is :torch.Size([149])
results:There have been several incidents of freeway shootings in Phoenix, Arizona, which caused panic among the residents. The police have arrested a man named Leslie Merritt Jr. and charged him with the crime. However, his lawyers have raised questions about the evidence and claimed that there is no proof linking him to the crimes. A judge has reduced his bond to zero and he is expected to be released from jail soon. In another incident, a driver suspected of firing at motorists on an Alabama highway was killed in a shootout with the police. The suspect's identity has not been released, and the police have imposed a gag order on the lawyers involved in the case.
 40%|████      | 16/40 [03:31<05:15, 13.14s/it]2024-12-22 03:05:36,941 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:37,081 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:37,081 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1965])
2024-12-22 03:05:37,150 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:05:38,312 - [Process 2/5] - INFO - res.shape is :torch.Size([250])
results:

The news passages highlight the ongoing tensions between North Korea and the international community, particularly with South Korea and the United States, over North Korea's nuclear weapons program. Recently, North Korea conducted its sixth nuclear test, which was described as the most powerful ever by the country. The test has led to increased military drills and rhetoric from South Korea and the United States, with the latter calling for the strongest sanctions possible to stop Pyongyang's nuclear program. Diplomats at the United Nations have also condemned the test, with the American ambassador, Nikki Haley, stating that the United States will circulate a resolution in response. Additionally, there are signs of a rift between South Korean President Moon Jae-in and US President Donald Trump, with Moon advocating for more dialogue and diplomacy with North Korea, while Trump has stated that "talking is not the answer." The North Korean government has also been accused of digging extra tunnels at its nuclear test site, and there are concerns that it may conduct another ICBM test soon, possibly on September 9th, which is North Korea's founding day.
 42%|████▎     | 17/40 [03:32<04:03, 10.58s/it]2024-12-22 03:05:38,456 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:40,476 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:40,476 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1971])
2024-12-22 03:05:40,549 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:05:41,163 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:41,163 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1489])
2024-12-22 03:05:41,217 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:05:43,394 - [Process 4/5] - INFO - res.shape is :torch.Size([174])
results:The House of Representatives passed a bill to rename the Dryden Flight Research Center as the Neil A. Armstrong Flight Research Center. However, the bill's passage was not without controversy, as some lawmakers used the opportunity to criticize the sequester and the federal budget. Republicans have embraced the sequester, despite President Obama's calls for alternative spending cuts and tax increases. Democrats, on the other hand, have expressed concern about the impact of the sequester on various government programs. The sequester is set to take effect on March 1, and Congress is planning another recess at the end of March, which could lead to a government shutdown. Meanwhile, lawmakers continue to receive their six-figure salaries, while some government programs face cuts.
 45%|████▌     | 18/40 [03:37<04:11, 11.41s/it]2024-12-22 03:05:43,537 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:43,747 - [Process 0/5] - INFO - res.shape is :torch.Size([208])
results:A bitter court battle is brewing between Frances Bean Cobain and her estranged husband Isaiah Silva over Kurt Cobain's most prized guitar, which is worth millions of dollars. Frances Bean claims that Silva is trying to take advantage of her and her father's legacy by asking for spousal support, despite the fact that she has already been paying him $15,000 a month in spousal support since their separation. Silva, on the other hand, claims that Frances Bean gave him the guitar as a wedding present and that he is entitled to a significant portion of Kurt Cobain's estate. The guitar is believed to be worth several times its original value due to its historical significance, and Love's manager has allegedly sent threatening messages to Silva's ex-girlfriend in an attempt to get the guitar back. The case is likely to go to trial, with both sides digging in their heels and refusing to back down.
 52%|█████▎    | 21/40 [03:38<02:45,  8.72s/it]2024-12-22 03:05:43,867 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:45,931 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:45,931 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1445])
2024-12-22 03:05:45,978 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:05:47,385 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:47,386 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 03:05:47,457 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:05:48,926 - [Process 2/5] - INFO - res.shape is :torch.Size([169])
results:
On Thursday, a tragic accident occurred in Schertz, Texas, when a 6-year-old boy, Kameron Prescott, was killed in a deputy-involved shooting. Four deputies were involved in the shooting, which occurred at a mobile home park after a woman, Amanda Lene Jones, was reported to be armed and dangerous. Witnesses stated that Jones threatened to shoot the deputies and made physical contact with one of them. The deputies fired their weapons, killing Jones and injuring Kameron, who later died at the hospital. The incident has left the community in shock, and a fund has been established to assist Kameron's family with expenses. The deputies involved in the shooting are on administrative leave while the investigation continues.
 45%|████▌     | 18/40 [03:43<03:52, 10.59s/it]2024-12-22 03:05:49,032 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:50,859 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:50,859 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1247])
2024-12-22 03:05:50,890 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:05:52,654 - [Process 1/5] - INFO - res.shape is :torch.Size([268])
results:
On Thursday, September 20, 2018, a mass shooting occurred at a Rite Aid distribution center in Harford County, Maryland, resulting in the deaths of three people and injuring three others. The shooter, identified as Snochia Moseley, 26, died of a self-inflicted gunshot wound after being taken to an area hospital. Moseley was a temporary employee at the facility and used a single handgun and multiple magazines in the attack. The shooting occurred at around 9:06 a.m. and the motive is still unknown. The incident is the third high-profile shooting in the US in two days and has sparked a massive response from law enforcement agencies. The area around the distribution center has been closed off, and people are being asked to avoid the area. The Harford County Emergency Operations Center has been partially activated, and the Harford County Information Hotline has been set up for people trying to reach their loved ones. The shooting has left many people in shock, including witnesses and survivors. The incident is a reminder of the growing concern about mass shootings in the US and the need for better security measures to prevent such incidents.
 42%|████▎     | 17/40 [03:47<05:21, 13.98s/it]2024-12-22 03:05:52,804 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:52,864 - [Process 3/5] - INFO - res.shape is :torch.Size([348])
results:
Tom Sizemore, a well-known actor, has been accused of touching an 11-year-old girl's genitals during a photo shoot for a movie in 2003. The incident occurred near the end of production on the film "Born Killers" in Utah. The young actress, who had a small role in the production, was seated on Sizemore's lap in a holiday tableau for a portrait session. According to multiple sources, Sizemore allegedly either rubbed his finger against the girl's vagina or inserted it inside. The incident was kept quiet at the time, but has recently come to light in the wake of the Harvey Weinstein sexual harassment scandal. Sizemore's representatives have declined to comment on the allegations. The actress' agent fielded a call from the actress' agent the next day, explaining that the girl had informed her mother that she'd been inappropriately touched. The mother noticed that her daughter was unusually quiet and told her she was going to take her to a swimming place that was the little girl's favorite thing. The young actress, now 26, has hired a lawyer to explore legal action against Sizemore as well as her parents. Sizemore has a history of drug addiction and aggressive behavior toward women, including previous arrests for suspected battery of another woman. He has worked steadily in the industry despite his troubles, including in recent projects such as "Felt" opposite Liam Neeson and "Twin Peaks."
 40%|████      | 16/40 [03:47<05:59, 14.99s/it]2024-12-22 03:05:53,074 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:55,254 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:55,254 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1463])
2024-12-22 03:05:55,303 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:05:56,642 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:56,643 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1977])
2024-12-22 03:05:56,717 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:05:56,949 - [Process 0/5] - INFO - res.shape is :torch.Size([236])
results:Matt Damon and Driver have different opinions on sexual misconduct. Damon recently made controversial statements about the spectrum of sexual misconduct, which Driver found tone-deaf and part of the problem. Driver believes that men cannot understand the extent of abuse women face daily and should not attempt to explain or differentiate sexual misconduct. Driver also expressed concern about her daughters living in a world where they cannot escape sexual harassment headlines. Driver and Damon have different views on how to address the issue, with Damon suggesting a need for dialogue and growth, while Driver emphasizes the importance of understanding and empathy.

In other news, Anita Hill will lead the fight against harassment in Hollywood, and Clarence Thomas' accuser has spoken out about the need for men to understand the extent of abuse. Driver also shared her personal experience with sexual misconduct and emphasized the importance of self-esteem and empowerment in addressing the issue. The interview with Damon sparked a wider conversation about the industry's response to sexual misconduct and the need for change.
 55%|█████▌    | 22/40 [03:51<03:01, 10.07s/it]2024-12-22 03:05:56,994 - [Process 2/5] - INFO - res.shape is :torch.Size([145])
results:
Sarah Markham, a vegan mother, was arrested in Seminole County, Florida, after she refused to take her newborn baby to the hospital when the baby was diagnosed as dehydrated. Markham wanted to try supplementing breast milk with vegan formula instead of following the doctor's orders. When police arrived at her apartment, she refused to answer the door and was arrested. Markham was charged with child neglect and is facing a judge in August. The baby was taken into state custody and is now receiving treatment. Markham's vegan beliefs were cited as the reason for her refusal to take the baby to the hospital.
 48%|████▊     | 19/40 [03:51<03:26,  9.83s/it]2024-12-22 03:05:57,047 - [Process 4/5] - INFO - res.shape is :torch.Size([254])
results:Senator Elizabeth Warren grilled Wells Fargo CEO John Stumpf over the bank's cross-selling scandal, accusing him of failing to hold himself or any other senior executives accountable for the company's actions. Warren called for a criminal investigation into Stumpf and suggested that he should resign, give back the money he took while the scandal was ongoing, and be criminally investigated by both the Department of Justice and the Securities and Exchange Commission. Stumpf defended the bank's culture and practices, saying that cross-selling is not a scam but rather a way of deepening relationships. The decision to recognize Jerusalem as Israel's capital is expected to be announced by Donald Trump on Wednesday, with the move seen as a compromise between the administration's previous pledge to move the US embassy to Jerusalem and international criticism of the decision. The decision has been met with criticism from Pope Francis and the Palestinian representative to the UK, who called for respect for the status quo in Jerusalem. The move comes amidst a broader shift in Middle East politics, with Saudi Arabia and the US growing closer to Israel.
 48%|████▊     | 19/40 [03:51<04:13, 12.09s/it]2024-12-22 03:05:57,061 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:57,143 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:57,159 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:05:58,756 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:58,757 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 877])
2024-12-22 03:05:58,788 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:05:59,964 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:05:59,964 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1676])
2024-12-22 03:06:00,020 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:06:00,577 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:00,577 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-22 03:06:00,649 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:06:03,250 - [Process 3/5] - INFO - res.shape is :torch.Size([145])
results:

Linda Hooper, a teacher in Lubbock, Texas, adopted a student named Cruz Riojas after he was repeatedly abused by his stepfather. Hooper took Cruz in after his mother asked her to keep him for a weekend, and he never left. Hooper and her husband, Gale, raised Cruz as their own son, and he eventually graduated from high school and went on to college. Cruz's biological mother eventually gave Hooper legal custody of him, and she and her husband adopted him. Cruz is now a successful man with a family of his own, and he credits Hooper with teaching him to never give up and persevere through difficult times.
 42%|████▎     | 17/40 [03:57<05:12, 13.60s/it]2024-12-22 03:06:03,409 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:05,726 - [Process 1/5] - INFO - res.shape is :torch.Size([239])
results:

A recent study published in JAMA Pediatrics suggests that taking acetaminophen during pregnancy may increase the risk of ADHD and similar disorders in children. The study found that pregnant women who frequently took acetaminophen were more likely to have children later diagnosed with ADHD and a rarer diagnosis called hyperkinetic disorder. The researchers advise pregnant women to avoid taking acetaminophen frequently and to consult their doctors before taking any medication. However, other experts suggest that the study does not show a clear cause-and-effect relationship and that more research is needed to determine the safety of acetaminophen during pregnancy. The study's authors also found that infections such as influenza during pregnancy may affect the brain development of babies and increase the risk of ADHD. The study highlights the importance of not taking a drug's safety during pregnancy for granted and suggests that there are non-pharmacological ways to deal with pain, such as massages, baths, and acupuncture.
 45%|████▌     | 18/40 [04:00<05:01, 13.71s/it]2024-12-22 03:06:05,825 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:06,223 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:06,223 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1563])
2024-12-22 03:06:06,282 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:06:06,542 - [Process 0/5] - INFO - res.shape is :torch.Size([147])
results:Microsoft unveiled its new motion-control system, Kinect, at an event in Los Angeles on Sunday night. The system allows users to play games and interact with the Xbox 360 console using gestures and spoken commands without the need for a controller. The system was demonstrated with a variety of games, including sports games, dance games, and a Star Wars game that lets players fight Darth Vader with a light saber. The system also includes a yoga game and a flying car game. Microsoft also showed off its new name for the system, Kinect, which replaces the previous Project Natal name. The company will provide more information on Kinect at its Monday press conference.
 57%|█████▊    | 23/40 [04:00<02:48,  9.93s/it]2024-12-22 03:06:06,604 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:07,490 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:07,490 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 971])
2024-12-22 03:06:07,522 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:06:08,624 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:08,624 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1131])
2024-12-22 03:06:08,664 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:06:10,323 - [Process 4/5] - INFO - res.shape is :torch.Size([264])
results:

Netflix's hit series "Orange Is the New Black" has been hit by a major hacking incident, with the first 10 episodes of season 5 leaked online by an anonymous hacker. The hacker, known as "thedarkoverlord," has demanded an unspecified ransom from Netflix to prevent the release of the remaining episodes. The hack is believed to have originated from a breach in a production vendor used by several major TV studios, including Netflix. The hacker claims to have stolen content from other studios as well, including ABC, Fox, National Geographic, and IFC. Netflix has described the incident as an "active situation" and is investigating with the help of the FBI. The leak of "Orange Is the New Black" could potentially dent Netflix's subscriber growth and stock price. The hacker has also threatened to release stolen content from other networks, including ABC, Fox, National Geographic, and IFC. Netflix is counting on "Orange" to help it add 3.2 million subscribers in the next quarter, and any shortfall in subscriber growth could negatively impact the company's stock price.
 50%|█████     | 20/40 [04:04<04:08, 12.44s/it]2024-12-22 03:06:10,540 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:12,588 - [Process 2/5] - INFO - res.shape is :torch.Size([281])
results:

A recent study published in the journal Cancer found that one in seven colon and rectal cancers in the US are now diagnosed in patients younger than age 50, which is when people are advised to begin routine screenings for these tumors. This is a significant increase from previous years, with nearly one in four rectal tumors and more than one in 10 colon cancers expected to be diagnosed in people under 50 by 2030. The study also found that younger patients are more likely to have advanced cancer and be diagnosed with tumors that have spread to the lymph nodes and other organs. Despite being diagnosed with more advanced cancer, younger patients lived slightly longer without a cancer recurrence. The study raises questions about whether screening for colon cancer should begin at an earlier age, as the incidence of colon cancer is increasing among people under 50. However, the study also notes that the risk is still low and more research is needed to understand the impact of expanding screening to younger age groups. The study's findings suggest that the medical community should be on the lookout for signs of colorectal cancer in patients younger than 50 and that young patients with tumors that have metastasized may be more likely to receive aggressive treatments.
 50%|█████     | 20/40 [04:06<03:51, 11.56s/it]2024-12-22 03:06:12,827 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:14,071 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:14,072 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1980])
2024-12-22 03:06:14,145 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:06:14,706 - [Process 0/5] - INFO - res.shape is :torch.Size([171])
results:

In the latest news, an auction of personal effects belonging to Ted Kaczynski, the Unabomber, is taking place. The auction includes items such as his handwritten manifesto, hoodie, and Smith-Corona typewriter. While some items have received significant bids, others have not. Kellan Lutz, an actor, is also participating in an auction where he is donating his travel luggage to raise funds for youth groups across the country. Meanwhile, a film reel initially bought for the look of the tin has been found to be a rare surviving copy of a 1917 film featuring Charlie Chaplin, and is expected to command a six-figure sum. Collector Morace Park bought the film reel in 2009.
 60%|██████    | 24/40 [04:09<02:30,  9.40s/it]2024-12-22 03:06:14,819 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:16,376 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:16,377 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2003])
2024-12-22 03:06:16,448 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:06:18,303 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:18,304 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2054])
2024-12-22 03:06:18,326 - [Process 1/5] - INFO - res.shape is :torch.Size([253])
results:Bill Cosby was sentenced to three to 10 years in prison for drugging and sexually assaulting a woman in 2004. His publicist, Andrew Wyatt, made fiery accusations of racism and sexism in statements outside the courthouse, framing Cosby's downfall as an unjust product of the #MeToo era. Wyatt compared Cosby's situation to the accusations against Supreme Court nominee Brett Kavanaugh, calling it a "sex war" happening in Washington. Cosby's team has repeatedly invoked Emmett Till, a black boy murdered after a white woman falsely accused him of whistling at her, and has called for the FBI to investigate their claims before a vote on Kavanaugh takes place. Cosby was taken to SCI Phoenix, a new 3,830-bed state prison in suburban Philadelphia, where he will serve his three-to-10-year sentence. Wyatt also claimed that Cosby was one of the greatest civil rights leaders and a great educator of men and boys, and criticized the psychologists who testified against Cosby.
 48%|████▊     | 19/40 [04:12<04:40, 13.37s/it]2024-12-22 03:06:18,372 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:06:18,410 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:19,766 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:19,767 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 812])
2024-12-22 03:06:19,791 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:06:23,216 - [Process 3/5] - INFO - res.shape is :torch.Size([382])
results:

Donald Trump has been attacking the media again, claiming that they are not covering him fairly and are putting false meanings into his words. He also accused the media of protecting Hillary Clinton. Trump's running mate, Mike Pence, said that Trump will offer "real specifics" this week on how to make the country safer. However, Trump has called President Barack Obama the "founder" of the Islamic State group, which Pence said Trump was trying to make the point that Obama is to blame for the group's rise in power. Trump also lashed out at the media on Sunday after more stories describing dysfunction inside his presidential campaign.

Meanwhile, a New York Times investigation revealed that Trump's top adviser, Paul Manafort, has ties to a large network that Ukrainian government investigators say was used to loot assets and influence elections. Manafort's involvement in Russian and Ukrainian politics has previously been reported but has come under increasing scrutiny as the U.S. election cycle's focus rests on the region. Trump is also expected to propose a new immigration policy under which the U.S. would stop issuing visas in cases where adequate screenings can't be performed, and he will create a new, ideological test for admission to the country that would assess a candidate's stances on issues like religious freedom.

Finally, former Trump campaign manager Corey Lewandowski tweeted a link to a New York Times article about Manafort's ties to Ukraine, which created a stir on Twitter. Many jumped on board, retweeting the article and noting the awkwardness of the former Trump adviser seemingly calling out his successor.
 45%|████▌     | 18/40 [04:17<05:41, 15.52s/it]2024-12-22 03:06:23,343 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:23,894 - [Process 4/5] - INFO - res.shape is :torch.Size([212])
results:
The article discusses the use of Bayesian statistics in the search for missing planes, specifically in the case of Malaysia Airlines Flight 370. Bayesian statisticians, led by Colleen Keller, were brought in to help refine the search efforts. They used a Bayesian approach to update their estimates of the probability of finding the plane based on new information and data. The team split the search area into a grid and applied Bayesian principles to each cell, assessing the probability of the plane being found in each location. They also took into account the probability of the plane being in a specific location based on historical data and the likelihood of different scenarios, such as mechanical failure or deliberate diversion. The team's efforts led to a new area of highest probability being identified, and ultimately, the plane was found in that location. The article highlights the flexibility and adaptability of the Bayesian approach in search and rescue efforts, and how it can help refine and focus the search process.
 52%|█████▎    | 21/40 [04:18<04:02, 12.78s/it]2024-12-22 03:06:23,996 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:25,435 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:25,436 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1170])
2024-12-22 03:06:25,479 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:06:25,561 - [Process 2/5] - INFO - res.shape is :torch.Size([201])
results:


A recent discovery in a rock shelter in southern Australia has revealed human occupation dating back 49,0000 years, challenging the previously held belief that humans arrived in the region 50,000 years ago. The find includes bone fragments from a massive bird and a mega fauna, as well as evidence of human interaction with these animals. Excavations at the site used hand excavation methods and found bone and quartz tools, as well as red ochre and gypsum pigments, indicating that these early technologies were developed locally. This discovery supports the idea that Indigenous Australians were not as innovative as similar populations elsewhere in the world. The find also includes evidence of human interaction with mega fauna, including a giant bird and a wombat-like species, Diprotodon optatum. The study has been published in Nature and is significant in understanding human settlement in Australia.
 52%|█████▎    | 21/40 [04:19<03:47, 11.99s/it]2024-12-22 03:06:25,712 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:25,921 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:25,921 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1080])
2024-12-22 03:06:25,958 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:06:26,164 - [Process 0/5] - INFO - res.shape is :torch.Size([194])
results:Kevin Spacey is facing multiple allegations of sexual misconduct, including sexual harassment, assault, and attempted rape, from over a dozen men. The allegations date back to the 1980s and 1990s, with some incidents occurring on set during filming of the movie "All the Money in the World." Spacey has been cut from the film and his representatives have not returned calls for comment. The allegations have led to a criminal investigation in Massachusetts, where the statute of limitations for indecent assault and battery is six years. The accusers include a former news anchor who alleges Spacey assaulted her son in 2016, and several other men who allege incidents of sexual misconduct. The allegations have sparked a wider conversation about sexual harassment and assault in the entertainment industry and the need for greater accountability.
 62%|██████▎   | 25/40 [04:20<02:30, 10.02s/it]2024-12-22 03:06:26,269 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:28,485 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:28,485 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1476])
2024-12-22 03:06:28,540 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:06:28,990 - [Process 1/5] - INFO - res.shape is :torch.Size([215])
results:Hillary Clinton has ruled out running for president in 2012 or 2016, according to her recent interview in New Zealand. She stated that she is enjoying her job as Secretary of State too much to get into the presidential race. Clinton also sought to defend President Barack Obama and put the Democrats' losses in the midterms in perspective, acknowledging that it was a big defeat but not out of the pattern of historical political elections. She also expressed hope that the United States is ready for a female president and mentioned that she is not planning to run for the top office through 2016. In a separate interview, Clinton defused speculation about another presidential run by reiterating her previous statement that she rules out standing for top office, including in 2016. The news comes as Clinton wraps up her nearly two-week Asia-Pacific tour, which includes a stop in Australia, where she will meet with Prime Minister Julia Gillard.
 50%|█████     | 20/40 [04:23<04:11, 12.56s/it]2024-12-22 03:06:29,074 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:29,555 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:29,555 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1901])
2024-12-22 03:06:29,623 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:06:30,418 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:30,418 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 796])
2024-12-22 03:06:30,443 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:06:33,505 - [Process 3/5] - INFO - res.shape is :torch.Size([178])
results:

A nine-year-old boy named Kieran was born without ears and had a pioneering surgery to create new ones from his own ribs at Great Ormond Street Hospital in London. The surgery took six hours and involved harvesting cartilage from Kieran's ribs and shaping it into frameworks for his ears. The procedure was successful, and Kieran is now able to hear for the first time in his life. The surgery has given him a huge boost in confidence, and he is no longer bullied at school. The hospital does about 40 of these operations each year, and the researchers are working on growing new ear frameworks from a child's own stem cells. This technology is still in its early stages, but it has the potential to revolutionize the field of reconstructive surgery.
 48%|████▊     | 19/40 [04:27<04:52, 13.94s/it]2024-12-22 03:06:33,627 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:35,745 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:35,746 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1204])
2024-12-22 03:06:35,787 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:06:37,229 - [Process 1/5] - INFO - res.shape is :torch.Size([150])
results:Amy Schumer has announced that she will not be participating in any Super Bowl LIII commercials this year, citing her support for Colin Kaepernick's protest against racism and police brutality. She has also revealed that she is expecting her first child. Schumer has been a vocal advocate for political causes, including gun control and supporting the transgender community. She has also been detained and arrested for protesting Brett Kavanaugh's nomination to the Supreme Court. Schumer has teamed up with Jessica Yellin to promote her #NewsNotNoise campaign, which aims to educate people about what they need to know in the upcoming midterm elections.
 52%|█████▎    | 21/40 [04:31<03:33, 11.26s/it]2024-12-22 03:06:37,363 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:37,571 - [Process 4/5] - INFO - res.shape is :torch.Size([263])
results:

Several explosions occurred in Damascus, Syria on Wednesday, targeting the army command headquarters and killing several people, including a correspondent for Iran's Press TV. The blasts were the latest in a series of attacks on security sites and symbols of regime power in a bid to turn the tide in the fighting. The Syrian government has denied any military commanders or personnel were hurt in the explosions, but witnesses reported heavy gunfire and panicked soldiers shooting in the air. The opposition has increasingly targeted security sites and symbols of regime power in recent months, and the conflict has intensified and become a regional calamity with global ramifications. The U.N. Secretary-General Ban Ki-moon demanded international action to stop the war in Syria, but Russia and China have vetoed three Western-backed resolutions aimed at pressuring Assad to end the violence. The U.S. President Obama pledged support for Syrians trying to oust Assad, who has been accused of massacring his own people. The death toll in the conflict has risen to nearly 30,000 people, and the situation remains dire with no end in sight.
 55%|█████▌    | 22/40 [04:31<03:54, 13.05s/it]2024-12-22 03:06:37,685 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:39,103 - [Process 0/5] - INFO - res.shape is :torch.Size([239])
results:
The political crisis in Mali has escalated as the country's neighbors impose economic sanctions on the country following a recent coup. The UN Security Council is set to hold an emergency meeting on the crisis. The Tuareg rebels have taken control of the northern part of the country, including the key towns of Gao, Kidal, and Timbuktu. The rebels are divided into two groups, with one wanting independence for the northern Tuareg homelands and the other wanting to remain part of Mali but impose Sharia law. The junta in power has been accused of mishandling the recent insurgency in the north, leading to the coup. The international community has expressed concern over the situation, with West African leaders calling for the restoration of democracy in Mali. The country's economy is also facing difficulties as the borders have been closed and the country's bank account has been frozen. The situation in Mali has raised concerns over the potential for a wider conflict in the region, with the UN warning that the crisis could have significant consequences for the whole of the Sahel.
 65%|██████▌   | 26/40 [04:33<02:32, 10.89s/it]2024-12-22 03:06:39,162 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:39,614 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:39,615 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1069])
2024-12-22 03:06:39,653 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:06:39,736 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:39,736 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1348])
2024-12-22 03:06:39,781 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:06:39,936 - [Process 2/5] - INFO - res.shape is :torch.Size([263])
results:


This week's news includes updates on several celebrities, including Britney Spears, Elvis Duran, and Beyonce. Britney Spears' handlers have been instructed to limit questions during radio interviews to three specific topics: her Las Vegas residency, new single, and album. However, during a recent interview, the deejay had difficulty getting Spears to answer questions, leading to speculation that she may not be fully aware of what's happening around her. Meanwhile, Elvis Duran and Eric Trump are walking across the Brooklyn Bridge to raise awareness of St. Jude Children’s Research Hospital’s fight against pediatric cancer. Beyonce has Instagrammed yet another flawless portrait of herself holding Blue Ivy, but neither of their faces is visible. Jamie Spears, Britney's father, has asked for more money to be paid to him for overseeing her life and career, as he has been doing for over five years. He claims that his rent has increased and he needs more money to cover office expenses. Charlie Hunnam is reportedly devastated over the backlash he received for pulling out of the movie "50 Shades of Grey."
 55%|█████▌    | 22/40 [04:34<03:48, 12.70s/it]2024-12-22 03:06:40,171 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:40,796 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:40,796 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 995])
2024-12-22 03:06:40,830 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:06:43,700 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:43,700 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2090])
2024-12-22 03:06:43,768 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:06:44,361 - [Process 3/5] - INFO - res.shape is :torch.Size([196])
results:

The news passages highlight the ongoing tensions between President Obama and President-elect Donald Trump during the transition of power. Trump has accused Obama of putting "roadblocks" in his way and not being helpful during the transition process. Obama has defended his administration's actions and claimed that he could have won the election if he had run against Trump. Trump has also expressed frustration with the Obama administration's actions on Israel, particularly the UN Security Council resolution condemning Israeli settlements in the West Bank and East Jerusalem. Trump has also criticized the Iran nuclear deal and the lack of veto by the Obama administration. Despite the tensions, Trump has said that he and Obama had a "nice conversation" and that the transition is going smoothly. However, Trump's spokesperson, Sean Spicer, declined to elaborate on Trump's tweets about roadblocks.
 50%|█████     | 20/40 [04:38<04:20, 13.02s/it]2024-12-22 03:06:44,422 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:45,436 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:45,436 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 542])
2024-12-22 03:06:45,455 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:06:47,168 - [Process 0/5] - INFO - res.shape is :torch.Size([183])
results:Lumos, a non-profit organization, has been working to end the practice of institutionalizing children worldwide. The organization has been successful in reducing the number of children in institutions in several countries, including Moldova, the Czech Republic, and Bulgaria. The reasons for institutionalization include poverty, disability, and discrimination, and the majority of children in institutions have parents who could care for them. The organization has also seen a reduction in the number of new admissions to institutions and an increase in the number of foster carers. The issue of institutionalization is particularly poignant during the holiday season, as it resembles the darkest of Grimms' fairytales. The organization is calling for donors to support its efforts to keep children with their families and end the practice of institutionalization by 2050.
 68%|██████▊   | 27/40 [04:41<02:10, 10.04s/it]2024-12-22 03:06:47,236 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:49,154 - [Process 4/5] - INFO - res.shape is :torch.Size([212])
results:
A decomposing body was found inside a column at a supermarket in Lancaster, California, leading to an investigation by the Los Angeles County Sheriff's Department. The body was believed to be that of a suspect who escaped from a traffic stop on Monday night and led deputies on a chase before crashing and running into the store. Witnesses reported seeing the suspect run into the store and climb onto the roof, but deputies were unable to locate him. The body was found after a store manager smelled a strong odor coming from the column and called a plumber to investigate. The body was identified as that of a 35-year-old man, and detectives believe he may have gotten stuck inside the column while trying to hide from deputies. The incident has left many in shock, including seasoned investigators, who described it as a "strange development." Anyone with information is encouraged to contact the Los Angeles County Sheriff's Department's Homicide Bureau.
 57%|█████▊    | 23/40 [04:43<03:34, 12.61s/it]2024-12-22 03:06:49,305 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:49,305 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1189])
2024-12-22 03:06:49,346 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:49,347 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:06:51,201 - [Process 3/5] - INFO - res.shape is :torch.Size([134])
results:In China, a man named Guan Quanzhi was attacked by a wild panda while working on his farmland. He required seven hours of surgery to treat his injuries and may need further operations. He sued local government officials and was awarded 520,000 yuan in compensation. This incident highlights the dangers of interacting with wild animals, even those considered cuddly and harmless like pandas. Meanwhile, the number of wild giant pandas in China has increased by nearly 17% over the past decade, according to government data, with conservation measures credited for the growth.
 52%|█████▎    | 21/40 [04:45<03:32, 11.16s/it]2024-12-22 03:06:51,366 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:51,488 - [Process 1/5] - INFO - res.shape is :torch.Size([272])
results:


Animal rights group Compassion Over Killing has released a new undercover video exposing mistreatment of chickens at Tyson Foods facilities in Virginia. The video shows workers at Tyson's chicken breeding farms in Mecklenburg, Lunenburg, and Buckingham counties abusing and mistreating chickens. The footage shows workers stomping, kicking, and suffocating chickens, as well as wringing their necks and running them over with forklifts. Tyson has fired ten workers caught on camera and has retraining all of its live poultry workers on the company's animal welfare policies. The company has also discontinued the practice of beak modification at two of the facilities. Tyson has called the behavior "inexcusable" and has promised to hold accountable those responsible. Animal Control officials in Mecklenburg and Buckingham counties are investigating the allegations, but no charges have been filed yet. Compassion Over Killing has also released a statement condemning the abuse and calling for Tyson to be held accountable. The group has also questioned whether there is any way to humanely kill and eat animals.
 55%|█████▌    | 22/40 [04:45<03:38, 12.16s/it]2024-12-22 03:06:51,623 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:52,111 - [Process 2/5] - INFO - res.shape is :torch.Size([183])
results:
Israel has continued its military operation in the Gaza Strip, with a focus on destroying Hamas' tunnel network. The operation has resulted in the deaths of over 1,360 Palestinians and 56 Israelis, according to latest reports. The UN has condemned Israel's shelling of a UN school in Jebaliya, which killed at least 17 people, as "outrageous" and "unjustifiable". The US and UN have called for an immediate humanitarian ceasefire, while Israel has said it will not agree to any offer that does not allow the military to complete its mission. In the meantime, Palestinian civilians continue to suffer, with many seeking shelter in UN schools. The situation remains dire, with ongoing violence and a lack of progress towards a lasting ceasefire.
 57%|█████▊    | 23/40 [04:46<03:33, 12.54s/it]2024-12-22 03:06:52,181 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:06:52,553 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:52,553 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1915])
2024-12-22 03:06:52,619 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:06:53,470 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:53,470 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 758])
2024-12-22 03:06:53,496 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:06:53,860 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:53,860 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1256])
2024-12-22 03:06:53,908 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:06:54,569 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:06:54,569 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1839])
2024-12-22 03:06:54,632 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:07:00,947 - [Process 0/5] - INFO - res.shape is :torch.Size([324])
results:

The U.S. and Pakistan have agreed to work together in any future actions against "high value targets" in Pakistan, according to a joint statement issued after a meeting between U.S. Secretary of State John Kerry and Pakistani leaders. This comes after the U.S. raid that killed Osama bin Laden in Abbottabad, Pakistan, which has strained the relationship between the two countries. Kerry has also announced that Pakistan has agreed to return the tail of a U.S. military helicopter that was damaged during the raid. The U.S. and Pakistan have also agreed to increase intelligence sharing and joint intelligence operations, but details of the agreed steps are unclear. The U.S. uses Pakistan as a key supply route for troops in Afghanistan, and Pakistan's assistance is crucial to a potential negotiated end to the war there. However, anti-U.S. sentiment is widespread in Pakistan, and some U.S. officials say cutting ties with Islamabad could destabilize the government and lead to Pakistan's nuclear arsenal falling into the hands of Islamists. Pakistan has also agreed to explore how increased cooperation on joint operations and intelligence sharing can maximize efforts to defeat common enemies. Meanwhile, a U.S. drone strike in Pakistan's tribal region has killed seven people, and a senior police official in Karachi has said that militants related to al-Qaeda may have carried out an attack on a Saudi diplomat in the city.
 70%|███████   | 28/40 [04:55<02:13, 11.17s/it]2024-12-22 03:07:01,061 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:01,988 - [Process 2/5] - INFO - res.shape is :torch.Size([189])
results:Snap Inc. has made its camera-enabled sunglasses, Spectacles, available for purchase online in the US through its website. Previously, they were only available through vending machines and a pop-up store in New York City. The glasses can record 10 to 30-second videos and are priced at $130. Snap is also selling charging cases and charging cords for $49.99 and $9.99 respectively. The company is expanding its distribution strategy to include online sales, but will still have pop-up vending machines around the country. This move comes after Snap's initial public offering (IPO) filing revealed that Spectacles have not generated significant revenue for the company. Despite this, Snap is optimistic about the potential of Spectacles and plans to continue to make them more readily available.
 60%|██████    | 24/40 [04:56<03:07, 11.74s/it]2024-12-22 03:07:02,057 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:02,566 - [Process 4/5] - INFO - res.shape is :torch.Size([221])
results:The upcoming Sochi Winter Olympics have raised concerns about security due to recent terrorist threats in the region. The US and other countries have taken measures to ensure the safety of their athletes, including hiring private security firms and working with the Russian authorities. The US Olympic Committee has declined to discuss the details of their security plans, but some teams have hired private security firms to provide additional protection. The Russian government has implemented extensive security measures, including deploying 40,000 police officers and 70,000 soldiers to provide security during the games. Despite these measures, some nations have expressed concerns about the safety of their athletes and have taken additional precautions, such as purchasing insurance for medical emergencies and evacuations. The US has also sent FBI agents to Sochi and Moscow to assist with security. The Russian government has raised the possibility of the US sharing bomb-detecting technology with them, but senior defense officials have said that it is unlikely that this technology could be in place in time.
 60%|██████    | 24/40 [04:56<03:25, 12.85s/it]2024-12-22 03:07:02,733 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:03,255 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:03,256 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 661])
2024-12-22 03:07:03,279 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:07:04,585 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:04,585 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2039])
2024-12-22 03:07:04,657 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:07:05,589 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:05,590 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1667])
2024-12-22 03:07:05,648 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:07:06,142 - [Process 3/5] - INFO - res.shape is :torch.Size([258])
results:A 9-year-old girl named Carlie Trent went missing on May 4th after being signed out of her elementary school by her uncle, Gary Simpson. Simpson has a criminal history and has been accused of kidnapping Carlie. An Amber Alert has been issued, and the TBI is offering a $10,000 reward for any information leading to Carlie's safe return. Authorities believe Simpson may have taken Carlie to a remote area or campground, and they have received several tips about possible sightings in 11 states. The TBI has also released new images of Simpson. The mother of Carlie, Shannon Trent, has expressed her concern for her daughter's safety and urged Simpson to come forward. The TBI has reached out to state park officials and private campgrounds in Tennessee, North Carolina, Virginia, and Kentucky, and is asking property owners in rural areas to be on the lookout for any suspicious activity. Anyone with information is asked to call the TBI at 1-800-TBI-FIND or the Rogersville Police Department at (423) 272-7555.
 55%|█████▌    | 22/40 [05:00<03:41, 12.30s/it]2024-12-22 03:07:06,254 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:08,238 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:08,238 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1069])
2024-12-22 03:07:08,278 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:07:10,167 - [Process 1/5] - INFO - res.shape is :torch.Size([363])
results:

The news passages highlight several recent events in the US political landscape. In the first passage, Donald Trump's campaign manager, Michael Cohen, appeared on CNN to defend the campaign's recent changes, including the appointment of new leaders Kellyanne Conway and Stephen Bannon. When anchor Brianna Keilar questioned the campaign's performance and suggested that they were "down" in the polls, Cohen responded with the phrase "says who?" several times. This exchange has sparked a trend on Twitter, with many users responding to Cohen's tweets with the same phrase.

In the second passage, Cohen appeared on CNN again to further defend the campaign's changes and to address polls showing Trump trailing Hillary Clinton. Cohen argued that the campaign was not "down" and that the hiring of Conway and Bannon was a positive move. When Keilar pressed him on the campaign's performance, Cohen repeated the phrase "says who?" again.

In the third passage, it is reported that Trump's campaign is struggling in the polls, with most polls showing him behind Clinton. However, a Monmouth University poll found that Trump leads Clinton in Indiana by 11 points. The article also mentions that Trump's running mate, Mike Pence, is from Indiana and that the state generally votes Republican.

Overall, the news passages suggest that the Trump campaign is facing challenges in the polls and that Cohen is defending the campaign's recent changes and strategy. The exchange on CNN has become a viral moment, with many users responding to Cohen's tweets with the phrase "says who?"
 57%|█████▊    | 23/40 [05:04<04:00, 14.12s/it]2024-12-22 03:07:10,344 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:13,175 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:13,175 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1639])
2024-12-22 03:07:13,234 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:07:13,391 - [Process 4/5] - INFO - res.shape is :torch.Size([175])
results:Robert Durst, a real estate heir and convicted felon, has been sentenced to 7 years and 1 month in prison on a weapons charge in Louisiana. He is also facing a murder charge in California related to the death of a friend in 20000. Durst has pleaded guilty to the gun charge and is expected to be transferred to California within 4-6 weeks to face the murder charge. Durst has maintained his innocence in both cases and his attorney has requested that he serve his time in California due to his poor health. The HBO documentary "The Jinx" renewed interest in Durst's past crimes, including the disappearance of his first wife in 1982. Durst's wealthy family runs 1 World Trade Center in New York.
 62%|██████▎   | 25/40 [05:07<03:03, 12.24s/it]2024-12-22 03:07:13,605 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:14,580 - [Process 2/5] - INFO - res.shape is :torch.Size([261])
results:
Zeta Beta Tau Fraternity is suing a group of University of Albany students connected with a 2014 hazing death for using its name. The group, which is not recognized by the fraternity or the university, claimed to be a part of the Epsilon Gamma chapter of Zeta Beta Tau, but the fraternity says it has no affiliation with the rogue group. The fraternity is retaining counsel for the purpose of bringing a lawsuit against the individuals involved, claiming they falsely represented themselves and damaged the fraternity's reputation. The men allegedly involved in the incident are not, nor have never been, members of Zeta Beta Tau Fraternity. Zeta Beta Tau was founded in 1898 in New York City and has chapters on more than 90 campuses across North America, with over 130,000 undergraduate and alumni brothers worldwide. The fraternity's symbols, slogans, and materials are protected by copyright and trademark law, and anyone found to be engaged in unauthorized use may be subject to legal action.
 62%|██████▎   | 25/40 [05:08<02:59, 12.00s/it]2024-12-22 03:07:14,799 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:17,174 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:17,174 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2015])
2024-12-22 03:07:17,245 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:07:18,349 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:18,349 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 03:07:18,420 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:07:19,231 - [Process 0/5] - INFO - res.shape is :torch.Size([361])
results:


The news passages highlight the assassination of Kim Jong-nam, the half-brother of North Korea's leader Kim Jong-un, who was killed by a highly toxic nerve agent, VX, at Kuala Lumpur airport. Malaysian authorities have confirmed that the toxicology report shows the presence of VX nerve agent in Kim's eyes and face, and one of the two women accused of carrying out the attack fell ill afterward. The use of VX raises serious questions about public safety, as the nerve agent can take days or even weeks to evaporate and can contaminate a large area. North Korea has denied any involvement in the assassination and accused Malaysia of having "sinister" purposes. The case has escalated into a diplomatic crisis, with international speculation pointing to Pyongyang as the perpetrator. Experts suggest that it would have taken only a tiny amount of VX to kill Kim, and the fact that the two women accused of carrying out the attack showed no ill effects raises questions about how they were exposed to the nerve agent. The use of VX also raises concerns about the safety of airports and other public areas, as the substance can remain lethal for a long period of time. The case has also highlighted North Korea's alleged chemical weapons capabilities, with estimates suggesting that the country has up to 12,000 tons of chemical weapons, including VX. The incident has led to a turnaround in relations between Malaysia and North Korea, with Malaysia seeking to decontaminate the airport and areas the suspects are known to have visited.
 72%|███████▎  | 29/40 [05:13<02:26, 13.30s/it]2024-12-22 03:07:19,303 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:20,753 - [Process 3/5] - INFO - res.shape is :torch.Size([289])
results:CBS News has undergone a major overhaul of its morning show, "The Early Show," replacing three of its co-hosts, Maggie Rodriguez, Harry Smith, and Dave Price, with new anchors Chris Wragge and Erica Hill. The new team will start on January 3rd. The move comes as a surprise, as there had been little indication that changes were afoot, and the new team has been tested on Saturday mornings. The network is also replacing weather forecaster Dave Price with Marysol Castro, formerly of ABC's "Good Morning America" weekend edition, and adding Jeff Glor as the regular news reader. CBS News President Sean McManus said he won't be expecting any immediate ratings changes, but he believes the new team will help the show grow. The move is seen as an attempt to freshen up the show, which has consistently ranked behind NBC's "Today" show and ABC's "Good Morning America" in the ratings. Harry Smith will become the primary substitute anchor for Katie Couric's evening newscast, "Face the Nation," and "Sunday Morning." Maggie Rodriguez and Dave Price will be given new roles at the network. The new team will have to work together to break the show out of third place in the ratings.
 57%|█████▊    | 23/40 [05:15<03:40, 12.99s/it]2024-12-22 03:07:20,957 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:21,509 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:21,510 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1285])
2024-12-22 03:07:21,551 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:07:22,374 - [Process 1/5] - INFO - res.shape is :torch.Size([211])
results:

A South Carolina school resource officer, Ben Fields, was fired after a video showed him forcefully removing a student from class. The incident occurred at Spring Valley High School in Columbia, where Fields had been a school resource officer for seven years. The video showed Fields flipping and tossing a black female student across a classroom, causing her to suffer injuries. The incident sparked outrage and calls for accountability, with civil rights groups praising the swift action taken against Fields. The FBI and Justice Department have opened investigations into the incident to determine if the student's civil rights were violated. The school district has also promised to review its training procedures for school resource officers. Fields' termination comes after previous allegations of excessive force and racial bias, and he has been the subject of racial bias and excessive force allegations in the past. The incident has raised concerns about the use of force by school resource officers and the need for better training and accountability.
 60%|██████    | 24/40 [05:16<03:36, 13.54s/it]2024-12-22 03:07:22,501 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:24,551 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:24,551 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1999])
2024-12-22 03:07:24,573 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:24,574 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1240])
2024-12-22 03:07:24,615 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:07:24,623 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:07:28,264 - [Process 4/5] - INFO - res.shape is :torch.Size([245])
results:The Benghazi Select Committee has released its final report, which reveals that the Obama administration knew the Benghazi attack was a terrorist attack from the beginning but led the public to believe it was caused by a protest and a video. The report includes new details from witness accounts, emails, and intelligence reports. The administration's statements on the attack were found to be inconsistent with the facts on the ground, and they ignored evidence that contradicted their narrative. The report also highlights the differences between the public and private statements made by Secretary Clinton and other officials. The committee's findings are at odds with the White House's claims that the attack was caused by a protest and a video. The report concludes that the administration's handling of the attack was influenced by political considerations and a desire to avoid blaming the video. The committee's findings have been met with criticism from Republicans, who argue that the report is biased and fails to provide a comprehensive account of the attack. The report has also been praised by Democrats, who say it provides a fair and balanced assessment of the events surrounding the Benghazi attack.
 65%|██████▌   | 26/40 [05:22<03:02, 13.03s/it]2024-12-22 03:07:28,358 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:29,203 - [Process 0/5] - INFO - res.shape is :torch.Size([213])
results:
A 15-year-old girl was gang-raped and beaten at Richmond High School in California after leaving a homecoming dance. The attack occurred in a dimly lit area near benches, and up to 24 people witnessed the crime without notifying police. Two suspects, a 19-year-old man and a 15-year-old boy, were arrested, and a $20,000 reward is being offered for information leading to the conviction of any of the assailants. The attack has raised questions about campus security, as the school recently approved surveillance cameras after a series of violent crimes. The police are still investigating and believe that as many as 10 assailants committed the crime. The victim is hospitalized with non-life-threatening injuries, and the police are offering counseling services to students. The incident has shocked the community and raised concerns about the safety of students in the school.
 75%|███████▌  | 30/40 [05:23<02:03, 12.30s/it]2024-12-22 03:07:29,319 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:29,970 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:29,970 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 903])
2024-12-22 03:07:30,001 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:07:31,496 - [Process 2/5] - INFO - res.shape is :torch.Size([288])
results:The news articles discuss the film "Detroit" directed by Kathryn Bigelow, which tells the story of the Algiers Motel siege in Detroit during the 1967 riots. The film recreates the events of the siege, which resulted in the deaths of three unarmed black men and the brutal treatment of the survivors by the police. The articles highlight the film's immersive and visceral depiction of the events, as well as the controversy surrounding the film's portrayal of the police and their actions. Some critics have praised the film for its unflinching portrayal of the violence and injustice, while others have criticized it for being too one-sided and not providing a full picture of the events. The film's use of animation and documentary footage is also noted, as well as the performances of the actors, particularly Will Poulter and John Boyega. The articles also mention the historical context of the events depicted in the film, including the systemic oppression and mistreatment of black people in the United States, and the role of the police in perpetuating these injustices. Overall, the articles suggest that "Detroit" is a powerful and thought-provoking film that sheds light on a dark chapter in American history.
 65%|██████▌   | 26/40 [05:25<03:08, 13.47s/it]2024-12-22 03:07:31,624 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:32,870 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:32,870 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1963])
2024-12-22 03:07:32,946 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:07:33,685 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:33,686 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1241])
2024-12-22 03:07:33,727 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:07:36,630 - [Process 1/5] - INFO - res.shape is :torch.Size([275])
results:

Gloria Chapman, the wife of Mark David Chapman, who was sentenced to 20 years to life in prison for murdering John Lennon in 1980, is hoping for her husband's freedom after 38 years behind bars. Chapman has been denied parole 10 times, but her latest parole hearing is scheduled for the week starting August 20. Gloria has been praying for Chapman's release and has remained faithful to him despite his crime. She revealed that Chapman had told her he was going to murder Lennon two months before the incident and had even cut her off completely for a year. However, she believed he never carried out the threat due to his love for her. Gloria and Chapman have only been allowed to spend 44 hours together each year during conjugal visits in a caravan. Despite the objections of Lennon's widow Yoko Ono, Gloria remains hopeful about Chapman's parole. Meanwhile, the Internet Archive is working to fix broken links on the web by archiving pages and preserving links to archived pages. The organization has launched a campaign called "No More 404" to archive pages as they are created and preserve links to archived pages.
 62%|██████▎   | 25/40 [05:31<03:26, 13.76s/it]2024-12-22 03:07:36,692 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:37,707 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:37,708 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 544])
2024-12-22 03:07:37,727 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:07:39,644 - [Process 4/5] - INFO - res.shape is :torch.Size([210])
results:
The latest news in the Hannah Graham case involves a mental health expert being brought in to examine Matthew before his trial. The defense is blaming police omission in the investigation, while the prosecution is seeking the death penalty. A former detective testified in court that a police bloodhound detected traces of Graham's scent inside Matthew's car and apartment, as well as at various locations around Charlottesville. The dog also found her scent on the passenger door of Matthew's vehicle and near a dumpster at his apartment complex. The defense unsuccessfully challenged the search warrant in court, with the judge ruling that there was probable cause for the searches. Matthew is set to appear in court for a pretrial hearing on Monday. Additionally, a man named Jesse LeRoy Matthew Jr. is set to appear in court for a pretrial hearing on Monday, charged with killing two college students in Virginia. Prosecutors are seeking the death penalty.
 68%|██████▊   | 27/40 [05:34<02:42, 12.54s/it]2024-12-22 03:07:39,762 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:41,357 - [Process 3/5] - INFO - res.shape is :torch.Size([369])
results:George H.W. Bush, the 41st President of the United States, passed away at the age of 94 on November 30, 2018. He was the last president to serve in World War II and led the US to a swift and decisive victory in the Gulf War. Bush was also known for his diplomatic and military web that trapped Iraqi leader Saddam Hussein and led to a lightning war. He was the father of former President George W. Bush and was described as a devoted husband, father, and public servant. Bush's legacy includes his leadership during the Cold War, his support for the Americans with Disabilities Act, and his efforts to advance literacy and education.

Bush's presidency was marked by several significant events, including the fall of the Berlin Wall in 1989 and the Gulf War in 1991. He also faced challenges such as high unemployment and a sluggish economy, which led to his loss in the 1992 election. Despite this, Bush remained a respected figure in politics and was remembered for his kindness and grace.

In addition to his political achievements, Bush was known for his personal interests and hobbies, including flying and baseball. He was also a skilled athlete and played baseball at Yale University. Bush's funeral was held on December 6, 2018, and was attended by numerous dignitaries and politicians.

Overall, George H.W. Bush's legacy is one of leadership, diplomacy, and public service. He will be remembered as a dedicated and devoted president who played a significant role in shaping American history.
 60%|██████    | 24/40 [05:35<04:04, 15.28s/it]2024-12-22 03:07:41,564 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:41,827 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:41,828 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1150])
2024-12-22 03:07:41,870 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:07:43,164 - [Process 1/5] - INFO - res.shape is :torch.Size([120])
results:The Pentagon's official Twitter account erroneously retweeted a call for President Trump to resign, amidst allegations of sexual misconduct against him and other public figures. The retweet was quickly deleted, and the Pentagon spokesperson issued a statement saying that the account's operator made the mistake. The White House has faced questions about the president's own behavior, with the press secretary declining to comment on the allegations. Additionally, Twitter has announced that it will now allow users to add location information to their tweets, including precise location.
 65%|██████▌   | 26/40 [05:37<02:42, 11.59s/it]2024-12-22 03:07:43,317 - [Process 2/5] - INFO - res.shape is :torch.Size([216])
results:Elon Musk, the CEO of Tesla, has proposed taking the company private in a deal valued at over $70 billion. The board of directors has confirmed that it will consider the proposal, but questions remain about the funding and timing of the deal. The board members stated that Musk had opened a discussion about taking the company private last week, and the talks included how being private could better serve Tesla's long-term interests and addressed the funding for this to occur. The board members did not include Musk, his brother, or two other board members in the statement. The announcement has caused wild swings in the company's stock price, and regulators are likely to be interested in the issue. The deal would be the largest in history, and the board will likely form a committee to evaluate the bid. The statement did not address whether the board was satisfied with Musk's method or timing for announcing the proposal, and questions remain about the funding and structure of the deal.
 68%|██████▊   | 27/40 [05:37<02:48, 12.98s/it]2024-12-22 03:07:43,371 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:43,381 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:44,377 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:44,377 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 552])
2024-12-22 03:07:44,397 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:07:45,161 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:45,161 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2043])
2024-12-22 03:07:45,232 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:07:46,945 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:46,945 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 03:07:47,017 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:07:48,792 - [Process 0/5] - INFO - res.shape is :torch.Size([392])
results:The news articles discuss the release of a memo by the House Intelligence Committee Chairman Devin Nunes regarding the FBI's use of the Foreign Intelligence Surveillance Act (FISA) during the 2016 presidential election. The memo alleges that the FBI abused its surveillance authority and that the initial FISA application notes Steele was working for a US. person, but does not name Fusion GPS and principal Glenn Simpson, who was paid by a US. law firm representing the DNC. The memo also mentions that Steele was terminated as an FBI source for what the FBI de?nes as the most serious of violations?an unauthorized disclosure to the media of his relationship with the FBI. Additionally, the memo notes that Steele's numerous encounters with the media violated the cardinal rule of source handling?maintaining con?dentiality?and demonstrated that Steele had become a less than reliable source for the FBI. The memo also reveals that the FBI's review process included input from the Of?ce of the Director of National Intelligence and the Department of Justice, and that the President has authorized the declassi?cation of the memo. Furthermore, the memo mentions that the FISA application relied on Steele's past record of credible reporting on other unrelated matters, but ignored or concealed his anti?Trump ?nancial and ideological motivations. Finally, the memo notes that the FBI's use of the Steele dossier was in its ?infancy? at the time of the initial Page FISA application, and that the Ohrs? relationship with Steele and Fusion GPS was inexplicably concealed from the FISC.
 78%|███████▊  | 31/40 [05:43<02:10, 14.49s/it]2024-12-22 03:07:48,916 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:52,414 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:52,414 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1955])
2024-12-22 03:07:52,487 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:07:54,833 - [Process 2/5] - INFO - res.shape is :torch.Size([240])
results:Serena Williams, a former world No. 1 tennis player, has been sidelined due to a severe strain of her medial and lateral ligaments in her right ankle, as well as torn ligaments in her ankle joint. She is currently wearing a cast and faces a recovery time of at least four to six weeks. Meanwhile, 23-year-old Ryan Sweeting, a former University of Florida player, won his first ATP title last week in the U.S. Clay Court Championship in Houston. The victory moved him up from No. 93 to No. 71 in the rankings. The U.S. Fed Cup team will rely on 18-year-old Christina McHale and 19-year-old Melanie Oudin in a world group playoff this weekend in Stuttgart, Germany, despite the absence of Bethanie Mattek-Sands due to a hip injury. Finally, Alexa Internet has been donating their crawl data to the Internet Archive since 1996, which is added to the Wayback Machine after an embargo period.
 70%|███████   | 28/40 [05:49<02:30, 12.54s/it]2024-12-22 03:07:54,961 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:56,017 - [Process 4/5] - INFO - res.shape is :torch.Size([322])
results:

Shannon Egeland, a convicted white-collar criminal, was sentenced to 13 years and 3 months in prison for orchestrating a staged shooting to delay his prison sentence. In June 2013, Egeland pleaded guilty to theft for shoplifting from Fred Meyer. In 2014, he staged a shooting with his 17-year-old son, Rylan, to delay his prison sentence. Egeland had his son shoot him in the legs with a shotgun, and then called 911 to report an assault. However, the shooting backfired when Egeland's left leg had to be amputated due to the injury. Egeland had previously pleaded guilty to fraud charges for his role in an extensive mortgage-fraud scheme, and was sentenced to 10 years in prison. Prosecutors sought a five-year sentence for the insurance scam, while Egeland's lawyer urged a two-year sentence. The judge extended Egeland's sentence by three years and 10 months due to his guilty plea in the insurance scam and his interstate failure to pay child support. The judge also ordered Egeland to undergo mental health treatment and counseling while in prison. The case highlights the unthinkable actions of Egeland, who put his son through emotional distress and manipulated the legal system for his own gain.
 70%|███████   | 28/40 [05:50<02:44, 13.69s/it]2024-12-22 03:07:56,172 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:56,994 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:56,994 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1237])
2024-12-22 03:07:57,036 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:07:57,262 - [Process 1/5] - INFO - res.shape is :torch.Size([227])
results:The Lovely Bones, a film adaptation of Alice Sebold's best-selling novel, has been released in theaters nationwide. The movie, directed by Peter Jackson, follows the story of a 14-year-old girl named Susie Salmon who is murdered and watches from the "in-between" as her family and killer move on with their lives. The film has received mixed reviews, with some praising Saoirse Ronan's performance as Susie and Stanley Tucci's portrayal of the killer, George Harvey. However, others have criticized the film's focus on the killer and its use of computer-generated effects in depicting heaven. The movie has also been noted for its departure from the novel's tone and complexity, with some arguing that it is too lighthearted and others finding it too heavy-handed. Overall, the film has been described as a "creepy" and "dreamy" adaptation that struggles to capture the book's power and emotion.
 68%|██████▊   | 27/40 [05:51<02:40, 12.34s/it]2024-12-22 03:07:57,384 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:58,668 - [Process 0/5] - INFO - res.shape is :torch.Size([154])
results:This news article discusses several movies related to the horror genre, including "The Wolfman," "An American Werewolf in London," and "Wolf." The articles highlight the remakes of classic horror movies, the use of special effects, and the casting of actors in these films. The articles also mention the original scripts and the directors of these movies. The overall tone of the articles is one of disappointment and frustration with the modern remakes of classic horror movies, which are seen as lacking in atmosphere and emotion compared to the originals. The articles also mention the use of gore and violence in these movies, which are seen as unnecessary and detracting from the overall quality of the films.
 80%|████████  | 32/40 [05:53<01:44, 13.10s/it]2024-12-22 03:07:58,748 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:58,951 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:58,952 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1518])
2024-12-22 03:07:58,961 - [Process 3/5] - INFO - res.shape is :torch.Size([303])
results:O.J. Simpson, the former football star and actor, may be released from prison as early as October 1st after a parole hearing on Thursday. Simpson was convicted of armed robbery and kidnapping in 2007 and is currently serving a 9-33 year sentence. The parole hearing is being held after Simpson has been deemed eligible for parole. The hearing will take place in Nevada and will be teleconferenced to the board in Carson City. The public is eagerly awaiting the outcome of the hearing as Simpson was acquitted of the murders of his ex-wife and her friend in 1995, but was found liable for their deaths in a civil trial. Simpson's net worth is estimated to be around $250,000, and he has been a model prisoner according to his previous parole hearing in 2013. The hearing will consider confidential information, including a pre-sentence investigation, a parole hearing report, a risk assessment, and letters of support or opposition. If granted parole, Simpson will be able to live on his NFL pension, which is estimated to be around $25,000 per month. The hearing is expected to be a spectacle, as Simpson's release from prison will renew the rabid curiosity surrounding his case.
 62%|██████▎   | 25/40 [05:53<03:59, 15.97s/it]2024-12-22 03:07:59,006 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:07:59,103 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:07:59,466 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:07:59,466 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1180])
2024-12-22 03:07:59,509 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:08:01,255 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:01,256 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1546])
2024-12-22 03:08:01,303 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:08:01,620 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:01,621 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1413])
2024-12-22 03:08:01,672 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:08:08,148 - [Process 0/5] - INFO - res.shape is :torch.Size([184])
results:
The Northern Ireland Minister of Justice, David Ford, has announced that Sinn Féin leader Gerry Adams has been arrested and questioned by police regarding the 1972 murder of Jean McConville. Adams was arrested under the Terrorism Act and taken to Antrim police station for questioning. The arrest has sparked controversy, with some political parties rejecting claims that the timing was politically motivated. Adams has maintained his innocence and stated that he supports the PSNI. The case against Adams is based on an interview he allegedly gave to researchers at Boston College in the US. The McConville family has called for justice and vowed to continue their fight for justice. Meanwhile, other political parties have rejected claims that the arrest was politically motivated and have expressed support for the PSNI. The investigation into the McConville murder is ongoing.
 82%|████████▎ | 33/40 [06:02<01:24, 12.02s/it]2024-12-22 03:08:08,197 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:08,849 - [Process 1/5] - INFO - res.shape is :torch.Size([206])
results:


The news passages describe a video that has been released showing a soldier executing a civilian lying face down in the street with a shot to the back of his head. The video was released by Mexican media outlets and shows an encounter between soldiers and suspected criminals in Palmarito, Puebla State, where four soldiers and six suspected criminals died. The video appears to show a soldier shooting the detained man in the head at point-blank range, and the Mexican military has been implicated in extrajudicial killings before, including the killing of 22 suspects at a warehouse in the Mexico state community of Tlatlaya. The defense ministry has urged an investigation of the video, and human rights groups have called for a swift and thorough investigation. The incident has sparked a new scandal over the army's use of extrajudicial force, and Mexico is facing a serious crisis of violence and impunity, according to a recent report.
 70%|███████   | 28/40 [06:03<02:25, 12.12s/it]2024-12-22 03:08:08,950 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:09,789 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:09,789 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 894])
2024-12-22 03:08:09,820 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:08:10,657 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:10,658 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 973])
2024-12-22 03:08:10,692 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:08:12,916 - [Process 4/5] - INFO - res.shape is :torch.Size([313])
results:


Tesla has confirmed that the fatal crash last week involving a Tesla Model X in Mountain View, California, did occur while the vehicle was on Autopilot. The National Transportation Safety Board (NTSB) is still investigating the accident, which took place on March 23rd and killed the driver, Walter Huang. According to Tesla, Autopilot was engaged with the adaptive cruise control follow-distance set to minimum when the crash occurred. The driver had received several visual and one audible hands-on warning earlier in the drive, but the driver's hands were not detected on the wheel for six seconds prior to the collision. The crash attenuator, a highway safety barrier, had been crushed in a prior accident without being replaced, which may have contributed to the severity of the accident. Huang's brother, Will, told San Francisco ABC station KGO that Walter had complained "seven to 10 times the car would swivel toward that same exact barrier during Autopilot." The California Department of Transportation (CalTrans) has been notified of the issue and is responsible for maintaining the barriers. The NTSB has recovered both the restraint control module and infotainment module from the vehicle, and Tesla has stated that Autopilot "unequivocally makes the world safer for the vehicle occupants, pedestrians, and cyclists."
 72%|███████▎  | 29/40 [06:07<02:41, 14.65s/it]2024-12-22 03:08:13,053 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:15,389 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:15,389 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1317])
2024-12-22 03:08:15,436 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:08:15,842 - [Process 3/5] - INFO - res.shape is :torch.Size([319])
results:Jamaica has decriminalized the possession of small amounts of marijuana, known locally as ganja, after passing an act through parliament. The new law makes possession of up to 2 ounces of marijuana a non-arrestable offense, resulting in a fine instead. This move comes after years of dialogue about the culturally entrenched drug and emboldened by changes to drug laws in US states. The legislation also establishes a licensing agency to regulate a lawful medical marijuana industry, paving the way for the development of a lawful industry for medical ganja and industrial hemp. The new law also permits the use of marijuana for religious, medical, scientific, and therapeutic purposes. This decision is seen as a victory for religious freedom, as Rastafarians can now freely use marijuana for sacramental purposes for the first time on the island. The law also decriminalizes the cultivation of five or fewer plants on any premises. Tourists who are prescribed medical marijuana abroad will soon be able to apply for permits authorizing them to legally buy small amounts of "ganja" in Jamaica. The move by Jamaican lawmakers adds to an international trend of easing restrictions on marijuana for medical or personal use, with more than 20 US states allowing some form of medical marijuana and Uruguay being the first nation to create a legal marijuana market.
 65%|██████▌   | 26/40 [06:10<03:47, 16.25s/it]2024-12-22 03:08:16,053 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:17,078 - [Process 2/5] - INFO - res.shape is :torch.Size([458])
results:

In a recent article published on Live Science, astronomer Jason Hooper proposes that advanced alien civilizations may harvest stars to power their civilizations, rather than just relying on their own planet's resources. According to Hooper, the universe is expanding at an accelerating rate, which means that distant stars will eventually become inaccessible to us. To prevent this, advanced civilizations may capture and harness the energy of these stars before they become unreachable. Hooper suggests that this could be done by creating a swarm of satellites in a spherical shell around the star, which would capture and transmit the star's energy back to the civilization. This idea is supported by science fiction authors such as Olaf Stapledon, who described similar structures in his novel "Star Maker." Additionally, physicist Freeman Dyson proposed similar structures in a scientific paper in 1960, and astrophysicist Nikolai Kardashev proposed that advanced civilizations would expand throughout the universe to harness energy. Hooper's idea is that advanced civilizations would recognize the grim reality of universal expansion and race against dark energy to harness as much energy as possible before the stars become inaccessible.

In another article, it is mentioned that WARCZone is a collection of outsider-uploaded WARCs that are contributed to the Internet Archive but may or may not be ingested into the Wayback Machine. These WARCs are being kept in this location for reference and clarity for the Wayback Team, while also being accessible to the general public.

Finally, in an article about advanced technologies, it is mentioned that Dyson spheres and other mega-structures appear frequently in science fiction. In his 1970 novel "Ringworld," Larry Niven features a ring-shaped artificial structure girdling an alien star. In the 1992 “Star Trek: The Next Generation” episode "Relics," the starship Enterprise encounters a Dyson sphere in the form of a rigid shell surrounding a star.
 72%|███████▎  | 29/40 [06:11<02:49, 15.45s/it]2024-12-22 03:08:17,149 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:17,647 - [Process 0/5] - INFO - res.shape is :torch.Size([230])
results:

Two women, Lizandra Cosme and Susana D. Gonzalez, have been charged in connection to an incident at a daycare center in Chicago where five toddlers were injured with hot glue. Cosme, 32, of Chicago, was arrested and charged with five counts of aggravated battery of a child under 13 years old causing great bodily harm. Gonzalez, 27, of Melrose Park, was charged with five misdemeanor counts of causing the circumstances of child endangerment. The incident occurred on December 1 at the Children's Learning Place in the 3100 block of West Fullerton, where Cosme, who had been working with children for 12 years, allegedly caused injury to five 2-year-old children - three girls and two boys. The daycare center has been working with investigators and has terminated two employees. The state Department of Children and Family Services is also investigating the incident. Cosme was denied bail and is scheduled to return to court on December 26.
 85%|████████▌ | 34/40 [06:12<01:07, 11.26s/it]2024-12-22 03:08:17,738 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:18,156 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:18,156 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 528])
2024-12-22 03:08:18,175 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:08:19,655 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:19,655 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-22 03:08:19,727 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:08:20,532 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:20,532 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1706])
2024-12-22 03:08:20,588 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:08:20,684 - [Process 1/5] - INFO - res.shape is :torch.Size([226])
results:Two young men, Connor Cummings and Dimitri Oliveras, aged 20 and 18 respectively, died after falling from the Four Seasons Hotel in Manhattan while attempting to take photos from the rooftop. Connor Cummings, a University of Massachusetts Amherst student, fell nine stories after climbing scaffolding to access the rooftop, while Oliveras, a high school student from Dover, New Jersey, climbed down the ladder and fled the scene before returning to tell hotel security what happened. The incident occurred around 10:40 pm on Wednesday night, and both men were pronounced dead at the scene. Police believe the scaffolding was wet due to the rain, which may have contributed to the accident. The hotel's management has released a statement expressing their condolences and assuring that they are cooperating with the investigation. The incident has raised concerns about the safety of the hotel's rooftop area, and police are investigating the cause of the accident.
 72%|███████▎  | 29/40 [06:15<02:12, 12.03s/it]2024-12-22 03:08:20,816 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:22,866 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:22,866 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1231])
2024-12-22 03:08:22,908 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:08:24,308 - [Process 4/5] - INFO - res.shape is :torch.Size([208])
results:Robin Williams, a renowned comedian and actor, passed away on Monday at the age of 63. He was known for his unnerving impact on his fans, particularly those in their mid-to-late 20s and early 30s. Williams was remembered for his edgy comedy and dramatic turns, and his ability to connect with his audience. He was a versatile actor who could play both comic roles and dark ones, and his legacy can be measured by the imprints his movies left on rec-room carpets. Williams was known for his generosity and willingness to please his audience, and he went to great lengths to make his performances enjoyable for children. Despite his success in adult movies, Williams' comedy was what defined him, and he was always excited to work on projects that his audience cared about. Williams' death has left a lasting impact on his fans, and his legacy will be remembered for years to come.
 75%|███████▌  | 30/40 [06:18<02:16, 13.67s/it]2024-12-22 03:08:24,405 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:26,017 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:26,017 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 936])
2024-12-22 03:08:26,048 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:08:26,365 - [Process 2/5] - INFO - res.shape is :torch.Size([190])
results:On Tuesday evening, a small explosion occurred in Disneyland's Toontown section, forcing the evacuation of the area for two hours. The explosion was caused by a dry ice bomb, which was placed in a trash can by a 22-year-old Disneyland employee, Christian Barnes. Barnes has been arrested on suspicion of possessing a destructive device and is cooperating with investigators. The incident is believed to be an isolated incident with unanticipated impacts, and Disneyland is working closely with authorities. In unrelated news, a 22-year-old man has been arrested in connection with the torture death of a 10-year-old boy in Los Angeles County, and two police officers who took a car on a joyride to Las Vegas have blamed the incident on a "momentary" lapse.
 75%|███████▌  | 30/40 [06:20<02:16, 13.60s/it]2024-12-22 03:08:26,484 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:28,511 - [Process 3/5] - INFO - res.shape is :torch.Size([194])
results:

AirAsia Flight 8501, an Airbus A320 aircraft, crashed into the Java Sea on December 28, 2014, while flying from Surabaya to Singapore with 162 people on board. The plane was not authorized to fly on the day of the crash, and the pilot had requested permission to climb to a higher altitude due to threatening clouds. Search teams have found four large objects believed to be from the plane, including an emergency exit door and slide, and a backpack with food and a camera inside. The cause of the crash remains unknown, but bad weather is suspected. The plane's black boxes, which contain the flight data and cockpit voice recorders, have yet to be found. Indonesian authorities have suspended AirAsia's Surabaya to Singapore flights and are investigating the incident.
 68%|██████▊   | 27/40 [06:22<03:17, 15.17s/it]2024-12-22 03:08:28,528 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:28,560 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:28,561 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1191])
2024-12-22 03:08:28,602 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:08:28,791 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:28,791 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 113])
2024-12-22 03:08:28,795 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:08:29,557 - [Process 0/5] - INFO - res.shape is :torch.Size([234])
results:

Wendy Davis, the Democratic candidate for Texas governor, has revealed in her upcoming memoir that she had an abortion in the 1990s after discovering that the fetus had a severe brain abnormality. She also writes about ending an earlier ectopic pregnancy. The memoir, "Forgetting to Be Afraid," goes on sale Tuesday and includes details about her personal life, including her relationship with her husband and parents. The book's title comes from a quote by Lady Bird Johnson, and the story of her abortion has been published ahead of the book's release. The news has sparked a reaction from anti-abortion groups, who expressed sympathy for her difficult decision but emphasized the value of life. Pro-choice advocates have praised Davis' "unwavering courage" and shared her story to highlight the importance of access to reproductive healthcare. The revelation of her abortion has also been seen as a personal and humanizing moment for Davis, who has been running for governor against Republican Attorney General Greg Abbott.
 88%|████████▊ | 35/40 [06:23<00:57, 11.46s/it]2024-12-22 03:08:29,621 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:31,638 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:31,638 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1149])
2024-12-22 03:08:31,678 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:08:32,685 - [Process 4/5] - INFO - res.shape is :torch.Size([146])
results:A man named Markus Austin was shot and killed outside his apartment in Montpelier, Vermont on Sunday morning. The suspect, Jayveon Caballero, is still at large and may have fled the state. Caballero is wanted for second-degree murder and an arrest warrant has been issued. Police believe Caballero and Austin were involved in an altercation outside a bar in Barre before the shooting. Caballero's girlfriend, Desiree Cary, was taken into custody on unrelated drug charges. Police are searching for Caballero and are asking anyone with information to contact them. This is the first murder in Montpelier in almost a century.
 78%|███████▊  | 31/40 [06:27<01:48, 12.08s/it]2024-12-22 03:08:32,749 - [Process 3/5] - INFO - res.shape is :torch.Size([87])
results:The Internet Archive is working to fix broken links on the web by creating archives of pages and their references. The organization is crawling supported sites to preserve links and provide a link to the original version of a page. This will help maintain the integrity of the web and ensure that important information is not lost. The project aims to fix all broken links on the web and provide a valuable resource for researchers and historians.
 70%|███████   | 28/40 [06:27<02:22, 11.89s/it]2024-12-22 03:08:32,808 - [Process 1/5] - INFO - res.shape is :torch.Size([228])
results:A group of truckers are planning to protest in Washington D.C. by circling the beltway three lanes deep to call attention to their frustration with national political leaders and their alleged violation of the Constitution. The protest, called "Truckers Ride for the Constitution," aims to arrest members of Congress for their alleged crimes and to restore the U.S. Constitution. The group claims that President Obama committed "treason" by allegedly funneling weapons to al-Qaida-linked rebels in Syria and that members of Congress who support arming Syrian rebels are accessories to the crime. The truckers are also complaining about fuel prices, environmental regulations, and the perceived deterioration of Fourth Amendment rights. The protest is scheduled to take place on October 11th and has gained attention on social media with close to 50,000 likes on Facebook. However, the group has since clarified that they will not be arresting members of Congress as previously planned.
 75%|███████▌  | 30/40 [06:27<02:00, 12.06s/it]2024-12-22 03:08:32,846 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:32,865 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:33,018 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:34,911 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:34,911 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1140])
2024-12-22 03:08:34,952 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:08:35,578 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:35,579 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1611])
2024-12-22 03:08:35,633 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:08:35,953 - [Process 2/5] - INFO - res.shape is :torch.Size([174])
results:Steve Vogelsang, a former Winnipeg broadcast personality and Red River College journalism instructor, has been charged with two counts of robbery in Alberta. He is accused of robbing two banks in Medicine Hat, Alta. without a disguise and demanding money. Police arrested him at a hotel in the same area as the banks. Vogelsang has a history of financial difficulties and legal issues, including a protection order filed against him by a former student he had been dating. The student alleges Vogelsang harassed her despite police asking him to leave her alone. Vogelsang is fighting the allegations. He had been a journalism instructor at Red River College from 2002 to 2011 and had worked at CTV Winnipeg.
 78%|███████▊  | 31/40 [06:30<01:51, 12.40s/it]2024-12-22 03:08:36,049 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:36,599 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:36,599 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2016])
2024-12-22 03:08:36,672 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:08:37,837 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:37,837 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1009])
2024-12-22 03:08:37,875 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:08:38,977 - [Process 0/5] - INFO - res.shape is :torch.Size([206])
results:

Police in Pittsfield, Massachusetts have arrested two men, Joseph VanWert and Randy Lambach, for running a prostitution ring out of an apartment in a senior living facility. The men, aged 65 and 45 respectively, were held on human trafficking charges and are accused of recruiting and transporting women, some of whom were as young as 15, to engage in sexual acts for money. The investigation began after police received complaints of increased prostitution in the area and identified Lambach as the owner of a blue station wagon used to shuttle the women. Witnesses identified Lambach as the man known as "Randy" and one woman claimed he had threatened to turn her in to the police if she stopped working for him. Lambach allegedly kept a significant cut of the women's earnings or paid them in drugs instead of cash. The investigation is ongoing and additional charges may be coming.
 90%|█████████ | 36/40 [06:33<00:43, 10.84s/it]2024-12-22 03:08:39,030 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:40,607 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:40,607 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 939])
2024-12-22 03:08:40,639 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:08:45,379 - [Process 1/5] - INFO - res.shape is :torch.Size([193])
results:

The Secret Service has been embroiled in a scandal involving 11 agents who brought prostitutes to their hotel rooms during a presidential visit to Colombia. Two high-level supervisors, David Chaney and Greg Stokes, have been fired, and three others have resigned. The scandal has raised questions of accountability and personal conduct within the agency, particularly given the involvement of two supervisors. The agents involved were part of the advance team preparing for President Obama's arrival, and the incident has become an election-year embarrassment. The Secret Service has moved quickly to quell the scandal, but more firings are expected. The agency's director has expressed confidence in the investigation, but lawmakers are calling for more resignations and firings. The scandal has also raised questions about the agency's culture and how it will be addressed.
 78%|███████▊  | 31/40 [06:39<01:49, 12.21s/it]2024-12-22 03:08:45,471 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:45,513 - [Process 2/5] - INFO - res.shape is :torch.Size([170])
results:

There are several news passages provided, each covering a different topic. Passage 1 discusses a baby girl named Oakleigh who has been hospitalized due to a severe infection caused by a "super strain" of E. coli. The infection has caused complications such as kidney reflux, and the baby is scheduled for surgery to fix the issue. Passage 2 provides information on Pseudomonas infections, explaining that they are caused by a common type of bacteria called Pseudomonas aeruginosa and can be difficult to treat due to antibiotic resistance. The article highlights that people in hospitals are at a higher risk of getting these infections and that burn victims and those with puncture wounds are also at risk.
 80%|████████  | 32/40 [06:39<01:32, 11.55s/it]2024-12-22 03:08:45,650 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:46,849 - [Process 3/5] - INFO - res.shape is :torch.Size([264])
results:

The Nonhuman Rights Project has filed a lawsuit against Stony Brook University, seeking to free two chimpanzees, Hercules and Leo, who are being held at the university's primate research center. The lawsuit argues that the chimpanzees are being held unlawfully and should be released to a sanctuary. The case has been granted a writ of habeas corpus, which allows the chimpanzees to challenge their detention in court. The university has not commented on the case, but the judge has ordered a hearing to determine whether the chimpanzees should be released. The case has implications for the rights of other animals, as it could set a precedent for their legal personhood. The Supreme Court has agreed to hear the case, which could lead to a major ruling on the rights of animals. The Military Commissions Act of 2006 has been criticized for revoking the right to habeas corpus for animals detained at Guantanamo Bay, and the case could challenge this act. The Nonhuman Rights Project plans to file more cases in the future, including one involving a captive elephant, and is working to defend the right to habeas corpus for all animals.
 72%|███████▎  | 29/40 [06:41<02:18, 12.56s/it]2024-12-22 03:08:47,024 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:47,024 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 845])
2024-12-22 03:08:47,056 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:08:47,065 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:48,021 - [Process 4/5] - INFO - res.shape is :torch.Size([274])
results:

The upcoming lunar eclipse on January 31, 2018, will be a rare event as it will be a total lunar eclipse of a Blue Moon, which occurs only every 152 years. The eclipse will be visible from Australia, western USA, and parts of Asia, and will be the first total lunar eclipse of a Blue Moon since 1866. The Virtual Telescope Project will be live streaming the event, and viewers can also catch a glimpse of the Super Blue Moon rising above Rome. The eclipse will begin at 11:48am UTC and will last for 77 minutes. To photograph the event, it is recommended to use a DSLR camera and adjust the white balance to account for the sunlight reflected off the moon. Additionally, there will be a Blood Moon lunar eclipse on January 31, which is a rare event that occurs when the Earth's shadow causes the moon to appear red. The best time to view the Blood Moon will be from eastern Asia across the Pacific to western North America. Finally, the Virtual Telescope Project will be hosting a fundraiser to support their work, and viewers can donate to receive limited edition images of the comet 46P/Wirtanen.
 80%|████████  | 32/40 [06:42<01:44, 13.06s/it]2024-12-22 03:08:48,120 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:48,211 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:48,211 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1444])
2024-12-22 03:08:48,265 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:08:49,848 - [Process 0/5] - INFO - res.shape is :torch.Size([268])
results:The recent archaeological discoveries in Wiltshire, England have shed light on the history of the area, revealing new insights into the lives of ancient people. Excavations at Marden Henge, a massive prehistoric monument, have uncovered a 4,000-year-old Bronze Age skeleton, believed to be that of a child, along with various artifacts such as flint arrowheads, decorated pottery, and a Roman brooch. The discovery was made during an open day event, which attracted visitors from around the country. The site, which is believed to be one of the best-preserved from the period, has also revealed evidence of a large Roman complex, including the foundations of an impressive barn. The excavations are part of a three-year project aimed at gaining a better understanding of the people who lived in the area surrounding Stonehenge. The project has also uncovered evidence of a massive earthwork enclosure, which is believed to have been used for status and power display. The discoveries have highlighted the importance of the area, which is linked to the River Avon and other nearby prehistoric monuments, and have provided new insights into the lives of ancient people.
 92%|█████████▎| 37/40 [06:44<00:32, 10.85s/it]2024-12-22 03:08:49,880 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:49,930 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:49,931 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1029])
2024-12-22 03:08:49,966 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:08:50,659 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:50,659 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 03:08:50,731 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:08:50,768 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:50,768 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 498])
2024-12-22 03:08:50,785 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:08:54,660 - [Process 1/5] - INFO - res.shape is :torch.Size([173])
results:

The news passages highlight the fall from grace of several high-profile individuals in various fields. In the first passage, it is revealed that New Jersey Governor Chris Christie has the lowest approval ratings of any governor in the country, with his reputation sullied by the Bridgegate scandal. In the second passage, it is mentioned that Anthony Scaramucci, the former White House Communications Director, was fired after less than two weeks in the position due to his tumultuous tenure. The article also highlights the fall of Tiger Woods, the professional golfer, who has not won a major tournament since 2008 and was recently arrested for DUI. The summary concludes by mentioning that these individuals are examples of how power and fame can be fleeting, and how circumstances can change rapidly.
 80%|████████  | 32/40 [06:49<01:30, 11.33s/it]2024-12-22 03:08:54,869 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:56,939 - [Process 0/5] - INFO - res.shape is :torch.Size([183])
results:

The news passages provide information on various financial services, including loans and credit checks. The first passage highlights the ease of obtaining loans with flexible repayment plans and the importance of planning ahead to avoid high interest rates. The second passage focuses on car enthusiasts and their love for beautiful automobiles, inviting readers to share their favorite cars.

In the loan section, it is emphasized that emergency loans come with a price tag, and it is essential to understand the terms and conditions before applying. The passage also mentions that drug abuse can affect approval for credit checks. Additionally, it is possible to get loans without a credit check, which can be a convenient option for those with poor credit history.

In conclusion, the news passages provide information on various financial services and the importance of planning ahead to avoid financial difficulties.
 95%|█████████▌| 38/40 [06:51<00:19,  9.72s/it]2024-12-22 03:08:57,052 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:57,222 - [Process 2/5] - INFO - res.shape is :torch.Size([205])
results:
A man urinated in the Mt. Tabor Reservoir in Portland, Oregon, causing the city to dump 38 million gallons of water down the drain. The man, Dallas Swonger, claims he did not pee in the drinking water, but rather on a wall near the reservoir. Despite this, city officials are proceeding with plans to flush the water, citing public health concerns. The incident has sparked debate and second-guessing among city leaders and residents, with some questioning the decision to dump so much water. The man's friend, Trey McDaniel, has come forward to defend Swonger, saying he is a good person who made a mistake. The Water Bureau is working with the Multnomah County District Attorney to determine if any criminal charges will be filed. The incident has also raised questions about the city's water treatment process and the safety of Portland's drinking water.
 82%|████████▎ | 33/40 [06:51<01:21, 11.60s/it]2024-12-22 03:08:57,407 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:08:58,446 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:08:58,446 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2014])
2024-12-22 03:08:58,518 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:09:00,575 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:00,576 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1991])
2024-12-22 03:09:00,647 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:09:00,898 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:00,899 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1934])
2024-12-22 03:09:00,970 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:09:01,568 - [Process 3/5] - INFO - res.shape is :torch.Size([236])
results:

A shooting incident occurred on the Las Vegas Strip early Thursday morning, leaving three people dead and at least six injured. The incident started with a dispute at a nearby hotel, which escalated into a high-speed chase involving two vehicles, a silver-gray Maserati and a black Range Rover SUV. Gunshots were fired from the SUV, hitting the Maserati, which then crashed into a taxi cab that burst into flames. The taxi driver and passenger were killed, and the male driver of the Maserati also died. The passenger in the Maserati was among the injured. The SUV fled the scene, and police have contacted authorities in three neighboring states to be on the lookout for the vehicle. The incident has raised concerns about safety on the Strip, but officials have assured visitors that they are doing everything possible to keep them safe. In other news, a convention-goer was unable to cross the Strip several hours after the attack, and tourists were advised to be cautious due to recent incidents of violence in the area.
 75%|███████▌  | 30/40 [06:55<02:12, 13.20s/it]2024-12-22 03:09:01,736 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:03,167 - [Process 4/5] - INFO - res.shape is :torch.Size([297])
results:

The US ambassador to Syria, Robert Ford, visited the city of Hama on Thursday to show support for the protesters and to make it clear that the US stands with those Syrians who are expressing their right to speak for change. This visit came after a series of peaceful demonstrations and a fierce crackdown by security forces in the area, which resulted in many arrests and deaths. The Syrian government accused the US of "interfering" in its affairs, but the US state department said that the embassy notified the Syrian government that an embassy team was traveling to Hama. The Syrian Observatory for Human Rights reported that an estimated 1,000 Syrians have fled Hama in fear of another military crackdown, and at least 23 civilians have been killed and many arrested since Tuesday. Activists say that the protests in Hama have become a symbolic city of opposition against the current government, and that the city has been the site of violent crackdowns in the past. Meanwhile, reports of the biggest crowd in Syria so far gathered in Hama on Friday, with over 500,000 protesters flooding through the city, and at least 13 protesters killed across the country, including five in Homs, two in Midan, and six in Dumair.
 82%|████████▎ | 33/40 [06:57<01:35, 13.69s/it]2024-12-22 03:09:03,212 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:03,958 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:03,958 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 385])
2024-12-22 03:09:03,972 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:09:04,584 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:04,584 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1669])
2024-12-22 03:09:04,641 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:09:09,715 - [Process 1/5] - INFO - res.shape is :torch.Size([249])
results:The United States government is facing a potential shutdown on Monday night as Congress is working through the weekend to resolve a standoff over funding for federal agencies. House Speaker John Boehner has moved to delay the Affordable Care Act, and Senate Majority Leader Harry Reid is pushing for a hard line with House Republicans. The Washington region could be particularly affected by a shutdown, with 700,000 jobs at risk and $200 million in daily losses to the local economy. Democrats are pushing for a short-term funding bill to avoid a shutdown, while Republicans are seeking changes to the Affordable Care Act. The White House and congressional Democrats have rejected a bill approved by the House to delay the health law, and a filibuster is expected in the Senate. The possible shutdown comes at a time when the region's economy is weak, with 26,500 net jobs lost in August due to sequestration cuts. A shutdown could have real costs, including delayed or canceled government services, furloughed workers, and a potential impact on the national economy.
 82%|████████▎ | 33/40 [07:04<01:27, 12.45s/it]2024-12-22 03:09:10,014 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:12,154 - [Process 2/5] - INFO - res.shape is :torch.Size([248])
results:
The two Koreas will march together under one flag at the Winter Olympics opening ceremony in Pyeongchang, marking a significant thaw in relations between the two countries. North Korea will send a delegation of over 400 athletes, including 230 cheerleaders, 140 artists, and 30 Taekwondo players, to the games, and will also participate in joint training with South Korea. The two countries have agreed to form a joint women's ice hockey team, and North Korea will also send a 150-strong delegation to the Paralympics. The move comes after a series of high-level talks between the two countries, which have been technically at war since the 1950s. However, Japan has warned against being too optimistic about North Korea's "charm offensive," and the US has expressed concern that the North may be using the Olympics to buy time to pursue its weapons program. Despite these concerns, the International Olympic Committee (IOC) has approved the joint Korean team, and the two countries will work together to ensure a successful and peaceful Olympics.
 85%|████████▌ | 34/40 [07:06<01:15, 12.60s/it]2024-12-22 03:09:12,193 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:12,200 - [Process 4/5] - INFO - res.shape is :torch.Size([183])
results:In the latest news, the Federal Bureau of Investigation (FBI) is looking into an incident of disinformation aimed at potentially disrupting the midterm elections. The Democratic National Committee (DNC) has also been targeted by hackers, with an attempt to breach its voter database. The DNC contacted the FBI after detecting a fake login page created to gather usernames and passwords. The White House cybersecurity coordinator, Rob Joyce, has warned that the focus of cyber attacks has shifted from stealing secrets to imposing national power. Microsoft has also detected hackers tied to Russian intelligence targeting the Senate and conservative think tanks in the US, creating fake websites to gather login details. With the midterm elections approaching, political groups and government organizations are on high alert for new hacks and foreign meddling.
 85%|████████▌ | 34/40 [07:06<01:13, 12.29s/it]2024-12-22 03:09:12,351 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:12,487 - [Process 0/5] - INFO - res.shape is :torch.Size([294])
results:

BP shareholders have rejected the company's executive pay policy for 2015, with 59% of investors voting against it. This is the first time a major British company has lost a vote on executive pay since the "shareholder spring" in 2012. The rejection comes despite BP Chief Executive Bob Dudley receiving a 20% increase in his total remuneration package for 2015, to $19.6 million. The company's pay policy is subject to a binding shareholder vote every three years, and the current policy was approved in 2014 with 96% of the vote. However, the current vote suggests that shareholders are not satisfied with the company's performance, particularly given the $5.2 billion loss the company incurred in 2015. Other major energy companies have also seen a decrease in their CEO's pay, with the median pay of an S&P 500 energy company CEO falling by 1.8% in 2015. BP's Chairman, Carl-Henric Svanberg, defended the pay package, saying it was based on "exceptional" company performance during a difficult year. However, the board may need to reconsider the package in light of the shareholder vote.
 98%|█████████▊| 39/40 [07:06<00:11, 11.47s/it]2024-12-22 03:09:12,540 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:12,829 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:12,829 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 354])
2024-12-22 03:09:12,841 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:09:12,942 - [Process 3/5] - INFO - res.shape is :torch.Size([184])
results:
Autumn Veatch, a 16-year-old girl from Bellingham, Washington, survived a small-plane crash in the North Cascades mountains after hiking for two days to find help. She was rescued by a motorist who drove her to safety and was taken to a hospital for treatment of minor injuries. The plane, carrying Autumn and her step-grandparents, Leland and Sharon Bowman, went missing on Saturday afternoon and was believed to have crashed in rugged terrain. Search efforts were ongoing, but no sign of the plane or its occupants was found until Autumn emerged from the wilderness. Autumn's father, David Veatch, was overjoyed to hear of his daughter's survival and thanked searchers for their efforts. The fate of the Bowmans remains unknown.
 78%|███████▊  | 31/40 [07:07<01:53, 12.65s/it]2024-12-22 03:09:13,036 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:13,597 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:13,597 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2027])
2024-12-22 03:09:13,669 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:09:14,124 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:14,124 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 901])
2024-12-22 03:09:14,155 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
2024-12-22 03:09:14,753 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:14,753 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 990])
2024-12-22 03:09:14,788 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:09:15,085 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:15,086 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1493])
2024-12-22 03:09:15,140 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:09:19,582 - [Process 2/5] - INFO - res.shape is :torch.Size([152])
results:A man named James Dirk Crudup, 41, was arrested by the Secret Service after he climbed over the White House fence on Tuesday. He was charged with unlawful entry and contempt of court. According to the Secret Service, Crudup had previously been ordered to stay away from the White House due to past incidents. The incident occurred around 9 p.m. and the area was locked down as a precaution. A backpack found near the fence was being examined. This is not the first time an individual has attempted to enter the White House without authorization. On Sunday night, a 6-year-old girl reached the lawn by going through the fence.
 88%|████████▊ | 35/40 [07:13<00:55, 11.04s/it]2024-12-22 03:09:19,830 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:21,826 - [Process 0/5] - INFO - res.shape is :torch.Size([225])
results:

Peru's top investigator, Eusebio Felix, has been put on leave after he misled the country by saying he had caught a gang of serial killers who were murdering people to sell their fat. The investigator claimed that the gang, led by a man named Felix Murga, had murdered up to 60 people and sold their fat for thousands of dollars a liter to Italian cosmetics makers. However, police have now admitted that there is no evidence to support this claim and that the case was likely a tall story or a big fat lie. The police chief, Miguel Hidalgo, has apologized for the confusion and embarrassment caused by the investigation. The case has raised questions about the reliability of the police and their ability to investigate crimes effectively. Some anthropologists have suggested that the police's story was a deliberate attempt to play on people's fears and distract from the real issues facing Peru. The case has also highlighted the problem of drug-trafficking and violence in the region.
100%|██████████| 40/40 [07:16<00:00, 10.83s/it]100%|██████████| 40/40 [07:16<00:00, 10.91s/it]
2024-12-22 03:09:23,384 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:23,384 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2042])
2024-12-22 03:09:23,455 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:09:24,462 - [Process 1/5] - INFO - res.shape is :torch.Size([242])
results:
NASA has announced a new mission to send a spacecraft, the Parker Solar Probe, to explore the sun's outer atmosphere. The mission is named after astrophysicist Eugene Parker, who predicted the existence of the solar wind in 1958. The probe will travel closer to the sun than any other spacecraft before, reaching an orbit within 4m miles of the sun's surface. The mission aims to answer questions about how the sun heats its corona and accelerates the solar wind, and will provide insight into space weather and its impact on Earth. The probe will carry a chip with Parker's photos and a plate with his message to the sun. The mission is set to launch in summer 2018 and will take seven flybys of Venus to reach the sun. The probe will be protected from the sun's heat by a 4.5-inch thick carbon-composite solar shield. The mission will also pass through the origin of the solar particles with the highest energy. The findings will help answer questions about how stars work and improve space weather forecasting.
 85%|████████▌ | 34/40 [07:18<01:18, 13.14s/it]2024-12-22 03:09:24,567 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:26,097 - [Process 3/5] - INFO - res.shape is :torch.Size([250])
results:Gibson Guitar Corporation was raided by the Justice Department due to allegations of illegally sourcing wood from Madagascar. The company denies the allegations, claiming that all of its purchases from Madagascar have complied with U.S. and Malagasy law. The Lacey Act, which was amended in 2008, requires end users of endangered wood to certify the legality of their supply chain. The Environmental Investigation Agency has conducted independent investigations and concluded that Gibson knowingly imported tainted wood. The company's CEO, Henry Juszkiewicz, is concerned about the retroactive nature of the Lacey Act and the potential for the government to charge the company without providing evidence of wrongdoing. The guitar industry is grappling with the implications of the Lacey Act, which has led to a decrease in the importation of vintage instruments due to concerns about compliance. The U.S. Justice Department has not commented on the case, but a court motion filed in June asserts that Gibson's Madagascar ebony was contraband.
 80%|████████  | 32/40 [07:20<01:42, 12.81s/it]2024-12-22 03:09:26,280 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:26,281 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 988])
2024-12-22 03:09:26,315 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:09:26,336 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:26,522 - [Process 4/5] - INFO - res.shape is :torch.Size([250])
results:

Yesterday, three people were killed and over 170 injured in the Boston Marathon bombings. The victims included an 8-year-old boy named Martin Richard, who was watching the race with his family, and two others who have been identified so far, Krystle Campbell, 29, and a third victim whose identity has not been released pending family approval. The bombings occurred at the finish line of the marathon, and the blasts were so powerful that they could be heard and felt miles away. The FBI has launched an investigation and is working to identify the perpetrators. The city of Boston is in a state of shock and grief, and the community has come together to support the victims and their families. The President of the United States, Barack Obama, has also expressed his condolences and vowed to find those responsible for the attack. The city is taking precautions to ensure the safety of its citizens, including increasing security at public events and transportation hubs. The tragedy has also brought out a show of solidarity from around the world, with many people expressing their support for Boston and its residents.
 88%|████████▊ | 35/40 [07:20<01:04, 12.90s/it]2024-12-22 03:09:26,626 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:28,596 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:28,596 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1041])
2024-12-22 03:09:28,636 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:09:29,948 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:29,949 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2052])
2024-12-22 03:09:30,020 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:09:31,891 - [Process 2/5] - INFO - res.shape is :torch.Size([187])
results:
Jon Stewart started his career as a correspondent on The Daily Show in 19996, where he quickly became known for his satirical take on politics. In 19999, he became the host of the show, and his segment on the Straight Talk Express bus with John McCain was a breakthrough moment for the show. The Daily Show was initially met with resistance from McCain's staff, but Stewart's question about McCain's inconsistencies on pork-barrel politics caught them off guard. The show's success led to a book deal for Stewart, and he became a household name. However, the show's success also led to pressure to be more serious and political, which Stewart resisted. Despite this, the show continued to be a cultural touchstone and a platform for Stewart's satirical take on politics.
 90%|█████████ | 36/40 [07:26<00:45, 11.42s/it]2024-12-22 03:09:31,987 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:33,584 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:33,584 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 937])
2024-12-22 03:09:33,615 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:09:41,292 - [Process 3/5] - INFO - res.shape is :torch.Size([245])
results:

The situation in the Middle East has escalated significantly as Israel has authorized the mobilization of up to 75,000 reservists in preparation for a possible ground invasion of Gaza. This comes after a rocket fired from Gaza struck Jerusalem for the first time in decades, and Israel has been conducting airstrikes against Hamas targets in response to ongoing rocket attacks. Egypt has also been involved in attempts to broker a ceasefire, but a three-hour truce declared by Egyptian Prime Minister Hisham Kandil failed to take hold. The conflict has resulted in the deaths of at least 30 people in Gaza, including 13 militants and 16 civilians, and three Israelis. The Israeli military has been criticized for its response, with some arguing that it is not doing enough to protect civilians. The international community has also been involved in attempts to resolve the conflict, with the US and EU calling on Hamas to stop its rocket attacks. The situation remains tense and volatile, with fears of a larger conflict escalating.
 82%|████████▎ | 33/40 [07:35<01:34, 13.52s/it]2024-12-22 03:09:41,444 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:41,733 - [Process 4/5] - INFO - res.shape is :torch.Size([292])
results:

A new report by the University of Washington's Institute for Health Metrics and Evaluation has found that obesity is a growing problem worldwide, with 2.1 billion people, or 30% of the global population, being either obese or overweight. The report, which was published in the Lancet medical journal, found that obesity rates have increased in both developed and developing countries over the past 33 years, with the biggest rises occurring in the Middle East and North Africa, Central America, and the Pacific and Caribbean islands. The report also found that obesity is now a major risk factor for heart disease, stroke, diabetes, arthritis, and certain cancers, and that it is affecting people of all ages, incomes, and regions. The researchers noted that population-wide weight gains and income gains generally go hand in hand around the world, and that a major cause of obesity is that food has become cheap relative to income. The report highlighted that more than half of the world's obese live in just 10 countries, including the United States, China, India, Russia, Brazil, Mexico, Egypt, Germany, Pakistan, and Indonesia. The World Health Organization has launched a commission to study rising obesity among children worldwide and suggest solutions to address the problem.
 90%|█████████ | 36/40 [07:36<00:54, 13.59s/it]2024-12-22 03:09:41,791 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:41,909 - [Process 1/5] - INFO - res.shape is :torch.Size([351])
results:
The recent news passages reveal a massive data breach at Neiman Marcus, a high-end retailer, which was discovered in mid-December but not disclosed to the public until last week. The breach involved the theft of customer information, including credit and debit card data, and is believed to have been committed by a criminal group in Eastern Europe. The breach is similar to the one at Target, which occurred earlier in the holiday season, and both attacks are thought to have been carried out by the same group. The investigation into the Neiman Marcus breach is ongoing, and the company has apologized for not disclosing the breach sooner. The report from iSight, a computer firm hired to investigate the breach, details the type of software used in the attack and the regions where the software coding prowess exists. The breach is believed to have been carried out using a type of malware that was placed in the store's point-of-sale systems, and the operational sophistication of the compromise activity is considered to be high. The report also notes that the issue at Neiman Marcus went on for significantly longer than the widespread attack on Target, and that the data stolen is ripe for fraud. The company has faced criticism for not telling customers about the breach sooner, and some have questioned the decision not to disclose anything until after the holiday season. The report concludes that since this style of POS hacking can net big rewards for the cyber crooks with little risk, cardholders can expect more of this type of breach in the future.
 88%|████████▊ | 35/40 [07:36<01:12, 14.43s/it]2024-12-22 03:09:42,123 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:42,799 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:42,799 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 577])
2024-12-22 03:09:42,818 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:09:44,085 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:44,086 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1519])
2024-12-22 03:09:44,136 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:09:44,814 - [Process 2/5] - INFO - res.shape is :torch.Size([257])
results:A 9111 dispatcher in Johnston County, North Carolina, Tim Medlin, used a cell phone to text a woman who was locked in the trunk of a moving car after she was unable to make herself heard over the phone. Medlin was able to locate the woman's location with the help of a cell phone provider and police were able to rescue her. The woman's boyfriend, Nicholas Mattevi, was arrested and charged with false imprisonment. This incident highlights the importance of technology in emergency situations and the quick thinking of the 911 dispatcher in saving the woman's life. In another incident, a 911 dispatcher in Cumberland County, North Carolina, helped locate a woman who was locked in the trunk of a car. The dispatcher, Lt. Todd Joyce, was able to locate the woman's location with the help of Verizon Wireless and police were able to rescue her. The woman's boyfriend, Nicholas Mattevi, was arrested and charged with false imprisonment. These incidents demonstrate the importance of 911 dispatchers in emergency situations and their ability to use technology to locate and rescue those in need.
 92%|█████████▎| 37/40 [07:39<00:35, 11.87s/it]2024-12-22 03:09:44,839 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:45,185 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:45,185 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 165])
2024-12-22 03:09:45,191 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:09:45,702 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:45,702 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2016])
2024-12-22 03:09:45,774 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:09:50,688 - [Process 2/5] - INFO - res.shape is :torch.Size([123])
results:The Internet Archive has launched a new initiative to crawl and archive websites that have a "No More 404" page. This is an effort to preserve the content of these pages as they are created and link to the archived versions of pages that they reference. In a separate incident, a Modoc County Sheriff's Deputy was killed while responding to a disturbance call. The suspect, Jack Lee Breiner, engaged in a gunbattle with another deputy and was shot and wounded. He has been charged with murder and other crimes in connection with the killing.
 95%|█████████▌| 38/40 [07:45<00:20, 10.07s/it]2024-12-22 03:09:50,827 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:51,377 - [Process 4/5] - INFO - res.shape is :torch.Size([192])
results:This week's LOOK magazine is out now, and it's packed with exciting news! Tom Cruise has written an emotional letter to his ex-wife Katie Holmes, reportedly in an attempt to rekindle their friendship. The letter was written after 12 months apart, and sources say Holmes has been "genuinely touched" by Cruise's words. Meanwhile, Katie has been building a new life for herself in New York City with her daughter Suri, and sources close to the former couple say Cruise would take her back in a second but Holmes isn't interested. In other news, LOOK magazine has revealed that they've been donating their crawl data to the Internet Archive since 1996, and the data is now available on the Wayback Machine after an embargo period. This is a great opportunity for anyone interested in internet history!
 92%|█████████▎| 37/40 [07:45<00:37, 12.41s/it]2024-12-22 03:09:51,555 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:53,311 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:53,311 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1425])
2024-12-22 03:09:53,362 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:09:54,725 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:54,726 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1748])
2024-12-22 03:09:54,788 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:09:55,089 - [Process 3/5] - INFO - res.shape is :torch.Size([250])
results:

On June 3, Rachel Borch, a 21-year-old woman from Hope, Maine, was attacked by a rabid raccoon while running near her home. The raccoon lunged at her without warning and bit her thumb, causing her to push it into a nearby puddle of water to drown it. The Maine CDC confirmed that the raccoon tested positive for rabies. This incident highlights the importance of being aware of the risk of rabies in Maine, especially in wooded areas. According to the Maine CDC, there have been 20 animals, including raccoons, red foxes, and skunks, that have tested positive for rabies in 2017, which is more than the average number of cases in recent years. The CDC advises people to avoid contact with wild animals, especially if they are acting strangely, and to seek medical attention immediately if they are bitten or scratched by an animal. Borch received four rabies shots and is expected to receive her last injection this weekend. She is grateful to have survived the attack but is still shaken by the experience.
 85%|████████▌ | 34/40 [07:49<01:21, 13.60s/it]2024-12-22 03:09:55,231 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:09:57,655 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:09:57,655 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1392])
2024-12-22 03:09:57,704 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:10:01,883 - [Process 2/5] - INFO - res.shape is :torch.Size([188])
results:Lauren Bacall, the legendary actress, passed away on August 12th at the age of 89. She left behind an estate worth $26.6 million, which was equally divided among her three children. In her will, she set aside $10,000 for the care of her Papillon dog, Sophie, and named her son as the executor of her estate. Additionally, she left $250,000 to each of her six grandchildren and $15,000 and $20,000 to two of her employees. Bacall's estate includes her apartment in the famous Dakota building and its contents, which will be split between her children. The actress was known for her love of dogs and had a special bond with Sophie. Her will also included provisions for her beloved dog's care.
 98%|█████████▊| 39/40 [07:56<00:10, 10.41s/it]2024-12-22 03:10:01,934 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:02,266 - [Process 1/5] - INFO - res.shape is :torch.Size([363])
results:

In recent news, a lion named Mohawk was killed by rangers in Kenya after it strayed from a national park and attacked a man. This incident has sparked outrage among Kenyans, who are questioning why the lion was not tranquilized instead of being killed. The lion, which was 13 years old and considered a celebrity in the area, had escaped from the park earlier in the week and was cornered by rangers when it attacked the man. The Kenya Wildlife Service has defended the decision to kill the lion, stating that it was a last resort to protect public safety.

In other news, a lion named Sylvester has escaped from a national park in South Africa and is being hunted by wildlife officials. The lion, which is believed to be a male, is considered a threat to humans and livestock, and officials are considering various options for its capture or killing. This incident comes after a recent study found that the African lion population has declined by 50% in the past 20 years due to habitat loss, human-wildlife conflict, and other factors.

Additionally, there have been several incidents of lions escaping from national parks in Kenya in recent weeks, including one incident where a lion mauled a pedestrian before being captured. The Kenya Wildlife Service has stated that they are working to improve the fencing of national parks to prevent further escapes.

Overall, these incidents highlight the challenges faced in balancing the protection of wildlife with the needs of humans, particularly in areas where wildlife and human populations are increasingly overlapping.
 90%|█████████ | 36/40 [07:56<01:04, 16.21s/it]2024-12-22 03:10:02,348 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:02,809 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:02,810 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 493])
2024-12-22 03:10:02,826 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
2024-12-22 03:10:03,086 - [Process 4/5] - INFO - res.shape is :torch.Size([187])
results:

Barry and Honey Sherman, a billionaire couple, were found dead in their Toronto mansion on Friday, with police calling the deaths "suspicious." The couple was known for their philanthropy and generosity, and their death has left friends, colleagues, and the community in shock. The police have taken over the investigation, and while they have not confirmed whether it was a murder-suicide, they have said there were no signs of forced entry and no suspects at large. The couple's business, Apotex, has been a major player in the generic drug industry, and the Shermans were known for their dedication to their work and their community. The couple had a reputation for being kind and generous, and their death has been described as a "big loss" for the Canadian business community. The investigation into their deaths continues.
 95%|█████████▌| 38/40 [07:57<00:24, 12.20s/it]2024-12-22 03:10:03,291 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:03,729 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:03,730 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 769])
2024-12-22 03:10:03,757 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:10:06,858 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:06,859 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-22 03:10:06,930 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:10:08,637 - [Process 3/5] - INFO - res.shape is :torch.Size([253])
results:


NASA scientists have discovered that Mars was once a wet planet, with an ocean covering nearly half of its surface. The ocean was a mile deep and held 20 million cubic kilometers of water, which is more than the Arctic Ocean today. The water was lost over time due to the planet's thin atmosphere, and the remaining water is now locked in the Martian polar caps. The discovery was made by analyzing the water vapor in the Martian atmosphere using infrared telescopes. The study also found that the Martian water contains a higher proportion of deuterium than Earth's water, which suggests that Mars lost a vast amount of water in the past. The findings support the idea that life may have existed on Mars in the past, as a warm and wet environment would have been conducive to life. Additionally, a recent study found that Mars had a global ocean that covered the entire surface of the planet to a depth of 137 meters, which is similar to the depth of the Atlantic Ocean on Earth. The study also suggests that the Martian ocean was likely deeper in the northern hemisphere, which could have been a habitable environment for life.
 88%|████████▊ | 35/40 [08:03<01:07, 13.59s/it]2024-12-22 03:10:08,718 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:10,024 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:10,025 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 755])
2024-12-22 03:10:10,049 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:10:11,467 - [Process 2/5] - INFO - res.shape is :torch.Size([237])
results:The latest news passages highlight the ongoing competition between Mitt Romney and Rick Santorum in the Republican presidential race. During a recent interview, Santorum expressed his disappointment with Romney's win in the CPAC presidential straw poll, suggesting that Romney's victory may have been due to the latter's ability to "rig" the poll. Romney's campaign has rejected these claims, with a spokesperson stating that the former Massachusetts governor's win was due to his support among conservative voters. Meanwhile, new White House chief of staff Jack Lew has defended President Obama's position on birth control and Catholic institutions, stating that the administration's "accommodation" is as far as they will go to address the issue. According to Lew, health insurers will pay less for birth control-inclusive plans, which are expected to reduce pregnancies. Overall, the news passages highlight the ongoing political maneuvering and debate within the Republican party, as well as the ongoing controversy surrounding the administration's position on birth control and religious institutions.
100%|██████████| 40/40 [08:05<00:00, 10.16s/it]100%|██████████| 40/40 [08:05<00:00, 12.15s/it]
2024-12-22 03:10:13,845 - [Process 1/5] - INFO - res.shape is :torch.Size([258])
results:A 2-year-old toddler was killed by a leopard at Queen Elizabeth National Park in Uganda while he was left in the care of a nanny at the staff quarters of a safari lodge. The leopard attacked the child on Friday night and ran away with him, with the nanny hearing the child scream for help but unable to stop the attack. The child's remains were later found with the leopard having eaten most of his body. The Uganda Wildlife Authority (UWA) is searching for the leopard and plans to relocate it elsewhere. This incident is the fourth unfortunate incident for UWA in recent weeks, following the deaths of 11 lions, a French tourist, and another lion. The UWA spokesman, Bashir Hangi, described the incident as unfortunate and said the authority is working towards establishing the leopard and evaluating options for its relocation. The child's father, Francis Manana Nabugyere, is seeking compensation from UWA for the loss of his son's life and wants heightened security measures to protect staff and children in the park.
 92%|█████████▎| 37/40 [08:08<00:44, 14.82s/it]2024-12-22 03:10:14,011 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:17,171 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:17,171 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1799])
2024-12-22 03:10:17,234 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:10:18,942 - [Process 3/5] - INFO - res.shape is :torch.Size([196])
results:A Swedish chemistry professor, Charlotta Turner, received a text message from one of her graduate students, Firas Jumaah, in 2014, who was hiding in a disused bleach factory in Iraq with his family due to the ISIS war. Jumaah was a member of the Yazidi ethno-religious group hated by ISIS. Turner was not willing to leave her student to die without trying to do something, so she contacted the university's security chief, Per Gustafson, who arranged for a rescue operation. Over a few days, Gustafson hired a security company that rescued Jumaah and his family from Iraq and brought them to Erbil Airport. Jumaah completed his PhD and now works for a pharmaceuticals company in Malmö, and the family has almost finished paying the university back for the rescue operation.
 90%|█████████ | 36/40 [08:13<00:50, 12.60s/it]2024-12-22 03:10:19,157 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:19,685 - [Process 4/5] - INFO - res.shape is :torch.Size([279])
results:

The latest news from Egypt indicates that the country's parliamentary elections are underway, with the third round of voting taking place on Tuesday. The Muslim Brotherhood is expected to consolidate its gains and potentially form a majority government, while former President Hosni Mubarak faces trial for his role in the killing of protesters during the uprising that ousted him. The trial has begun, with prosecutors calling for the harshest penalty against Mubarak. The former president is accused of complicity in the killing of over 800 protesters and faces charges of corruption and conspiring to kill protesters. The trial is expected to continue over the next two days, with the prosecution presenting its case. Meanwhile, the military rulers have said they will not cede power before presidential elections are held by the end of June. The slowing growth of the economy and the unrest deters tourists and investors, with foreign-currency reserves declining by 44 percent. The election and Mubarak's trial take place amid growing opposition to the ruling generals, with protesters planning to take to the streets on January 25 to mark the anniversary of the mass rallies that drove Mubarak from power.
 98%|█████████▊| 39/40 [08:14<00:13, 13.52s/it]2024-12-22 03:10:19,713 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:20,145 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:20,145 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 214])
2024-12-22 03:10:20,152 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
2024-12-22 03:10:22,759 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:22,760 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1985])
2024-12-22 03:10:22,833 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:10:24,194 - [Process 4/5] - INFO - res.shape is :torch.Size([98])
results:A car was suspended from a telephone pole in Mendon after the driver reacted too quickly to a GPS device's instructions. The driver, Nabila Altahan, was not injured, but her passenger was not present in the car at the time of the incident. Witnesses reported seeing Altahan jump out of the car as it was suspended from the pole. State police and Rutland City Fire Department responded to the scene.
100%|██████████| 40/40 [08:18<00:00, 10.81s/it]100%|██████████| 40/40 [08:18<00:00, 12.47s/it]
2024-12-22 03:10:31,544 - [Process 3/5] - INFO - res.shape is :torch.Size([201])
results:

Pippa Middleton, the sister of Duchess of Cambridge, Kate Middleton, was involved in a controversial incident in Paris where a driver of her car pointed a gun at a photographer. The driver, identified as Vicomte de Soultrait, a French aristocrat, was seen holding a gun that looked real but turned out to be a fake. The incident has sparked outrage in France, and the driver could face up to two years in prison if the gun was real. Pippa was attending a costume birthday party at the time of the incident. The party was hosted by de Soultrait, who is a successful young entrepreneur and runs the upmarket Vicomte A clothing range. The party featured a decadent 18th-century-style theme and included a risque burlesque show. The incident has raised questions about Pippa's security and the Palace's response to the situation.
 92%|█████████▎| 37/40 [08:25<00:37, 12.60s/it]2024-12-22 03:10:31,609 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:32,736 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:32,737 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 674])
2024-12-22 03:10:32,757 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:10:33,503 - [Process 1/5] - INFO - res.shape is :torch.Size([392])
results:


The news passages describe the discovery of a lead tube reliquary containing ashes from the grave of John the Apostle in Ephesus, Turkey, by Bulgarian archaeologists during excavations in the ancient and medieval port of Burgos (also known as Poros) on Cape Foros in Burgas. The reliquary is dated to the 6th century AD and is one of only 43 lead tubes known from this time period. The discovery is significant because it provides evidence of the pilgrimage route from the fortress of Poros to the grave of John the Apostle in Ephesus. The archaeologists also discovered a 10th century Bulgarian royal seal at the fortress site. The findings suggest that the fortress and port of Burgos was an important center of Christian pilgrimage in the 6th century AD. Additionally, the discovery of the reliquary is one of the most important discoveries in the history of the Burgas Regional Museum of History. The lead tube reliquary is small, measuring 2.2 cm in length and 1.7 cm in diameter, and contains a cross with equal arms inside a medallion on one side and two overlapping crosses with equal arms on the other side. The neck of the tube is also decorated with crosses. The discovery of the reliquary is significant because it provides evidence of the beliefs of early Christians that every year on May 8, the date of John the Apostle’s death, there is manna, a holy curing powder, on the site of his grave. The discovery also highlights the importance of the fortress and port of Burgos (Poros) as a center of Christian pilgrimage in the 6th century AD.
 95%|█████████▌| 38/40 [08:27<00:32, 16.27s/it]2024-12-22 03:10:33,579 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:34,831 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:34,831 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 708])
2024-12-22 03:10:34,855 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:10:39,830 - [Process 3/5] - INFO - res.shape is :torch.Size([183])
results:A woman has been suspected of robbing six jewelry stores across five states in the southeast region of the United States since April. The suspect is described as a white woman in her 20s or 30s, 5'8" and 130 pounds, and is considered armed and dangerous. The FBI has released surveillance images of the suspect and is asking for the public's help in identifying her. The suspect is believed to have committed similar robberies in Florida, Georgia, South Carolina, and Tennessee, and may be working with an accomplice. The most recent robbery occurred in Mebane, North Carolina, where the suspect forced two employees into a back room and stole jewelry worth over $400,000. Police are urging anyone with information to contact them immediately.
 95%|█████████▌| 38/40 [08:34<00:22, 11.31s/it]2024-12-22 03:10:39,935 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:41,976 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:41,976 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1113])
2024-12-22 03:10:42,017 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:10:42,155 - [Process 1/5] - INFO - res.shape is :torch.Size([191])
results:Netflix has announced an unlimited maternity and paternity leave policy for new parents, allowing them to take off as much time as they want during the first year after a child's birth or adoption. This policy is part of the company's effort to create a "freedom and responsibility" culture that gives employees context about the business and the freedom to make decisions along with the accompanying responsibility. The policy allows parents to return part-time, full-time, or return and then go back out as needed, and Netflix will continue to pay them normally. The company believes that this policy will help employees balance the needs of their growing families without worrying about work or finances, and will allow them to return to work more focused and dedicated. This policy update is seen as a clear reminder that Netflix wants the best talent, especially when and after they become new parents.
 98%|█████████▊| 39/40 [08:36<00:13, 13.98s/it]2024-12-22 03:10:42,208 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:43,204 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:43,204 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 527])
2024-12-22 03:10:43,223 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-22 03:10:49,423 - [Process 3/5] - INFO - res.shape is :torch.Size([191])
results:Ariel Winter, a 19-year-old actress from the TV show "Modern Family," has been making headlines for her bold fashion choices. At a recent event, she wore a glamorous gold minidress that sparked mixed reactions from fans and critics. Some praised her for being confident and expressing herself, while others criticized her for being overdressed and inappropriate. Winter responded to the criticism by posting a message on Instagram, encouraging people to mind their own business and be themselves. She also emphasized the importance of feeling good about oneself and not caring about what others think. Meanwhile, Winter's co-stars, including Ty Burrell, Eric Stonestreet, and Julie Bowen, opted for more subdued outfits at the event. The cast was promoting their show's eighth season finale.
 98%|█████████▊| 39/40 [08:43<00:10, 10.79s/it]2024-12-22 03:10:49,575 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-22 03:10:50,700 - [Process 1/5] - INFO - res.shape is :torch.Size([195])
results:

In the latest news, a Jewish student at the London School of Economics (LSE) was subjected to violence and Nazi ideology glorification during a skiing trip to Val d'Isère. The student suffered a broken nose after objecting to a Nazi-themed drinking game, which included playing cards arranged in the shape of a swastika and requiring players to "salute the Führer." The LSE and the students' union are investigating the incident and have condemned the actions of the perpetrators. The university's student newspaper reported that a similar incident occurred at Oxford University last November, where members of the Oxford University Conservative Association were accused of singing an antisemitic song with Nazi references. These incidents highlight the ongoing problem of antisemitism and racism on university campuses, and the need for greater awareness and action to address these issues.
100%|██████████| 40/40 [08:45<00:00, 12.35s/it]100%|██████████| 40/40 [08:45<00:00, 13.13s/it]
2024-12-22 03:10:52,392 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-22 03:10:52,392 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1590])
2024-12-22 03:10:52,451 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
2024-12-22 03:11:01,735 - [Process 3/5] - INFO - res.shape is :torch.Size([230])
results:The news is about the movie "Focus," starring Will Smith and Margot Robbie. The movie is a caper film about a veteran con artist who takes on a young apprentice, and it has been described as a "dapper-con genre" movie. The filmmakers, Glenn Ficarra and John Requa, have been praised for their ability to create a movie that is both entertaining and well-made. The actors, Will Smith and Margot Robbie, have been praised for their performances, with Smith being particularly commended for his ability to convey confidence and charm. The movie has been described as a "little gem" and a "charm barrage," and it has been compared to classic movies such as "The Lady Eve" and "The Sting." However, some critics have noted that the movie's convoluted plot and lack of emotional depth may detract from its overall impact. Despite this, the movie is expected to be a commercial success and has been praised for its stylish direction and witty dialogue.
100%|██████████| 40/40 [08:56<00:00, 11.25s/it]100%|██████████| 40/40 [08:56<00:00, 13.40s/it]
2024-12-22 03:11:01,755 - [Process 3/5] - DEBUG - datasets_name:multi_news
2024-12-22 03:11:01,756 - [Process 2/5] - DEBUG - datasets_name:multi_news
2024-12-22 03:11:01,756 - [Process 4/5] - DEBUG - datasets_name:multi_news
2024-12-22 03:11:01,756 - [Process 1/5] - DEBUG - datasets_name:multi_news
2024-12-22 03:11:01,756 - [Process 0/5] - DEBUG - datasets_name:multi_news
