
CondaError: Run 'conda init' before 'conda activate'

Running evaluation for dataset: narrativeqa
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.53s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:33:25,081 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:33:25,081 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:33:25,082 - [Process 4/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:33:25,102 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:33:25,102 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:33:25,102 - [Process 0/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里!!!!!!!!!!!!!!!!!!!!!!!! 这里

2024-12-21 17:33:25,107 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:33:25,108 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:33:25,108 - [Process 1/5] - INFO - output_max_len: 128
2024-12-21 17:33:25,108 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:33:25,108 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:33:25,108 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:33:25,108 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:33:25,108 - [Process 3/5] - INFO - output_max_len: 128
2024-12-21 17:33:25,108 - [Process 2/5] - INFO - output_max_len: 128
2024-12-21 17:33:25,146 - [Process 4/5] - INFO - Max Length is 36418
2024-12-21 17:33:25,146 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:33:25,147 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:33:25,201 - [Process 0/5] - INFO - Max Length is 36418
2024-12-21 17:33:25,202 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:33:25,202 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:33:25,207 - [Process 1/5] - INFO - Max Length is 36418
2024-12-21 17:33:25,207 - [Process 3/5] - INFO - Max Length is 36418
2024-12-21 17:33:25,207 - [Process 2/5] - INFO - Max Length is 36418
2024-12-21 17:33:25,208 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:33:25,208 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:33:25,208 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:33:25,208 - [Process 1/5] - INFO - get_predicted begin
2024-12-21 17:33:25,208 - [Process 3/5] - INFO - get_predicted begin
2024-12-21 17:33:25,208 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:33:29,870 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:33:29,954 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:33:29,958 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:33:29,959 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:33:29,960 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:33:33,976 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:33:33,976 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1985])
2024-12-21 17:33:34,049 - [Process 2/5] - DEBUG - predict_token:tensor([[9672]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:33:34,189 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:33:34,190 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1950])
2024-12-21 17:33:34,215 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:33:34,215 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1939])
2024-12-21 17:33:34,229 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:33:34,229 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1981])
2024-12-21 17:33:34,259 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:33:34,259 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1981])
2024-12-21 17:33:34,262 - [Process 0/5] - DEBUG - predict_token:tensor([[15914]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:33:34,288 - [Process 1/5] - DEBUG - predict_token:tensor([[874]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:33:34,302 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:33:34,332 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:33:34,343 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'R'
2024-12-21 17:33:34,534 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'I'
2024-12-21 17:33:34,622 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'E'
2024-12-21 17:33:34,740 - [Process 1/5] - INFO - res.shape is :torch.Size([8])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'E'
2024-12-21 17:33:34,788 - [Process 3/5] - INFO - res.shape is :torch.Size([8])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'B'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2332813 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2332814 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2332816 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2332817 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 2332815) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:33:35
  host      : nwonga100.local
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2332815)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: qasper
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.64s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.48s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:35:31,273 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:35:31,273 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:35:31,273 - [Process 0/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:35:31,281 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:35:31,282 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:35:31,282 - [Process 4/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:35:31,292 - [Process 0/5] - INFO - Max Length is 14660
2024-12-21 17:35:31,292 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:35:31,292 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:35:31,292 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:35:31,292 - [Process 3/5] - INFO - output_max_len: 128
2024-12-21 17:35:31,292 - [Process 0/5] - INFO - get_predicted begin
!!!!!!!!!!!!!!!!!!!!!!!! 这里  0%|          | 0/40 [00:00<?, ?it/s]!!!!!!!!!!!!!!!!!!!!!!!! 这里

2024-12-21 17:35:31,293 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:35:31,293 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:35:31,294 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:35:31,294 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:35:31,294 - [Process 1/5] - INFO - output_max_len: 128
2024-12-21 17:35:31,294 - [Process 2/5] - INFO - output_max_len: 128
2024-12-21 17:35:31,311 - [Process 4/5] - INFO - Max Length is 14660
2024-12-21 17:35:31,311 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:35:31,312 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:35:31,321 - [Process 3/5] - INFO - Max Length is 14660
2024-12-21 17:35:31,322 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:35:31,322 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:35:31,323 - [Process 1/5] - INFO - Max Length is 14660
2024-12-21 17:35:31,323 - [Process 2/5] - INFO - Max Length is 14660
2024-12-21 17:35:31,324 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:35:31,324 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:35:31,324 - [Process 1/5] - INFO - get_predicted begin
2024-12-21 17:35:31,324 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:35:36,054 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:35:36,135 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:35:36,138 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:35:36,139 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:35:36,141 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:35:40,168 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:35:40,168 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2099])
2024-12-21 17:35:40,239 - [Process 0/5] - DEBUG - predict_token:tensor([[470]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:35:40,446 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:35:40,446 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2092])
2024-12-21 17:35:40,458 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:35:40,458 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2016])
2024-12-21 17:35:40,490 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:35:40,490 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-21 17:35:40,492 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:35:40,492 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2111])
2024-12-21 17:35:40,517 - [Process 4/5] - DEBUG - predict_token:tensor([[2]], device='cuda:4')
2024-12-21 17:35:40,517 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'U'
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:35:40,531 - [Process 2/5] - DEBUG - predict_token:tensor([[761]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:35:40,563 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-21 17:35:40,563 - [Process 3/5] - DEBUG - predict_token:tensor([[13]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:35:40,769 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'Y'
2024-12-21 17:35:40,846 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'u'
2024-12-21 17:35:40,860 - [Process 1/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'U'
2024-12-21 17:35:40,998 - [Process 3/5] - INFO - res.shape is :torch.Size([7])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: '.'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2333294 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2333295 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2333296 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2333297 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2333293) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:35:41
  host      : nwonga100.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2333293)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: multifieldqa_en
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.16s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.80s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.86s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:37:47,356 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:37:47,357 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:37:47,357 - [Process 4/5] - INFO - output_max_len: 64
2024-12-21 17:37:47,357 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:37:47,357 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:37:47,357 - [Process 2/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:37:47,358 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:37:47,358 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:37:47,358 - [Process 3/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:37:47,368 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:37:47,369 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:37:47,369 - [Process 1/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:37:47,371 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:37:47,371 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:37:47,371 - [Process 0/5] - INFO - output_max_len: 64
2024-12-21 17:37:47,374 - [Process 2/5] - INFO - Max Length is 10337
2024-12-21 17:37:47,374 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:37:47,374 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-21 17:37:47,381 - [Process 4/5] - INFO - Max Length is 10337
2024-12-21 17:37:47,381 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:37:47,382 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-21 17:37:47,384 - [Process 3/5] - INFO - Max Length is 10337
2024-12-21 17:37:47,385 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:37:47,385 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-21 17:37:47,395 - [Process 1/5] - INFO - Max Length is 10337
2024-12-21 17:37:47,395 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:37:47,395 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-21 17:37:47,397 - [Process 0/5] - INFO - Max Length is 10337
2024-12-21 17:37:47,398 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:37:47,398 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/30 [00:00<?, ?it/s]2024-12-21 17:37:52,126 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:37:52,136 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:37:52,188 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:37:52,209 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:37:52,218 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:37:54,503 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:37:54,504 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 731])
2024-12-21 17:37:54,543 - [Process 0/5] - DEBUG - predict_token:tensor([[1008]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:37:54,803 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
  0%|          | 0/30 [00:07<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'S'
2024-12-21 17:37:55,685 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:37:55,686 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1582])
2024-12-21 17:37:55,743 - [Process 4/5] - DEBUG - predict_token:tensor([[2197]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2333788 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2333789 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2333790 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2333791 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2333787) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:37:55
  host      : nwonga100.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2333787)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: hotpotqa
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.77s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.95s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:40:05,199 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:40:05,199 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:40:05,199 - [Process 0/5] - INFO - output_max_len: 32
2024-12-21 17:40:05,199 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:40:05,199 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:40:05,200 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:40:05,200 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:40:05,200 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:40:05,200 - [Process 2/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:40:05,209 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:40:05,209 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:40:05,209 - [Process 3/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:40:05,211 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:40:05,211 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:40:05,211 - [Process 1/5] - INFO - output_max_len: 32
2024-12-21 17:40:05,246 - [Process 2/5] - INFO - Max Length is 12697
2024-12-21 17:40:05,247 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:40:05,247 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:40:05,276 - [Process 4/5] - INFO - Max Length is 12697
2024-12-21 17:40:05,276 - [Process 0/5] - INFO - Max Length is 12697
2024-12-21 17:40:05,277 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:40:05,277 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:40:05,277 - [Process 4/5] - INFO - get_predicted begin
2024-12-21 17:40:05,277 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:40:05,284 - [Process 1/5] - INFO - Max Length is 12697
2024-12-21 17:40:05,285 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:40:05,285 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:40:05,287 - [Process 3/5] - INFO - Max Length is 12697
2024-12-21 17:40:05,287 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:40:05,288 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:40:09,977 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:40:10,058 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:40:10,062 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:40:10,063 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:40:10,063 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:40:14,056 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:40:14,057 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1950])
2024-12-21 17:40:14,130 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:40:14,214 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:40:14,214 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1906])
2024-12-21 17:40:14,282 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:40:14,283 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1960])
2024-12-21 17:40:14,287 - [Process 4/5] - DEBUG - predict_token:tensor([[287]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:40:14,330 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:40:14,330 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2060])
2024-12-21 17:40:14,344 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'N'
2024-12-21 17:40:14,355 - [Process 0/5] - DEBUG - predict_token:tensor([[985]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:40:14,375 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:40:14,376 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1985])
2024-12-21 17:40:14,398 - [Process 2/5] - DEBUG - predict_token:tensor([[305]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:40:14,447 - [Process 3/5] - DEBUG - predict_token:tensor([[7694]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:40:14,602 - [Process 4/5] - INFO - res.shape is :torch.Size([5])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'M'
2024-12-21 17:40:14,669 - [Process 0/5] - INFO - res.shape is :torch.Size([5])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'M'
2024-12-21 17:40:14,683 - [Process 3/5] - INFO - res.shape is :torch.Size([3])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'd'
2024-12-21 17:40:14,760 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'P'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2334252 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2334254 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2334255 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2334256 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 2334253) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:40:15
  host      : nwonga100.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2334253)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: 2wikimqa
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.43s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  5.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.39s/it]

!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:42:21,600 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:42:21,600 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:42:21,600 - [Process 3/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:42:21,612 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:42:21,612 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:42:21,612 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:42:21,621 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:42:21,621 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:42:21,621 - [Process 1/5] - INFO - output_max_len: 32
2024-12-21 17:42:21,622 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:42:21,622 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:42:21,622 - [Process 2/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:42:21,624 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:42:21,624 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:42:21,624 - [Process 0/5] - INFO - output_max_len: 32
2024-12-21 17:42:21,625 - [Process 3/5] - INFO - Max Length is 11950
2024-12-21 17:42:21,625 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:42:21,626 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:42:21,655 - [Process 4/5] - INFO - Max Length is 11950
2024-12-21 17:42:21,655 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:42:21,656 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:42:21,660 - [Process 1/5] - INFO - Max Length is 11950
2024-12-21 17:42:21,660 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:42:21,661 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:42:21,663 - [Process 2/5] - INFO - Max Length is 11950
2024-12-21 17:42:21,664 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:42:21,664 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:42:21,666 - [Process 0/5] - INFO - Max Length is 11950
2024-12-21 17:42:21,666 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:42:21,666 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:42:26,387 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:42:26,468 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:42:26,469 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:42:26,470 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:42:26,472 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:42:30,623 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:42:30,624 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1822])
2024-12-21 17:42:30,653 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:42:30,653 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2106])
2024-12-21 17:42:30,658 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:42:30,659 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1904])
2024-12-21 17:42:30,703 - [Process 4/5] - DEBUG - predict_token:tensor([[29891]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:42:30,709 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:42:30,709 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1979])
2024-12-21 17:42:30,718 - [Process 1/5] - DEBUG - predict_token:tensor([[515]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:42:30,738 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:42:30,738 - [Process 3/5] - DEBUG - predict_token:tensor([[263]], device='cuda:3')
2024-12-21 17:42:30,738 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2010])
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:42:30,780 - [Process 2/5] - DEBUG - predict_token:tensor([[13]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:42:30,808 - [Process 0/5] - DEBUG - predict_token:tensor([[749]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:42:30,944 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'M'
2024-12-21 17:42:31,010 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'N'
2024-12-21 17:42:31,053 - [Process 2/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'F'
2024-12-21 17:42:31,076 - [Process 4/5] - INFO - res.shape is :torch.Size([7])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'H'
2024-12-21 17:42:31,257 - [Process 0/5] - INFO - res.shape is :torch.Size([8])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'G'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2334730 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2334731 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2334732 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2334733 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 2334734) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:42:32
  host      : nwonga100.local
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 2334734)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: musique
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]multi_gpus:True

model_name:/mnt/Data/xiongjing/llama2chattorch.cuda.device_count():5

n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:Truemulti_gpus:True

torch.cuda.device_count():5torch.cuda.device_count():5

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.90s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.36s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:44:39,333 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:44:39,333 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:44:39,333 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:44:39,344 - [Process 2/5] - INFO - loading datasets finished
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:44:39,344 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:44:39,345 - [Process 2/5] - INFO - output_max_len: 32
2024-12-21 17:44:39,345 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:44:39,345 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:44:39,345 - [Process 1/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:44:39,351 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:44:39,352 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:44:39,352 - [Process 3/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:44:39,354 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:44:39,354 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:44:39,354 - [Process 0/5] - INFO - output_max_len: 32
2024-12-21 17:44:39,391 - [Process 4/5] - INFO - Max Length is 17355
2024-12-21 17:44:39,391 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:44:39,392 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:44:39,439 - [Process 2/5] - INFO - Max Length is 17355
2024-12-21 17:44:39,439 - [Process 1/5] - INFO - Max Length is 17355
2024-12-21 17:44:39,439 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:44:39,439 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:44:39,440 - [Process 2/5] - INFO - get_predicted begin
2024-12-21 17:44:39,440 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:44:39,446 - [Process 3/5] - INFO - Max Length is 17355
2024-12-21 17:44:39,446 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:44:39,447 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:44:39,448 - [Process 0/5] - INFO - Max Length is 17355
2024-12-21 17:44:39,449 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:44:39,449 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:44:44,115 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:44:44,201 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:44:44,202 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:44:44,202 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:44:44,204 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:44:48,201 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:44:48,202 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2012])
2024-12-21 17:44:48,272 - [Process 4/5] - DEBUG - predict_token:tensor([[787]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:44:48,470 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:44:48,470 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1983])
2024-12-21 17:44:48,526 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:44:48,526 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1904])
2024-12-21 17:44:48,526 - [Process 4/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'L'
2024-12-21 17:44:48,542 - [Process 3/5] - DEBUG - predict_token:tensor([[29890]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:44:48,570 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:44:48,570 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2234])
2024-12-21 17:44:48,571 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:44:48,572 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2119])
2024-12-21 17:44:48,605 - [Process 0/5] - DEBUG - predict_token:tensor([[8168]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:44:48,632 - [Process 2/5] - DEBUG - predict_token:tensor([[2998]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:44:48,637 - [Process 1/5] - DEBUG - predict_token:tensor([[768]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:44:48,814 - [Process 3/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'B'
2024-12-21 17:44:48,866 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'M'
2024-12-21 17:44:48,877 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'R'
2024-12-21 17:44:48,949 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'E'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2335202 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2335203 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2335204 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2335205 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 2335206) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:44:49
  host      : nwonga100.local
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 2335206)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: trec
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 4
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.75s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.59s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:46:45,990 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:46:45,990 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:46:45,991 - [Process 3/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:46:46,000 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:46:46,001 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:46:46,001 - [Process 4/5] - INFO - output_max_len: 64
2024-12-21 17:46:46,008 - [Process 3/5] - INFO - Max Length is 8714
2024-12-21 17:46:46,008 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:46:46,008 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:46:46,010 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:46:46,010 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:46:46,011 - [Process 2/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:46:46,013 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:46:46,013 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:46:46,013 - [Process 1/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:46:46,014 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:46:46,015 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:46:46,015 - [Process 0/5] - INFO - output_max_len: 64
2024-12-21 17:46:46,027 - [Process 4/5] - INFO - Max Length is 8714
2024-12-21 17:46:46,028 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:46:46,029 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:46:46,037 - [Process 2/5] - INFO - Max Length is 8714
2024-12-21 17:46:46,038 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:46:46,038 - [Process 2/5] - INFO - get_predicted begin
2024-12-21 17:46:46,039 - [Process 1/5] - INFO - Max Length is 8714
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:46:46,040 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:46:46,041 - [Process 1/5] - INFO - get_predicted begin
2024-12-21 17:46:46,041 - [Process 0/5] - INFO - Max Length is 8714
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:46:46,042 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:46:46,042 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:46:50,759 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:46:50,863 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:46:50,863 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:46:50,865 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:46:50,867 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:46:54,044 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:46:54,045 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1604])
2024-12-21 17:46:54,099 - [Process 3/5] - DEBUG - predict_token:tensor([[16492]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:46:55,125 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:46:55,126 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1940])
2024-12-21 17:46:55,146 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:46:55,146 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2000])
2024-12-21 17:46:55,161 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:46:55,162 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1995])
2024-12-21 17:46:55,183 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:46:55,183 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1996])
2024-12-21 17:46:55,196 - [Process 0/5] - DEBUG - predict_token:tensor([[9445]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:46:55,217 - [Process 4/5] - DEBUG - predict_token:tensor([[2297]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:46:55,232 - [Process 2/5] - DEBUG - predict_token:tensor([[29890]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:46:55,254 - [Process 1/5] - DEBUG - predict_token:tensor([[2]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:46:56,570 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
  0%|          | 0/40 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'I'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2335684 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2335685 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2335686 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2335688 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 2335687) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:46:57
  host      : nwonga100.local
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2335687)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: triviaqa
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]multi_gpus:True

model_name:/mnt/Data/xiongjing/llama2chat
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.40s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.49s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.57s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:49:02,402 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:49:02,403 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:49:02,403 - [Process 1/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:49:02,406 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:49:02,406 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:49:02,406 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:49:02,407 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:49:02,407 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:49:02,407 - [Process 3/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:49:02,416 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:49:02,416 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:49:02,416 - [Process 2/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:49:02,418 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:49:02,418 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:49:02,418 - [Process 0/5] - INFO - output_max_len: 32
2024-12-21 17:49:02,448 - [Process 3/5] - INFO - Max Length is 16633
2024-12-21 17:49:02,449 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:49:02,449 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:49:02,476 - [Process 1/5] - INFO - Max Length is 16633
2024-12-21 17:49:02,476 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:49:02,477 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:49:02,479 - [Process 4/5] - INFO - Max Length is 16633
2024-12-21 17:49:02,480 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:49:02,480 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:49:02,489 - [Process 2/5] - INFO - Max Length is 16633
2024-12-21 17:49:02,490 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:49:02,490 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:49:02,491 - [Process 0/5] - INFO - Max Length is 16633
2024-12-21 17:49:02,492 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:49:02,492 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:49:07,183 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:49:07,263 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:49:07,265 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:49:07,268 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:49:07,268 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:49:11,294 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:49:11,294 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1997])
2024-12-21 17:49:11,366 - [Process 3/5] - DEBUG - predict_token:tensor([[265]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:49:11,416 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:49:11,417 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2112])
2024-12-21 17:49:11,482 - [Process 2/5] - DEBUG - predict_token:tensor([[2]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:49:11,536 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:49:11,537 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2019])
2024-12-21 17:49:11,557 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:49:11,557 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1994])
2024-12-21 17:49:11,584 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:49:11,584 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1897])
2024-12-21 17:49:11,607 - [Process 4/5] - DEBUG - predict_token:tensor([[367]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:49:11,628 - [Process 1/5] - DEBUG - predict_token:tensor([[674]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:49:11,663 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:49:12,042 - [Process 0/5] - INFO - res.shape is :torch.Size([4])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'U'
2024-12-21 17:49:12,673 - [Process 4/5] - INFO - res.shape is :torch.Size([3])
  0%|          | 0/40 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'C'
2024-12-21 17:49:12,767 - [Process 3/5] - INFO - res.shape is :torch.Size([5])
  0%|          | 0/40 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'K'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2336133 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2336134 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2336135 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2336136 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2336132) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:49:13
  host      : nwonga100.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2336132)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: passage_count
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.75s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里!!!!!!!!!!!!!!!!!!!!!!!! 这里

2024-12-21 17:51:19,350 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:51:19,350 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:51:19,351 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:51:19,351 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:51:19,351 - [Process 0/5] - INFO - output_max_len: 32
2024-12-21 17:51:19,351 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:51:19,351 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:51:19,352 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:51:19,352 - [Process 1/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:51:19,362 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:51:19,363 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:51:19,362 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:51:19,363 - [Process 2/5] - INFO - output_max_len: 32
2024-12-21 17:51:19,363 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:51:19,363 - [Process 3/5] - INFO - output_max_len: 32
2024-12-21 17:51:19,404 - [Process 1/5] - INFO - Max Length is 22099
2024-12-21 17:51:19,404 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:51:19,405 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:51:19,435 - [Process 4/5] - INFO - Max Length is 22099
2024-12-21 17:51:19,436 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:51:19,436 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:51:19,438 - [Process 0/5] - INFO - Max Length is 22099
2024-12-21 17:51:19,438 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:51:19,438 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:51:19,449 - [Process 3/5] - INFO - Max Length is 22099
2024-12-21 17:51:19,449 - [Process 2/5] - INFO - Max Length is 22099
2024-12-21 17:51:19,449 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:51:19,449 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:51:19,450 - [Process 3/5] - INFO - get_predicted begin
2024-12-21 17:51:19,450 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:51:24,126 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:24,209 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:24,211 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:24,213 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:24,213 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:28,264 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:28,265 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2064])
2024-12-21 17:51:28,336 - [Process 3/5] - DEBUG - predict_token:tensor([[283]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:51:28,460 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:28,461 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2088])
2024-12-21 17:51:28,504 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:28,504 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2013])
2024-12-21 17:51:28,509 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:28,509 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
2024-12-21 17:51:28,510 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2049])
results:
  2%|▎         | 1/40 [00:09<05:53,  9.06s/it]2024-12-21 17:51:28,529 - [Process 2/5] - DEBUG - predict_token:tensor([[30006]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:51:28,576 - [Process 0/5] - DEBUG - predict_token:tensor([[1238]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:51:28,580 - [Process 4/5] - DEBUG - predict_token:tensor([[8034]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:51:28,652 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:28,652 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1926])
2024-12-21 17:51:28,694 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:28,706 - [Process 2/5] - INFO - res.shape is :torch.Size([2])
results:
  2%|▎         | 1/40 [00:09<06:00,  9.26s/it]2024-12-21 17:51:28,732 - [Process 1/5] - DEBUG - predict_token:tensor([[6496]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:51:28,758 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results:
  2%|▎         | 1/40 [00:09<06:03,  9.32s/it]2024-12-21 17:51:28,763 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results: 
  2%|▎         | 1/40 [00:09<06:03,  9.33s/it]2024-12-21 17:51:28,916 - [Process 1/5] - INFO - res.shape is :torch.Size([2])
results:
  2%|▎         | 1/40 [00:09<06:10,  9.51s/it]2024-12-21 17:51:29,006 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:29,037 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:29,085 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:29,210 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:32,210 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:32,211 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1961])
2024-12-21 17:51:32,285 - [Process 3/5] - DEBUG - predict_token:tensor([[680]], device='cuda:3')
2024-12-21 17:51:32,455 - [Process 3/5] - INFO - res.shape is :torch.Size([2])
results:
  5%|▌         | 2/40 [00:13<03:49,  6.05s/it]2024-12-21 17:51:32,548 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:32,548 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2029])
2024-12-21 17:51:32,556 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:32,556 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2057])
2024-12-21 17:51:32,618 - [Process 0/5] - DEBUG - predict_token:tensor([[5733]], device='cuda:0')
2024-12-21 17:51:32,627 - [Process 2/5] - DEBUG - predict_token:tensor([[1809]], device='cuda:2')
2024-12-21 17:51:32,642 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:32,643 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1855])
2024-12-21 17:51:32,643 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:32,722 - [Process 4/5] - DEBUG - predict_token:tensor([[12358]], device='cuda:4')
2024-12-21 17:51:32,795 - [Process 0/5] - INFO - res.shape is :torch.Size([2])
results: 
  5%|▌         | 2/40 [00:13<03:56,  6.21s/it]2024-12-21 17:51:32,809 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:51:32,810 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2166])
2024-12-21 17:51:32,875 - [Process 1/5] - DEBUG - predict_token:tensor([[13]], device='cuda:1')
2024-12-21 17:51:32,902 - [Process 4/5] - INFO - res.shape is :torch.Size([2])
results: 
  5%|▌         | 2/40 [00:13<03:58,  6.28s/it]2024-12-21 17:51:32,975 - [Process 2/5] - INFO - res.shape is :torch.Size([6])
  2%|▎         | 1/40 [00:13<08:47, 13.52s/it]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: ' '
2024-12-21 17:51:33,028 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:33,164 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:51:33,667 - [Process 1/5] - INFO - res.shape is :torch.Size([16])
  2%|▎         | 1/40 [00:14<09:16, 14.26s/it]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: '\n'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2336576 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2336577 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2336579 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2336580 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 2336578) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:51:34
  host      : nwonga100.local
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2336578)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: passage_retrieval_en
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]n_windows:[2]

model_name:/mnt/Data/xiongjing/llama2chatmodel_name:/mnt/Data/xiongjing/llama2chat

multi_gpus:True
multi_gpus:Truetorch.cuda.device_count():5

torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.43s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.55s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.01s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.40s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:53:40,664 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:53:40,664 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:53:40,664 - [Process 0/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:53:40,671 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:53:40,672 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:53:40,672 - [Process 4/5] - INFO - output_max_len: 32
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:53:40,683 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:53:40,683 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:53:40,683 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:53:40,683 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:53:40,683 - [Process 3/5] - INFO - output_max_len: 32
2024-12-21 17:53:40,683 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:53:40,683 - [Process 2/5] - INFO - output_max_len: 32
2024-12-21 17:53:40,683 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:53:40,684 - [Process 1/5] - INFO - output_max_len: 32
2024-12-21 17:53:40,709 - [Process 0/5] - INFO - Max Length is 11516
2024-12-21 17:53:40,709 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:53:40,710 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:53:40,746 - [Process 4/5] - INFO - Max Length is 11516
2024-12-21 17:53:40,746 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:53:40,747 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:53:40,755 - [Process 2/5] - INFO - Max Length is 11516
2024-12-21 17:53:40,755 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:53:40,756 - [Process 2/5] - INFO - get_predicted begin
2024-12-21 17:53:40,756 - [Process 1/5] - INFO - Max Length is 11516
2024-12-21 17:53:40,756 - [Process 1/5] - INFO - Finish loading dataset
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:53:40,757 - [Process 1/5] - INFO - get_predicted begin
2024-12-21 17:53:40,757 - [Process 3/5] - INFO - Max Length is 11516
2024-12-21 17:53:40,758 - [Process 3/5] - INFO - Finish loading dataset
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:53:40,758 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:53:45,444 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:53:45,528 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:53:45,532 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:53:45,533 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:53:45,534 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:53:49,483 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:53:49,484 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1945])
2024-12-21 17:53:49,556 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:53:49,695 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:53:49,695 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1901])
2024-12-21 17:53:49,768 - [Process 2/5] - DEBUG - predict_token:tensor([[450]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:53:49,775 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:53:49,775 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2063])
2024-12-21 17:53:49,799 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:53:49,799 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2058])
2024-12-21 17:53:49,830 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:53:49,830 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1988])
2024-12-21 17:53:49,842 - [Process 4/5] - DEBUG - predict_token:tensor([[13]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:53:49,867 - [Process 1/5] - DEBUG - predict_token:tensor([[8449]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:53:49,901 - [Process 3/5] - DEBUG - predict_token:tensor([[354]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:53:49,976 - [Process 0/5] - INFO - res.shape is :torch.Size([6])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'P'
2024-12-21 17:53:50,314 - [Process 2/5] - INFO - res.shape is :torch.Size([5])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'P'
2024-12-21 17:53:50,331 - [Process 1/5] - INFO - res.shape is :torch.Size([3])
results:
  2%|▎         | 1/40 [00:09<06:13,  9.57s/it]2024-12-21 17:53:50,367 - [Process 4/5] - INFO - res.shape is :torch.Size([6])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'P'
2024-12-21 17:53:50,430 - [Process 3/5] - INFO - res.shape is :torch.Size([6])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'P'
2024-12-21 17:53:50,634 - [Process 1/5] - INFO - len(per_windows_prompt):2
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337026 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337027 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337028 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337029 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2337025) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:53:50
  host      : nwonga100.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2337025)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: qmsum
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.47s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.76s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:55:55,710 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:55:55,710 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:55:55,710 - [Process 1/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:55:55,720 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:55:55,721 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:55:55,721 - [Process 4/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:55:55,729 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:55:55,729 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:55:55,729 - [Process 3/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:55:55,732 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:55:55,732 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:55:55,732 - [Process 2/5] - INFO - output_max_len: 512
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:55:55,734 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:55:55,734 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:55:55,734 - [Process 0/5] - INFO - output_max_len: 512
2024-12-21 17:55:55,740 - [Process 1/5] - INFO - Max Length is 24585
2024-12-21 17:55:55,740 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:55:55,741 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:55:55,771 - [Process 4/5] - INFO - Max Length is 24585
2024-12-21 17:55:55,772 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:55:55,772 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:55:55,779 - [Process 3/5] - INFO - Max Length is 24585
2024-12-21 17:55:55,779 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:55:55,780 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:55:55,782 - [Process 2/5] - INFO - Max Length is 24585
2024-12-21 17:55:55,783 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:55:55,783 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:55:55,784 - [Process 0/5] - INFO - Max Length is 24585
2024-12-21 17:55:55,784 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:55:55,785 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:56:00,495 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:56:00,577 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:56:00,578 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:56:00,581 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:56:00,581 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:56:04,532 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:56:04,533 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2048])
2024-12-21 17:56:04,601 - [Process 1/5] - DEBUG - predict_token:tensor([[825]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:56:04,809 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:56:04,810 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1983])
2024-12-21 17:56:04,813 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:56:04,814 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1972])
2024-12-21 17:56:04,843 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:56:04,843 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1969])
2024-12-21 17:56:04,867 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:56:04,867 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2030])
2024-12-21 17:56:04,880 - [Process 4/5] - DEBUG - predict_token:tensor([[2]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:56:04,884 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:56:04,915 - [Process 3/5] - DEBUG - predict_token:tensor([[1497]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:56:04,937 - [Process 2/5] - DEBUG - predict_token:tensor([[426]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:56:07,927 - [Process 1/5] - INFO - res.shape is :torch.Size([81])
  0%|          | 0/40 [00:12<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'A'
2024-12-21 17:56:08,597 - [Process 2/5] - INFO - res.shape is :torch.Size([78])
  0%|          | 0/40 [00:12<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'T'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337442 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337444 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337445 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337446 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 2337443) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:56:08
  host      : nwonga100.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2337443)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: samsum
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.55s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:58:05,129 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 17:58:05,129 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 17:58:05,129 - [Process 3/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:58:05,149 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 17:58:05,149 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 17:58:05,150 - [Process 4/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:58:05,151 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 17:58:05,151 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 17:58:05,151 - [Process 1/5] - INFO - output_max_len: 128
2024-12-21 17:58:05,151 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 17:58:05,152 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 17:58:05,152 - [Process 0/5] - INFO - output_max_len: 128
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 17:58:05,160 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 17:58:05,160 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 17:58:05,160 - [Process 2/5] - INFO - output_max_len: 128
2024-12-21 17:58:05,177 - [Process 3/5] - INFO - Max Length is 12252
2024-12-21 17:58:05,177 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 17:58:05,177 - [Process 3/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:58:05,224 - [Process 1/5] - INFO - Max Length is 12252
2024-12-21 17:58:05,224 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 17:58:05,225 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:58:05,227 - [Process 0/5] - INFO - Max Length is 12252
2024-12-21 17:58:05,228 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 17:58:05,228 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:58:05,239 - [Process 4/5] - INFO - Max Length is 12252
2024-12-21 17:58:05,239 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 17:58:05,240 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:58:05,250 - [Process 2/5] - INFO - Max Length is 12252
2024-12-21 17:58:05,251 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 17:58:05,251 - [Process 2/5] - INFO - get_predicted begin
  0%|          | 0/40 [00:00<?, ?it/s]2024-12-21 17:58:09,915 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:58:09,998 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:58:10,000 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:58:10,000 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:58:10,002 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 17:58:14,008 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:58:14,008 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2006])
2024-12-21 17:58:14,080 - [Process 3/5] - DEBUG - predict_token:tensor([[29892]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:58:14,221 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:58:14,222 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1980])
2024-12-21 17:58:14,229 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:58:14,230 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2055])
2024-12-21 17:58:14,256 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:58:14,257 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 2007])
2024-12-21 17:58:14,259 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 17:58:14,259 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1927])
2024-12-21 17:58:14,292 - [Process 0/5] - DEBUG - predict_token:tensor([[664]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:58:14,297 - [Process 2/5] - DEBUG - predict_token:tensor([[18673]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:58:14,327 - [Process 4/5] - DEBUG - predict_token:tensor([[29896]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:58:14,332 - [Process 1/5] - DEBUG - predict_token:tensor([[322]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 17:58:14,669 - [Process 0/5] - INFO - res.shape is :torch.Size([7])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'E'
2024-12-21 17:58:14,875 - [Process 3/5] - INFO - res.shape is :torch.Size([17])
  0%|          | 0/40 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'E'
2024-12-21 17:58:15,534 - [Process 1/5] - INFO - res.shape is :torch.Size([24])
  0%|          | 0/40 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'A'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337884 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337885 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337886 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337888 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 2337887) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_17:58:15
  host      : nwonga100.local
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2337887)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: lcc
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]n_windows:[2]

model_name:/mnt/Data/xiongjing/llama2chatmodel_name:/mnt/Data/xiongjing/llama2chat

multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.70s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.53s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 18:00:11,701 - [Process 4/5] - INFO - loading datasets finished
2024-12-21 18:00:11,701 - [Process 4/5] - INFO - model_max_len: 3950
2024-12-21 18:00:11,701 - [Process 4/5] - INFO - output_max_len: 64
2024-12-21 18:00:11,723 - [Process 4/5] - INFO - Max Length is 10029
2024-12-21 18:00:11,724 - [Process 4/5] - INFO - Finish loading dataset
2024-12-21 18:00:11,724 - [Process 4/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]!!!!!!!!!!!!!!!!!!!!!!!! 这里!!!!!!!!!!!!!!!!!!!!!!!! 这里

2024-12-21 18:00:11,731 - [Process 3/5] - INFO - loading datasets finished
2024-12-21 18:00:11,731 - [Process 2/5] - INFO - loading datasets finished
2024-12-21 18:00:11,732 - [Process 2/5] - INFO - model_max_len: 3950
2024-12-21 18:00:11,732 - [Process 3/5] - INFO - model_max_len: 3950
2024-12-21 18:00:11,732 - [Process 2/5] - INFO - output_max_len: 64
2024-12-21 18:00:11,732 - [Process 3/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 18:00:11,732 - [Process 1/5] - INFO - loading datasets finished
2024-12-21 18:00:11,733 - [Process 1/5] - INFO - model_max_len: 3950
2024-12-21 18:00:11,733 - [Process 1/5] - INFO - output_max_len: 64
!!!!!!!!!!!!!!!!!!!!!!!! 这里
2024-12-21 18:00:11,735 - [Process 0/5] - INFO - loading datasets finished
2024-12-21 18:00:11,735 - [Process 0/5] - INFO - model_max_len: 3950
2024-12-21 18:00:11,736 - [Process 0/5] - INFO - output_max_len: 64
2024-12-21 18:00:11,766 - [Process 2/5] - INFO - Max Length is 10029
2024-12-21 18:00:11,766 - [Process 1/5] - INFO - Max Length is 10029
2024-12-21 18:00:11,766 - [Process 3/5] - INFO - Max Length is 10029
2024-12-21 18:00:11,767 - [Process 2/5] - INFO - Finish loading dataset
2024-12-21 18:00:11,767 - [Process 3/5] - INFO - Finish loading dataset
2024-12-21 18:00:11,767 - [Process 1/5] - INFO - Finish loading dataset
2024-12-21 18:00:11,768 - [Process 2/5] - INFO - get_predicted begin
2024-12-21 18:00:11,768 - [Process 3/5] - INFO - get_predicted begin
2024-12-21 18:00:11,768 - [Process 1/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]2024-12-21 18:00:11,769 - [Process 0/5] - INFO - Max Length is 10029
2024-12-21 18:00:11,770 - [Process 0/5] - INFO - Finish loading dataset
2024-12-21 18:00:11,771 - [Process 0/5] - INFO - get_predicted begin
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]2024-12-21 18:00:16,496 - [Process 4/5] - INFO - len(per_windows_prompt):2
2024-12-21 18:00:16,519 - [Process 3/5] - INFO - len(per_windows_prompt):2
2024-12-21 18:00:16,529 - [Process 1/5] - INFO - len(per_windows_prompt):2
2024-12-21 18:00:16,569 - [Process 0/5] - INFO - len(per_windows_prompt):2
2024-12-21 18:00:16,572 - [Process 2/5] - INFO - len(per_windows_prompt):2
2024-12-21 18:00:19,505 - [Process 3/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 18:00:19,505 - [Process 3/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1273])
2024-12-21 18:00:19,554 - [Process 3/5] - DEBUG - predict_token:tensor([[849]], device='cuda:3')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 18:00:19,802 - [Process 1/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 18:00:19,803 - [Process 1/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1298])
2024-12-21 18:00:19,857 - [Process 1/5] - DEBUG - predict_token:tensor([[1024]], device='cuda:1')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 18:00:20,527 - [Process 4/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 18:00:20,528 - [Process 4/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1940])
2024-12-21 18:00:20,598 - [Process 4/5] - DEBUG - predict_token:tensor([[1599]], device='cuda:4')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 18:00:20,688 - [Process 0/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 18:00:20,689 - [Process 0/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1702])
2024-12-21 18:00:20,767 - [Process 0/5] - DEBUG - predict_token:tensor([[13]], device='cuda:0')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 18:00:20,780 - [Process 2/5] - DEBUG - len of windows in get_contexts_cache:2
2024-12-21 18:00:20,780 - [Process 2/5] - DEBUG - predict_token[0].shape:torch.Size([1, 1992])
2024-12-21 18:00:20,848 - [Process 2/5] - DEBUG - predict_token:tensor([[4750]], device='cuda:2')
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
2024-12-21 18:00:22,277 - [Process 1/5] - INFO - res.shape is :torch.Size([59])
  0%|          | 0/100 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: 'O'
2024-12-21 18:00:22,290 - [Process 3/5] - INFO - res.shape is :torch.Size([64])
  0%|          | 0/100 [00:10<?, ?it/s]
Traceback (most recent call last):
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 137, in <module>
    run_pcw_experiment(**vars(args))
  File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 94, in run_pcw_experiment
    em.run_experiment(batch_size=1,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 254, in run_experiment
    results_gathered=self.get_predicted(eval_batch_size=batch_size,output_max_len=output_max_len)
  File "/home/xiongjing/sjh/parallel_window_size/experiment_manager.py", line 223, in get_predicted
    output = self.model.pcw_generate_longbench(per_windows_prompt,output_max_len,self.parallel_pattern,question=question)
  File "/home/xiongjing/sjh/parallel_window_size/pcw_wrapper.py", line 527, in pcw_generate_longbench
    res = self.tokenizer.decode(res, skip_special_tokens=True)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3486, in decode
    return self._decode(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 931, in _decode
    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/transformers/tokenization_utils.py", line 906, in convert_ids_to_tokens
    index = int(index)
ValueError: invalid literal for int() with base 10: ' '
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2338329 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2338331 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2338332 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2338333 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 2338330) of binary: /home/xiongjing/miniconda3/envs/parallel/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/parallel/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/parallel/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_18:00:23
  host      : nwonga100.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2338330)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: repobench-p
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 4
Added key: store_based_barrier_key:1 to store for rank: 1
Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 5 nodes.
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
n_windows:[2]n_windows:[2]

model_name:/mnt/Data/xiongjing/llama2chatmodel_name:/mnt/Data/xiongjing/llama2chat

multi_gpus:Truemulti_gpus:True

torch.cuda.device_count():5torch.cuda.device_count():5

n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
n_windows:[2]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
multi_gpus:True
torch.cuda.device_count():5
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]slurmstepd-nwonga100: error: *** JOB 5774 ON nwonga100 CANCELLED AT 2024-12-21T18:01:23 ***
