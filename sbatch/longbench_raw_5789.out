
CondaError: Run 'conda init' before 'conda activate'

Running evaluation for dataset: lcc
n_windows:[3]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 138, in <module>
[rank2]:     run_pcw_experiment(**vars(args))
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 57, in run_pcw_experiment
[rank2]:     pcw_model = load_pcw_wrapper(model, cache_dir, right_indentation, max(n_windows), prompt_method=prompt_method, model_class=model_class, accelerator=accelerator, capacity=capacity)
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/model_loaders.py", line 99, in load_pcw_wrapper
[rank2]:     model = model_obj.from_pretrained(model_name,**model_args).eval()
[rank2]:   File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4130, in from_pretrained
[rank2]:     model = cls(config, *model_args, **model_kwargs)
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 32, in __init__
[rank2]:     self.model = LlamaModelPCW(config)
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 105, in __init__
[rank2]:     self.layers = nn.ModuleList([LlamaDecoderLayerPCW(config) for _ in range(config.num_hidden_layers)])
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 105, in <listcomp>
[rank2]:     self.layers = nn.ModuleList([LlamaDecoderLayerPCW(config) for _ in range(config.num_hidden_layers)])
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 115, in __init__
[rank2]:     super().__init__(config)
[rank2]: TypeError: LlamaDecoderLayer.__init__() missing 1 required positional argument: 'layer_idx'
n_windows:[3]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 138, in <module>
[rank1]:     run_pcw_experiment(**vars(args))
[rank1]:   File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 57, in run_pcw_experiment
[rank1]:     pcw_model = load_pcw_wrapper(model, cache_dir, right_indentation, max(n_windows), prompt_method=prompt_method, model_class=model_class, accelerator=accelerator, capacity=capacity)
[rank1]:   File "/home/xiongjing/sjh/parallel_window_size/model_loaders.py", line 99, in load_pcw_wrapper
[rank1]:     model = model_obj.from_pretrained(model_name,**model_args).eval()
[rank1]:   File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4130, in from_pretrained
[rank1]:     model = cls(config, *model_args, **model_kwargs)
[rank1]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 32, in __init__
[rank1]:     self.model = LlamaModelPCW(config)
[rank1]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 105, in __init__
[rank1]:     self.layers = nn.ModuleList([LlamaDecoderLayerPCW(config) for _ in range(config.num_hidden_layers)])
[rank1]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 105, in <listcomp>
[rank1]:     self.layers = nn.ModuleList([LlamaDecoderLayerPCW(config) for _ in range(config.num_hidden_layers)])
[rank1]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 115, in __init__
[rank1]:     super().__init__(config)
[rank1]: TypeError: LlamaDecoderLayer.__init__() missing 1 required positional argument: 'layer_idx'
W1222 18:54:07.911518 140142097520448 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2410874 closing signal SIGTERM
W1222 18:54:07.911841 140142097520448 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2410875 closing signal SIGTERM
W1222 18:54:07.912000 140142097520448 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2410877 closing signal SIGTERM
W1222 18:54:07.912123 140142097520448 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2410878 closing signal SIGTERM
E1222 18:54:08.240133 140142097520448 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 2410876) of binary: /home/xiongjing/miniconda3/envs/sjh/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/sjh/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-22_18:54:07
  host      : nwonga100.local
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2410876)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Running evaluation for dataset: repobench-p
n_windows:[3]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 138, in <module>
[rank4]:     run_pcw_experiment(**vars(args))
[rank4]:   File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 57, in run_pcw_experiment
[rank4]:     pcw_model = load_pcw_wrapper(model, cache_dir, right_indentation, max(n_windows), prompt_method=prompt_method, model_class=model_class, accelerator=accelerator, capacity=capacity)
[rank4]:   File "/home/xiongjing/sjh/parallel_window_size/model_loaders.py", line 99, in load_pcw_wrapper
[rank4]:     model = model_obj.from_pretrained(model_name,**model_args).eval()
[rank4]:   File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4130, in from_pretrained
[rank4]:     model = cls(config, *model_args, **model_kwargs)
[rank4]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 32, in __init__
[rank4]:     self.model = LlamaModelPCW(config)
[rank4]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 105, in __init__
[rank4]:     self.layers = nn.ModuleList([LlamaDecoderLayerPCW(config) for _ in range(config.num_hidden_layers)])
[rank4]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 105, in <listcomp>
[rank4]:     self.layers = nn.ModuleList([LlamaDecoderLayerPCW(config) for _ in range(config.num_hidden_layers)])
[rank4]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 115, in __init__
[rank4]:     super().__init__(config)
[rank4]: TypeError: LlamaDecoderLayer.__init__() missing 1 required positional argument: 'layer_idx'
n_windows:[3]
model_name:/mnt/Data/xiongjing/llama2chat
multi_gpus:True
torch.cuda.device_count():5
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 138, in <module>
[rank2]:     run_pcw_experiment(**vars(args))
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/run_evaluation_longbench_multi_gpu.py", line 57, in run_pcw_experiment
[rank2]:     pcw_model = load_pcw_wrapper(model, cache_dir, right_indentation, max(n_windows), prompt_method=prompt_method, model_class=model_class, accelerator=accelerator, capacity=capacity)
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/model_loaders.py", line 99, in load_pcw_wrapper
[rank2]:     model = model_obj.from_pretrained(model_name,**model_args).eval()
[rank2]:   File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4130, in from_pretrained
[rank2]:     model = cls(config, *model_args, **model_kwargs)
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 32, in __init__
[rank2]:     self.model = LlamaModelPCW(config)
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 105, in __init__
[rank2]:     self.layers = nn.ModuleList([LlamaDecoderLayerPCW(config) for _ in range(config.num_hidden_layers)])
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 105, in <listcomp>
[rank2]:     self.layers = nn.ModuleList([LlamaDecoderLayerPCW(config) for _ in range(config.num_hidden_layers)])
[rank2]:   File "/home/xiongjing/sjh/parallel_window_size/modeling_llama_with_pcw.py", line 115, in __init__
[rank2]:     super().__init__(config)
[rank2]: TypeError: LlamaDecoderLayer.__init__() missing 1 required positional argument: 'layer_idx'
W1222 18:54:18.527356 140253232158528 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2410955 closing signal SIGTERM
W1222 18:54:18.527952 140253232158528 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2410956 closing signal SIGTERM
W1222 18:54:18.528328 140253232158528 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2410957 closing signal SIGTERM
W1222 18:54:18.528444 140253232158528 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 2410958 closing signal SIGTERM
E1222 18:54:18.856619 140253232158528 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 4 (pid: 2410959) of binary: /home/xiongjing/miniconda3/envs/sjh/bin/python
Traceback (most recent call last):
  File "/home/xiongjing/miniconda3/envs/sjh/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xiongjing/miniconda3/envs/sjh/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_evaluation_longbench_multi_gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-22_18:54:18
  host      : nwonga100.local
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 2410959)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
