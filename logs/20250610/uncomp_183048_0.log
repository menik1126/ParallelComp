2025-06-10 18:30:48,954 - [Process 0/1] - INFO - loading datasets finished
2025-06-10 18:30:48,955 - [Process 0/1] - INFO - context_max_len: 6000
2025-06-10 18:30:48,955 - [Process 0/1] - INFO - raw_model_max_len: 8000
2025-06-10 18:30:48,955 - [Process 0/1] - INFO - output_max_len: 128
2025-06-10 18:30:48,955 - [Process 0/1] - INFO - parallel_pattern: default
2025-06-10 18:31:41,018 - [Process 0/1] - INFO - success load tokenizer
2025-06-10 18:31:51,675 - [Process 0/1] - INFO - Max ids is 62
2025-06-10 18:31:51,675 - [Process 0/1] - INFO - Max Length is 36418
2025-06-10 18:31:51,675 - [Process 0/1] - INFO - Max Length string is 210429
2025-06-10 18:31:51,675 - [Process 0/1] - INFO - query_max_len tokens is 26
2025-06-10 18:31:51,676 - [Process 0/1] - INFO - length_context_len tokens is 65301
2025-06-10 18:31:51,676 - [Process 0/1] - INFO - Finish loading dataset
2025-06-10 18:31:51,676 - [Process 0/1] - INFO - get_predicted begin
2025-06-10 18:31:51,677 - [Process 0/1] - INFO - context length string:127701
2025-06-10 18:31:51,763 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 18:31:51,766 - [Process 0/1] - INFO - after truncation context length string:32648
2025-06-10 18:31:51,767 - [Process 0/1] - INFO - window_size string: 32648
2025-06-10 18:31:51,767 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 18:31:57,026 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 18:31:57,039 - [Process 0/1] - INFO - cache['sum_windows_size']: 8029
2025-06-10 18:31:57,039 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 8029])
2025-06-10 18:31:57,039 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 28, 8029, 128])
2025-06-10 18:31:57,039 - [Process 0/1] - INFO - input is: 

2025-06-10 18:31:57,041 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 18:31:57,041 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 8030
2025-06-10 18:31:57,041 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 8030])
2025-06-10 18:31:57,041 - [Process 0/1] - INFO - input_max_window_size: 8029
2025-06-10 18:31:57,041 - [Process 0/1] - INFO - sum_windows_size: 8029
2025-06-10 18:31:57,041 - [Process 0/1] - INFO - interval: 1
2025-06-10 18:31:57,519 - [Process 0/1] - INFO - context length string:134629
2025-06-10 18:31:57,568 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 18:31:57,571 - [Process 0/1] - INFO - after truncation context length string:33904
2025-06-10 18:31:57,571 - [Process 0/1] - INFO - window_size string: 33904
2025-06-10 18:31:57,571 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 18:31:58,894 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 18:31:58,905 - [Process 0/1] - INFO - cache['sum_windows_size']: 8029
2025-06-10 18:31:58,905 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 8029])
2025-06-10 18:31:58,905 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 28, 8029, 128])
2025-06-10 18:31:58,905 - [Process 0/1] - INFO - input is: 

2025-06-10 18:31:58,906 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 18:31:58,906 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 8030
2025-06-10 18:31:58,906 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 8030])
2025-06-10 18:31:58,906 - [Process 0/1] - INFO - input_max_window_size: 8029
2025-06-10 18:31:58,906 - [Process 0/1] - INFO - sum_windows_size: 8029
2025-06-10 18:31:58,906 - [Process 0/1] - INFO - interval: 1
2025-06-10 18:31:59,489 - [Process 0/1] - INFO - context length string:34099
2025-06-10 18:31:59,502 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 18:31:59,505 - [Process 0/1] - INFO - after truncation context length string:32541
2025-06-10 18:31:59,505 - [Process 0/1] - INFO - window_size string: 32541
2025-06-10 18:31:59,505 - [Process 0/1] - INFO - len(raw_prompt):1
