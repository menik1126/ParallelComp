2025-06-10 15:51:19,441 - [Process 0/1] - INFO - loading datasets finished
2025-06-10 15:51:19,442 - [Process 0/1] - INFO - context_max_len: 3600
2025-06-10 15:51:19,442 - [Process 0/1] - INFO - raw_model_max_len: 3950
2025-06-10 15:51:19,442 - [Process 0/1] - INFO - output_max_len: 128
2025-06-10 15:51:19,442 - [Process 0/1] - INFO - parallel_pattern: default
2025-06-10 15:52:11,839 - [Process 0/1] - INFO - success load tokenizer
2025-06-10 15:52:17,286 - [Process 0/1] - INFO - Max ids is 62
2025-06-10 15:52:17,287 - [Process 0/1] - INFO - Max Length is 36418
2025-06-10 15:52:17,287 - [Process 0/1] - INFO - Max Length string is 210429
2025-06-10 15:52:17,287 - [Process 0/1] - INFO - query_max_len tokens is 29
2025-06-10 15:52:17,287 - [Process 0/1] - INFO - length_context_len tokens is 84123
2025-06-10 15:52:17,287 - [Process 0/1] - INFO - Finish loading dataset
2025-06-10 15:52:17,287 - [Process 0/1] - INFO - get_predicted begin
2025-06-10 15:52:17,288 - [Process 0/1] - INFO - context length string:127701
2025-06-10 15:52:17,365 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:17,367 - [Process 0/1] - INFO - after truncation context length string:14263
2025-06-10 15:52:17,367 - [Process 0/1] - INFO - window_size string: 14263
2025-06-10 15:52:17,367 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:22,404 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:22,415 - [Process 0/1] - INFO - cache['sum_windows_size']: 3951
2025-06-10 15:52:22,415 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:22,415 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3951, 128])
2025-06-10 15:52:22,415 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:22,417 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:22,417 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3952
2025-06-10 15:52:22,417 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3952])
2025-06-10 15:52:22,417 - [Process 0/1] - INFO - input_max_window_size: 3951
2025-06-10 15:52:22,417 - [Process 0/1] - INFO - sum_windows_size: 3951
2025-06-10 15:52:22,417 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:22,762 - [Process 0/1] - INFO - context length string:134629
2025-06-10 15:52:22,829 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:22,831 - [Process 0/1] - INFO - after truncation context length string:14646
2025-06-10 15:52:22,831 - [Process 0/1] - INFO - window_size string: 14646
2025-06-10 15:52:22,831 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:23,416 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:23,425 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:23,425 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:23,425 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:23,425 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:23,425 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:23,425 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:23,425 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:23,425 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:23,426 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:23,426 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:24,144 - [Process 0/1] - INFO - context length string:34099
2025-06-10 15:52:24,160 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:24,162 - [Process 0/1] - INFO - after truncation context length string:13673
2025-06-10 15:52:24,162 - [Process 0/1] - INFO - window_size string: 13673
2025-06-10 15:52:24,162 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:24,746 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:24,754 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:24,754 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:24,754 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:24,754 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:24,755 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:24,755 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:24,755 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:24,755 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:24,755 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:24,755 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:24,947 - [Process 0/1] - INFO - context length string:36068
2025-06-10 15:52:24,964 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:24,966 - [Process 0/1] - INFO - after truncation context length string:14536
2025-06-10 15:52:24,966 - [Process 0/1] - INFO - window_size string: 14536
2025-06-10 15:52:24,966 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:25,565 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:25,573 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:25,573 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:25,573 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:25,573 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:25,574 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:25,574 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:25,574 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:25,575 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:25,575 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:25,575 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:26,068 - [Process 0/1] - INFO - context length string:150584
2025-06-10 15:52:26,147 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:26,148 - [Process 0/1] - INFO - after truncation context length string:14295
2025-06-10 15:52:26,149 - [Process 0/1] - INFO - window_size string: 14295
2025-06-10 15:52:26,149 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:26,734 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:26,742 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:26,742 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:26,742 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:26,742 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:26,743 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:26,743 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:26,743 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:26,743 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:26,743 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:26,743 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:27,498 - [Process 0/1] - INFO - context length string:56191
2025-06-10 15:52:27,524 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:27,525 - [Process 0/1] - INFO - after truncation context length string:14645
2025-06-10 15:52:27,526 - [Process 0/1] - INFO - window_size string: 14645
2025-06-10 15:52:27,526 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:28,111 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:28,119 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:28,119 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:28,119 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:28,119 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:28,119 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:28,120 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:28,120 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:28,120 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:28,120 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:28,120 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:28,311 - [Process 0/1] - INFO - context length string:102593
2025-06-10 15:52:28,361 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:28,362 - [Process 0/1] - INFO - after truncation context length string:10982
2025-06-10 15:52:28,362 - [Process 0/1] - INFO - window_size string: 10982
2025-06-10 15:52:28,362 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:28,947 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:28,955 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:28,955 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:28,955 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:28,955 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:28,956 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:28,956 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:28,956 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:28,956 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:28,956 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:28,956 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:29,034 - [Process 0/1] - INFO - context length string:153865
2025-06-10 15:52:29,112 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:29,114 - [Process 0/1] - INFO - after truncation context length string:9517
2025-06-10 15:52:29,114 - [Process 0/1] - INFO - window_size string: 9517
2025-06-10 15:52:29,114 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:29,713 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:29,721 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:29,721 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:29,721 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:29,721 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:29,722 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:29,722 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:29,722 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:29,722 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:29,722 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:29,722 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:29,876 - [Process 0/1] - INFO - context length string:127697
2025-06-10 15:52:29,942 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:29,944 - [Process 0/1] - INFO - after truncation context length string:14271
2025-06-10 15:52:29,944 - [Process 0/1] - INFO - window_size string: 14271
2025-06-10 15:52:29,944 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:30,530 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:30,538 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:30,538 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:30,538 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:30,538 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:30,539 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:30,539 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:30,539 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:30,539 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:30,539 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:30,539 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:30,617 - [Process 0/1] - INFO - context length string:34126
2025-06-10 15:52:30,634 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:30,635 - [Process 0/1] - INFO - after truncation context length string:13676
2025-06-10 15:52:30,635 - [Process 0/1] - INFO - window_size string: 13676
2025-06-10 15:52:30,635 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:31,221 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:31,229 - [Process 0/1] - INFO - cache['sum_windows_size']: 3950
2025-06-10 15:52:31,229 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3950])
2025-06-10 15:52:31,229 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3950, 128])
2025-06-10 15:52:31,229 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:31,230 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:31,230 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3951
2025-06-10 15:52:31,230 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:31,230 - [Process 0/1] - INFO - input_max_window_size: 3950
2025-06-10 15:52:31,230 - [Process 0/1] - INFO - sum_windows_size: 3950
2025-06-10 15:52:31,230 - [Process 0/1] - INFO - interval: 1
2025-06-10 15:52:31,384 - [Process 0/1] - INFO - context length string:157712
2025-06-10 15:52:31,465 - [Process 0/1] - INFO - fullkv truncation
2025-06-10 15:52:31,466 - [Process 0/1] - INFO - after truncation context length string:15157
2025-06-10 15:52:31,466 - [Process 0/1] - INFO - window_size string: 15157
2025-06-10 15:52:31,467 - [Process 0/1] - INFO - len(raw_prompt):1
2025-06-10 15:52:32,053 - [Process 0/1] - INFO - raw_location: [0]
2025-06-10 15:52:32,062 - [Process 0/1] - INFO - cache['sum_windows_size']: 3951
2025-06-10 15:52:32,062 - [Process 0/1] - INFO - past_attention_mask.shape: torch.Size([1, 3951])
2025-06-10 15:52:32,062 - [Process 0/1] - INFO - cache['past_key_values'][0][0].shape: torch.Size([1, 32, 3951, 128])
2025-06-10 15:52:32,062 - [Process 0/1] - INFO - input is: 

2025-06-10 15:52:32,063 - [Process 0/1] - INFO - input tokens length: 1
2025-06-10 15:52:32,063 - [Process 0/1] - INFO - A window: after truncation generate all tokens length: 3952
2025-06-10 15:52:32,063 - [Process 0/1] - INFO - combined_attention_mask.shape: torch.Size([1, 3952])
2025-06-10 15:52:32,063 - [Process 0/1] - INFO - input_max_window_size: 3951
2025-06-10 15:52:32,063 - [Process 0/1] - INFO - sum_windows_size: 3951
2025-06-10 15:52:32,063 - [Process 0/1] - INFO - interval: 1
